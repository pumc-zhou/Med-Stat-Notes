[
  {
    "objectID": "Quarto/quarto-LaTeX.html",
    "href": "Quarto/quarto-LaTeX.html",
    "title": "Quarto与LaTeX的记录",
    "section": "",
    "text": "在2025-02-23迁移博客到本网站时遇到一些问题，主要是因为在相关与回归部分使用plotly包制作了一个三维图形，在以前的Rmarkdown博客中，因为未知原因导致从2024年中的一次更新开始就无法在生成PDF文件，所以这个问题没有被发现。\n简而言之就是使用plotly绘制如图所示的双元正态分布图形后，如果Quarto被指定要生成PDF文件，需要载入更多的R包且更新TeXLive的宏包来提供支持。\n## 安装和加载所需的包\n# install.packages(\"plotly\")\n# install.packages(\"mvtnorm\")\nlibrary(plotly)\nlibrary(mvtnorm)\nlibrary(webshot2)\n\n# 创建网格数据\nx &lt;- seq(150, 190, length.out = 100)\n#身高150-190，等距的100个值\ny &lt;- seq(50, 80, length.out = 100)\n#体重50-80，等距的100个值\ngrid &lt;- expand.grid(X = x, Y = y)\n#生成 x 和 y 的所有组合，用于构建一个网格数据框，以便计算多元正态分布的概率密度。\n\n# 设置均值和协方差矩阵\nmu &lt;- c(170, 65)\n#设置双元正态分布的均值向量，表示均值分别为身高 170 cm 和体重 65 kg\n\nsigma &lt;- matrix(c(100, 20, 20, 25), nrow = 2)\n#设置协方差矩阵，表示身高的方差为 100，体重的方差为 25，身高和体重之间的协方差为 20\n\n# 计算概率密度\nz &lt;- dmvnorm(as.matrix(grid), mean = mu, sigma = sigma)\n#计算每个网格点上双元正态分布的概率密度。\n\n# 将概率密度矩阵转换为适合绘图的形状\nz_matrix &lt;- matrix(z, nrow = 100, ncol = 100)\n\n# 绘制三维表面图\nplot_ly(x = x, y = y, z = z_matrix, type = \"surface\") %&gt;%\n  layout(title = list(text = \"双元正态分布的三维概率密度图\", y=0.95),\n         scene = list(xaxis = list(title = \"身高 (cm)\"),\n                      yaxis = list(title = \"体重 (kg)\"),\n                      zaxis = list(title = \"概率密度\")))\n\n\n双元正态分布示例\n上述图像在被转换为PDF文件时，会发生报错：Quarto 文档中包含了一些生成 HTML 输出的函数（比如交互式图表或其他 HTML 小部件），但你当前的目标输出格式是 PDF。由于 PDF 是静态格式，无法直接渲染 HTML 内容，Quarto 会报错并停止执行。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/quarto-LaTeX.html#解决方案1",
    "href": "Quarto/quarto-LaTeX.html#解决方案1",
    "title": "Quarto与LaTeX的记录",
    "section": "\n1 解决方案1",
    "text": "1 解决方案1\n此章节不转换为PDF格式。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/quarto-LaTeX.html#解决方案2",
    "href": "Quarto/quarto-LaTeX.html#解决方案2",
    "title": "Quarto与LaTeX的记录",
    "section": "\n2 解决方案2",
    "text": "2 解决方案2\n增加支持：如果你仍想输出 PDF，但希望将 HTML 小部件作为静态截图嵌入，可以安装 R 的 webshot 或 webshot2 包。Quarto 会利用它们将 HTML 内容转换为图片。\n需要安装：\n\ninstall.packages(\"webshot2\")\n\n然后在这段程序的前部导入该包：library(webshot2)。\n也可以使用\n\ninstall.packages(\"webshot\")\n\n但是使用 webshot 还需要安装 PhantomJS\n\nwebshot::install_phantomjs()",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/quarto-LaTeX.html#解决方案3启用-prefer-html-true-选项",
    "href": "Quarto/quarto-LaTeX.html#解决方案3启用-prefer-html-true-选项",
    "title": "Quarto与LaTeX的记录",
    "section": "\n3 解决方案3：启用 prefer-html: true 选项\n",
    "text": "3 解决方案3：启用 prefer-html: true 选项\n\n如果你不在乎 HTML 内容在 PDF 中不可见，可以通过添加 prefer-html: true 来跳过这个错误。这种方法会忽略 HTML 输出，PDF 中不会显示相关内容。\n\n\n在 .qmd 文件的 YAML 前置元数据中添加：\n\nformat: pdf: toc: true # 可选{yaml}",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/quarto-LaTeX.html#解决方法-4检查并移除-html-输出代码",
    "href": "Quarto/quarto-LaTeX.html#解决方法-4检查并移除-html-输出代码",
    "title": "Quarto与LaTeX的记录",
    "section": "\n4 解决方法 4：检查并移除 HTML 输出代码\n",
    "text": "4 解决方法 4：检查并移除 HTML 输出代码\n\n如果你的目标是纯 PDF 输出，且不需要 HTML 小部件，可以检查文档中的代码块，移除或调整生成 HTML 的部分。例如：\n\n如果使用了 R 的 plotly 或 htmlwidgets，将其替换为静态图形库（如 ggplot2）。\n检查是否有 {r, results='asis'} 或其他生成 HTML 的设置，改为适合 PDF 的输出。\n\n示例：\n将交互式图表改为静态图表：\n\n# 原代码（生成 HTML）\nlibrary(plotly)\nplot_ly(data, x = ~x, y = ~y, type = \"scatter\")\n\n# 修改后（适合 PDF）\nlibrary(ggplot2)\nggplot(data, aes(x = x, y = y)) + geom_point()",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/quarto-LaTeX.html#quarto调用texlive报错",
    "href": "Quarto/quarto-LaTeX.html#quarto调用texlive报错",
    "title": "Quarto与LaTeX的记录",
    "section": "\n5 Quarto调用TeXLive报错",
    "text": "5 Quarto调用TeXLive报错\n在使用方法一进行完善的过程中，terminal输出了一些关于TeX Live的信息：\n\nTeX Live infrastructure update in progress ... \nTeX Live infrastructure update in progress ... \nDetailed command logging to \"C:\\texlive\\2024\\temp\\update-self.log\" \nself update: texlive.infra (70084 -&gt; 73495) \ntexlive.infra.windows (69813 -&gt; 71447) \ntlperl.windows (69939 -&gt; 71515) \nInfrastructure update finished successfully. \nYou may now close this window.\n\n这是因为Quarto的预览过程中触发了TeX Live的更新，这里的输出显示已完成更新，但是preview .qmd文档仍然不顺利，怀疑没有正确更新，且因为太晚关闭了电脑，导致更新中断。\n试着从terminal对TeX Live进行更新，在cmd中使用如下命令\n\ntlmgr update --self --all\n\n你的Windows电脑可能会和我一样报错：\n\nC:\\Users\\asus&gt;tlmgr update --self --all\nLocale 'English_United States.936' is unsupported, and may crash the interpreter.\n\n这是因为：\n当你运行 tlmgr update --self --all 来更新 TeX Live 时，出现了 Locale 'English_United States.936' is unsupported, and may crash the interpreter 的警告。这表明 TeX Live 的包管理器（tlmgr）在你的系统区域设置（locale）下遇到了兼容性问题，可能是因为你的 Windows 系统使用了中文（代码页 936，简体中文 GBK）作为默认区域设置，而 tlmgr 默认期望一个支持的区域设置（如 UTF-8 或英语）。\n以下是解决这个问题的步骤：\n\n\n5.1 方案 1：检查系统区域设置\n\n\n查看当前区域设置：\n\n按 Win + R，输入 intl.cpl，回车，打开“区域”设置。\n在“格式”选项卡中，查看当前格式（可能是“中文（简体，中国）”）。\n点击“管理”选项卡 -&gt; “更改系统区域设置”，查看“当前系统区域设置”（可能是“中文（简体，中国）”）。\n\n\n\n问题原因：\n\n代码页 936（中文 GBK）不是 TeX Live 的 tlmgr 完全支持的区域设置，可能导致编码或解释器错误。\n\n\n\n\n5.2 方案 2：临时更改区域设置运行 tlmgr\n\n为了让 tlmgr 正常工作，可以临时将区域设置为英语（美国），运行更新后再改回。\n操作步骤：\n\n\n更改系统区域设置：\n\n按 Win + R，输入 intl.cpl。\n点击“管理”选项卡 -&gt; “更改系统区域设置”。\n在下拉菜单中选择“英语（美国）”（English (United States)）。\n勾选“Beta: 使用 UTF-8 提供全球语言支持”（推荐），然后点击“确定”。\n系统会提示重启，点击“立即重启”。\n\n\n\n运行更新：\n\n重启后，打开命令行（cmd）。\n\n输入：\n\ntlmgr update --self --all\n\n\n观察是否还有错误，等待更新完成。\n\n\n\n恢复区域设置（可选）：\n\n更新完成后，重复上述步骤，将区域设置改回“中文（简体，中国）”，再次重启。\n\n\n\n\n5.3 方案 3：使用环境变量绕过区域问题\n如果你不想更改系统区域设置，可以通过设置环境变量临时调整 tlmgr 的运行环境。\n操作步骤：\n\n打开命令行（cmd）。\n\n输入以下命令设置临时环境变量：\n\nset LC_ALL=en_US.UTF-8\n\n\n\n紧接着运行更新：\n\ntlmgr update --self --all\n\n\n\n检查是否成功执行。如果仍然报错，尝试：\n\nset LANG=en_US.UTF-8\ntlmgr update --self --all\n\n\n说明：\n\n\nLC_ALL 或 LANG 是控制区域设置的环境变量，设置为 en_US.UTF-8 可以让 tlmgr 在英语环境下运行，避免中文编码问题。\n这种方法无需重启，适合临时解决。\n\n\n5.4 方案 4：验证更新结果\n更新完成后，检查 TeX Live 是否正常工作： 1. 运行： tlmgr info --list installed 查看已安装的包列表，确保更新生效。 2. 返回 Quarto 项目，运行： quarto preview 确认是否还有 TeX 相关的问题。\n\n5.5 额外建议\n\n\n检查 TeX Live 安装： 如果问题持续存在，可能是 TeX Live 安装不完整。可以尝试重新安装：\n\n下载最新版 TeX Live（tug.org/texlive）。\n安装时选择“完整安装”以确保所有组件齐全。\n\n\n\n使用 PowerShell 或 WSL： 如果 cmd 仍然有问题，可以尝试在 PowerShell 或 Windows Subsystem for Linux (WSL) 中运行 tlmgr，这些环境可能更好地处理区域设置。\n\n\n5.6 推荐方案\n\n\n快速解决：用 Step 3 设置 LC_ALL=en_US.UTF-8，无需重启，最简单。\n\n彻底解决：用 Step 2 临时改为英语区域设置，更新后再改回。\n\n我采用了方案 1，在cmd中输入命令后，出现如下：\n\nC:\\Users\\asus&gt;tlmgr update --self --all\ntlmgr.pl: package repository https://mirrors.hust.edu.cn/CTAN/systems/texlive/tlnet (not verified: gpg unavailable)\ntlmgr.pl: saving backups to C:/texlive/2024/tlpkg/backups\ntlmgr.pl: no self-updates for tlmgr available\ntlmgr.pl: skipping forcibly removed package: extractbb\ntlmgr.pl: skipping forcibly removed package: extractbb.windows\n[  1/764, ??:??/??:??] update: ebgaramond [8780k] (66604 -&gt; 71069) ... done\n[  2/764, 00:16/42:39] update: ebgaramond-maths [567k] (52168 -&gt; 74174) ... done\n\n表示在更新中，这个过程大概需要45分钟左右。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/config-giscus-github.html",
    "href": "Quarto/config-giscus-github.html",
    "title": "Qmd config giscus",
    "section": "",
    "text": "[giscus](https://giscus.app/zh-CN)\n\n\n\nGiscus是一个基于Github Discussion的评论插件，可以为无服务器端的博客运营者提供简易的部署和拓展。根据官网，其存在以下特性：\n开源！\n无跟踪，无广告，永久免费；\n无需数据库。全部数据均储存在 GitHub Discussions 中；\n支持自定义主题、多种语言、高度可配置；\n自动从 GitHub 拉取新评论与编辑；\n可自建服务！\n工作原理:\n  Giscus在加载时，会使用 GitHub Discussions 搜索 API 根据选定的映射方式（如 URL、pathname等）来查找与当前页面关联的discussion。如果找不到匹配的discussion，giscus bot就会在第一次有人留下评论或回应时自动创建一个discussion。\n\n  在评论时，访客必须按 GitHub OAuth 流程授权 giscus app 代表他发帖。或者访客也可以直接在 GitHub Discussion 里评论。你可以在 GitHub 上管理评论。\n\n\n\n\n首先要在GitHub建一个开放、可以使用Discussion的仓库，命名可以根据自己需要来，不能同网站仓库一致，因为独立存放评论内容。\n\n创建仓库完成后，请点击仓库最后一个选项setting，往下滑，找到Features功能区中的Discussion，点击勾选，开启仓库的Discussion功能，并安装giscus app ，点击连接并且install到对应新建的仓库就可以了。\n\n\n\n\n\n\nhttps://github.com/apps/giscus\n\nGiscus 应用库可以帮助你更方便地管理设置。通过上述链接进入网页进行安装：\n\n安装完成后转到如下页面，选择已创建好的comments仓库作为评论仓库。\n\n\n\n\n进入&lt;https://giscus.app/zh-CN&gt;配置页面，在该页面中根据自己的需求，选择配置。\n\n仓库选项是必须的，你需要把你创建好的comments仓库输入其中。\n选择合适的分类：\n\n嵌入到网站，在上述配置完成后，会生成一段script，将其放置在你想要展示评论的位置即可。\n\nend.",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Qmd config giscus"
    ]
  },
  {
    "objectID": "Quarto/config-giscus-github.html#什么是giscus",
    "href": "Quarto/config-giscus-github.html#什么是giscus",
    "title": "Qmd config giscus",
    "section": "",
    "text": "Giscus是一个基于Github Discussion的评论插件，可以为无服务器端的博客运营者提供简易的部署和拓展。根据官网，其存在以下特性：\n开源！\n无跟踪，无广告，永久免费；\n无需数据库。全部数据均储存在 GitHub Discussions 中；\n支持自定义主题、多种语言、高度可配置；\n自动从 GitHub 拉取新评论与编辑；\n可自建服务！\n工作原理:\n  Giscus在加载时，会使用 GitHub Discussions 搜索 API 根据选定的映射方式（如 URL、pathname等）来查找与当前页面关联的discussion。如果找不到匹配的discussion，giscus bot就会在第一次有人留下评论或回应时自动创建一个discussion。\n\n  在评论时，访客必须按 GitHub OAuth 流程授权 giscus app 代表他发帖。或者访客也可以直接在 GitHub Discussion 里评论。你可以在 GitHub 上管理评论。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Qmd config giscus"
    ]
  },
  {
    "objectID": "Quarto/config-giscus-github.html#如何使用giscus",
    "href": "Quarto/config-giscus-github.html#如何使用giscus",
    "title": "Qmd config giscus",
    "section": "",
    "text": "首先要在GitHub建一个开放、可以使用Discussion的仓库，命名可以根据自己需要来，不能同网站仓库一致，因为独立存放评论内容。\n\n创建仓库完成后，请点击仓库最后一个选项setting，往下滑，找到Features功能区中的Discussion，点击勾选，开启仓库的Discussion功能，并安装giscus app ，点击连接并且install到对应新建的仓库就可以了。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Qmd config giscus"
    ]
  },
  {
    "objectID": "Quarto/config-giscus-github.html#安装giscus-app",
    "href": "Quarto/config-giscus-github.html#安装giscus-app",
    "title": "Qmd config giscus",
    "section": "",
    "text": "https://github.com/apps/giscus\n\nGiscus 应用库可以帮助你更方便地管理设置。通过上述链接进入网页进行安装：\n\n安装完成后转到如下页面，选择已创建好的comments仓库作为评论仓库。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Qmd config giscus"
    ]
  },
  {
    "objectID": "Quarto/config-giscus-github.html#配置giscus",
    "href": "Quarto/config-giscus-github.html#配置giscus",
    "title": "Qmd config giscus",
    "section": "",
    "text": "进入&lt;https://giscus.app/zh-CN&gt;配置页面，在该页面中根据自己的需求，选择配置。\n\n仓库选项是必须的，你需要把你创建好的comments仓库输入其中。\n选择合适的分类：\n\n嵌入到网站，在上述配置完成后，会生成一段script，将其放置在你想要展示评论的位置即可。\n\nend.",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Qmd config giscus"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html",
    "href": "Learn/Bayes/01-Bayes-PGM.html",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "",
    "text": "概率图模型是一种结合概率论与图论的理论框架，用于表示和分析复杂系统中的不确定性及概率关系。\n\n\n贝叶斯网络\n马尔科夫随机场（MRF)\n隐马尔科夫模型（HMM)\n\n\n安装：\n\ninstall.packages(\"gRain\")\n\n加载：\n\nlibrary(\"gRbase\")\n\n首先定义一个带有变量的A、B、C、D、E的简单无向图\n\ngraph &lt;- ug(\"A:B:E + C:E:D\")\nclass(graph)\n\n[1] \"igraph\"\n\n\n安装可视化程序包，使用流行的Rgraphviz\nRgraphviz是 Bioconductor 生态系统的一部分，所以可以通过 Bioconductor 来安装它。首先要确保已经安装了BiocManager，若未安装，可使用以下代码进行安装：\n\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\n安装好BiocManager之后，使用它来安装Rgraphviz：\n\nBiocManager::install(\"Rgraphviz\")\n\n载入并使用：\n\nlibrary(\"Rgraphviz\")\nplot(graph)\n\n\n\n\n\n\n\n定义有向图：\n\ndag &lt;- dag(\"A + B:A + C:B + D:B + E:C:D\")\ndag\n\nIGRAPH 28898ca DN-- 1 0 -- \n+ attr: name (v/c)\n+ edges from 28898ca (vertex names):\n\nplot(dag)\n\n\n\n\n\n\n\n\n报错：无法正确显示图形\n原因：Rgraphviz 没有被正确的安装。\n需要从 bioconductor 中安装：\n\n&gt; install.packages(\"Rgraphviz\")\nInstalling package into 'C:/Users/asus/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\nWarning message:\npackage 'Rgraphviz' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n当使用 bidconductor 源来安装 Rgraphviz ，首先安装 BiocManager ：\n\n&gt; if (!require(\"BiocManager\", quietly = TRUE))\n+     install.packages(\"BiocManager\")\nInstalling package into 'C:/Users/asus/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/BiocManager_1.30.25.zip'\nContent type 'application/zip' length 506482 bytes (494 KB)\ndownloaded 494 KB\n\npackage 'BiocManager' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\n\n然后使用 BiocManager 安装 Rgraphviz ，需要安装的配套程序包很多，需要几分钟，但是这里出现了报错。\n\n&gt; BiocManager::install(\"Rgraphviz\")\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.sustech.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nInstalling package(s) 'BiocVersion', 'Rgraphviz'\nalso installing the dependencies 'BiocGenerics', 'graph'\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/BiocGenerics_0.52.0.zip'\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nContent type 'application/zip' length 639558 bytes (624 KB)\ndownloaded 624 KB\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/graph_1.84.1.zip'\nContent type 'application/zip' length 2173761 bytes (2.1 MB)\ndownloaded 2.1 MB\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/BiocVersion_3.20.0.zip'\nContent type 'application/zip' length 8386 bytes\ndownloaded 8386 bytes\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/Rgraphviz_2.50.0.zip'\nContent type 'application/zip' length 1457153 bytes (1.4 MB)\ndownloaded 1.4 MB\n\npackage 'BiocGenerics' successfully unpacked and MD5 sums checked\npackage 'graph' successfully unpacked and MD5 sums checked\npackage 'BiocVersion' successfully unpacked and MD5 sums checked\npackage 'Rgraphviz' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\nInstallation paths not writeable, unable to update packages\n  path: C:/Program Files/R/R-4.4.2/library\n  packages:\n    class, cli, cluster, curl, data.table, foreign, glue, jsonlite, KernSmooth,\n    lintr, MASS, Matrix, nlme, nnet, processx, ps, purrr, R.utils, R6, renv,\n    reticulate, rlang, rpart, sessioninfo, spatial, survival, tinytex, xfun,\n    xml2, zoo\nOld packages: 'httpgd', 'readxl', 'unigd'\napdate all/some/none? [a/s/n]:\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/httpgd_2.0.3.zip'\nContent type 'application/zip' length 874316 bytes (853 KB)\ndownloaded 853 KB\n\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/readxl_1.4.4.zip' \nContent type 'application/zip' length 750422 bytes (732 KB)\ndownloaded 732 KB\n\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/unigd_0.1.3.zip'\nContent type 'application/zip' length 5591347 bytes (5.3 MB)\ndownloaded 5.3 MB\n\npackage 'httpgd' successfully unpacked and MD5 sums checked\npackage 'readxl' successfully unpacked and MD5 sums checked\nWarning: cannot remove prior installation of package 'readxl'\nWarning: restored 'readxl'\npackage 'unigd' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\nWarning message:\nIn file.copy(savedcopy, lib, recursive = TRUE) :\n  problem copying C:\\Users\\asus\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\readxl\\libs\\x64\\readxl.dll to C:\\Users\\asus\\AppData\\Local\\R\\win-library\\4.4\\readxl\\libs\\x64\\readxl.dll: Permission denied\n\n主要问题是没有写入权限，这可能和在 vscode 中安装有关，因此，换到 R 的终端中进行安装（注意，可能需要使用管理员权限），但是仍然免不了一系列的包冲突问题，需要根据提示选择操作：\n\n使用管理员身份运行 R 。\n选择合适的源来下载。\n\n处理有冲突的包（卸载），或者重新创建一个 env 。\n\n # 删除 readxl 包\n readxl_path &lt;- system.file(package = \"readxl\")\n if (!is.na(readxl_path)) {\n unlink(readxl_path, recursive = TRUE)\n }\n # 删除 cli 包\n cli_path &lt;- system.file(package = \"cli\")\n if (!is.na(cli_path)) {\n unlink(cli_path, recursive = TRUE)\n }\n # 再次尝试安装 Rgraphviz\n BiocManager::install(\"Rgraphviz\")\n\n\n强制更新，忽略冲突\n\n有些包的版本已经是最新的，但安装过程仍然提示需要更新，你可以使用 force = TRUE 参数强制重新安装。\n\nBiocManager::install(\"Rgraphviz\", force = TRUE)\n\n\n\n具体信息如下：\n\n&gt; BiocManager::install(\"Rgraphviz\")\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.nju.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nWarning message:\npackage(s) not installed when version(s) same as or greater than current; use\n  `force = TRUE` to re-install: 'Rgraphviz' \n&gt; BiocManager::install(\"Rgraphviz\", force = TRUE)\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.nju.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nInstalling package(s) 'Rgraphviz'\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/Rgraphviz_2.50.0.zip'\nContent type 'application/zip' length 1457153 bytes (1.4 MB)\ndownloaded 1.4 MB\n\npackage ‘Rgraphviz’ successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\Rtmp2j7SlA\\downloaded_packages\n\n\n\n首先，为每一个节点定义取值：\n\nmachine_val &lt;- c(\"working\", \"broken\")\nlight_bulb_val &lt;- c(\"good\", \"bad\")\n\n为两个随机变量定义百分比数值：\n\nmachine_val &lt;- c(99,1)\nlight_bulb_val &lt;- c(99,1,60,40)\n\n使用 gRain 定义随机变量：\n\nlibrary(gRain)\nM &lt;- cptable(~machine, values = machine_prob,\n            levels = machine_val)\nL &lt;- cptable(~light_bulb | machine,\n            values = light_bulb_prob,\n            levels = light_bulb_val)\n\n这里的 cptable 表示条件概率表1：它是离散型随机变量概率分布的内存表示2。\n\n\nplist &lt;- compileCPT(list(M,L))\nplist\n\n输出结果如上，这里可以清楚地看到之前定义的概率分布\n2025.04.05 再次尝试复现程序，失败，暂时停止概率图R程序的复现工作。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html#常见类型",
    "href": "Learn/Bayes/01-Bayes-PGM.html#常见类型",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "",
    "text": "贝叶斯网络\n马尔科夫随机场（MRF)\n隐马尔科夫模型（HMM)",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html#用r来展示概率图模型",
    "href": "Learn/Bayes/01-Bayes-PGM.html#用r来展示概率图模型",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "",
    "text": "安装：\n\ninstall.packages(\"gRain\")\n\n加载：\n\nlibrary(\"gRbase\")\n\n首先定义一个带有变量的A、B、C、D、E的简单无向图\n\ngraph &lt;- ug(\"A:B:E + C:E:D\")\nclass(graph)\n\n[1] \"igraph\"\n\n\n安装可视化程序包，使用流行的Rgraphviz\nRgraphviz是 Bioconductor 生态系统的一部分，所以可以通过 Bioconductor 来安装它。首先要确保已经安装了BiocManager，若未安装，可使用以下代码进行安装：\n\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\n安装好BiocManager之后，使用它来安装Rgraphviz：\n\nBiocManager::install(\"Rgraphviz\")\n\n载入并使用：\n\nlibrary(\"Rgraphviz\")\nplot(graph)\n\n\n\n\n\n\n\n定义有向图：\n\ndag &lt;- dag(\"A + B:A + C:B + D:B + E:C:D\")\ndag\n\nIGRAPH 28898ca DN-- 1 0 -- \n+ attr: name (v/c)\n+ edges from 28898ca (vertex names):\n\nplot(dag)",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html#报错",
    "href": "Learn/Bayes/01-Bayes-PGM.html#报错",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "",
    "text": "报错：无法正确显示图形\n原因：Rgraphviz 没有被正确的安装。\n需要从 bioconductor 中安装：\n\n&gt; install.packages(\"Rgraphviz\")\nInstalling package into 'C:/Users/asus/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\nWarning message:\npackage 'Rgraphviz' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n当使用 bidconductor 源来安装 Rgraphviz ，首先安装 BiocManager ：\n\n&gt; if (!require(\"BiocManager\", quietly = TRUE))\n+     install.packages(\"BiocManager\")\nInstalling package into 'C:/Users/asus/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/BiocManager_1.30.25.zip'\nContent type 'application/zip' length 506482 bytes (494 KB)\ndownloaded 494 KB\n\npackage 'BiocManager' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\n\n然后使用 BiocManager 安装 Rgraphviz ，需要安装的配套程序包很多，需要几分钟，但是这里出现了报错。\n\n&gt; BiocManager::install(\"Rgraphviz\")\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.sustech.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nInstalling package(s) 'BiocVersion', 'Rgraphviz'\nalso installing the dependencies 'BiocGenerics', 'graph'\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/BiocGenerics_0.52.0.zip'\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nContent type 'application/zip' length 639558 bytes (624 KB)\ndownloaded 624 KB\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/graph_1.84.1.zip'\nContent type 'application/zip' length 2173761 bytes (2.1 MB)\ndownloaded 2.1 MB\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/BiocVersion_3.20.0.zip'\nContent type 'application/zip' length 8386 bytes\ndownloaded 8386 bytes\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/Rgraphviz_2.50.0.zip'\nContent type 'application/zip' length 1457153 bytes (1.4 MB)\ndownloaded 1.4 MB\n\npackage 'BiocGenerics' successfully unpacked and MD5 sums checked\npackage 'graph' successfully unpacked and MD5 sums checked\npackage 'BiocVersion' successfully unpacked and MD5 sums checked\npackage 'Rgraphviz' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\nInstallation paths not writeable, unable to update packages\n  path: C:/Program Files/R/R-4.4.2/library\n  packages:\n    class, cli, cluster, curl, data.table, foreign, glue, jsonlite, KernSmooth,\n    lintr, MASS, Matrix, nlme, nnet, processx, ps, purrr, R.utils, R6, renv,\n    reticulate, rlang, rpart, sessioninfo, spatial, survival, tinytex, xfun,\n    xml2, zoo\nOld packages: 'httpgd', 'readxl', 'unigd'\napdate all/some/none? [a/s/n]:\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/httpgd_2.0.3.zip'\nContent type 'application/zip' length 874316 bytes (853 KB)\ndownloaded 853 KB\n\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/readxl_1.4.4.zip' \nContent type 'application/zip' length 750422 bytes (732 KB)\ndownloaded 732 KB\n\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/unigd_0.1.3.zip'\nContent type 'application/zip' length 5591347 bytes (5.3 MB)\ndownloaded 5.3 MB\n\npackage 'httpgd' successfully unpacked and MD5 sums checked\npackage 'readxl' successfully unpacked and MD5 sums checked\nWarning: cannot remove prior installation of package 'readxl'\nWarning: restored 'readxl'\npackage 'unigd' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\nWarning message:\nIn file.copy(savedcopy, lib, recursive = TRUE) :\n  problem copying C:\\Users\\asus\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\readxl\\libs\\x64\\readxl.dll to C:\\Users\\asus\\AppData\\Local\\R\\win-library\\4.4\\readxl\\libs\\x64\\readxl.dll: Permission denied\n\n主要问题是没有写入权限，这可能和在 vscode 中安装有关，因此，换到 R 的终端中进行安装（注意，可能需要使用管理员权限），但是仍然免不了一系列的包冲突问题，需要根据提示选择操作：\n\n使用管理员身份运行 R 。\n选择合适的源来下载。\n\n处理有冲突的包（卸载），或者重新创建一个 env 。\n\n # 删除 readxl 包\n readxl_path &lt;- system.file(package = \"readxl\")\n if (!is.na(readxl_path)) {\n unlink(readxl_path, recursive = TRUE)\n }\n # 删除 cli 包\n cli_path &lt;- system.file(package = \"cli\")\n if (!is.na(cli_path)) {\n unlink(cli_path, recursive = TRUE)\n }\n # 再次尝试安装 Rgraphviz\n BiocManager::install(\"Rgraphviz\")\n\n\n强制更新，忽略冲突\n\n有些包的版本已经是最新的，但安装过程仍然提示需要更新，你可以使用 force = TRUE 参数强制重新安装。\n\nBiocManager::install(\"Rgraphviz\", force = TRUE)\n\n\n\n具体信息如下：\n\n&gt; BiocManager::install(\"Rgraphviz\")\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.nju.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nWarning message:\npackage(s) not installed when version(s) same as or greater than current; use\n  `force = TRUE` to re-install: 'Rgraphviz' \n&gt; BiocManager::install(\"Rgraphviz\", force = TRUE)\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.nju.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nInstalling package(s) 'Rgraphviz'\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/Rgraphviz_2.50.0.zip'\nContent type 'application/zip' length 1457153 bytes (1.4 MB)\ndownloaded 1.4 MB\n\npackage ‘Rgraphviz’ successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\Rtmp2j7SlA\\downloaded_packages",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html",
    "href": "Learn/Basic/10-regression-correlation.html",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "two variables relationship\n\n\n\n\n\nThe basic process of straight-line regression analysis\n\n\n\n\n\n\n\n\n名称\n适用条件\n\n\n\nPearson直线相关系数\n双变量正态分布的资料\\(\\rightarrow\\)定量\\(\\rightarrow\\)类比t检验、方差分析\n\n\n列联系数\n非等级资料\\(\\rightarrow\\)分类\\(\\rightarrow\\)类比卡方检验\n\n\nSpearman秩相关系数\n不满足双变量正态分布、分布未知、等级资料\\(\\rightarrow\\)定量+分类\\(\\rightarrow\\)类比秩和检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#两变量关系分析",
    "href": "Learn/Basic/10-regression-correlation.html#两变量关系分析",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "two variables relationship",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#常见相关系数",
    "href": "Learn/Basic/10-regression-correlation.html#常见相关系数",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "The basic process of straight-line regression analysis\n\n\n\n\n\n\n\n\n名称\n适用条件\n\n\n\nPearson直线相关系数\n双变量正态分布的资料\\(\\rightarrow\\)定量\\(\\rightarrow\\)类比t检验、方差分析\n\n\n列联系数\n非等级资料\\(\\rightarrow\\)分类\\(\\rightarrow\\)类比卡方检验\n\n\nSpearman秩相关系数\n不满足双变量正态分布、分布未知、等级资料\\(\\rightarrow\\)定量+分类\\(\\rightarrow\\)类比秩和检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#简单直线回归",
    "href": "Learn/Basic/10-regression-correlation.html#简单直线回归",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "Draw a scatterplot\n\n\n选择一组数据集的“最佳拟合直线”，需要设法通过观测数据确定参数\\(\\alpha\\)与\\(\\beta\\)的估计值a和b，使得直线\n\\[\\hat y=a+bx\\]\n能最佳地反映\\((x_i,y_i)\\)之间的变化关系，该直线称为一元回归直线。\n常用最小二乘估计法（least squares estimation）来最佳直线，其基本原理是通过最小化残差平方和，使得各观测点到回归直线的纵向距离的平方和最小。\n\\[\n\\begin{cases}\na=\\bar y-b \\bar x\\\\\nb=\\frac{\\sum\\limits_{i=1}^n x_i y_i - \\frac{1}{n}(\\sum\\limits_{i=1}^n x_i)(\\sum\\limits_{i=1}^n y_i)}{\\sum\\limits_{i=1}^n x_i^2 - \\frac{1}{n}(\\sum\\limits_{i=1}^n x_i)^2}\n\\end{cases}\n\\] 为方便，引入以下记号： \\[\nSS_{xx}=\\sum_{i}(x_i-\\bar x)^2=\\sum_{i}x_i^2-\\frac{1}{n}(\\sum_{i}x_i)^2\\\\\nSS_{yy}=\\sum_{i}(y_i-\\bar y)^2=\\sum_{i}y_i^2-\\frac{1}{n}(\\sum_{i}y_i)^2\\\\\nSS_{xy}=\\sum_{i}(x_i-\\bar x)(y_i-\\bar y)=\\sum_{i}x_i y_i-\\frac{1}{n}(\\sum_{i}x_i)(\\sum_{i}y_i)\n\\] 其中，\\(SS_{xx}\\)和\\(SS_{yy}\\)是离均差平方和，\\(SS_{xy}\\)称为离均差积和。\n这样可以简化为： \\[\n\\begin{cases}\na=\\bar y-b \\bar x\\\\\nb=\\frac{SS_{xy}}{SS_{xx}}\n\\end{cases}\n\\]\n\n\nF检验\n\n\\(y_i\\)的总离均差平方和为：\n\\[SS_{yy}=\\sum_{i}(y_i-\\bar y)^2\\] 对其做分解，得到等式：\n\\[SS_{yy}=\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\] \\(\\sum_{i}^{n}(\\hat y_i-\\bar y)^2\\)为回归平方和（regression sum of squares），记为\\(SS_R\\)，表示回归估计值\\(\\hat y_i\\)与均数\\(\\bar y\\)的离差平方和，其公式为：\n\\[\n\\begin{align}\nSS_{yy} &= \\sum_{i=1}^{n}(\\hat y_i - \\bar y)^2 \\\\\n        &= \\sum_{i=1}^{n}[a + bx_i - (a + b\\bar x)]^2 \\\\\n        &= SS_{xx}b^2 \\\\\n        &= SS_{xy}b\n\\end{align}\n\\] 显然，回归平方和\\(SS_{R}\\)反映的是在y的总变异中由x与y的直线回归关系解释的那部分变异。\\(SS_R\\)值越大，说明回归直线的拟合效果就越好。\n\\(\\sum_{i}^{n}(y_i-\\hat y_i)^2\\)为残差平方和（residual sum of squares），记为\\(SS_E\\)，表示观测值\\(y_i\\)与回归估计值\\(\\hat y_i\\)的离差平方和，其公式为： \\[SS_E=\\sum_{i=1}^{n}(y_i-\\hat y_i)^2\\] \\(SS_E\\)反映了在总变异中扣除自变量x对因变量y的线性影响以后的其他因素（包括x对y的非线性影响和随机误差等）对y变异的影响，也就是在总平方和中无法用y和x线性回归关系解释的部分。\\(SS_E\\)值越小，说明回归直线的拟合效果就越好。\n对公式进行简化： \\[\\begin{align}\nSS_{yy}=&\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\\\\n=&SS_R+SS_E\n\\end{align}\\] 上述三个平方和，各有其相应的自由度\\(v\\)，并有如下关系： \\[v_{yy}=v_R+v_E\\\\\nv_{yy}=n-1,v_R=1,v_E=n-2\\]\n在\\(H_0\\)成立的条件下，有： \\[\\frac{SS_R}{\\sigma^2}\\sim \\chi^2(v_R),\\frac{SS_E}{\\sigma^2}\\sim \\chi^2(v_E)\\] 且\\(SS_R\\)和\\(SS_E\\)相互独立。\n检验统计量：\n\\[F=\\frac{SS_R/v_R}{SS_E/v_E}\\] 服从自由度\\(v_R=1,v_E=n-2\\)的F分布。如果y和x确实存在直线回归关系，那么回归所解释的变异\\(SS_R\\)应大于其他因素所解释的变异\\(SS_E\\)。由此可见，F检验正是建立在这个基础上的。\n对于给定的检验水准\\(\\alpha\\)， 如果\\(F&gt;F_{(v_R,v_E),1-\\alpha}\\)，则拒绝\\(H_0\\)，认为直线回归方程有统计学显著性；\n如果\\(F\\leq F_{(v_R,v_E),1-\\alpha}\\)，则不拒绝\\(H_0\\)，尚不能认为直线回归方程有统计学显著性。\n\nt检验法",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#直线相关与直线回归的比较",
    "href": "Learn/Basic/10-regression-correlation.html#直线相关与直线回归的比较",
    "title": "简单线性相关和回归",
    "section": "\n2.3 直线相关与直线回归的比较",
    "text": "2.3 直线相关与直线回归的比较\n\n\n\n\n\n\n\n区别与联系\n类目\n内容\n\n\n\n区别\n资料要求\n1. 线性相关要求X,Y服从双变量正态分布，对这种资料进行回归分析称为\\(\\textrm{II}\\)型回归，即可以把X当自变量，也可以当因变量，反之亦然。2. 线性回归要求Y在给定X值时服从正态分布，X可以是精确测量和严格控制的变量，这时的回归称为型回归，即不可以把X当因变量，Y当自变量进行回归分析。\n\n\n\n\n应用\n1. 线性相关用来表达两个变量间的互依关系，两个变量的研究地位是相等的，谁做X，谁做Y都可以；2. 线性回归用来表达两个变量间的依存变化的数量关系，即一个变量（为因变量Y）如何依存于另一个变量（为自变量X）而变化，两个变量的研究地位是不相等的。\n\n\n\n意义\n1. 相关系数r说明具有线性关系的两个变量之间的密切程度和相关方向；2. 回归系数b表示X每变化一个单位所导致的Y的平均变化量。\n\n\n\nr和b的取值范围\nr没有单位，而b有单位（其单位是：Y的单位/X的单位），所以导致两者的取值范围不同；\\(-1 \\le r \\le 1\\),\\(-\\infty&lt;b&lt;+\\infty\\)\n\n\n\n\nr和b的计算公式不同\n\n\\(r=\\frac{l_{xy}}{\\sqrt{l_{xx}l_{yy}}}\\),\\(b=\\frac{SS_{xy}}{SS_{xx}}\\)\n\n\n\n联系\n符号\n对于既可以做相关又可作回归的同一组资料，计算出r与b的正负号相同\n\n\n\n假设检验\n对于同一组资料，相关系数和回归系数的假设检验等价。即有：\\(t_b=t_r\\)\n\n\n\n\n相互换算\n对于同一组资料，相关系数和回归系数可通过下式换算：\\(b=r\\frac{S_Y}{S_X}\\)，式中的\\(S_X,S_Y\\)分别是\\(X,Y\\)的标准差\n\n\n\n用回归解释相关\n又决定系数\\(R^2=\\frac{SS_{回}}{SS_{总}}\\in [0,1]\\)当总平方和的大小决定了相关的密切程度，回归平方和越接近总平方和，则\\(R^2\\)越接近1，相关的效果越好，说明回归效果越好，相关的密切程度也越高。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/08-anova-test.html",
    "href": "Learn/Basic/08-anova-test.html",
    "title": "方差分析",
    "section": "",
    "text": "类目\n完全随机设计的方差分析\n\n\n\n数据要求\n独立性、正态性、方差齐性\n\n\n检验目的\n推断多个样本所代表的总体均数是否不等\n\n\n\n\\(H_0\\)与\\(H_1\\)\n\n\n\\(H_0\\):\\(\\mu_1=\\mu_2=\\dots =\\mu_a\\)，各组所代表的总体均数相等。\\(H_1\\):\\(\\mu_1,\\mu_2,\\dots,\\mu_a\\)各组总体均数不全相等（至少有一个不等式成立）\n\n\n检验统计量\n\\(F=\\frac{MS_{组间}}{MS_{组内}}\\sim(v_{组间}=k-1，v_{组内}=n-k）\\)\n\n\n关键要点\n总变异分解为组间变异和组内变异\n\n\n\n\n\n\n\n\n\n类目\n随机区组设计的方差分析\n\n\n\n数据要求\n处理组间、区组间数据满足独立、正态性和方差齐性\n\n\n处理组假设\n\n\\(H_0\\)：不同处理组水平的均数相同；\\(H_1\\)：不同处理组水平的均属不全相同\n\n\n区组假设\n\n\\(H_0\\)：不同区组对观测指标的影响很大；\\(H_1\\)：不同区组对观测指标的影响不全相同\n\n\n检验统计量\n\n\\(F=\\frac{MS_{处理}}{MS_{误差}}\\sim(v_{处理}=k-1,v_{误差}=(b-1)×(k-1))\\)\\(F=\\frac{MS_{区组}}{MS_{误差}}\\sim(v_{区组}=k-1,v_{误差}=(b-1)×(k-1))\\)\n\n\n\n关键要点\n总变异分解为处理组变异、区组变异和随机误差变异\n\n\n\n\n\n\n\n\n\n\n类目\n析因设计的方差分析\n\n\n\n数据要求\n因素之间的数据独立，样本数据满足正态性和方差齐性假设\n\n\n检验目的\n推断多个因素及其交互作用是否对观测指标存在显著影响\n\n\n主效应假设\n\n\\(H_0\\): 各因素的水平对观测指标的均数无显著影响；\\(H_1\\): 各因素的水平对观测指标的均数存在显著影响\n\n\n交互作用假设\n\n\\(H_0\\): 不同因素水平的交互作用对观测指标的均数无显著影响；\\(H_1\\): 不同因素水平的交互作用对观测指标的均数存在显著影响\n\n\n检验统计量\n\\(F=\\frac{MS_{主效应或交互作用}}{MS_{误差}}\\sim F(v_{效应},v_{误差})\\)\n\n\n关键要点\n- 总变异分解为主效应变异、交互作用变异和随机误差变异- 各因素主效应和交互作用的显著性需要单独检验- 每个因素包含多个水平，可能是固定效应或随机效应",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "方差分析"
    ]
  },
  {
    "objectID": "Learn/Basic/08-anova-test.html#完全随机设计的方差分析",
    "href": "Learn/Basic/08-anova-test.html#完全随机设计的方差分析",
    "title": "方差分析",
    "section": "",
    "text": "类目\n完全随机设计的方差分析\n\n\n\n数据要求\n独立性、正态性、方差齐性\n\n\n检验目的\n推断多个样本所代表的总体均数是否不等\n\n\n\n\\(H_0\\)与\\(H_1\\)\n\n\n\\(H_0\\):\\(\\mu_1=\\mu_2=\\dots =\\mu_a\\)，各组所代表的总体均数相等。\\(H_1\\):\\(\\mu_1,\\mu_2,\\dots,\\mu_a\\)各组总体均数不全相等（至少有一个不等式成立）\n\n\n检验统计量\n\\(F=\\frac{MS_{组间}}{MS_{组内}}\\sim(v_{组间}=k-1，v_{组内}=n-k）\\)\n\n\n关键要点\n总变异分解为组间变异和组内变异",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "方差分析"
    ]
  },
  {
    "objectID": "Learn/Basic/08-anova-test.html#随机区组设计的方差分析",
    "href": "Learn/Basic/08-anova-test.html#随机区组设计的方差分析",
    "title": "方差分析",
    "section": "",
    "text": "类目\n随机区组设计的方差分析\n\n\n\n数据要求\n处理组间、区组间数据满足独立、正态性和方差齐性\n\n\n处理组假设\n\n\\(H_0\\)：不同处理组水平的均数相同；\\(H_1\\)：不同处理组水平的均属不全相同\n\n\n区组假设\n\n\\(H_0\\)：不同区组对观测指标的影响很大；\\(H_1\\)：不同区组对观测指标的影响不全相同\n\n\n检验统计量\n\n\\(F=\\frac{MS_{处理}}{MS_{误差}}\\sim(v_{处理}=k-1,v_{误差}=(b-1)×(k-1))\\)\\(F=\\frac{MS_{区组}}{MS_{误差}}\\sim(v_{区组}=k-1,v_{误差}=(b-1)×(k-1))\\)\n\n\n\n关键要点\n总变异分解为处理组变异、区组变异和随机误差变异",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "方差分析"
    ]
  },
  {
    "objectID": "Learn/Basic/08-anova-test.html#析因设计的总结",
    "href": "Learn/Basic/08-anova-test.html#析因设计的总结",
    "title": "方差分析",
    "section": "",
    "text": "类目\n析因设计的方差分析\n\n\n\n数据要求\n因素之间的数据独立，样本数据满足正态性和方差齐性假设\n\n\n检验目的\n推断多个因素及其交互作用是否对观测指标存在显著影响\n\n\n主效应假设\n\n\\(H_0\\): 各因素的水平对观测指标的均数无显著影响；\\(H_1\\): 各因素的水平对观测指标的均数存在显著影响\n\n\n交互作用假设\n\n\\(H_0\\): 不同因素水平的交互作用对观测指标的均数无显著影响；\\(H_1\\): 不同因素水平的交互作用对观测指标的均数存在显著影响\n\n\n检验统计量\n\\(F=\\frac{MS_{主效应或交互作用}}{MS_{误差}}\\sim F(v_{效应},v_{误差})\\)\n\n\n关键要点\n- 总变异分解为主效应变异、交互作用变异和随机误差变异- 各因素主效应和交互作用的显著性需要单独检验- 每个因素包含多个水平，可能是固定效应或随机效应",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "方差分析"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html",
    "href": "Learn/Basic/06-parameter-estimation.html",
    "title": "参数估计",
    "section": "",
    "text": "统计量实际上是一种对样本数据信息的压缩。一个好的统计量，应该能把样本中包含总体的信息全部提炼出来，而不损失任何信息，这样的统计量称为充分统计量（sufficient statistic）。\n\n\n抽样误差是抽样研究固有的属性，不可避免，它是由个体变异和抽样共同引起的。\n\n总体方差已知，或总体方差未知但样本量足够大时\n\n\\[\\bar X \\sim N(\\mu,\\sigma_{\\bar X}^2)\\] 将\\(\\bar X\\)标准化，有： \\[U=\\frac{\\bar X-\\mu}{\\sigma_{\\bar X}}=\\frac{\\bar X-\\mu}{\\sigma_ X/\\sqrt{n}}\\] U为标准化随机变量，\\(U\\sim N(0,1)\\)。\n若从一个非正态总体中随机抽样，且样本量足够大\\((n\\geq30)\\)，样本均数\\(\\bar X\\)的抽样分布又该如何？\n中心极限定理(Central limit theorems):中心极限定理指的是给定一个任意分布的总体\\(X\\)，只要存在有限的方差\\(\\sigma^2(\\sigma^2\\neq0)\\)，则当样本量n足够大时，样本均数\\(\\bar X\\)的抽样分布将近似的服从均数为\\(\\mu\\)和方差为\\(\\sigma_{\\bar X}^2\\)的正态分布。 \\[\\bar X\\simeq N(\\mu,\\frac{\\sigma^2}{n})\\] 在大样本量条件下，由于样本方差\\(S^2\\)对总体方差\\(\\sigma^2\\)的估计误差非常小，实践中我们可以直接用\\(S^2\\)替代\\(\\sigma^2\\)进行计算。\n每次从这些总体中随机抽取\\(n\\)个抽样，一共抽\\(m\\)次。然后把这\\(m\\)组抽样分别求出平均值。这些平均值的分布接近正态分布。\n\n\\[\\frac{(n-1)S^2}{\\sigma^2}\\sim \\chi^2(v)\\] \\(\\chi^2\\)分布式赫尔默特（F.R. Helmert）于1875年研究来自正态总体的样本方差的抽样分布时得出的，其密度函数为： \\[f_v(x)=\\begin{cases} \\frac{1}{2^{\\frac{v}{2}}\\Gamma\\left(\\frac{v}{2}\\right)}y^{\\frac{v}{2}-1}\\mathrm{e}^{-\\frac{\\chi^2}{2}},&\\chi^2&gt;0\\\\ 0,&\\chi^2\\leq0\\end{cases}\\] \\(\\chi^2\\)分布和\\(t\\)分布一样，是依赖于参数（自由度）的一簇分布。随着自由度的增加，其分布曲线由正偏态分布趋近于正态分布。\n\n\n\n\n\n\n\n率的统计指标\n计算公式\n\n\n\n样本率\\(p\\)的总体均数\n\\(\\mu_{p}=\\pi\\)\n\n\n样本量\\(p\\)的方差\n\n\\(\\sigma_p^2=\\frac{\\pi(1-\\pi)}{n}\\)(理论值);\\(S_p^2=\\frac{p(1-p)}{n}\\)(估计值)\n\n\n样本率\\(p\\)的标准差\n\n\\(\\sigma_p=\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\)(理论值);\\(S_p=\\sqrt{\\frac{p(1-p)}{n}}\\)(估计值)\n\n\n率的标准误\n\n\\(\\sigma_p=\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\)(理论值);\\(S_p=\\sqrt{\\frac{p(1-p)}{n}}\\)(估计值)\n\n\n\n\n\n\n\n\n\n均数的统计指标\n计算公式\n\n\n\n样本均数\n\\(\\bar X=\\frac{\\sum_\\limits{i=1}^{n}X_i}{n}\\)\n\n\n样本方差\n\n\\(\\sigma^2=\\frac{\\sum_\\limits{i=1}^{n}(\\mu-\\bar \\mu)^2}{n}\\)(理论值);\\(S^2=\\frac{\\sum_\\limits{i=1}^{n}(X_i-\\bar X)^2}{n-1}\\)(估计值) 1\n\n\n\n样本均数标准误(SE)\n\n\\(\\sigma_{\\bar X}=\\frac{\\sigma}{\\sqrt{n}}\\)(理论值);\\(S_{\\bar X}=\\frac{S}{\\sqrt{n}}\\)(估计值)\n\n\n\n大数定律(Law of large Numbers):当随机事件发生的次数足够多时，随机事件发生的频率趋近于预期的概率。可以简单理解为样本数量越多，其平概率越接近于期望值。大数定律的条件：\n\n独立重复事件；\n重复次数足够多。\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n标准差\n均数的标准误\n\n\n\n定义\n描述一组变量的离散程度，并可以作为总体标准差的点估计\n描述多个样本均数的离散程度，并且是样本均数的标准差估计值\n\n\n应用\n1. 标准差越小，个体资料的离散程度就越小，说明变量值围绕均数分布越紧密，均数的代表性越好  2.估计医学参考值范围，计算变异系数和标准误\n1. 标准误越小，统计量的平均抽样误差就越小，说明样本均数和总体均数的平均差异越小，用样本均数估计总体均数的可靠性越大；2. 计算置信区间、进行假设检验\n\n\n与n的关系\nn越大，样本标准差随机波动的幅度越来越小，并且稳定在总体标准差附近\nn越大，样本均数的标准误越小，并且趋向于0\n\n\n控制方法\n个体差异，不能通过统计方法控制\n增加n，可以减小标准误\n\n\n二者联系\n1. 两者都是变异指标  2. 在n相同的情况下，标准差越大，标准误相对越大；标准差越小，标准误也相对越小。正比关系  3. \\(\\sigma_{\\bar x}=\\frac{\\sigma}{\\sqrt{n}}\\),\\(\\sigma_{\\bar x}\\)与\\(\\sigma\\)成正比，与\\(\\sqrt{n}\\)成反比。\n\n\n\n\n\n\n\n\n\n\n\n类目\n总体均数的置信区间\n医学参考值范围\n\n\n\n含义\n按照预先给定的概率，确定的包含未知总体参数\\(\\mu\\)（总体均数）的可能范围\n指特定的“正常”人群（排除了对所研究指标有影响的疾病和有关因素的人群）的生理生化指标中大多数个体的取值所在的范围\n\n\n举例\n若某一样本的均值为10，其95%可信区间为（9.5,10.5），这就表示总体均数介于（9.5,10.5）之间的可信度为95%\n假设空腹血糖95%正常值范围为（3.6,6.1），这就是指95%正常人的空腹血糖值介于（3.6,6.1）之间\n\n\n计算\n1. 总体标准差位置，且样本量n不大，根据t分布计算；2. 总体标准差未知，n足够大，正态近似法；3. 总体标准差已知，根据Z分布计算\n1. 正态分布法； 2. 偏态分布法\n\n\n用途\n总体均数的区间估计（估计未知的总体均数所在范围\n1. 个体值的波动范围；  2. 绝大多数观察对象某指标分布范围； 3. 医学判断个体某指标是否正常\n\n\n95%理解的常见误区\n\n\n\n\n区别\n\n\n\n\n\n\n在95%置信区间内有95%的总体参数在该区间？×\n在95%置信区间内，该区间包含了95%的总体参数？×\n以\\(1-\\alpha=95\\%\\)算得的100个可信区间中，平均有95个可信区间包含了总体均数，而另外5个未包含总体均数。√\n在95%置信区间，该区间有95%的可能包含总体参数？×\n该区间包含总体参数，可信度在95%。√\n总体参数有95%的可能落在该区间。×\n\n\n\n置信水平（Confidence Level）\n置信水平是指在多次重复抽样时，置信区间覆盖总体参数的比例。常见的置信水平有95%、99%等。置信水平越高，表示对总体参数的估计越保守，但置信区间也会变得更宽。\n置信区间的宽度（Width of Confidence Interval）\n置信区间的宽度是指上下限之间的距离，反映了估计的精确程度。置信区间越窄，说明估计的精度越高，越宽则精度越低。\n\n\n样本大小（Sample Size）\n样本大小是置信区间宽度的一个关键决定因素。样本量越大，标准误差越小，置信区间越窄，从而提高估计的精确度。\n样本标准差（Sample Standard Deviation）\n样本标准差反映数据的离散程度。样本的波动越大，置信区间越宽；样本的波动越小，置信区间越窄。\n置信水平\n提高置信水平（例如从95%提高到99%）会导致置信区间变宽，因为要包含更多可能的参数值范围。\n总体分布的形状\n如果数据服从正态分布且样本量较大，置信区间估计会更精确；对于非正态分布，特别是在样本量较小时，置信区间的估计可能不够准确。\n估计方法（Point Estimate and Statistical Technique）\n使用不同的统计方法（如t分布、z分布）会对置信区间的范围造成影响。通常情况下，样本量较小时采用t分布，样本量较大时可以近似采用z分布。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#统计量",
    "href": "Learn/Basic/06-parameter-estimation.html#统计量",
    "title": "参数估计",
    "section": "",
    "text": "统计量实际上是一种对样本数据信息的压缩。一个好的统计量，应该能把样本中包含总体的信息全部提炼出来，而不损失任何信息，这样的统计量称为充分统计量（sufficient statistic）。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#抽样分布",
    "href": "Learn/Basic/06-parameter-estimation.html#抽样分布",
    "title": "参数估计",
    "section": "",
    "text": "抽样误差是抽样研究固有的属性，不可避免，它是由个体变异和抽样共同引起的。\n\n总体方差已知，或总体方差未知但样本量足够大时\n\n\\[\\bar X \\sim N(\\mu,\\sigma_{\\bar X}^2)\\] 将\\(\\bar X\\)标准化，有： \\[U=\\frac{\\bar X-\\mu}{\\sigma_{\\bar X}}=\\frac{\\bar X-\\mu}{\\sigma_ X/\\sqrt{n}}\\] U为标准化随机变量，\\(U\\sim N(0,1)\\)。\n若从一个非正态总体中随机抽样，且样本量足够大\\((n\\geq30)\\)，样本均数\\(\\bar X\\)的抽样分布又该如何？\n中心极限定理(Central limit theorems):中心极限定理指的是给定一个任意分布的总体\\(X\\)，只要存在有限的方差\\(\\sigma^2(\\sigma^2\\neq0)\\)，则当样本量n足够大时，样本均数\\(\\bar X\\)的抽样分布将近似的服从均数为\\(\\mu\\)和方差为\\(\\sigma_{\\bar X}^2\\)的正态分布。 \\[\\bar X\\simeq N(\\mu,\\frac{\\sigma^2}{n})\\] 在大样本量条件下，由于样本方差\\(S^2\\)对总体方差\\(\\sigma^2\\)的估计误差非常小，实践中我们可以直接用\\(S^2\\)替代\\(\\sigma^2\\)进行计算。\n每次从这些总体中随机抽取\\(n\\)个抽样，一共抽\\(m\\)次。然后把这\\(m\\)组抽样分别求出平均值。这些平均值的分布接近正态分布。\n\n\\[\\frac{(n-1)S^2}{\\sigma^2}\\sim \\chi^2(v)\\] \\(\\chi^2\\)分布式赫尔默特（F.R. Helmert）于1875年研究来自正态总体的样本方差的抽样分布时得出的，其密度函数为： \\[f_v(x)=\\begin{cases} \\frac{1}{2^{\\frac{v}{2}}\\Gamma\\left(\\frac{v}{2}\\right)}y^{\\frac{v}{2}-1}\\mathrm{e}^{-\\frac{\\chi^2}{2}},&\\chi^2&gt;0\\\\ 0,&\\chi^2\\leq0\\end{cases}\\] \\(\\chi^2\\)分布和\\(t\\)分布一样，是依赖于参数（自由度）的一簇分布。随着自由度的增加，其分布曲线由正偏态分布趋近于正态分布。\n\n\n\n\n\n\n\n率的统计指标\n计算公式\n\n\n\n样本率\\(p\\)的总体均数\n\\(\\mu_{p}=\\pi\\)\n\n\n样本量\\(p\\)的方差\n\n\\(\\sigma_p^2=\\frac{\\pi(1-\\pi)}{n}\\)(理论值);\\(S_p^2=\\frac{p(1-p)}{n}\\)(估计值)\n\n\n样本率\\(p\\)的标准差\n\n\\(\\sigma_p=\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\)(理论值);\\(S_p=\\sqrt{\\frac{p(1-p)}{n}}\\)(估计值)\n\n\n率的标准误\n\n\\(\\sigma_p=\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\)(理论值);\\(S_p=\\sqrt{\\frac{p(1-p)}{n}}\\)(估计值)\n\n\n\n\n\n\n\n\n\n均数的统计指标\n计算公式\n\n\n\n样本均数\n\\(\\bar X=\\frac{\\sum_\\limits{i=1}^{n}X_i}{n}\\)\n\n\n样本方差\n\n\\(\\sigma^2=\\frac{\\sum_\\limits{i=1}^{n}(\\mu-\\bar \\mu)^2}{n}\\)(理论值);\\(S^2=\\frac{\\sum_\\limits{i=1}^{n}(X_i-\\bar X)^2}{n-1}\\)(估计值) 1\n\n\n\n样本均数标准误(SE)\n\n\\(\\sigma_{\\bar X}=\\frac{\\sigma}{\\sqrt{n}}\\)(理论值);\\(S_{\\bar X}=\\frac{S}{\\sqrt{n}}\\)(估计值)\n\n\n\n大数定律(Law of large Numbers):当随机事件发生的次数足够多时，随机事件发生的频率趋近于预期的概率。可以简单理解为样本数量越多，其平概率越接近于期望值。大数定律的条件：\n\n独立重复事件；\n重复次数足够多。\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n标准差\n均数的标准误\n\n\n\n定义\n描述一组变量的离散程度，并可以作为总体标准差的点估计\n描述多个样本均数的离散程度，并且是样本均数的标准差估计值\n\n\n应用\n1. 标准差越小，个体资料的离散程度就越小，说明变量值围绕均数分布越紧密，均数的代表性越好  2.估计医学参考值范围，计算变异系数和标准误\n1. 标准误越小，统计量的平均抽样误差就越小，说明样本均数和总体均数的平均差异越小，用样本均数估计总体均数的可靠性越大；2. 计算置信区间、进行假设检验\n\n\n与n的关系\nn越大，样本标准差随机波动的幅度越来越小，并且稳定在总体标准差附近\nn越大，样本均数的标准误越小，并且趋向于0\n\n\n控制方法\n个体差异，不能通过统计方法控制\n增加n，可以减小标准误\n\n\n二者联系\n1. 两者都是变异指标  2. 在n相同的情况下，标准差越大，标准误相对越大；标准差越小，标准误也相对越小。正比关系  3. \\(\\sigma_{\\bar x}=\\frac{\\sigma}{\\sqrt{n}}\\),\\(\\sigma_{\\bar x}\\)与\\(\\sigma\\)成正比，与\\(\\sqrt{n}\\)成反比。\n\n\n\n\n\n\n\n\n\n\n\n类目\n总体均数的置信区间\n医学参考值范围\n\n\n\n含义\n按照预先给定的概率，确定的包含未知总体参数\\(\\mu\\)（总体均数）的可能范围\n指特定的“正常”人群（排除了对所研究指标有影响的疾病和有关因素的人群）的生理生化指标中大多数个体的取值所在的范围\n\n\n举例\n若某一样本的均值为10，其95%可信区间为（9.5,10.5），这就表示总体均数介于（9.5,10.5）之间的可信度为95%\n假设空腹血糖95%正常值范围为（3.6,6.1），这就是指95%正常人的空腹血糖值介于（3.6,6.1）之间\n\n\n计算\n1. 总体标准差位置，且样本量n不大，根据t分布计算；2. 总体标准差未知，n足够大，正态近似法；3. 总体标准差已知，根据Z分布计算\n1. 正态分布法； 2. 偏态分布法\n\n\n用途\n总体均数的区间估计（估计未知的总体均数所在范围\n1. 个体值的波动范围；  2. 绝大多数观察对象某指标分布范围； 3. 医学判断个体某指标是否正常\n\n\n95%理解的常见误区\n\n\n\n\n区别",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#置信区间的含义与常见说法辨析",
    "href": "Learn/Basic/06-parameter-estimation.html#置信区间的含义与常见说法辨析",
    "title": "参数估计",
    "section": "",
    "text": "在95%置信区间内有95%的总体参数在该区间？×\n在95%置信区间内，该区间包含了95%的总体参数？×\n以\\(1-\\alpha=95\\%\\)算得的100个可信区间中，平均有95个可信区间包含了总体均数，而另外5个未包含总体均数。√\n在95%置信区间，该区间有95%的可能包含总体参数？×\n该区间包含总体参数，可信度在95%。√\n总体参数有95%的可能落在该区间。×",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#置信区间的两要素及影响因素",
    "href": "Learn/Basic/06-parameter-estimation.html#置信区间的两要素及影响因素",
    "title": "参数估计",
    "section": "",
    "text": "置信水平（Confidence Level）\n置信水平是指在多次重复抽样时，置信区间覆盖总体参数的比例。常见的置信水平有95%、99%等。置信水平越高，表示对总体参数的估计越保守，但置信区间也会变得更宽。\n置信区间的宽度（Width of Confidence Interval）\n置信区间的宽度是指上下限之间的距离，反映了估计的精确程度。置信区间越窄，说明估计的精度越高，越宽则精度越低。\n\n\n样本大小（Sample Size）\n样本大小是置信区间宽度的一个关键决定因素。样本量越大，标准误差越小，置信区间越窄，从而提高估计的精确度。\n样本标准差（Sample Standard Deviation）\n样本标准差反映数据的离散程度。样本的波动越大，置信区间越宽；样本的波动越小，置信区间越窄。\n置信水平\n提高置信水平（例如从95%提高到99%）会导致置信区间变宽，因为要包含更多可能的参数值范围。\n总体分布的形状\n如果数据服从正态分布且样本量较大，置信区间估计会更精确；对于非正态分布，特别是在样本量较小时，置信区间的估计可能不够准确。\n估计方法（Point Estimate and Statistical Technique）\n使用不同的统计方法（如t分布、z分布）会对置信区间的范围造成影响。通常情况下，样本量较小时采用t分布，样本量较大时可以近似采用z分布。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#假设检验的基本步骤",
    "href": "Learn/Basic/06-parameter-estimation.html#假设检验的基本步骤",
    "title": "参数估计",
    "section": "\n3.1 假设检验的基本步骤",
    "text": "3.1 假设检验的基本步骤\n\n\n\n\n\n\n步骤\n内容\n\n\n\n建立假设检验，确定检验水准\n1. 双侧检验：\\(H_{0}:\\mu_{d}=0;H_{1}:\\mu_{d}\\neq 0,\\alpha=0.05\\)  2. 单侧检验：\\(H_{0}:\\mu_{d}=0;H_{1}:\\mu_{d}&lt;0或\\mu_{d}&gt;0,\\alpha=0.05\\)\n\n\n\n\n1. 假设检验是针对总体的，而非样本；2. 单双侧检验主要根据专业知识预先确定，并且还需要考虑差异的方向； 3. 单侧检验的检验效能更高。\n\n\n计算并选择检验统计量\n1. 根据研究设计方案、资料类型、样本含量大小及分析目的选用适当的检验方法，并根据样本资料计算相应的检验统计量；2. 不同的检验方法要用不同的公式计算现有样本的检验统计量（\\(t\\)检验、\\(\\chi^2\\)检验、\\(F\\)检验）；3. 检验统计量是在\\(H_{0}\\)成立的前提下计算的。\n\n\n确定P值，做出推断\n假设检验的统计学结论：1. 若\\(P\\le \\alpha\\)，按所取\\(\\alpha\\)检验水准，拒绝\\(H_{0}\\)，接受\\(H_{1}\\)，可以认为…有差异；2. 若\\(P&gt;\\alpha\\)时，现有样本信息还不足以拒绝H0，尚不能认为…有差异\n\n\n\n假设检验所做出的的结论是具有概率性质的，不是绝对的肯定或否定。不论拒绝或不拒绝\\(H_{0}\\)都可能发生错误。下结论时，只能两种：1. 两总体有无差异；2. 两样本差异有无统计学意义。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#假设检验的两型错误检验效能",
    "href": "Learn/Basic/06-parameter-estimation.html#假设检验的两型错误检验效能",
    "title": "参数估计",
    "section": "\n3.2 假设检验的两型错误、检验效能",
    "text": "3.2 假设检验的两型错误、检验效能\n\n\n\n\n\n\n\n客观实际\n拒绝\\(H_{0}\\)，接受\\(H_{1}\\)\n\n不拒绝\\(H_{0}\\)\n\n\n\n\n\n\\(H_{0}\\)成立\n\n\\(\\textrm{I}\\)型错误(\\(\\alpha\\))(假阳性) 错误拒绝实际成立的\\(H_{0}\\)\n\n正确推断(\\(1-\\alpha\\))\n\n\n\n\\(H_{0}\\)不成立\n正确推断(\\(1-\\beta\\))\\(H_{1}\\)为真，能够拒绝\\(H_{0}\\)的概率称为发现该\\(H_{1}\\)的检验效能，用\\(1-\\beta\\)表示\n\n\\(\\textrm{II}\\)型错误(\\(\\beta\\))(假阴性)不拒绝实际不成立的\\(H_{0}\\)\n\n\n\n\n\n3.2.1 \\(1-\\beta\\)的影响因素：\n\n检验水准\\(\\alpha\\)（正向）——检验水准\\(\\alpha\\)越大，检验效能越大\n\n\\(H_{0}\\)与\\(H_{1}\\)的差异大小（正向）——差异越大，检验效能越大\n样本量（正向）——样本量越大，检验效能越大\n标准差越大（反向）——个体差异（标准差）越小，检验效能越大\n单双侧检验：单侧检验效能高于双侧检验效能\n\n3.2.2 \\(\\alpha 、\\beta 、1-\\beta\\)关系：\n\n当样本量确定时，\\(\\alpha\\)与\\(\\beta\\)呈反向变化关系，与\\(1-\\beta\\)呈正向变化关系。如果把\\(\\alpha\\)设置得很小，势必增加犯\\(\\textrm{II}\\)型错误的概率，从而降低检验效能；反之，如果把重点放在减少\\(\\beta\\)上，势必增加犯\\(\\textrm{I}\\)型错误的概率，从而降低了置信度。\n要同时减小\\(\\alpha\\)和\\(\\beta\\)，只有通过增加样本含量来计算。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#假设检验与置信区间的关系",
    "href": "Learn/Basic/06-parameter-estimation.html#假设检验与置信区间的关系",
    "title": "参数估计",
    "section": "\n3.3 假设检验与置信区间的关系",
    "text": "3.3 假设检验与置信区间的关系\n\n\n\n\n\n\n\n基本思想\n假设检验\n置信区间\n\n\n\n基本思想\n假设检验的假设是指我们对总体特征（如参数、分布）的某种推测，从而用概率来判断样本数据所提供的的信息和我们对总体特征猜想的一致性，进而结合专业知识判断这一猜想的正确性\n置信区间是指有样本统计量所构造的总体参数的估计区间，区间估计是按照一定的概率和可信度\\((1-\\alpha)\\)用一个区间估计总体参数所在的范围，这个范围称作可信度为\\((1-\\alpha)\\)的可信区间\n\n\n区别\n1. 假设检验用于推断总体参数之间是否不同\n1. 置信区间用于推断总体参数所在范围； 2.置信区间比假设检验提供更多的信息，置信区间能够回答假设检验的问题； 3. 置信区间在回答差别有无统计学意义时，还可以提示差别是否具有实际意义。\n\n\n联系\n1. 假设检验与置信区间都属于统计推断方法；2. 置信区间估计总体参数所采用的的统计量与假设检验的检验统计量相同；\n3.置信区间能够回答假设检验的问题。根据置信度\\(1-\\alpha\\)构造置信区间，如果统计量在置信区间内，那么不拒绝原假设；如果不在置信区间中，那么拒绝原假设；4. 双侧检验时，置信区间确定的\\(z'\\)与检验水准\\(\\alpha\\)确定的检验统计量的分布界值相同，因此，在双侧检验时\\(C=1-\\alpha\\)。根据显著水平\\(\\alpha\\)，可以构造置信度为\\(1-\\alpha\\)的置信区间",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#footnotes",
    "href": "Learn/Basic/06-parameter-estimation.html#footnotes",
    "title": "参数估计",
    "section": "脚注",
    "text": "脚注\n\n无偏方差:\\(S^2\\)作为样本方差，称之为无偏方差。样本方差是度量样本离散程度的统计量，其中n为样本量， \\(\\sum_{i=1}^{n}(x_i-\\bar x)^2\\)为偏差平方和，\\(n-1\\)称为偏差平方和的自由度，因为在\\(\\bar x\\)确定后，\\(x_i(i=1,2,\\dots,n)\\)中只有\\(n-1\\)个可以自由变动。↩︎",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/04-discrete-type-random-variable.html",
    "href": "Learn/Basic/04-discrete-type-random-variable.html",
    "title": "离散型随机变量的概率分布",
    "section": "",
    "text": "type of data\n\n\n定义：\\(n\\)次伯努利试验，成功的次数为\\(X\\)的离散概率分布，其中每次试验的成功概率为\\(\\pi\\)，失败的概率为\\(1-\\pi\\)。\n\n\n\\(X\\)的总体均数\\(\\mu_{x}=n\\pi\\)\n\n总体方差\\(\\sigma_{x}=n\\pi(1-\\pi)\\)\n\n\nnotice：\n\n实际上，当\\(n=1\\)时，二项分布就是伯努利试验。\n伯努利试验要求：互斥、独立、重复\n\n\n\n\n\n\nBinomial Distribution with Different n/π\n\n\n\n\n定义：描述在单位面积、单位时间或单位空间中罕见事件发生次数的概率分布为泊松分布，记作\\(P(\\mu)\\)。泊松分布是二项分布的极限形式，当一个二项分布的\\(n\\)很大，\\(\\pi\\)很小时，此时，这个二项分布近似于泊松分布。\n\n其总体均数与总体方差相等，记为\\(\\mu\\)\n\n可加性：\\(X\\sim P(\\mu_{1})\\)，\\(Y\\sim P(\\mu_{2})\\)，若\\(X\\)与\\(Y\\) 独立，则\\(X+Y \\sim P(\\mu_{1}+\\mu_{1})\\)\n\n泊松分布只有一个参数\\(\\lambda(\\mu)\\)\n\n服从泊松分布的随机变量，其取值为\\(0\\)到\\(+\\infty\\)的概率之和为1\n一般来说，当\\(\\mu \\ge20\\)时，可以认为近似正态分布\n\n\nlibrary(ggplot2)\n# Define the range for x\nx &lt;- 0:40\n\n# Define the lambda values\nlambdas &lt;- c(1, 4, 10, 20)\n\n# Set up the plot area\nplot(x, dpois(x, lambdas[1]), type=\"n\", ylim=c(0, max(dpois(x, lambdas))), \n     xlab=\"x\", ylab=\"Probability\", main=\"Poisson Distribution with Different λ Values\")\n\n# Plot the Poisson distributions for each lambda\ncolors &lt;- c(\"blue\", \"green\", \"red\", \"purple\")\nfor (i in 1:length(lambdas)) {\n  lines(x, dpois(x, lambdas[i]), type=\"b\", pch=19, col=colors[i])\n}\n\n# Add a legend\nlegend(\"topright\", legend=paste(\"λ =\", lambdas), col=colors, pch=19)\n\n\n\n\n\n\n\n\n\n\n\nPoisson Distribution with Different λ=nπ\n\n\n\n\n\n统计描述角度：直接法计算概率 [ Pr(X=K)={k}(1-){n-k},k=0,1,2,3,,n ]\n统计推断角度：区间估计、假设检验\n\n\n统计描述角度：直接法计算概率 [ Pr(X=K)=,k=0,1,2,]\n统计推断角度：区间估计、假设检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "离散型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/04-discrete-type-random-variable.html#二项分布binomial-distribution",
    "href": "Learn/Basic/04-discrete-type-random-variable.html#二项分布binomial-distribution",
    "title": "离散型随机变量的概率分布",
    "section": "",
    "text": "定义：\\(n\\)次伯努利试验，成功的次数为\\(X\\)的离散概率分布，其中每次试验的成功概率为\\(\\pi\\)，失败的概率为\\(1-\\pi\\)。\n\n\n\\(X\\)的总体均数\\(\\mu_{x}=n\\pi\\)\n\n总体方差\\(\\sigma_{x}=n\\pi(1-\\pi)\\)\n\n\nnotice：\n\n实际上，当\\(n=1\\)时，二项分布就是伯努利试验。\n伯努利试验要求：互斥、独立、重复\n\n\n\n\n\n\nBinomial Distribution with Different n/π",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "离散型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/04-discrete-type-random-variable.html#泊松分布poission-distribution",
    "href": "Learn/Basic/04-discrete-type-random-variable.html#泊松分布poission-distribution",
    "title": "离散型随机变量的概率分布",
    "section": "",
    "text": "定义：描述在单位面积、单位时间或单位空间中罕见事件发生次数的概率分布为泊松分布，记作\\(P(\\mu)\\)。泊松分布是二项分布的极限形式，当一个二项分布的\\(n\\)很大，\\(\\pi\\)很小时，此时，这个二项分布近似于泊松分布。\n\n其总体均数与总体方差相等，记为\\(\\mu\\)\n\n可加性：\\(X\\sim P(\\mu_{1})\\)，\\(Y\\sim P(\\mu_{2})\\)，若\\(X\\)与\\(Y\\) 独立，则\\(X+Y \\sim P(\\mu_{1}+\\mu_{1})\\)\n\n泊松分布只有一个参数\\(\\lambda(\\mu)\\)\n\n服从泊松分布的随机变量，其取值为\\(0\\)到\\(+\\infty\\)的概率之和为1\n一般来说，当\\(\\mu \\ge20\\)时，可以认为近似正态分布\n\n\nlibrary(ggplot2)\n# Define the range for x\nx &lt;- 0:40\n\n# Define the lambda values\nlambdas &lt;- c(1, 4, 10, 20)\n\n# Set up the plot area\nplot(x, dpois(x, lambdas[1]), type=\"n\", ylim=c(0, max(dpois(x, lambdas))), \n     xlab=\"x\", ylab=\"Probability\", main=\"Poisson Distribution with Different λ Values\")\n\n# Plot the Poisson distributions for each lambda\ncolors &lt;- c(\"blue\", \"green\", \"red\", \"purple\")\nfor (i in 1:length(lambdas)) {\n  lines(x, dpois(x, lambdas[i]), type=\"b\", pch=19, col=colors[i])\n}\n\n# Add a legend\nlegend(\"topright\", legend=paste(\"λ =\", lambdas), col=colors, pch=19)\n\n\n\n\n\n\n\n\n\n\n\nPoisson Distribution with Different λ=nπ",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "离散型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/04-discrete-type-random-variable.html#二项分布的应用",
    "href": "Learn/Basic/04-discrete-type-random-variable.html#二项分布的应用",
    "title": "离散型随机变量的概率分布",
    "section": "",
    "text": "统计描述角度：直接法计算概率 [ Pr(X=K)={k}(1-){n-k},k=0,1,2,3,,n ]\n统计推断角度：区间估计、假设检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "离散型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/04-discrete-type-random-variable.html#泊松分布的应用",
    "href": "Learn/Basic/04-discrete-type-random-variable.html#泊松分布的应用",
    "title": "离散型随机变量的概率分布",
    "section": "",
    "text": "统计描述角度：直接法计算概率 [ Pr(X=K)=,k=0,1,2,]\n统计推断角度：区间估计、假设检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "离散型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html",
    "href": "Learn/Basic/02-descriptive-statistics.html",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "指标\n定义→本质\n表示方法\n计算方法\n应用条件\n\n\n\n\n算术均数\n先求和再平均\n(1)样本均数:\\(\\bar x\\)  (2)总体均数:\\(\\mu\\)\n(1)直接法：\\(\\bar X=\\frac{\\sum{X_{i}}}{n}\\)  (2)加权法：\\(\\bar X = \\frac{\\sum{f_{i}X_{i}}}{\\sum{f}}\\)，（\\(X\\)为组中值，\\(f\\)为频数）\n(1)对称分布，尤其是正态分布  (2)不含极端值\n\n\n几何均数\n先乘积再开方\n\\(G\\)\n(1)直接法：\\(G=\\sqrt[n]{x_{1}·x_{2}·x_{3}\\cdots}x_{n}\\)  (2)加权法：\\(G=\\ln^{-1}(\\frac{\\sum{f_{i}\\ln X_{i}}}{\\sum{f_{i}}})\\)\n(1)数据呈倍数变化或对数正态分布→正偏态分布  (2)观察值中不能有零且不能同时有正数和负数→对数性质\n\n\n中位数\n从小到大找中间  还要注意奇偶性\n\\(M\\)\n(1)直接法：n为奇数，\\(M=X_{\\frac{n+1}{x}}\\)  n为偶数，\\(M=\\frac{1}{2} (X_{\\frac{n}{2}}+X_{\\frac{n}{2}+1})\\)  (2)加权法：百分位数法\\(P_{X}=L_{X}+\\frac{i}{f_{x}}(nX\\%-\\sum{f_{i}})\\)\n任何资料\n\n\n众数\n出现次数最多\n\\\n(1)直接法：一组数据中出现次数最多的数值  加权法：f最多的组段的组中值\\(X\\)\n任何数据\n\n\n\n\n\n\n\n对称分布：算术均数\\(\\approx\\)中位数\n右偏态：算术均数\\(&gt;\\)中位数\n左偏态：算术均数\\(&lt;\\)中位数\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n指标\n本质→定义\n表示方式\n计算公式\n适用条件\n\n\n\n\n极差\n\\(X_{Max}-X_{Min}\\)\n\\(R\\)\n\n\n\n\n四分位间距/范围\n位置指标\n\\(IQR\\)\n\\(IQR=P_{75}-P_{25}\\)\n任何资料\n\n\n方差\n离均差平方和求平均\n样本：\\(s^{2}\\)  总体：\\(\\sigma^{2}\\)\n\\(s^{2}=\\frac{\\sum(x_{i}-x)^2}{(n-1)}\\)\n对称分布，尤其是正态分布;不含极端值\n\n\n标准差\n方差开根号\n样本：\\(s\\)  总体：\\(\\sigma\\)\n\\(s=\\sqrt{\\frac{\\sum(x_{i}-x)^2}{(n-1)}}\\)\n同上\n\n\n变异系数\n测量数据变异程度的相对统计量\n\\(CV\\)\n\\(CV=\\frac{s}{\\bar x}×100%\\)\n(1)单位相同：但均数相差悬殊；(2)单位不同\n\n\n\n\n\n\n\n直接法 \\[s=\\sqrt{\\frac{\\sum\\limits_{i=1}^nx_i^2-\\frac{\\left(\\sum\\limits_{i=1}^nx_i\\right)^2}{n}}{n-1}}\\]\n加权法：与讨算均数的方法类似，对频数表资料采用加权法，讨算公式为\n\n\\[s=\\sqrt{\\frac{\\sum\\limits_{k=1}^gf_kx_{mk}^2-\\left(\\sum\\limits_{k=1}^gf_kx_{mk}\\right)^2 \\left(\\sum\\limits_{k=1}^gf_k\\right)}{\\sum\\limits_{k=1}^gf_k-1}}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n指标\n相对比\n构成比\n频率型指标\n强度性指标\n\n\n\n\n定义\n两个有关联的指标之比\n某一部分与总体之比\n某时期内累计出现的频率\n单位时间内某件事发生的频率\n\n\n计算公式\n\\(\\frac{A指标}{B指标}\\)\n\\(\\frac{某一事物总体中某一部分}{某一事物所有组成部分的总体}×100\\%\\)\n\\(\\frac{同时期实际发生某现象的观察单位数}{某时期可能发生某现象的观察单位总数}×K\\)\n\\(\\frac{发生某件事的观察单位数}{\\sum(观察单位×观察时间)}×K\\)\n\n\n量纲\n可有可无\n一般无量纲\n无\n有\n\n\n取值\n没有限制\n[0,1]\n[0,1]\n可大于1\n\n\n举例\nRR,变异系数CV\n死因构成比\n病死率，累计发病率\n发病率，发病密度\n\n\n\n\n\n\n\n绝对量指标\n\n累计增长量\n逐年增长量\n\n定基类指标\n\n定基发展速度\n定基增长速度\n\n环比类指标\n\n环比发展速度\n环比增长速度\n\n平均类指标\n\n平均发展速度\n平均增长速度\n\n\n\n\n\n\n频率型指标的解释要紧扣总体和属性\n计算相对数分母应该有足够的观察单位数 -如果观察例数太少，则相对数波动较大\n\n若因实际因素，观察例数确实过少，建议直接采用绝对数\n\n正确计算合计率：分子分母分别相加，再求合计率\n不能用结构相对数代替强度相对数，不能混淆频率型指标和强度型指标，不能以比代率\n注意资料的可比性\n不能仅用样本率比较，因为样本和总体之间存在抽样误差，需要进行假设检验推断总体的情况\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n指标\n参照人数\n目标人群\n过程\n\n\n\n\n直接标化法\n人口构成\n率\n各年龄组期望死亡数→期望死亡合计数→直接标化率\n\n\n简介标化法\n率\n人口构成\n各年龄组期望死亡数→期望死亡合计数→变化死亡比→间接标化率\n\n\n\nnotice：\n\n变化标化比\\(SMR=\\frac{实际死亡人数}{期望死亡人数}\\)\n直接标准化选择的标准是：各年龄组标准人口构成比或各年龄组标准人口数\n\n\n\n\n\n\n\n连续型变量\n\n计量资料\n定量资料\n\n离散型变量\n\n不具有分类性质的资料\n离散型定量资料\n\n分类资料\n\n有序分类资料\n等级资料\n半定量资料\n无序分类资料\n名义变量\n\n\nnotice：一般来说，统计图的选择，是综合考量了变量取值特点+研究目的。\n\n\n\n\n\n\n\n\n\n\n\n统计图\n资料类型\n分析目的\n\n\n\n\n圆图和百分条图\n构成比资料\n用圆的扇形面积或直条各段的长度表示事物各组成部分的构成情况\n\n\n直条图\n相互独立资料\n用直条长短表示相互独立的各指标的数值大小，一般用于比较不同组别的指标大小\n\n\n直方图\n连续型变量的频数分布\n用矩阵面积表示各组段的频数（频率）\n\n\n箱式图\n连续型资料\n描述数据的分布特征（包括中位数、四分位范围、最大值和最小值）\n\n\n普通线图\n连续型资料\n用线段的升降表示某事物在时间上的变化趋势、或某一现象随着另一现象变化的情况\n\n\n半对数线图\n连续型资料\n用线段的升降表示事物的相对变化速度\n\n\n散点图\n双变量连续型资料\n表示两种事物变化的相关性和趋势\n\n\n\n\n\n\n\n\n\nChoice of Statistical Charts",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#定量资料的描述指标",
    "href": "Learn/Basic/02-descriptive-statistics.html#定量资料的描述指标",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "指标\n定义→本质\n表示方法\n计算方法\n应用条件\n\n\n\n\n算术均数\n先求和再平均\n(1)样本均数:\\(\\bar x\\)  (2)总体均数:\\(\\mu\\)\n(1)直接法：\\(\\bar X=\\frac{\\sum{X_{i}}}{n}\\)  (2)加权法：\\(\\bar X = \\frac{\\sum{f_{i}X_{i}}}{\\sum{f}}\\)，（\\(X\\)为组中值，\\(f\\)为频数）\n(1)对称分布，尤其是正态分布  (2)不含极端值\n\n\n几何均数\n先乘积再开方\n\\(G\\)\n(1)直接法：\\(G=\\sqrt[n]{x_{1}·x_{2}·x_{3}\\cdots}x_{n}\\)  (2)加权法：\\(G=\\ln^{-1}(\\frac{\\sum{f_{i}\\ln X_{i}}}{\\sum{f_{i}}})\\)\n(1)数据呈倍数变化或对数正态分布→正偏态分布  (2)观察值中不能有零且不能同时有正数和负数→对数性质\n\n\n中位数\n从小到大找中间  还要注意奇偶性\n\\(M\\)\n(1)直接法：n为奇数，\\(M=X_{\\frac{n+1}{x}}\\)  n为偶数，\\(M=\\frac{1}{2} (X_{\\frac{n}{2}}+X_{\\frac{n}{2}+1})\\)  (2)加权法：百分位数法\\(P_{X}=L_{X}+\\frac{i}{f_{x}}(nX\\%-\\sum{f_{i}})\\)\n任何资料\n\n\n众数\n出现次数最多\n\\\n(1)直接法：一组数据中出现次数最多的数值  加权法：f最多的组段的组中值\\(X\\)\n任何数据\n\n\n\n\n\n\n\n对称分布：算术均数\\(\\approx\\)中位数\n右偏态：算术均数\\(&gt;\\)中位数\n左偏态：算术均数\\(&lt;\\)中位数",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#离散数据的描述指标",
    "href": "Learn/Basic/02-descriptive-statistics.html#离散数据的描述指标",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "指标\n本质→定义\n表示方式\n计算公式\n适用条件\n\n\n\n\n极差\n\\(X_{Max}-X_{Min}\\)\n\\(R\\)\n\n\n\n\n四分位间距/范围\n位置指标\n\\(IQR\\)\n\\(IQR=P_{75}-P_{25}\\)\n任何资料\n\n\n方差\n离均差平方和求平均\n样本：\\(s^{2}\\)  总体：\\(\\sigma^{2}\\)\n\\(s^{2}=\\frac{\\sum(x_{i}-x)^2}{(n-1)}\\)\n对称分布，尤其是正态分布;不含极端值\n\n\n标准差\n方差开根号\n样本：\\(s\\)  总体：\\(\\sigma\\)\n\\(s=\\sqrt{\\frac{\\sum(x_{i}-x)^2}{(n-1)}}\\)\n同上\n\n\n变异系数\n测量数据变异程度的相对统计量\n\\(CV\\)\n\\(CV=\\frac{s}{\\bar x}×100%\\)\n(1)单位相同：但均数相差悬殊；(2)单位不同\n\n\n\n\n\n\n\n直接法 \\[s=\\sqrt{\\frac{\\sum\\limits_{i=1}^nx_i^2-\\frac{\\left(\\sum\\limits_{i=1}^nx_i\\right)^2}{n}}{n-1}}\\]\n加权法：与讨算均数的方法类似，对频数表资料采用加权法，讨算公式为\n\n\\[s=\\sqrt{\\frac{\\sum\\limits_{k=1}^gf_kx_{mk}^2-\\left(\\sum\\limits_{k=1}^gf_kx_{mk}\\right)^2 \\left(\\sum\\limits_{k=1}^gf_k\\right)}{\\sum\\limits_{k=1}^gf_k-1}}\\]",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#分类资料的描述指标",
    "href": "Learn/Basic/02-descriptive-statistics.html#分类资料的描述指标",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "指标\n相对比\n构成比\n频率型指标\n强度性指标\n\n\n\n\n定义\n两个有关联的指标之比\n某一部分与总体之比\n某时期内累计出现的频率\n单位时间内某件事发生的频率\n\n\n计算公式\n\\(\\frac{A指标}{B指标}\\)\n\\(\\frac{某一事物总体中某一部分}{某一事物所有组成部分的总体}×100\\%\\)\n\\(\\frac{同时期实际发生某现象的观察单位数}{某时期可能发生某现象的观察单位总数}×K\\)\n\\(\\frac{发生某件事的观察单位数}{\\sum(观察单位×观察时间)}×K\\)\n\n\n量纲\n可有可无\n一般无量纲\n无\n有\n\n\n取值\n没有限制\n[0,1]\n[0,1]\n可大于1\n\n\n举例\nRR,变异系数CV\n死因构成比\n病死率，累计发病率\n发病率，发病密度\n\n\n\n\n\n\n\n绝对量指标\n\n累计增长量\n逐年增长量\n\n定基类指标\n\n定基发展速度\n定基增长速度\n\n环比类指标\n\n环比发展速度\n环比增长速度\n\n平均类指标\n\n平均发展速度\n平均增长速度\n\n\n\n\n\n\n频率型指标的解释要紧扣总体和属性\n计算相对数分母应该有足够的观察单位数 -如果观察例数太少，则相对数波动较大\n\n若因实际因素，观察例数确实过少，建议直接采用绝对数\n\n正确计算合计率：分子分母分别相加，再求合计率\n不能用结构相对数代替强度相对数，不能混淆频率型指标和强度型指标，不能以比代率\n注意资料的可比性\n不能仅用样本率比较，因为样本和总体之间存在抽样误差，需要进行假设检验推断总体的情况",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#率的标准化",
    "href": "Learn/Basic/02-descriptive-statistics.html#率的标准化",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "指标\n参照人数\n目标人群\n过程\n\n\n\n\n直接标化法\n人口构成\n率\n各年龄组期望死亡数→期望死亡合计数→直接标化率\n\n\n简介标化法\n率\n人口构成\n各年龄组期望死亡数→期望死亡合计数→变化死亡比→间接标化率\n\n\n\nnotice：\n\n变化标化比\\(SMR=\\frac{实际死亡人数}{期望死亡人数}\\)\n直接标准化选择的标准是：各年龄组标准人口构成比或各年龄组标准人口数",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#常见统计图",
    "href": "Learn/Basic/02-descriptive-statistics.html#常见统计图",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "连续型变量\n\n计量资料\n定量资料\n\n离散型变量\n\n不具有分类性质的资料\n离散型定量资料\n\n分类资料\n\n有序分类资料\n等级资料\n半定量资料\n无序分类资料\n名义变量\n\n\nnotice：一般来说，统计图的选择，是综合考量了变量取值特点+研究目的。\n\n\n\n\n\n\n\n\n\n\n\n统计图\n资料类型\n分析目的\n\n\n\n\n圆图和百分条图\n构成比资料\n用圆的扇形面积或直条各段的长度表示事物各组成部分的构成情况\n\n\n直条图\n相互独立资料\n用直条长短表示相互独立的各指标的数值大小，一般用于比较不同组别的指标大小\n\n\n直方图\n连续型变量的频数分布\n用矩阵面积表示各组段的频数（频率）\n\n\n箱式图\n连续型资料\n描述数据的分布特征（包括中位数、四分位范围、最大值和最小值）\n\n\n普通线图\n连续型资料\n用线段的升降表示某事物在时间上的变化趋势、或某一现象随着另一现象变化的情况\n\n\n半对数线图\n连续型资料\n用线段的升降表示事物的相对变化速度\n\n\n散点图\n双变量连续型资料\n表示两种事物变化的相关性和趋势\n\n\n\n\n\n\n\n\n\nChoice of Statistical Charts",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#常见的概率抽样",
    "href": "Learn/Basic/02-descriptive-statistics.html#常见的概率抽样",
    "title": "不同资料的统计描述",
    "section": "2.1 常见的概率抽样",
    "text": "2.1 常见的概率抽样\n\n\n\n\n\n\n\n\n\n\n\n类别\n简单随机抽样\n系统抽样\n整群抽样\n分层抽样\n多阶段抽样\n\n\n\n\n概念\n将全部的观察单位编号，形成抽样框，在抽样框中随机抽取部分观察单位组成样本\n先将总体的观察单位按照某一顺序分成n个部分，再从第一部分随机抽取第k号观察单位，依次用相等间隔，从每一部分各抽取一个观察单位组成样本\n是以“群”为基本单位的抽样方法，先将总体分成若干群，从中随机抽取一些群，被抽中群内的全部个体组成调查的样本\n先将总体中全部个体按某种特征分成若干“层”，再从每一层内随机抽取一定数量的个体组成样本\n将整个抽样过程分成若干阶段进行，在初级抽样单位中抽取二级抽样单位，又在二级抽样单位中抽取三级抽样单位\n\n\n优点\n简单直观；均数（率）及其标准误计算简便\n易于理解、简便易行；可得到按比例分配的样本；样本在总体中的分布均匀\n便于组织调查；节约成本；容易控制调查质量\n抽样误差相对较小；可对不同层采用不同的抽样方法；可对不同层进行独立分析\n充分利用各种抽样方法的优势，克服各自的不足，并能节省人力、物力\n\n\n缺点\n观察单位较多，编号在实际工作中难以实现；当总体变异大时，抽样误差较分层抽样误差大\n观察单位按顺序有周期趋势或递增（减）时易产生偏差\n样本例数一定时，抽样误差大于简单随机抽样（因样本为广泛散布于总体中\n若分层变量选择不当，层内变异较大，层间变异较小，则分层抽样失去意义\n在抽样之前要掌握各级调查单位的人口资料及特点\n\n\n适用范围\n是其他抽样方法的基础，主要用于总体不太大的情形\n主要用于按抽样顺序个体随机分布的情形\n主要用于群间差异较小的情形\n主要用于层间差异较大的情形\n大型流行病学调查\n\n\n\n误差大小： 整群抽样&gt;简单随机抽样&gt;系统抽样&gt;分层抽样\n样本量大小：整群抽样&gt;简单随机抽样&gt;系统抽样&gt;分层抽样\n概率抽样：是指每个个体被抽样抽中的概率是非零的、已知的或可计算的。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#常见的非概率抽样",
    "href": "Learn/Basic/02-descriptive-statistics.html#常见的非概率抽样",
    "title": "不同资料的统计描述",
    "section": "2.2 常见的非概率抽样",
    "text": "2.2 常见的非概率抽样\n\n特点\n\n不需要考虑等概率原则\n依赖研究人员的经验和专业知识\n简便易行、节约资源\n结果的稳定性容易受主观影响\n\n\n\n\n\n\n\n\n\n类别\n概念\n\n\n\n\n偶遇抽样\n又称便利抽样，指研究者根据实际情况而采用最便利的方法来选取样本，可以抽取偶然遇到的人，或选择那些距离最近的、最容易找到的人作为调查对象\n\n\n目的抽样\n又称判断抽样，指研究者根据研究目标和对情况的主观判断来选择和确定调查对象的方法，是“有目的”地去选择对总体具有代表性的样本\n\n\n滚雪球抽样\n又称链式抽样或网络抽样，指当无法了解总体情形时，可以从能找到的少数个体入手，对他们进行调查，并请他们介绍其他符合条件的人，扩大调查面，如此重复下去直到达到所需的样本量\n\n\n定额抽样\n又称配额抽样，是按照总体的某种特征（年龄、性别、社会阶层等）进行分层（组），然后在每一层（组）中按照事先规定的比例或数量（即定额）用便利抽样或目的抽样的方法选取样本\n\n\n空间抽样\n指对具有空间关联性的各种调查对象及资源进行抽样的一种方法\n\n\n\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/00-basic.html",
    "href": "Learn/Basic/00-basic.html",
    "title": "医学统计学基础",
    "section": "",
    "text": "本章主要介绍基本的医学统计学内容，主要参照内容为：人卫第八版《卫生统计学》、科社第二版《医学统计学（基础版）》。\n\n章节主要分布如下：\n\n绪论\n\n医学统计学\n统计学的基本概念\n统计学在医学科研中的基本步骤\n统计学与相关学科的关系\n\n描述性统计\n随机事件与概率\n离散型随机变量\n连续型随机变量\n参数估计\n一个正态总体参数的假设检验\n两个正态总体参数的假设检验\n单因素方差分析\n多因素方差分析\n\\(\\chi^2\\)检验\n基于秩次的非参数检验\n一元线性回归\n相关分析\n\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-intro.html",
    "href": "Guide/SAS/SAS-intro.html",
    "title": "SAS",
    "section": "",
    "text": "SAS introduction\nSAS的历史很长，很强大，但是现代化做的一般（交互界面）。\n使用场景也是有限的，至少一般情况下用不上这么高级的工具。\n但是在某些领域又是极其重要的，像银行和药企，他们要追求足够的稳定和严谨，那么多年不曾有重大改变且一向以稳定著称的 SAS 自然可以很好的满足这一需求。\n\nSAS 对学术研究的支持是比较不友好的。正版太贵，除非学校有提供，个人基本不可能使用正版，这里下载破解版，搞SID(SAS的授权证书)需要时间成本，还容易有安装问题，没错你可以选择使用SAS的教育版，不过谁用谁知道。\n安装比较麻烦，尤其是在Linux上，我曾用两周的时间折腾在 Linux 上安装一个 SAS ，最后以失败告终，且在互联网上找不到解决的方案，AI也束手无策。\nSAS不开源，意味着你看到某些论文，里面使用一些比较新的统计分析方法，SAS不大可能有现成代码可以使用，而 Python 和 R 则大几率有现成的包可以调用，这里也会节省不少时间。\nSAS的强大一方面是性能稳定，可以处理几十上百GB的数据而不容易崩溃，但是医学数据一般容量比较小，并不是非得SAS才能跑的动。\nSAS相比其他编程语言来说是独树一帜(奇葩)的存在（proc和data步独步天下），从语法上面来说并没有什么和它相接近的语言，相反 R 和 Python 则会和一般的编程语言例如 Java, C 等有一些类似的地方，对以后万一还需要学习其他语言或者学习以后新诞生的编程软件诞有一定帮助。\nSAS 的支持有限，互联网上关于 SAS 的使用信息较少，一般都在出版的书中有可复现的内容，也没有像 Python 和 R 等活跃的社区可以提供较多的互动和支持，编程遇上问题不容易找到答案。\n\n用肯定能用，但是在使用中占多大的比重，就需要权衡一下，在 AI 时代，不一定要全部掌握，看得懂，知道怎么做，应该也可以了，当然如果要深入，那就另说。\n每个人的资源和时间都是有限的，用最少的资源和时间做最多的事才是最重要的。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "SAS"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html",
    "href": "Guide/SAS/2025-02-16-CLHLS.html",
    "title": "使用SAS处理CLHLS数据",
    "section": "",
    "text": "“中国老年健康影响因素跟踪调查”（简称“中国老年健康调查”，英文缩写CLHLS），以下简称CLHLS数据，是由北京大学健康老龄与发展研究中心/国家发展研究院组织的老年人追踪调查，调查范围覆盖全国23个省市自治区，调查对象为65岁及以上老年人和35-64岁成年子女，调查问卷分为存活被访者问卷和死亡老人家属问卷两种。存活被访者问卷的调查内容包括老人及家庭基本状况、社会经济背景及家庭结构、经济来源和经济状况、健康和生活质量自评、认知功能、性格心理特征、日常活动能力、生活方式、生活照料、疾病治疗和医疗费承担；死亡老人家属问卷的调查内容包括老人死亡时间、死因等内容。该调查项目在1998年进行基线调查后分别于2000 年、2002年、2005年、2008-2009年、2011-2012年、2014年和2017-2018年进行了跟踪调查，最近的一次跟踪调查(2017-2018年)共访问15,874 名65+岁老年人,收集了2014-2018年期间死亡的2,226位老年人的信息。“中国老年健康调查”累计入户访问11.3万人次，其中最需照料的80岁及以上高龄老人占总样本67.4%，其余为较低龄老人和中年对照组；同时访问2.89万位65+岁已死亡被访老人的直接家庭成员，收集了老人死亡前健康状况、生活质量与医疗和照料需求成本等详细数据。CLHLS数据共有15874例样本，761个变量，由于本文主要研究老年人的自评和客观健康水平以及代际支持模式对健康水平的影响，因此需要对样本和变量进行筛选，保留符合要求的样本和变量进行后续的分析。DVN/WBO7LK_2020",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html#因变量的选取",
    "href": "Guide/SAS/2025-02-16-CLHLS.html#因变量的选取",
    "title": "使用SAS处理CLHLS数据",
    "section": "2.1 因变量的选取",
    "text": "2.1 因变量的选取\n老年人自评健康状况（SHEALTH）：对应变量 b12 （self-reported health），为有序变量（Ordinal），可用于衡量老年人对自身健康的主观评价，其赋值为：很好=1，好=2，一般=3，不好=4，很不好=5。\n生活自理能力（ADL）：对应变量 e1 （bathing，洗澡）、e2 （dressing，穿衣）、e3 （toileting，如厕）、e4 （indoor transferring，室内移动）、e5 （continence，大小便控制）、e6 （feeding，进食），均为有序变量（Ordinal）。通过这些变量的得分计算可体现老年人的生活自理能力。\n变量 e1b 、e2b 、e3b 、e4b 、e5b 、e6b 分别记录了在需要他人帮助情况下，这些活动的帮助持续天数，可作为辅助信息进一步分析生活自理能力情况。其变量赋值为：不需要帮扶=1，需要一个帮扶=2，需要多个帮扶=3，将其合并为ADL变量，相加值越低则表示生活自理能力越好，反之越差。\n工具型生活自理能力（IADL）：对应变量 e7 （able to go outside to visit neighbors，能外出拜访邻居）、e8 （able to go shopping by yourself，能自己去购物）、e9 （able to make food by yourself，能自己做饭）、e10 （able to wash clothes，能洗衣服）、e11 （able to walk one kilometer，能步行一公里）、e12 （able to carry 5kg weight，能搬运 5 公斤重物）、e13 （able to crouch and stand three times，能蹲下并站立三次）、e14 （able to take public transportation，能乘坐公共交通工具），均为有序变量（Ordinal）。这些变量可综合反映老年人在更复杂日常生活任务上的能力。其变量赋值为：可以=1，些微困难=2，不能=3，将其合并为IADL变量，相加值越低则表示工具型生活自理能力越好，反之越差。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html#自变量的选取",
    "href": "Guide/SAS/2025-02-16-CLHLS.html#自变量的选取",
    "title": "使用SAS处理CLHLS数据",
    "section": "2.2 自变量的选取",
    "text": "2.2 自变量的选取\n经济支持：新设变量为 economic-support，对应的原始变量 f12a （how much did you receive from your son (s) or daughter (s)-in-law last year，去年从儿子或儿媳处收到多少钱）、f12b （how much did you receive from your daughter (s) or son (s)-in-law last year，去年从女儿或女婿处收到多少钱）、f12c （how much did you receive from your grandchild (ren) last year，去年从孙辈处收到多少钱），均为尺度变量（Scale），其中给与物质支持=99998，不知道=88888，缺失=99999，通过统计99998的个数来估计经济支持的程度，如果没有99998则赋值为0,。通过这些变量可衡量子女及孙辈在物质上给予老年人的经济支持。\n照料支持：新设变量为：residence，对应原始变量a51（co-residence of interviewee，受访者居住情况），有人同住=1，独居=2，住在机构=3；新设变量为living，对应变量 a52（how many people are living with you，有多少人居住在一起），数值型变量；新设变量 visit-fren，对应细分变量为f103a5 （frequent visits of the 1st child，第一个孩子的探访频率）、f103b5 （frequent visits of the 2nd child，第二个孩子的探访频率）、f103c5 （frequent visits of the 3rd child，第三个孩子的探访频率）等（一直到 f103k5 等关于各个孩子的探访频率变量），均为有序变量（Ordinal），有探访=1，没有=2，任一一个孩子=1即可认为有探访，同时给新变量赋值为1，如果没有则给新变量赋值为0。\n设置care-support变量，如果residence=1或visit-fren=1则给care-support赋值为1，否则赋值为0。\n用这些变量以老年人与子女相处和见面的频率来间接反映子女对老人的照料支持情况。\n情感支持（emotion support）：对应细分变量 f103a6 （contact with the 1st child，与第一个孩子的联系情况）、f103b6 （contact with the 2nd child，与第二个孩子的联系情况）、f103c6 （contact with the 3rd child，与第三个孩子的联系情况）等（一直到 f103k6 等关于各个孩子的联系情况变量），均为名义变量（Nominal）。根据这些变量所反映的与子女联系的情况来体现子女对老人的情感关怀。有联系=1，没联系=2，任一一个为1即可说明有孩子联系。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html#控制变量的选取",
    "href": "Guide/SAS/2025-02-16-CLHLS.html#控制变量的选取",
    "title": "使用SAS处理CLHLS数据",
    "section": "2.3 控制变量的选取",
    "text": "2.3 控制变量的选取\n年龄：问卷中的 trueage “真实年龄”，用于获取老年人的实际年龄。\n性别：问卷中 a1 “性别”，男性赋值为 1，女性赋值为 2。\n受教育程度：问卷中的 f1 “受教育年限”，数值型变量，但是其中如果赋值为88或99则表示“不知道”或“缺失”。\n退休前的工作类型：问卷中 f2 “60 岁之前的主要职业”，名义变量，专业技术人员=0，政府、机构或管理人员=1，商业、服务或工业工人=2，自雇人士=3，农、林、牧或渔业工人=4，家政工人=5，军事人员=6，从未工作过=7，其他=8。\n婚姻状况：问卷中 f41 “您目前的婚姻状态是？”，将 “已婚或与伴侣同居” 赋值为 1，其他赋值为 0。\n户口类型：问卷中的 hukou “hukou type of the elderly being visited，被访老人的户口类型”， “城镇” 赋值为 1，“农村” 赋值为 2。\n社保和养老保险：问卷中的 nf64a f64b f64c（do you have any social security and social insurance now? 你是否有任何社保和社会保险？），有任何一种社保或养老保险则赋值为 1，没有赋值为 0；。\n医疗保险：问卷中f64d f64e f64f f64h(是否有医疗保险)，有任何一种医疗保险则赋值为 1，没有赋值为 0。\n子女年龄：通过对问卷中的 f103a4 f103b4 到 f103m4 来检查该老人的子女的年龄，如果最后一个还活着的孩子年龄&gt;60，赋值为1，则表示该老人的子女也是老年状态，否则为0。\n慢性病：问卷中 g15a1 g15b1 一直到 g15x1 表示是否患有某种慢性病，有任何一种慢性病赋值为 1，否则为 0。\n抽烟：问卷中 g151 “你24小时内抽烟吗？”，是赋值为 1，否赋值为 2。\n喝酒：问卷中 g161 “你24小时内饮酒吗？”，是赋值为 1，否赋值为 2。\n体育锻炼：问卷中d91 d92 分别表示“目前是否锻炼？”和“过去是否锻炼过？”，是赋值为 1，否赋值为 2。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html#纳入标准",
    "href": "Guide/SAS/2025-02-16-CLHLS.html#纳入标准",
    "title": "使用SAS处理CLHLS数据",
    "section": "3.1 纳入标准",
    "text": "3.1 纳入标准\n年龄要求：年龄大于 60 岁，通过实际年龄 &gt; 60进行筛选。60 岁以上人群处于老年阶段，其健康、社会角色和代际关系有老年群体特征，符合本研究对老年人的研究范围。\n生育情况：生育过子女，通过反映生育子女数量的变量（如 f10 生育子女数），要求f10 &gt; 0。代际支持模式研究需存在代际关系，生育子女是形成代际关系的前提。\n子女存活情况：所生育子女中至少有一个存活。可通过检查如 f103a3 - f103m3 等表示孩子存活情况的变量，只要有一个值为表示存活的标识即可纳入（原始数据的变量赋值为存活=1，去世=2）。有在世子女才能开展代际支持研究。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html#排除标准",
    "href": "Guide/SAS/2025-02-16-CLHLS.html#排除标准",
    "title": "使用SAS处理CLHLS数据",
    "section": "3.2 排除标准",
    "text": "3.2 排除标准\n关键变量缺失值：排除关键变量存在缺失值和无效值的样本。关键变量涵盖上述控制变量及后续分析的因变量、自变量等。缺失值或无效值会影响数据质量和分析结果准确性。\n不合理生育数量：生育子女数量超过合理上限（结合研究背景和数据确定，上限为 7）或者为缺失值，则需要排除（f10&gt;7 or f10 = null）。不合理生育数量可能是数据录入错误或特殊情况，干扰研究结果。\n通过多次尝试，以下是用于筛选的SAS代码：\n\n#| eval: false\n/* 导入必要的库 */\nproc import \n    datafile = 'C:\\Users\\asus\\Desktop\\test\\CLHLS\\Analysis-0214\\clhls_2018_15874.sav'  \n    /* 请将这里替换为你的SAV文件的实际路径 */\n    out = raw_data \n    dbms = sav \n    replace;\nrun;\n\n/* 筛选合适的变量并排序 */\ndata selected_data;\n    set raw_data;\n    \n    /* 因变量 */\n    /* 老年人自评健康状况（SHEALTH） */\n    SHEALTH = b12;\n    \n    /* 生活自理能力（ADL） */\n    array adl_vars(*) e1 e2 e3 e4 e5 e6;\n    array adl_help_days(*) e1b e2b e3b e4b e5b e6b;\n    ADL = 0;\n    do i = 1 to dim(adl_vars);\n        if adl_help_days[i] = 1 then ADL = ADL + adl_vars[i];\n        else if adl_help_days[i] = 2 then ADL = ADL + adl_vars[i] * 2;\n        else if adl_help_days[i] = 3 then ADL = ADL + adl_vars[i] * 3;\n    end;\n    \n    /* 新增：生活自理障碍二分类变量 ADL2 */\n    ADL2 = 0;\n    do i = 1 to dim(adl_vars);\n        if adl_vars[i] &gt; 1 then do;\n            ADL2 = 1;\n            leave;\n        end;\n    end;\n    \n    /* 工具型生活自理能力（IADL） */\n    array iadl_vars(*) e7 e8 e9 e10 e11 e12 e13 e14;\n    IADL = 0;\n    do i = 1 to dim(iadl_vars);\n        IADL = IADL + iadl_vars[i];\n    end;\n    \n    /* 新增：工具型生活自理障碍二分类变量 IADL2 */\n    IADL2 = 0;\n    do i = 1 to dim(iadl_vars);\n        if iadl_vars[i] &gt; 1 then do;\n            IADL2 = 1;\n            leave;\n        end;\n    end;\n    \n    /* 自变量 */\n    /* 经济支持 */\n    array economic_vars(*) f12a f12b f12c;\n    economic_support = 0;\n    do i = 1 to dim(economic_vars);\n        if economic_vars[i] = 99998 then\n            economic_support = economic_support + 10000;\n        else if economic_vars[i] in (88888, 99999) then\n            continue; /* 跳过无效值或缺失值 */\n        else\n            economic_support = economic_support + economic_vars[i];\n    end;\n    \n    /* 照料支持 */\n    residence = a51;\n    living = a52;\n    array visit_freq(*) f103a5 f103b5 f103c5 f103d5 f103e5 f103f5 f103g5 f103h5 f103i5 f103j5 f103k5;\n    visit_fren = 0;\n    do i = 1 to dim(visit_freq);\n        if visit_freq[i] = 1 then do;\n            visit_fren = 1;\n            leave;\n        end;\n    end;\n    \n    /* 情感支持 */\n    array contact_vars(*) f103a6 f103b6 f103c6 f103d6 f103e6 f103f6 f103g6 f103h6 f103i6 f103j6 f103k6;\n    emotion_support = 2; /* 先假设没联系 */\n    do i = 1 to dim(contact_vars);\n        if contact_vars[i] = 1 then do;\n            emotion_support = 1;\n            leave;\n        end;\n    end;\n    \n    /* 控制变量 */\n    /* 年龄 */\n    age = trueage;\n    \n    /* 性别 */\n    gender = a1;\n    \n    /* 受教育程度 */\n    education = f1;\n    \n    /* 退休前的工作类型 */\n    job_type = f2;\n    \n    /* 婚姻状况 */\n    if f41 = 1 then marriage_status = 1; /* 假设 1 代表已婚或与伴侣同居 */\n    else marriage_status = 0;\n    \n    /* 户口类型 */\n    hukou_type = hukou;\n    \n    /* 社保和养老保险 */\n    if nf64a = 0 or f64b = 1 or f64c = 1 or f64i = 1 then social_insurance = 1;\n    else social_insurance = 0;\n    \n    /* 医疗保险 */\n    if f64d = 1 or f64e = 1 or f64g = 1 or f64h = 1 then medical_insurance = 1;\n    else medical_insurance = 0;\n    \n    /* 慢性病 */\n    array chronic_vars(*) g15a1 g15b1 g15c1 g15d1 g15e1 g15f1 g15g1 g15h1 g15i1 g15j1 g15k1 g15l1 g15m1 g15n1 g15o1 g15p1 g15q1 g15r1 g15s1 g15t1 g15u1 g15v1 g15w1 g15x1;\n    chronic_disease = 0;\n    do i = 1 to dim(chronic_vars);\n        if chronic_vars[i] = 1 then chronic_disease = 1;\n        if chronic_disease = 1 then leave;\n    end;\n    \n    /* 抽烟 */\n    smoking = g151;\n    \n    /* 喝酒 */\n    drinking = g161;\n    \n    /* 体育锻炼 */\n    if d91 = 1 or d92 = 1 then exercise = 1;\n    else exercise = 2;\n\n    /* 子女年龄状态 */\n    array child_ages(*) f103a4 f103b4 f103c4 f103d4 f103e4 f103f4 f103g4 f103h4 f103i4 f103j4 f103k4 f103l4 f103m4;\n    array child_alive(*) f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\n    last_alive_child_age = .;\n    do i = dim(child_ages) to 1 by -1;\n        if child_alive[i] = 1 then do;\n            last_alive_child_age = child_ages[i];\n            leave;\n        end;\n    end;\n    if last_alive_child_age &gt; 60 then child_elderly_status = 1;\n    else child_elderly_status = 0;\n\n    /* 生成 care - support 变量 */\n    if residence = 1 or visit_fren = 1 then care_support = 1;\n    else care_support = 0;\n    \n    /* 选择需要的变量 */\n    keep SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support \n         f10 age gender education job_type marriage_status hukou_type \n         social_insurance medical_insurance chronic_disease smoking drinking exercise\n         child_elderly_status care_support f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\nrun;\n\n/* 将筛选后的保存为 XLSX 格式文件 */\n/*\nproc export data=selected_data\n    outfile='C:\\Users\\asus\\Desktop\\test\\CLHLS\\CLHLS数据\\CLHLS数据\\clhls_2018_sort0220.xlsx'\n    dbms=xlsx\n    replace;\nrun;\n*/\n\n/*检查f10 生育子女数的分布情况*/\nproc freq data=selected_data;\n    tables f10;\nrun;\n\n/*检查子女存活状态*/\nproc freq data=selected_data;\n    tables f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\nrun;\n\n/* 样本筛选 */\ndata temp_data;\n    set selected_data;\n\n    /* 纳入标准 */\n    /* 年龄要求 */\n    /*age_include = (age &gt; 60);*/\n    /* 生育情况 */\n    fertility_include = (f10 &gt; 0);\n\n    /* 子女存活情况 */\n    array child_alive(*) f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\n    child_alive_include = 0;\n    do i = 1 to dim(child_alive);\n        if child_alive[i] = 1 then do;\n            child_alive_include = 1;\n            leave;\n        end;\n    end;\n    meet_include = fertility_include and child_alive_include;\n\n    /* 排除标准 */ \n    /* 关键变量缺失值检查 */\n    /*array key_vars(*) SHEALTH ADL IADL;*/\n    array key_vars(*) SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support \n                      age gender education job_type marriage_status hukou_type \n                      social_insurance medical_insurance chronic_disease smoking drinking exercise\n                      child_elderly_status care_support;\n    has_missing = 0;\n    do i = 1 to dim(key_vars);\n        if missing(key_vars[i]) then do;\n            has_missing = 1;\n            leave;\n        end;\n    end;\n    /* 不合理生育数量检查 */\n    unreasonable_fertility = (f10 &gt; 7 or missing(f10));\n    meet_exclude = has_missing or unreasonable_fertility;\n\n    /* 筛选符合条件的样本 */\n    if meet_include and not meet_exclude;\n\n\n    /* 移除临时判断变量 */\n    drop fertility_include child_alive_include has_missing unreasonable_fertility meet_include meet_exclude;\n    /*移除部分原始变量*/\n    drop f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\nrun;\n\n/*打印所有变量的频数分布表，检查是否含有异常值*/\n/*\nproc freq data=final_data;\n    tables _all_;\nrun;\n*/\n\n/*还需要删除含有的样本，即某些变量中赋值为9（not applicable）和88（don't know）的样本*/\n/*具体需要剔除变量满足这些条件的样本：SHEALTH&gt;8,ADL&gt;18,IADL&gt;24,residence&gt;3,eudcation&gt;22,smoking&gt;2,drinking&gt;2*/\n/* 删除满足特定条件的样本 */\ndata final_data;\n    set temp_data;\n    if (SHEALTH &lt;= 8) and (ADL &lt;= 18) and (IADL &lt;= 24) and (residence &lt;= 3) and (age&gt;=60) and (education &lt;= 22) and (smoking &lt;= 2) and (drinking &lt;= 2);\nrun;\n\nproc freq data=final_data;\n    tables _all_;\nrun;\n\n/* 保存筛选后的数据为 XLSX 格式 */\nproc export\n    data = final_data\n    outfile = 'C:\\Users\\asus\\Desktop\\test\\CLHLS\\Analysis-0214\\final_data.xlsx' /* 请替换为实际保存路径 */\n    dbms = xlsx\n    replace;\nrun;\n\n/* 对 age 变量进行分组 */\ndata final_data_grouped;\n    set final_data;\n    if age &lt; 70 then age_group = '60 - 69';\n    else if age &lt; 80 then age_group = '70 - 79';\n    else if age &lt; 90 then age_group = '80 - 89';\n    else age_group = '90+';\nrun;\n\n/* 探查每个变量的基本统计信息，查看是否有异常值 */\nproc means data=final_data n nmiss min max mean std;\n    var SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support\n        age gender education job_type marriage_status hukou_type\n        social_insurance medical_insurance chronic_disease smoking drinking exercise\n        child_elderly_status care_support;\nrun;\n\n/* 查看字符型变量的唯一值，看是否有异常字符 */\nproc freq data=final_data_grouped;\n    tables age_group gender education job_type marriage_status hukou_type;\nrun;\n\n/* 打印因变量、自变量和控制变量的频数分布表并汇总 */\nproc freq data=final_data_grouped noprint;\n    tables SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support\n           age_group gender education job_type marriage_status hukou_type\n           social_insurance medical_insurance chronic_disease smoking drinking exercise\n           child_elderly_status care_support / out=freq_summary;\nrun;\n\n/* 导出频数汇总表到 Excel */\nproc export\n    data = freq_summary\n    outfile = 'C:\\Users\\asus\\Desktop\\test\\CLHLS\\Analysis-0214\\frequency_summary.xlsx' /* 请替换为实际保存路径 */\n    dbms = xlsx\n    replace;\nrun;\n\n\u00145                                                             SAS 系统                                  23:00 Saturday, May 24, 2025\n\n24         ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n24       ! ods graphics on / outputfmt=png;\nNOTE: 正在写入 HTML5(SASPY_INTERNAL) Body（主体）文件: _TOMODS1\n25         \n26         #| eval: false\n           _\n           180\nERROR 180-322: 语句无效或未按正确顺序使用。\n\n27         /* 导入必要的库 */\n28         proc import\n29             datafile = 'C:\\Users\\asus\\Desktop\\test\\CLHLS\\Analysis-0214\\clhls_2018_15874.sav'\n30             /* 请将这里替换为你的SAV文件的实际路径 */\n31             out = raw_data\n32             dbms = sav\n33             replace;\n\n34         run;\n35         \n36         /* 筛选合适的变量并排序 */\n37         data selected_data;\n38             set raw_data;\nERROR: 文件“WORK.RAW_DATA.DATA”不存在。\n39         \n40             /* 因变量 */\n41             /* 老年人自评健康状况（SHEALTH） */\n42             SHEALTH = b12;\n43         \n44             /* 生活自理能力（ADL） */\n45             array adl_vars(*) e1 e2 e3 e4 e5 e6;\n46             array adl_help_days(*) e1b e2b e3b e4b e5b e6b;\n47             ADL = 0;\n48             do i = 1 to dim(adl_vars);\n49                 if adl_help_days[i] = 1 then ADL = ADL + adl_vars[i];\n50                 else if adl_help_days[i] = 2 then ADL = ADL + adl_vars[i] * 2;\n51                 else if adl_help_days[i] = 3 then ADL = ADL + adl_vars[i] * 3;\n52             end;\n53         \n54             /* 新增：生活自理障碍二分类变量 ADL2 */\n55             ADL2 = 0;\n56             do i = 1 to dim(adl_vars);\n57                 if adl_vars[i] &gt; 1 then do;\n58                     ADL2 = 1;\n59                     leave;\n60                 end;\n61             end;\n62         \n63             /* 工具型生活自理能力（IADL） */\n64             array iadl_vars(*) e7 e8 e9 e10 e11 e12 e13 e14;\n65             IADL = 0;\n66             do i = 1 to dim(iadl_vars);\n67                 IADL = IADL + iadl_vars[i];\n68             end;\n69         \n70             /* 新增：工具型生活自理障碍二分类变量 IADL2 */\n71             IADL2 = 0;\n72             do i = 1 to dim(iadl_vars);\n73                 if iadl_vars[i] &gt; 1 then do;\n74                     IADL2 = 1;\n75                     leave;\n76                 end;\n77             end;\n78         \n79             /* 自变量 */\n80             /* 经济支持 */\n81             array economic_vars(*) f12a f12b f12c;\n82             economic_support = 0;\n83             do i = 1 to dim(economic_vars);\n84                 if economic_vars[i] = 99998 then\n85                     economic_support = economic_support + 10000;\n86                 else if economic_vars[i] in (88888, 99999) then\n87                     continue; /* 跳过无效值或缺失值 */\n88                 else\n89                     economic_support = economic_support + economic_vars[i];\n90             end;\n91         \n92             /* 照料支持 */\n93             residence = a51;\n94             living = a52;\n95             array visit_freq(*) f103a5 f103b5 f103c5 f103d5 f103e5 f103f5 f103g5 f103h5 f103i5 f103j5 f103k5;\n96             visit_fren = 0;\n97             do i = 1 to dim(visit_freq);\n98                 if visit_freq[i] = 1 then do;\n99                     visit_fren = 1;\n100                    leave;\n101                end;\n102            end;\n103        \n104            /* 情感支持 */\n105            array contact_vars(*) f103a6 f103b6 f103c6 f103d6 f103e6 f103f6 f103g6 f103h6 f103i6 f103j6 f103k6;\n106            emotion_support = 2; /* 先假设没联系 */\n107            do i = 1 to dim(contact_vars);\n108                if contact_vars[i] = 1 then do;\n109                    emotion_support = 1;\n110                    leave;\n111                end;\n112            end;\n113        \n114            /* 控制变量 */\n115            /* 年龄 */\n116            age = trueage;\n117        \n118            /* 性别 */\n119            gender = a1;\n120        \n121            /* 受教育程度 */\n122            education = f1;\n123        \n124            /* 退休前的工作类型 */\n125            job_type = f2;\n126        \n127            /* 婚姻状况 */\n128            if f41 = 1 then marriage_status = 1; /* 假设 1 代表已婚或与伴侣同居 */\n129            else marriage_status = 0;\n130        \n131            /* 户口类型 */\n132            hukou_type = hukou;\n133        \n134            /* 社保和养老保险 */\n135            if nf64a = 0 or f64b = 1 or f64c = 1 or f64i = 1 then social_insurance = 1;\n136            else social_insurance = 0;\n137        \n138            /* 医疗保险 */\n139            if f64d = 1 or f64e = 1 or f64g = 1 or f64h = 1 then medical_insurance = 1;\n140            else medical_insurance = 0;\n141        \n142            /* 慢性病 */\n143            array chronic_vars(*) g15a1 g15b1 g15c1 g15d1 g15e1 g15f1 g15g1 g15h1 g15i1 g15j1 g15k1 g15l1 g15m1 g15n1 g15o1 g15p1\n143      !  g15q1 g15r1 g15s1 g15t1 g15u1 g15v1 g15w1 g15x1;\n144            chronic_disease = 0;\n145            do i = 1 to dim(chronic_vars);\n146                if chronic_vars[i] = 1 then chronic_disease = 1;\n147                if chronic_disease = 1 then leave;\n148            end;\n149        \n150            /* 抽烟 */\n151            smoking = g151;\n152        \n153            /* 喝酒 */\n154            drinking = g161;\n155        \n156            /* 体育锻炼 */\n157            if d91 = 1 or d92 = 1 then exercise = 1;\n158            else exercise = 2;\n159        \n160            /* 子女年龄状态 */\n161            array child_ages(*) f103a4 f103b4 f103c4 f103d4 f103e4 f103f4 f103g4 f103h4 f103i4 f103j4 f103k4 f103l4 f103m4;\n162            array child_alive(*) f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\n163            last_alive_child_age = .;\n164            do i = dim(child_ages) to 1 by -1;\n165                if child_alive[i] = 1 then do;\n166                    last_alive_child_age = child_ages[i];\n167                    leave;\n168                end;\n169            end;\n170            if last_alive_child_age &gt; 60 then child_elderly_status = 1;\n171            else child_elderly_status = 0;\n172        \n173            /* 生成 care - support 变量 */\n174            if residence = 1 or visit_fren = 1 then care_support = 1;\n175            else care_support = 0;\n176        \n177            /* 选择需要的变量 */\n178            keep SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support\n179                 f10 age gender education job_type marriage_status hukou_type\n180                 social_insurance medical_insurance chronic_disease smoking drinking exercise\n181                 child_elderly_status care_support f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3\n181      ! f103l3 f103m3;\n182        run;\n\nWARNING: DROP、KEEP 或 RENAME 列表中的变量 f10 从未被引用。\nNOTE: 由于出错，SAS 系统停止处理该步。\nWARNING: 数据集 WORK.SELECTED_DATA 可能不完整。该步停止时，共有 0 个观测和 37 个变量。\nNOTE: “DATA 语句”所用时间（总处理时间）:\n      实际时间          0.02 秒\n      CPU 时间          0.01 秒\n      \n\n183        \n184        /* 将筛选后的保存为 XLSX 格式文件 */\n185        /*\n186        proc export data=selected_data\n187            outfile='C:\\Users\\asus\\Desktop\\test\\CLHLS\\CLHLS数据\\CLHLS数据\\clhls_2018_sort0220.xlsx'\n188            dbms=xlsx\n189            replace;\n190        run;\n191        */\n192        \n193        /*检查f10 生育子女数的分布情况*/\n194        proc freq data=selected_data;\n195            tables f10;\nERROR: 变量 F10 没有找到。\n196        run;\n\nNOTE: 由于出错，SAS 系统停止处理该步。\nNOTE: “PROCEDURE FREQ”所用时间（总处理时间）:\n      实际时间          0.00 秒\n      CPU 时间          0.00 秒\n      \n197        \n198        /*检查子女存活状态*/\n\n\n199        proc freq data=selected_data;\n200            tables f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\n201        run;\n\nNOTE: 数据集 WORK.SELECTED_DATA 中没有观测。\nNOTE: “PROCEDURE FREQ”所用时间（总处理时间）:\n      实际时间          0.00 秒\n      CPU 时间          0.00 秒\n      \n\n202        \n203        /* 样本筛选 */\n204        data temp_data;\n205            set selected_data;\n206        \n207            /* 纳入标准 */\n208            /* 年龄要求 */\n209            /*age_include = (age &gt; 60);*/\n210            /* 生育情况 */\n211            fertility_include = (f10 &gt; 0);\n212        \n213            /* 子女存活情况 */\n214            array child_alive(*) f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\n215            child_alive_include = 0;\n216            do i = 1 to dim(child_alive);\n217                if child_alive[i] = 1 then do;\n218                    child_alive_include = 1;\n219                    leave;\n220                end;\n221            end;\n222            meet_include = fertility_include and child_alive_include;\n223        \n224            /* 排除标准 */\n225            /* 关键变量缺失值检查 */\n226            /*array key_vars(*) SHEALTH ADL IADL;*/\n227            array key_vars(*) SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support\n228                              age gender education job_type marriage_status hukou_type\n229                              social_insurance medical_insurance chronic_disease smoking drinking exercise\n230                              child_elderly_status care_support;\n231            has_missing = 0;\n232            do i = 1 to dim(key_vars);\n233                if missing(key_vars[i]) then do;\n234                    has_missing = 1;\n235                    leave;\n236                end;\n237            end;\n238            /* 不合理生育数量检查 */\n239            unreasonable_fertility = (f10 &gt; 7 or missing(f10));\n240            meet_exclude = has_missing or unreasonable_fertility;\n241        \n242            /* 筛选符合条件的样本 */\n243            if meet_include and not meet_exclude;\n244        \n245        \n246            /* 移除临时判断变量 */\n247            drop fertility_include child_alive_include has_missing unreasonable_fertility meet_include meet_exclude;\n248            /*移除部分原始变量*/\n249            drop f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\n250        run;\n\nNOTE: 变量 f10 未初始化。\nNOTE: 从数据集 WORK.SELECTED_DATA. 读取了 0 个观测\nNOTE: 数据集 WORK.TEMP_DATA 有 0 个观测和 26 个变量。\nNOTE: “DATA 语句”所用时间（总处理时间）:\n      实际时间          0.01 秒\n      CPU 时间          0.00 秒\n      \n\n251        \n252        /*打印所有变量的频数分布表，检查是否含有异常值*/\n253        /*\n254        proc freq data=final_data;\n255            tables _all_;\n256        run;\n257        */\n258        \n259        /*还需要删除含有的样本，即某些变量中赋值为9（not applicable）和88（don't know）的样本*/\n260        /*具体需要剔除变量满足这些条件的样本：SHEALTH&gt;8,ADL&gt;18,IADL&gt;24,residence&gt;3,eudcation&gt;22,smoking&gt;2,drinking&gt;2*/\n261        /* 删除满足特定条件的样本 */\n262        data final_data;\n263            set temp_data;\n264            if (SHEALTH &lt;= 8) and (ADL &lt;= 18) and (IADL &lt;= 24) and (residence &lt;= 3) and (age&gt;=60) and (education &lt;= 22) and\n264      ! (smoking &lt;= 2) and (drinking &lt;= 2);\n265        run;\n\nNOTE: 从数据集 WORK.TEMP_DATA. 读取了 0 个观测\nNOTE: 数据集 WORK.FINAL_DATA 有 0 个观测和 26 个变量。\nNOTE: “DATA 语句”所用时间（总处理时间）:\n      实际时间          0.00 秒\n      CPU 时间          0.00 秒\n      \n\n266        \n267        proc freq data=final_data;\n268            tables _all_;\n269        run;\n\nNOTE: 数据集 WORK.FINAL_DATA 中没有观测。\nNOTE: “PROCEDURE FREQ”所用时间（总处理时间）:\n      实际时间          0.00 秒\n      CPU 时间          0.00 秒\n      \n\n270        \n271        /* 保存筛选后的数据为 XLSX 格式 */\n272        proc export\n273            data = final_data\n274            outfile = 'C:\\Users\\asus\\Desktop\\test\\CLHLS\\Analysis-0214\\final_data.xlsx' /* 请替换为实际保存路径 */\n275            dbms = xlsx\n276            replace;\n277        run;\n\nERROR: Temporary file for XLSX file can not be created -&gt; C:\\Users\\asus\\Desktop\\test\\CLHLS\\Analysis-0214\\final_data.$$1.  Make sure \nthe path name is correct and that you have write permission.\n\nNOTE: 由于出错，SAS 系统停止处理该步。\nNOTE: “PROCEDURE EXPORT”所用时间（总处理时间）:\n      实际时间          0.02 秒\n      CPU 时间          0.01 秒\n      \n278        \n279        /* 对 age 变量进行分组 */\n\n\n280        data final_data_grouped;\n281            set final_data;\n282            if age &lt; 70 then age_group = '60 - 69';\n283            else if age &lt; 80 then age_group = '70 - 79';\n284            else if age &lt; 90 then age_group = '80 - 89';\n285            else age_group = '90+';\n286        run;\n\nNOTE: 从数据集 WORK.FINAL_DATA. 读取了 0 个观测\nNOTE: 数据集 WORK.FINAL_DATA_GROUPED 有 0 个观测和 27 个变量。\nNOTE: “DATA 语句”所用时间（总处理时间）:\n      实际时间          0.00 秒\n      CPU 时间          0.00 秒\n      \n\n287        \n288        /* 探查每个变量的基本统计信息，查看是否有异常值 */\n289        proc means data=final_data n nmiss min max mean std;\n290            var SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support\n291                age gender education job_type marriage_status hukou_type\n292                social_insurance medical_insurance chronic_disease smoking drinking exercise\n293                child_elderly_status care_support;\n294        run;\n\nNOTE: 数据集 WORK.FINAL_DATA 中没有观测。\nNOTE: “PROCEDURE MEANS”所用时间（总处理时间）:\n      实际时间          0.00 秒\n      CPU 时间          0.01 秒\n      \n\n295        \n296        /* 查看字符型变量的唯一值，看是否有异常字符 */\n297        proc freq data=final_data_grouped;\n298            tables age_group gender education job_type marriage_status hukou_type;\n299        run;\n\nNOTE: 数据集 WORK.FINAL_DATA_GROUPED 中没有观测。\nNOTE: “PROCEDURE FREQ”所用时间（总处理时间）:\n      实际时间          0.00 秒\n      CPU 时间          0.00 秒\n      \n\n300        \n301        /* 打印因变量、自变量和控制变量的频数分布表并汇总 */\n302        proc freq data=final_data_grouped noprint;\n303            tables SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support\n304                   age_group gender education job_type marriage_status hukou_type\n305                   social_insurance medical_insurance chronic_disease smoking drinking exercise\n306                   child_elderly_status care_support / out=freq_summary;\n307        run;\n\nNOTE: 数据集 WORK.FINAL_DATA_GROUPED 中没有观测。\nNOTE: 数据集 WORK.FREQ_SUMMARY 有 0 个观测和 3 个变量。\nNOTE: “PROCEDURE FREQ”所用时间（总处理时间）:\n      实际时间          0.01 秒\n      CPU 时间          0.01 秒\n      \n\n308        \n309        /* 导出频数汇总表到 Excel */\n310        proc export\n311            data = freq_summary\n312            outfile = 'C:\\Users\\asus\\Desktop\\test\\CLHLS\\Analysis-0214\\frequency_summary.xlsx' /* 请替换为实际保存路径 */\n313            dbms = xlsx\n314            replace;\n315        run;\n\nERROR: Temporary file for XLSX file can not be created -&gt; C:\\Users\\asus\\Desktop\\test\\CLHLS\\Analysis-0214\\frequency_summary.$$1.  \nMake sure the path name is correct and that you have write permission.\n\nNOTE: 由于出错，SAS 系统停止处理该步。\nNOTE: “PROCEDURE EXPORT”所用时间（总处理时间）:\n      实际时间          0.00 秒\n      CPU 时间          0.00 秒\n      \n316        \n317        \n318        ods html5 (id=saspy_internal) close;ods listing;\n319        \n\u00146                                                             SAS 系统                                  23:00 Saturday, May 24, 2025\n\n320        \n\n\nNotes:在Qmd中输入SAS代码时，不可以直接使用\n\n::: {#ab0a8276 .cell execution_count=2}\n``` {.sas .cell-code}\nyour SAS code here \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\u00147                                                             SAS 系统                                  23:00 Saturday, May 24, 2025\n\n323        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n323      ! ods graphics on / outputfmt=png;\nNOTE: 正在写入 HTML5(SASPY_INTERNAL) Body（主体）文件: _TOMODS1\n324        \n325        your SAS code here\n326        \n327        \n328        ods html5 (id=saspy_internal) close;ods listing;\n329        \n\u00148                                                             SAS 系统                                  23:00 Saturday, May 24, 2025\n\n330        \n```\n:::\n:::\n\n进行标注，应使用如下形式：\n\n::: {#90b27631 .cell execution_count=3}\n``` {.sas .cell-code}\n#| eval: false \nyour code here \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\u00149                                                             SAS 系统                                  23:00 Saturday, May 24, 2025\n\n333        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n333      ! ods graphics on / outputfmt=png;\nNOTE: 正在写入 HTML5(SASPY_INTERNAL) Body（主体）文件: _TOMODS1\n334        \n335        #| eval: false\n336        your code here\n337        \n338        \n339        ods html5 (id=saspy_internal) close;ods listing;\n340        \n\u001410                                                            SAS 系统                                  23:00 Saturday, May 24, 2025\n\n341        \n```\n:::\n:::\n\n否则会在输出报错，使用了错误的\n``````````````````` :::\n原因是因为，在 Quarto 中，fenced div 的正确语法是：\n::: {.class-name}  \nContent here  \n:::",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/R/R-install.html",
    "href": "Guide/R/R-install.html",
    "title": "MacOS安装 R 和 Rstudio",
    "section": "",
    "text": "进入官网：https://www.r-project.org/\n找到适合操作系统的安装包与合适的版本\n\n点击download R\n找个离你近的学校或者随便哪个都行\n根据电脑选择一下安装包\nmacOS需要看一下是Intel还是arm的CPU，arm目前在M1/M2上搭载\n下载到download中\n在download中查看，选中R-4.3.2-x86_64.pkg这个文件，右键后选择第一个option“open”\n\n进入安装界面\n\n然后一直continue\n出现上面页面后，点击install后需要输入电脑密码，解锁后安装\n安装完成后可以选择保留安装包或者将其移入trash（废纸篓）\n安装完成后在Launchpad中找到R，双击进入操作界面\n进入后发现系统有点缺陷，因为是新电脑，有很多不完善的地方，找了一些办法解决\nDuring startup - Warning messages:  \n1: Setting LC_CTYPE failed, using \"C\"   \n2: Setting LC_COLLATE failed, using \"C\"   \n3: Setting LC_TIME failed, using \"C\"   \n4: Setting LC_MESSAGES failed, using \"C\"   \n5: Setting LC_MONETARY failed, using \"C\"   \n[R.app GUI 1.80 (8281) x86_64-apple-darwin20]    \nWARNING: You're using a non-UTF8 locale, therefore only ASCII characters will work. Please read R for Mac OS X FAQ (see Help) section 9 and adjust your system preferences accordingly.\n以上是warning，解决办法是\n⌘+space(空格键）调出Spotlight Search，然后输入terminal再点击open，打开后输入locale查看本地的设置，得到如下\n{bash,eval = FALSE} LANG=\"\" LC_COLLATE=\"C\" LC_CTYPE=\"UTF-8\" LC_MESSAGES=\"C\" LC_MONETARY=\"C\" LC_NUMERIC=\"C\" LC_TIME=\"C\" LC_ALL=\n这些还没有配置好，进行一些配置：\n{bash，eval = FALSE}        export LC_CTYPE=en_US.UTF-8 export LC_COLLATE=en_US.UTF-8 export LC_TIME=en_US.UTF-8 export LC_MESSAGES=en_US.UTF-8 export LC_MONETARY=en_US.UTF-8\n这样就可以了。\n然后再安装Rstudio，RStudio是为R语言设计的一种跨平台集成开发环境。其特色包括可客制化的软件套件视觉化界面与同团队开发的一系列数据可视化与出版工具。\n在Rstudio中下载合适的版本，按照上述操作再来亿遍即可。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "R introduction",
      "MacOS安装 R 和 Rstudio"
    ]
  },
  {
    "objectID": "Guide/Python/Python-intro.html",
    "href": "Guide/Python/Python-intro.html",
    "title": "Python",
    "section": "",
    "text": "Python（英式发音：/ˈpaɪθən/；美式发音：/ˈpaɪθɑːn/），是一种广泛使用的解释型、高级和通用的编程语言。Python支持多种编程范型，包括结构化、过程式、反射式、面向对象和函数式编程。它拥有动态类型系统和垃圾回收功能，能够自动管理内存使用，并且其本身拥有一个巨大而广泛的标准库。它的语言结构以及面向对象的方法，旨在帮助程序员为小型的和大型的项目编写逻辑清晰的代码。\n和Python结缘在2020年末，当时朋友正在学，我也跟着一起，跑了几个代码，做了几个图，但是没有啥应用计划，后来就没咋关注了。\n等毕了业，有感觉应该学一学，要是工作了也算有一个技能，于是又捡了起来，还行，随着理解力的提升，逐渐能掌握这一门语言了，当然，AI 的辅助也是功不可没。\nPython 的故事很多，关于它的内容在互联网可能看一年都看不完，全世界的人都在使用它，因此我这里也不做过多的赘述，显得很多余。\n使用Python的第一步，就是来一个电脑，当然没有电脑也行，因为可以用Google的 Colab ，挺好用，如果做机器学习，是一个很好的选择（当然是要开Pro）。\n然后安装，新手就安装普通的 Python 吧，先试试，来个编辑器，用 VS code、vim、pycharm 都可以，个人推荐用 VS code（这个也要学，没那么简单）。\n再然后学会 pip 安装包/库，创建一个文件 .py 开始运行。\nMOOC和B站上有很多教程，市面上的书也很多，选一本自己看得过去/感兴趣的。\n关于书的话，这些可以看看：\n\nPython Cookbook 3rd\nPython编程:从入门到实践\nPython基础教程 \n利用Python进行数据分析（做数据分析的可以着重看这一本）\n\n等逐渐熟悉Python了，可以用下 anaconda ，对环境的管理要好不少，且可以使用 spyder（神器，就是还不支持 AI）。\nend.",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python"
    ]
  },
  {
    "objectID": "Guide/Guide-intro.html",
    "href": "Guide/Guide-intro.html",
    "title": "统计软件",
    "section": "",
    "text": "一些统计软件的使用记录。\n最早是学的Python，应该是20年的时候，看的MOOC上的北理嵩天老师的课程，过了一遍，没有深入研究。\n后来学的SPSS，21年，因为开始上统计学课了，课上老师用SPSS做分析演示，我还负责给全专业分发SPSS安装包和密钥，并帮他们安装。\n21年还试着用过MATLAB，因为20年的时候数学老师说可以准备数学建模的比赛，20年疫请然后没人管事，21年计算机学院的一位老师担下了这个活，随机我们组队在21年秋天比赛，但是数理教研组所承诺的培训和指导都是泡影，我和队友在数理教研室办公室待了两个晚上，不断地从互联网上查找相关的资料和代码，然后在MATLAB中进行复现，其实这个时候的认识和技能是很糟糕的，数学上了微积分和线代，但是和建模关系最大的运筹学要在大二的下学期才上。就这样，紧张又无聊的三天就这么过去了，现在来看（2025年）真的是既心酸又好笑，但是也不失为一个有趣的记忆。\n21年写完大创的论文后，就逐渐没有再关注这些软件的使用了，因为用不上，直到23年毕业的时候，又拿着SPSS做了一下毕业论文的分析。\n23年毕业后，选择了二战，暑假又回了学校和同学租房备考，在逛丁香园的时候，发现本校的一位老师在上面更新了Rmarkdown的课程，我好奇，随机花了一周的时间学了一下，是一个很好用的“thing”，结合了R、Markdown和LaTeX。正好当时需要整理统计学的笔记，随开始尝试并使用。\n其实23年的时候Posit已经在开发Quarto了，但是新事物到达普通人的视野中总是需要多耗费一些时间，到了24年我才知道有这么个“thing”，但是后面的学习节奏，不允许我抽出大量的时间来“改换门庭”。同时我的Rstudio一直无法正确的创建和使用Quarto，和社区交流后无果，只能使用VS Code来作为编辑器。\n直到24年考完空闲下来，我才开始系统地转换这个笔记并重新部署在GitHub上。\n24年上半年在长沙的时候，二战失败，五月找工作也失败。安慰自己说，没事，我可以干点别的，就重新开始学Python，并尝试学了SAS。\n25年开年朋友请我帮他做一些Stata的分析，我便拿出了22年朋友送的《Stata统计分析·社会科学应用指南》开始速成，得益于AI的发展，专供某一个方向也是不难的。在这种理解下，我开始系统地学习统计学和数据分析的内容，这个网站被用来记录和整理相关的内容与想法。\n我也不知道最后会学成什么样，但是希望能留下一些有意义的东西，以供后来者。\n2025-04-28 Stata作为一个商业软件，确有其独到之处，稳定，简便，支持很好是很好的有点，相较于R的开源与不稳定，这在多次使用与复现中是很重要的一点。最近在用R做概率图模型的时候就复现不出来，功能强大是优点，但是无法复现也就意味着断层，对于学习者来说是一件很麻烦的事情。\n2025-04-29 昨晚弄好 Stata 18 以后，晚上想起 Stata 官网说可以和 Python 联用，开发了包在 Ipython 中调用 Stata ，这很有意思，我在想，既然 Quarto 可以编译 .ipynb 文件为 PDF、html 等文件，那么是否可以在 Quarto 中使用 .ipynb 文件做一个容器，然后将 Satat 程序放在里面进行编译后，再由 Quarto 生成 html 文件，最后再组成网站，可以无损/流畅地展示 Stata 程序和输出结果。网上有 Python + Stata 结合使用的相关信息，但是没有和 Quarto 配合的，于是自己进行尝试，成功，能够实现在 .ipynb 中编辑 Stata 代码，并通过 Quarto 编译成网页，在网站进行展示。\n2025-05-05 看着桌边的 《SAS统计软件应用》，想了一下 SAS ，软件自带的编辑器很糟糕，又想，vscode 是万能编辑器，是不是也有 SAS 的扩展，搜了一下，嘿，还真有，那是不是也可以像 Stata 一样通过 .ipynb 文件进行编辑和运行，然后通过 Quarto 进行编译转为 .html 文件然后网页输出，看了下也是可以的，有趣，感觉 Quarto 真的可以借助 .ipynb 和 .qmd/rmd 实现统计软件的展现大一统。\n2025-05-11 昨天学完了stata的初级课程，今天开始捣鼓advanced course，一直听闻实证分析的妙用，有点抽象，而且没有找到特别合适的课程，Princeton 的一些教案挺好的，对着教案翻译然后跑代码，复现还可以。\n2025-05-24 实现在 .ipynb 文件中插入 SAS 代码并运行，但是输出网页有一定“畸变”。\n2025-07-07 成功实现在 .qmd 文件中插入 execute cell 调用 Python kernel 后运行 SAS 代码，略有复杂。"
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "本篇博客用来记录2025年1月到2月帮助学弟处理一份某二级医院2018-2023年的医疗费用，最开始用的Stata，但是越往后，越感觉到Stata的难用，以及AI对这种程序的支持程度极其有限，随改用 Python + Stata 来继续完成相关的分析。\n\n\n运行此文档需要电脑上以安装Python，并且下列包已被安装并且能被调用：\nnumpy jupyter-cache pandas openpyxl\n你可以使用 pip 或 conda 进行安装： pip install jupyter-cache\n\n\n\n我们可以使用pandas包来查看部分原始数据，数据的基本样式如下：\n\n# 安装并加载必要的包\nimport pandas as pd\nimport numpy as np\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\ndata = pd.read_excel(file_path, sheet_name=0, engine='openpyxl')     \n\n# 数据脱敏，删除地方\ncolumns_to_drop = [\"籍贯\", \"出生地\"]\ndata = data.drop(columns=columns_to_drop, errors='ignore')  # errors='ignore' 防止列不存在时报错\n\n# 随机抽取10个样本数据\nsample_data = data.sample(n=10, random_state=42)\n\n# 打印样本数据\nprint(sample_data)\n\n      次数        出生日期 性别   年龄      医疗付费方式  国籍 新生儿出生体重 新生儿入院体重  民族     职业  ...  \\\n1138   1  1956-09-06  男  61岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n2024   1  1959-08-15  女  59岁  城镇居民基本医疗保险  中国       －       －  汉族     居民  ...   \n1605   1  1973-02-05  女  45岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1975  11  2012-09-20  男   6岁    新型农村合作医疗  中国       －       －  汉族  学龄前儿童  ...   \n1701   1  1963-01-23  男  55岁         全自费  中国       －       －  汉族      无  ...   \n218    1  1969-06-08  女  48岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1344   1  1960-09-11  女  57岁    新型农村合作医疗  中国       －       －  汉族     务农  ...   \n252    1  1944-05-01  女  73岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1921   5  2013-10-17  女   5岁    新型农村合作医疗  中国       －       －  汉族  学龄前儿童  ...   \n643    1  1997-09-02  男  20岁         全自费  中国       －       －  汉族     战士  ...   \n\n     麻醉开始时间3 麻醉结束时间3 麻醉方式3 麻醉分级3  切口部位3 切口等级3 NNIS分级3  手术部位感染3  术前住院天数3  \\\n1138     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n2024     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1605     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1975     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1701     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n218      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1344     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n252      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1921     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n643      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n\n     手术持续时间3  \n1138     NaN  \n2024     NaN  \n1605     NaN  \n1975     NaN  \n1701     NaN  \n218      NaN  \n1344     NaN  \n252      NaN  \n1921     NaN  \n643      NaN  \n\n[10 rows x 200 columns]\n\n\n我们可以看到，该数据的列很多，第一张表中有200列，我们需要对其进行一些筛选。\n\n\n\n从哪里开始是一个需要思考的问题，对于数据的认识决定了你处理问题的方向和效率。首先，理解数据的来源至关重要，这包括了解数据是如何收集的、收集过程中可能出现的偏差或错误。其次，明确数据的类型与结构也是关键步骤之一，不同类型的数据（如定量数据、定性数据）需要采用不同的分析方法。再者，对数据进行初步探索，比如通过可视化手段观察数据分布特征，或是计算一些基本统计量来了解数据的基本情况，能够帮助你更好地制定数据处理策略。\n在真正开始处理数据之前，还需要考虑你的目标是什么。是为了回答一个具体的问题，还是为了探索潜在的模式？明确了目标之后，才能有针对性地选择合适的工具和技术。此外，考虑到数据质量的问题，数据清洗是不可跳过的一步，它包括去除异常值、填补缺失值等操作，这对于提高分析结果的准确性非常关键。\n最后，保持对数据伦理的关注同样重要，在整个数据分析的过程中，确保遵循相关的隐私保护法规和道德标准，这样才能确保你的工作不仅有效，而且负责任。通过对数据全面而深刻的理解，你可以更加自信地从数据中提取有价值的信息，并为决策提供有力支持。\n\n\n这份Excel文件有6张sheet，分别是2018-2023年，首先需要检查这六张sheet中的变量是否一致：\n\nimport pandas as pd\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\nsheet_names = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n\n# 读取所有 sheet 的数据\nsheets_data = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}\n\n# 获取每个 sheet 的列名\nsheets_columns = {sheet: set(data.columns) for sheet, data in sheets_data.items()}\n\n# 找出所有 sheet 的共同变量和不一致的变量\ncommon_columns = set.intersection(*sheets_columns.values())\nall_columns = set.union(*sheets_columns.values())\ninconsistent_columns = all_columns - common_columns\n\n# 打印结果\nprint(\"一致的变量名:\")\nprint(common_columns)\n\nprint(\"\\n不一致的变量名:\")\nprint(inconsistent_columns)\n\n# 打印每个 sheet 的变量\nfor sheet, columns in sheets_columns.items():\n    print(f\"\\n{sheet} 的变量: {columns}\")\n\n一致的变量名:\n{'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '1愈合', '死亡患者尸检', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '3级别', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '级别1'}\n\n不一致的变量名:\n{'愈合2', '手术开始时间3', '7手术编码', '7级别', '7切口', '手术部位感染3', '手术次数3', '8麻醉方式', '7手术时间', '手术持续时间3', '是否微创手术2', '8手术时间', '8愈合', '4麻醉方式', '麻醉开始时间3', '术前住院天数3', '切口部位3', 'NNIS分级3', '8切口', '麻醉结束时间3', '麻醉分级3', '8手术编码', '切口等级3', '手术操作名称3', '麻醉方式3', '手术操作编码3', '8级别', '4麻醉医师', '8手术名称', '7愈合', '手术结束时间3', '择期手术3', '7手术名称', '7麻醉方式'}\n\n2018 的变量: {'目的', '愈合2', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '手术开始时间3', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉结束时间3', '麻醉开始时间1', '切口等级3', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '麻醉方式3', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '2切口', '手术部位感染3', '3手术时间', '是否有使用抗菌药物1', '入院日期', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '麻醉开始时间3', '6愈合', '切口部位3', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '手术操作名称3', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '手术操作编码3', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '手术结束时间3', '1麻醉方式', '3.1非手术治疗项目费', '择期手术3', '4愈合', '1手术名称', '病案质量', '5级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '3级别', '手术次数3', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '手术持续时间3', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '是否微创手术2', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '4麻醉方式', '3.3手术治疗费', '术前住院天数3', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '麻醉分级3', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', 'NNIS分级3', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '级别1'}\n\n2019 的变量: {'目的', '愈合2', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '手术开始时间3', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '手术操作名称3', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '手术操作编码3', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '手术结束时间3', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '手术次数3', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '是否微创手术2', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '4麻醉方式', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2020 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '8愈合', '6手术编码', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2021 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2022 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2023 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '4麻醉医师', '血管介入治疗抗菌药物使用天数', '是否非计划重返手术室病例1', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '输液反应次数', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n\n然后剔除不一致的变量数据，同时创建一个新变量year，用sheet的年份对其进行填充，再按变量名对应合并6张表格的数据称为一张总表，命名为merge-sheet.xlsx输出到你需要存放数据的文件夹中。\n变量还是太多了，那接下来对变量进行筛选，首先我们可以对所有键值为空的变量进行剔除，或者根据实际的研究需要，剔除一部分键值全部为null的变量。\n这里我选择对键值全部为null或0的变量进行剔除。\n第一次尝试的时候，打开表后进行查看，发现变量顺序很乱，没有按照原始顺序进行排列，处理办法则是在前面的变量筛选部分使用DataFrame的loc方法选择列，同时保持列的顺序。\n同时为了节省时间，因为在Quarto中运行Python代码很慢，暂时还不知道原因，待以后调试一下。所以最后用一个程序解决上述这些问题，节省时间。\n\nimport pandas as pd\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\noutput_path = \"C:/Users/asus/Desktop/test/stata/data/merge-data.xlsx\"\nfinal_output_path = \"C:/Users/asus/Desktop/test/stata/data/cleaned-merge-data.xlsx\"\nsheet_names = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n\n# 读取所有 sheet 的数据\nsheets_data = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}\n\n# 获取每个 sheet 的列名\nsheets_columns = {sheet: set(data.columns) for sheet, data in sheets_data.items()}\n\n# 找出所有 sheet 的共同变量\ncommon_columns = set.intersection(*sheets_columns.values())\n# 保持原始顺序\ncommon_columns = list(common_columns)  \n\n# 剔除不一致的变量数据，并添加 year 变量\nfor sheet, data in sheets_data.items():\n    sheets_data[sheet] = data[list(common_columns)]\n    sheets_data[sheet]['year'] = sheet\n\n# 合并所有 sheet 的数据\nmerged_data = pd.concat(sheets_data.values(), ignore_index=True)\n\n# 输出合并后的数据到指定路径\nmerged_data.to_excel(output_path, index=False)\n\n# 重新导入合并后的数据\nmerged_data = pd.read_excel(output_path)\n\n# 剔除键值全部为 null 或 0 的变量，同时保持原始变量的顺序\nnon_null_columns = merged_data.dropna(axis=1, how='all').columns\nnon_zero_columns = merged_data.loc[:, (merged_data != 0).any(axis=0)].columns\nvalid_columns = [col for col in merged_data.columns if col in non_null_columns and col in non_zero_columns]\n\ncleaned_data = merged_data.loc[:, valid_columns]\n\n# 输出清理后的数据到指定路径\ncleaned_data.to_excel(final_output_path, index=False)\n\nprint(f\"清理后的数据已输出到 {final_output_path}\")\n\n# 展示部分数据\n\n# 随机抽取10个样本数据\nsample_data = cleaned_data.sample(n=10)\n\n# 打印样本数据\nprint(sample_data)\n\n清理后的数据已输出到 C:/Users/asus/Desktop/test/stata/data/cleaned-merge-data.xlsx\n      目的 断脐后预防性使用抗菌药物给药时间1 6麻醉方式  1愈合 死亡患者尸检 入院途径  2.4临床诊断项目费  \\\n14170  -               NaN   NaN  NaN    NaN   急诊      1553.0   \n20116  -               NaN   NaN  NaN    NaN   门诊      1272.5   \n37053  -               NaN   NaN  NaN    NaN   门诊         0.0   \n43824  -               NaN   NaN    甲      否   门诊      1106.0   \n32348  -               NaN   NaN  NaN    NaN   门诊       398.5   \n37403  -               NaN   NaN  NaN    NaN   门诊        29.0   \n31026  -               NaN   NaN    甲    NaN   门诊      1814.0   \n12348  -               NaN   NaN    乙    NaN   门诊       592.0   \n28312  -               NaN   NaN   其他    NaN   门诊      2022.5   \n35851  -               NaN   NaN   其他    NaN   门诊      1417.5   \n\n       清洁手术预防使用抗菌药物总天数              麻醉结束时间1 麻醉结束时间2  ...  2.3影像学诊断费 麻醉方式2  \\\n14170              NaN                  NaN     NaN  ...          0   NaN   \n20116              NaN                  NaN     NaN  ...          0   NaN   \n37053              NaN                  NaN     NaN  ...          0   NaN   \n43824              1.0  2023-07-07 12:05:33     NaN  ...        256   NaN   \n32348              NaN                  NaN     NaN  ...          0   NaN   \n37403              NaN                  NaN     NaN  ...          0   NaN   \n31026              NaN                  NaN     NaN  ...          0   NaN   \n12348              NaN                  NaN     NaN  ...          0   NaN   \n28312              NaN                  NaN     NaN  ...        640   NaN   \n35851              NaN  2022-07-07 16:25:49     NaN  ...          0   NaN   \n\n       4手术时间 入院后颅脑损伤昏迷时间  级别2  细菌名称1 是否药物过敏 细菌名称3  级别1  year  \n14170    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n20116    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n37053    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n43824    NaN      -天-时-分  NaN      -      无     -  4.0  2023  \n32348    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n37403    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n31026    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2021  \n12348    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n28312    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2021  \n35851    NaN      -天-时-分  NaN      -      无     -  2.0  2022  \n\n[10 rows x 160 columns]\n\n\n\n\n\n\n经过上述筛选后的变量依然还有很多，其中不乏无用信息变量或无效信息变量，对变量做进一步筛选。\n对样本进行筛选，需要满足在当年收治且出院，满足常规医保使用条件，关键变量含有缺失值的样本。\n使用 pandas 进行处理（根据Excel视图挑选的缺失数据的变量或含有较多缺失值的变量）：\n\nimport pandas as pd\n\n# 文件路径\ninput_file = r\"C:\\Users\\asus\\Desktop\\test\\stata\\data\\cleaned-merge-data.xlsx\"\noutput_file = r\"C:\\Users\\asus\\Desktop\\test\\stata\\data\\allclean.xlsx\"\n\n# 读取 Excel 文件\ndf = pd.read_excel(input_file)\n\n# 要删除的列列表\ncolumns_to_drop = [\n    \"新生儿出生体重\", \"新生儿入院体重\", \"国籍\", \"籍贯\", \"病室\", \"病室.1\", \"是否有出院31天内再住院计划\",\n    \"病理诊断\", \"院内感染\", \"药物过敏\", \"死亡患者尸检\", \"血型\", \"RH\",\n    \"1手术编码\", \"1手术时间\", \"1级别\", \"1切口\", \"1愈合\", \"1麻醉方式\",\n    \"2手术名称\", \"2手术编码\", \"2手术时间\", \"2级别\", \"2切口\", \"2愈合\", \"2麻醉方式\",\n    \"3手术名称\", \"3手术编码\", \"3手术时间\", \"3级别\", \"3切口\", \"3愈合\", \"3麻醉方式\",\n    \"4手术名称\", \"4手术编码\", \"4手术时间\", \"4级别\", \"4切口\", \"4愈合\", \"4麻醉方式\",\n    \"5手术名称\", \"5手术编码\", \"5手术时间\", \"5级别\", \"5切口\", \"5愈合\", \"5麻醉方式\",\n    \"6手术名称\", \"6手术编码\", \"6手术时间\", \"6级别\", \"6切口\", \"6愈合\", \"6麻醉方式\",\n    \"7手术名称\", \"7手术编码\", \"7手术时间\", \"7级别\", \"7切口\", \"7愈合\", \"7麻醉方式\",\n    \"8手术名称\", \"8手术编码\", \"8手术时间\", \"8级别\", \"8切口\", \"8愈合\", \"8麻醉方式\",\n    \"目的\", \"入院前颅脑损伤昏迷时间\", \"入院后颅脑损伤昏迷时间\",\n    \"抗菌药物使用天数\", \"清洁手术预防使用抗菌药物品种数\", \"是否临床路径\", \"清洁手术预防使用抗菌药物总天数\",\n    \"患者入住重症监护室（ICU）情况\", \"感染情况\", \"输血反应\", \"输血反应次数\", \"输液反应\", \"输液反应次数\",\n    \"细菌名称1\", \"细菌名称2\", \"细菌名称3\", \"细菌名称4\", \"血管介入治疗\", \"血管介入治疗抗菌药物使用天数\",\n    \"手术次数1\", \"手术操作名称1\", \"手术操作编码1\", \"手术开始时间1\", \"手术结束时间1\", \"择期手术1\",\n    \"麻醉开始时间1\", \"麻醉结束时间1\", \"麻醉方式1\", \"麻醉分级1\", \"切口部位1\", \"切口等级1\", \"NNIS分级1\",\n    \"手术部位感染1\", \"术前住院天数1\", \"手术持续时间1\", \"是否非计划重返手术室病例1\", \"术前使用预防性抗菌药物1\",\n    \"术前预防性抗菌药物给药时间1\", \"是否在术后使用预防性抗菌药物1\", \"术后预防性抗菌药物结束时间1\",\n    \"手术预防性使用抗菌药物天数1\", \"是否有使用抗菌药物1\", \"预防性抗菌药物使用时机1\",\n    \"断脐后预防性使用抗菌药物给药时间1\", \"本次住院期间有无重返手术室的计划1\", \"腔镜手术名称1\", \"级别1\", \"愈合1\",\n    \"是否微创手术1\", \"手术次数2\", \"手术操作名称2\", \"手术操作编码2\", \"手术开始时间2\", \"手术结束时间2\",\n    \"择期手术2\", \"麻醉开始时间2\", \"麻醉结束时间2\", \"麻醉方式2\", \"麻醉分级2\", \"切口部位2\", \"切口等级2\",\n    \"NNIS分级2\", \"手术部位感染2\", \"术前住院天数2\", \"手术持续时间2\", \"是否非计划重返手术室病例2\",\n    \"术前使用预防性抗菌药物2\", \"术前预防性抗菌药物给药时间2\", \"是否在术后使用预防性抗菌药物2\",\n    \"术后预防性抗菌药物结束时间2\", \"手术预防性使用抗菌药物天数2\", \"是否有使用抗菌药物2\",\n    \"预防性抗菌药物使用时机2\", \"断脐后预防性使用抗菌药物给药时间2\", \"本次住院期间有无重返手术室的计划2\",\n    \"腔镜手术名称2\", \"级别2\", \"愈合2\", \"是否微创手术2\", \"手术次数3\", \"手术操作名称3\", \"手术操作编码3\", \"手术开始时间3\",\n    \"手术结束时间3\", \"择期手术3\", \"麻醉开始时间3\", \"麻醉结束时间3\", \"麻醉方式3\", \"麻醉分级3\", \"切口部位3\", \"切口等级3\",\n    \"NNIS分级3\", \"手术部位感染3\", \"术前住院天数3\", \"手术持续时间3\", \"4麻醉医师\", \"出生地\", \"籍贯\"\n]\n\n# 删除指定的列\ndf = df.drop(columns=columns_to_drop, errors='ignore')\n\n# 过滤掉 '公安病区'\nif '入院科别' in df.columns and '出院科别' in df.columns:\n    df = df[~df['入院科别'].isin(['公安病区'])]\n    df = df[~df['出院科别'].isin(['公安病区'])]\n\n# 打印随机 10 个样本\nprint(\"随机 10 个样本：\")\nprint(df.sample(10))\n\n# 将处理后的 DataFrame 写入新的 Excel 文件\n# df.to_excel(output_file, index=False)\n\n# print(f\"数据清洗完成，已保存到 {output_file}\")\n\n随机 10 个样本：\n      入院途径  2.4临床诊断项目费   10.其他费  7.2中草药费 性别    总药品费  \\\n45540   门诊       225.0    28.98      0.0  女   22.17   \n27023   门诊      1105.0  9923.46      0.0  男   77.87   \n10102   门诊       178.3  3442.00      0.0  女  168.30   \n29291   门诊       977.5   511.57      0.0  女  626.99   \n18117   门诊       624.0   139.90      0.0  女  526.41   \n48037   门诊       264.9  3286.00      0.0  男   88.97   \n35602   门诊       201.0   336.92      0.0  男  723.32   \n17413   门诊      1278.5   357.29      0.0  女  702.13   \n966     门诊         0.0    24.75      0.0  女    0.00   \n14589   门诊       147.3  4724.00      0.0  女  196.45   \n\n                                                    入院诊断     职业  1.3护理费  \\\n45540                       老年核性白内障|H25.100,翼状胬肉|H11.000     农民    25.0   \n27023              结肠恶性肿瘤个人史|Z85.006,手术后恶性肿瘤化学治疗|Z51.102     农民    75.0   \n10102                       老年性白内障|H25.900,玻璃体混浊|H43.300     农民    50.0   \n29291                           结肠息肉|K63.500,胃息肉|K31.703  自由职业者   196.0   \n18117  大疱性类天疱疮|L12.000,冠状动脉粥样硬化性心脏病|I25.103,心功能Ⅲ级|I50...     居民    85.0   \n48037                                    老年核性白内障|H25.100     农民    50.0   \n35602                       节肢动物咬伤|T63.402,过敏性皮炎|L23.901     农民   200.6   \n17413                                         腹痛|R10.400     居民   125.0   \n966                                      脑外伤后综合征|F07.201      -   156.0   \n14589  老年性白内障|H25.900,翼状胬肉|H11.000,玻璃体混浊|H43.300,特指手术...     农民    50.0   \n\n       1.1一般医疗服务费  ...  婚姻  3.5手术费   年龄  民族 2.2实验室诊断费  住院天数       总费用  \\\n45540        32.0  ...  已婚     0.0  61岁  汉族     372.0     1    716.25   \n27023       105.0  ...  已婚     0.0  29岁  汉族     499.0     3  11888.53   \n10102        52.0  ...  已婚  1976.0  76岁  汉族     387.0     2   6314.60   \n29291       245.0  ...  已婚     0.0  67岁  汉族     589.0     7  11239.16   \n18117       294.0  ...  已婚     0.0  92岁  汉族    1095.0     3   3133.31   \n48037        32.0  ...  已婚  1976.0  77岁  汉族     381.0     1   6164.87   \n35602       140.0  ...  已婚     0.0  56岁  汉族     731.0     4   2475.84   \n17413       160.0  ...  已婚     0.0  60岁  汉族     608.0     5   4195.02   \n966         414.0  ...  未婚     0.0   8岁  汉族       0.0    13   7004.75   \n14589        52.0  ...  已婚  1976.0  78岁  汉族       0.0     2   7166.75   \n\n      2.3影像学诊断费 是否药物过敏  year  \n45540         0      无  2023  \n27023         0      无  2021  \n10102        36      无  2019  \n29291         0      无  2021  \n18117         0      无  2020  \n48037        36      无  2023  \n35602         0      无  2022  \n17413         0      无  2020  \n966           0      无  2018  \n14589         0      无  2020  \n\n[10 rows x 39 columns]\n\n\n\n\n\n\n\n因为需要对疾病进行分类与根据诊断信息确定来生成共病信息，根据出院诊断来对疾病进行分类与赋值，这里使用 Stata 来完成。\n**************\n* 1. 清理环境并导入数据\n**************\nclear all\n\n* 读取 Excel 文件，假设第一行为列名\nimport excel \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\allclean.xlsx\", ///\n    firstrow case(lower) clear\n\n* 注意：\n*  - firstrow 表示将第一行作为变量名\n*  - case(lower) 将变量名转换为小写，避免中文或大小写冲突\n*  - 如果您的表格存在中文列名，可能需要手动 rename\n\n************************\n* 2. 处理、提取与分类: 以“出院诊断”列为例\n************************\n\n*------------------\ngen disease = 出院诊断\n*------------------\n\n* 假设您已经将\"出院诊断\"重命名为了 \"disease\"\n* 现在要从 disease 里提取 ICD 编码到 icd10 列。\n* 如果原数据已包含 icd10 这列，可跳过此步。\n* 这里只是示例，具体提取逻辑需根据实际字符串格式做 parsing:\n* 例如： 出院诊断 字符串为 \"急性化脓性阑尾炎|K35.902|有,高血压病|I10.x00|有\"\n\n*（示例）如果 disease 形如 \"XXX|K35.902|有,YYY|I10.x00|有\"\n* 可以先把逗号换成某种分隔，然后再拆分，这里仅给示例逻辑\n* 注意：以下只是思路示例，可能需正则表达式、substr、split 等更复杂处理\n\n// 对disease进行拆分\n* 1. 按 | 分隔 disease 列，生成多个新变量\nsplit disease, parse(\"|\") generate(disease_part)\n \n// 提取第一个 ICD 编码\n* 2. 提取第二部分（part2）作为 icd10，使用正则表达式剔除多余编码\n* 保留 disease_part2 的前7个字符作为 icd10，形如 C15.900\ngen icd10 = substr(disease_part2, 1, 7)\ngen icd_com = substr(disease_part4, 1, 7)\n* 去除前后的空格\nreplace icd10 = trim(icd10)\n\n* 3. 删除所有拆分部分\ndrop disease_part1-disease_part55\n\n* 4. 检查结果\nlist disease icd10 in 1/10\n\n***************************************\n* 按 ICD 数量判断是否共病\n***************************************\ngen comorbidity = 0  // 初始值为 0\nreplace comorbidity = 1 if !missing(icd10) & !missing(icd_com) \n// 如果ICD10和ICD_com都不为空，则赋值为1\n\n// 查看前10行的数据\nlist icd10 icd_com comorbidity in 1/10\n\n***************************************\n* 筛除部分变量\n***************************************\n\ndrop 入院日期 入院科别 出院日期 出院科别 出院诊断 disease 离院方式 病案质量\n\n***************************************\n* 按 ICD 编码生成截取变量\n***************************************\n\n* 如果 icd10 是数值型，转换为字符串型\ntostring icd10, replace  \n\n* 检查并创建 icd_3c 变量\ngen icd_3c = \"\"   // 如果 icd_3c 不存在，创建一个空的字符串变量\n\n* 截取 icd10 的前三位并赋值给 icd_3c\nreplace icd_3c = substr(icd10, 1, 3)  \n\n* 如果 icd_3c 是数值型，转换为字符串型\ntostring icd_3c, replace \n\n* 创建 icd_str1 变量\ngen icd_str1 = \"\"\n\n* icd_str1: ICD 编码首位\nreplace icd_str1 = substr(icd10,1,1)\n\n* 如果 icd_str1 是数值型，转换为字符串型\ntostring icd_str1, replace \n\n* 使用 trim() 来去除空格\nreplace icd_str1 = trim(icd_str1)\n\n* 查看 icd_str1 的数据类型\ndescribe icd_str1\n\n* 查看是否有空值或特殊字符\n* list icd_str1 if missing(icd_str1)\n\n************************\n* 按照ICD编码归为22类\n************************\n\n* 创建icd分类变量：icd_chapter\ngen icd_chapter = \"\" \n\n* 字符转换为数值\ndestring icd_chapter,replace\n\n// 为icd_chapter赋值\n\nreplace icd_chapter=1 if icd_str1==\"A\"|icd_str1==\"B\"\nreplace icd_chapter=2 if icd_str1==\"C\"|(icd_3c&gt;=\"D00\"&icd_3c&lt;=\"D48\")\nreplace icd_chapter=3 if icd_3c&gt;=\"D50\"&icd_3c&lt;=\"D89\"\nreplace icd_chapter=4 if icd_3c&gt;=\"E00\"&icd_3c&lt;=\"E90\"\nreplace icd_chapter=5 if icd_3c&gt;=\"F00\"&icd_3c&lt;=\"F99\"\nreplace icd_chapter=6 if icd_3c&gt;=\"G00\"&icd_3c&lt;=\"G99\"\nreplace icd_chapter=7 if icd_3c&gt;=\"H00\"&icd_3c&lt;=\"H59\"\nreplace icd_chapter=8 if icd_3c&gt;=\"H60\"&icd_3c&lt;=\"H99\"\nreplace icd_chapter=9 if icd_str1==\"I\"\nreplace icd_chapter=10 if icd_str1==\"J\"\nreplace icd_chapter=11 if icd_str1==\"K\"\nreplace icd_chapter=12 if icd_str1==\"L\"\nreplace icd_chapter=13 if icd_str1==\"M\"\nreplace icd_chapter=14 if icd_str1==\"N\"\nreplace icd_chapter=15 if icd_str1==\"O\"\nreplace icd_chapter=16 if icd_str1==\"P\"\nreplace icd_chapter=17 if icd_str1==\"Q\"\nreplace icd_chapter=18 if icd_str1==\"R\"\nreplace icd_chapter=19 if icd_str1==\"S\"| icd_str1==\"T\"\nreplace icd_chapter=20 if icd_str1==\"V\"| icd_str1==\"Y\"\nreplace icd_chapter=21 if icd_str1==\"Z\"\nreplace icd_chapter=22 if icd_str1==\"U\"\nreplace icd_chapter=20 if icd_str1==\"V\"| icd_str1==\"Y\"|icd_str1==\"X\"|icd_str1==\"W\"\nreplace icd_chapter=21 if icd_str1==\"Z\"| substr(trim(icd10),1,2)==\"WW\"\nreplace icd_chapter=22 if icd_str1==\"U\"\n\n***************************************\n* 检查分类缺失\n***************************************\ntab icd_3c if icd_chapter==.\n\n****************************\n* 3. 导出处理后的数据\n****************************\n\n* 导出为 Stata 格式\nsave \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\ICD-result.dta\", replace\n\n\n\n// 数据处理\nclear all\nuse \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\ICD-result.dta\",clear\n\ncapture drop Cost  // 捕获可能发生的错误，如果变量不存在则继续执行\n\n* 创建 id 变量并赋值\ngen id = _n\n\nsort id year\nxtset id year\n\n*- 次均费用，需要查看总费用是否和各项目费用加总一致，此处不一致\n* gen Cost = 总费用 / 次数 \n\n*- 住院天数\ngen Day = 住院天数\n\n*- DIP政策\ngen DIP = .\nreplace DIP = 0 if year == 2018\nreplace DIP = 0 if year == 2019\nreplace DIP = 0 if year == 2020\nreplace DIP = 0 if year == 2021\nreplace DIP = 1 if year == 2022\nreplace DIP = 1 if year == 2023\n\n*- 控制变量序列\n*- 年龄\ngen Age = 年龄\n\n*- 性别（虚拟变量；当受访者性别为女性时，赋值为\"0\"，否则为\"1\"）\ngen Gender = 0 if 性别 == \"女\"\nrecode Gender .= 1\n\n*- 婚姻（虚拟变量；当受访者已婚时，赋值为\"1\"，否则为\"0\"）\ngen Marriage = 1 if 婚姻 == \"已婚\"\nrecode Marriage .= 0\n\n*- 药物过敏\ngen Sensitive = 1 if 是否药物过敏 == \"有\"\nrecode Sensitive .= 0\n\n*- 是否手术\ngen Opera = 1 if 手术费 &gt; 0\nreplace Cmedicine = 0 if 手术费 == 0\n\n*- 职业\ngen Career = 1 if strmatch(职业, \"*农*\")\nreplace Career = 2 if strmatch(职业, \"*职*\")\nreplace Career = 3 if strmatch(职业, \"*无业*\")\nreplace Career = 4 if Career == .\n\n*- 是否共病\ngen Comorbidity = 1 if comorbidity == 1\nreplace Comorbidity = 0 if comorbidity == 0\n\n* 疾病类型按照 ICD-10 划分\ngen Disease = icd_chapter\n\ngen Insurance = 1 if strmatch(医疗付费方式, \"*自费*\")\nreplace Insurance = 2 if strmatch(医疗付费方式, \"*商业*\")\nreplace Insurance = 3 if strmatch(医疗付费方式, \"*城乡居民*\")\nreplace Insurance = 3 if strmatch(医疗付费方式, \"*城镇居民*\")\nreplace Insurance = 4 if strmatch(医疗付费方式, \"*城镇职工*\")\nreplace Insurance = 5 if strmatch(医疗付费方式, \"*贫困救助*\")\nreplace Insurance = 6 if strmatch(医疗付费方式, \"*新型农村合作*\")\nreplace Insurance = 7 if strmatch(医疗付费方式, \"*全公费*\")\nreplace Insurance = 2 if Insurance == .\n\n*- 是否使用中药\ngen Cmedicine = 1 if 中成药费 &gt; 0\nreplace Cmedicine = 1 if 中草药费 &gt; 0\nreplace Cmedicine = 0 if Cmedicine == .\n\n*- 入院途径\ngen category = 1 if 入院途径 == \"门诊\"\nreplace category = 0 if 入院途径 == \"急诊\"\n\n*- 自付金额\ngen SelfCost = 自付金额 / 次数\n\n*- 除去空值变量\ndrop 其他费用 病理诊断费 临床物理治疗费 手术治疗费 康复费 中医治疗费 抗菌药物费用 白蛋白类制品费 球蛋白类制品费 凝血因子类制品费 细胞因子类制品费 检查用一次性医用材料费 治疗用一次性医用材料费 手术用一次性医用材料费\n\n*- 费用变量数据\ngen GService = 一般医疗服务费 / 次数\n* gen GOperate = 一般治疗操作费 / 次数\ngen GSurgery = 手术费 / 次数\ngen GNurse = 护理费 / 次数\ngen GNonoperate = 非手术治疗项目费 / 次数\ngen GNarcosis = 麻醉费 / 次数\ngen GDrug = 西药费 / 次数\ngen GBlood = 血费 / 次数\ngen Others = 其他费 / 次数\n// 计算行总和\negen temp_total_diagnose = rowtotal(实验室诊断费 影像学诊断费 临床诊断项目费)\n\n// 进行除法运算\ngen GDiagnose = temp_total_diagnose / 次数\n\n// 删除临时变量\ndrop temp_total_diagnose\n\n// 计算行总和\negen temp_total_cdrug = rowtotal(中成药费 中草药费)\n\n// 进行除法运算\ngen GCDrug = temp_total_cdrug / 次数\n\n// 删除临时变量\ndrop temp_total_cdrug\n\n*- 次均费用\negen Cost = rowtotal(GService GSurgery GNurse GNonoperate GNarcosis GDrug GBlood GDiagnose GCDrug Others) \n\n*- 对变量进行排序\norder id year Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others\n\n\n\n\n\n\n// 描述性统计 保留小数点后两位\nestpost summarize Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose, detail\nesttab, cells(\"count mean(fmt(2)) sd(fmt(2)) min(fmt(2)) p50(fmt(2)) max(fmt(2))\") noobs compress replace title(Descriptive statistics)\nesttab using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result\\描述性统计0218.rtf\", cells(\"count mean(fmt(2)) sd(fmt(2)) min(fmt(2)) p50(fmt(2)) max(fmt(2))\") noobs compress replace title(Descriptive statistics)\n\n\n// 全样本费用指标\ntabstat Cost SelfCost Day GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others, s(mean) by(year)\n\n// 变量指标\ntabstat Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category, s(mean) by(year)\n\n* 计算所有年份的均值\nsummarize Cost SelfCost Day GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others\n\n\n\n为了稳健性，对因变量进行缩尾处理。\n*- 缩尾处理（目的：剔除异常值；剔除的比例根据研究而定）\nwinsor2 Cost Day Age, replace cuts(1, 99)\n\n// 全局暂元\nglobal Control Age Gender Career Marriage category Disease Opera Comorbidity Cmedicine Insurance \n\n// 基准模型\nreg Cost DIP $Control, r\nest store m1\nreg SelfCost DIP $Control, r\nest store m2\nreg Day DIP $Control Sensitive, r // Sensitive 只在Day的模型中出现\nest store m3\nreg Cost DIP $Control Day, r\nest store m4\nreg SelfCost DIP $Control Day, r\nest store m5\n\n* 输出基准模型结果\nesttab m1 m2 m3 m4 m5 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\基准模型结果.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\n// 调节效应分析\nglobal Control Age Gender Career Marriage Disease Opera Comorbidity Cmedicine Insurance\n\nreg Cost DIP $Control Day if category == 1, r\nest store m1\nreg Cost DIP $Control Day if category == 0, r\nest store m2\n\n\n*- esttab m1 m2 m3 using \nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-cost&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\nreg SelfCost DIP $Control Day if category==1, r\nest store m1\nreg SelfCost DIP $Control Day if category==0, r\nest store m2\n\n\nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-self&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\nreg Day DIP $Control Sensitive if category==1, r\nest store m1\nreg Day DIP $Control Sensitive if category==0, r\nest store m2\n\n\nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-day&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#必须配置",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#必须配置",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "运行此文档需要电脑上以安装Python，并且下列包已被安装并且能被调用：\nnumpy jupyter-cache pandas openpyxl\n你可以使用 pip 或 conda 进行安装： pip install jupyter-cache",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#初步认识数据",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#初步认识数据",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "我们可以使用pandas包来查看部分原始数据，数据的基本样式如下：\n\n# 安装并加载必要的包\nimport pandas as pd\nimport numpy as np\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\ndata = pd.read_excel(file_path, sheet_name=0, engine='openpyxl')     \n\n# 数据脱敏，删除地方\ncolumns_to_drop = [\"籍贯\", \"出生地\"]\ndata = data.drop(columns=columns_to_drop, errors='ignore')  # errors='ignore' 防止列不存在时报错\n\n# 随机抽取10个样本数据\nsample_data = data.sample(n=10, random_state=42)\n\n# 打印样本数据\nprint(sample_data)\n\n      次数        出生日期 性别   年龄      医疗付费方式  国籍 新生儿出生体重 新生儿入院体重  民族     职业  ...  \\\n1138   1  1956-09-06  男  61岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n2024   1  1959-08-15  女  59岁  城镇居民基本医疗保险  中国       －       －  汉族     居民  ...   \n1605   1  1973-02-05  女  45岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1975  11  2012-09-20  男   6岁    新型农村合作医疗  中国       －       －  汉族  学龄前儿童  ...   \n1701   1  1963-01-23  男  55岁         全自费  中国       －       －  汉族      无  ...   \n218    1  1969-06-08  女  48岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1344   1  1960-09-11  女  57岁    新型农村合作医疗  中国       －       －  汉族     务农  ...   \n252    1  1944-05-01  女  73岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1921   5  2013-10-17  女   5岁    新型农村合作医疗  中国       －       －  汉族  学龄前儿童  ...   \n643    1  1997-09-02  男  20岁         全自费  中国       －       －  汉族     战士  ...   \n\n     麻醉开始时间3 麻醉结束时间3 麻醉方式3 麻醉分级3  切口部位3 切口等级3 NNIS分级3  手术部位感染3  术前住院天数3  \\\n1138     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n2024     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1605     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1975     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1701     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n218      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1344     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n252      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1921     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n643      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n\n     手术持续时间3  \n1138     NaN  \n2024     NaN  \n1605     NaN  \n1975     NaN  \n1701     NaN  \n218      NaN  \n1344     NaN  \n252      NaN  \n1921     NaN  \n643      NaN  \n\n[10 rows x 200 columns]\n\n\n我们可以看到，该数据的列很多，第一张表中有200列，我们需要对其进行一些筛选。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#对数据的思考",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#对数据的思考",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "从哪里开始是一个需要思考的问题，对于数据的认识决定了你处理问题的方向和效率。首先，理解数据的来源至关重要，这包括了解数据是如何收集的、收集过程中可能出现的偏差或错误。其次，明确数据的类型与结构也是关键步骤之一，不同类型的数据（如定量数据、定性数据）需要采用不同的分析方法。再者，对数据进行初步探索，比如通过可视化手段观察数据分布特征，或是计算一些基本统计量来了解数据的基本情况，能够帮助你更好地制定数据处理策略。\n在真正开始处理数据之前，还需要考虑你的目标是什么。是为了回答一个具体的问题，还是为了探索潜在的模式？明确了目标之后，才能有针对性地选择合适的工具和技术。此外，考虑到数据质量的问题，数据清洗是不可跳过的一步，它包括去除异常值、填补缺失值等操作，这对于提高分析结果的准确性非常关键。\n最后，保持对数据伦理的关注同样重要，在整个数据分析的过程中，确保遵循相关的隐私保护法规和道德标准，这样才能确保你的工作不仅有效，而且负责任。通过对数据全面而深刻的理解，你可以更加自信地从数据中提取有价值的信息，并为决策提供有力支持。\n\n\n这份Excel文件有6张sheet，分别是2018-2023年，首先需要检查这六张sheet中的变量是否一致：\n\nimport pandas as pd\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\nsheet_names = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n\n# 读取所有 sheet 的数据\nsheets_data = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}\n\n# 获取每个 sheet 的列名\nsheets_columns = {sheet: set(data.columns) for sheet, data in sheets_data.items()}\n\n# 找出所有 sheet 的共同变量和不一致的变量\ncommon_columns = set.intersection(*sheets_columns.values())\nall_columns = set.union(*sheets_columns.values())\ninconsistent_columns = all_columns - common_columns\n\n# 打印结果\nprint(\"一致的变量名:\")\nprint(common_columns)\n\nprint(\"\\n不一致的变量名:\")\nprint(inconsistent_columns)\n\n# 打印每个 sheet 的变量\nfor sheet, columns in sheets_columns.items():\n    print(f\"\\n{sheet} 的变量: {columns}\")\n\n一致的变量名:\n{'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '1愈合', '死亡患者尸检', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '3级别', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '级别1'}\n\n不一致的变量名:\n{'愈合2', '手术开始时间3', '7手术编码', '7级别', '7切口', '手术部位感染3', '手术次数3', '8麻醉方式', '7手术时间', '手术持续时间3', '是否微创手术2', '8手术时间', '8愈合', '4麻醉方式', '麻醉开始时间3', '术前住院天数3', '切口部位3', 'NNIS分级3', '8切口', '麻醉结束时间3', '麻醉分级3', '8手术编码', '切口等级3', '手术操作名称3', '麻醉方式3', '手术操作编码3', '8级别', '4麻醉医师', '8手术名称', '7愈合', '手术结束时间3', '择期手术3', '7手术名称', '7麻醉方式'}\n\n2018 的变量: {'目的', '愈合2', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '手术开始时间3', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉结束时间3', '麻醉开始时间1', '切口等级3', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '麻醉方式3', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '2切口', '手术部位感染3', '3手术时间', '是否有使用抗菌药物1', '入院日期', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '麻醉开始时间3', '6愈合', '切口部位3', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '手术操作名称3', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '手术操作编码3', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '手术结束时间3', '1麻醉方式', '3.1非手术治疗项目费', '择期手术3', '4愈合', '1手术名称', '病案质量', '5级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '3级别', '手术次数3', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '手术持续时间3', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '是否微创手术2', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '4麻醉方式', '3.3手术治疗费', '术前住院天数3', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '麻醉分级3', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', 'NNIS分级3', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '级别1'}\n\n2019 的变量: {'目的', '愈合2', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '手术开始时间3', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '手术操作名称3', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '手术操作编码3', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '手术结束时间3', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '手术次数3', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '是否微创手术2', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '4麻醉方式', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2020 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '8愈合', '6手术编码', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2021 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2022 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2023 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '4麻醉医师', '血管介入治疗抗菌药物使用天数', '是否非计划重返手术室病例1', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '输液反应次数', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n\n然后剔除不一致的变量数据，同时创建一个新变量year，用sheet的年份对其进行填充，再按变量名对应合并6张表格的数据称为一张总表，命名为merge-sheet.xlsx输出到你需要存放数据的文件夹中。\n变量还是太多了，那接下来对变量进行筛选，首先我们可以对所有键值为空的变量进行剔除，或者根据实际的研究需要，剔除一部分键值全部为null的变量。\n这里我选择对键值全部为null或0的变量进行剔除。\n第一次尝试的时候，打开表后进行查看，发现变量顺序很乱，没有按照原始顺序进行排列，处理办法则是在前面的变量筛选部分使用DataFrame的loc方法选择列，同时保持列的顺序。\n同时为了节省时间，因为在Quarto中运行Python代码很慢，暂时还不知道原因，待以后调试一下。所以最后用一个程序解决上述这些问题，节省时间。\n\nimport pandas as pd\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\noutput_path = \"C:/Users/asus/Desktop/test/stata/data/merge-data.xlsx\"\nfinal_output_path = \"C:/Users/asus/Desktop/test/stata/data/cleaned-merge-data.xlsx\"\nsheet_names = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n\n# 读取所有 sheet 的数据\nsheets_data = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}\n\n# 获取每个 sheet 的列名\nsheets_columns = {sheet: set(data.columns) for sheet, data in sheets_data.items()}\n\n# 找出所有 sheet 的共同变量\ncommon_columns = set.intersection(*sheets_columns.values())\n# 保持原始顺序\ncommon_columns = list(common_columns)  \n\n# 剔除不一致的变量数据，并添加 year 变量\nfor sheet, data in sheets_data.items():\n    sheets_data[sheet] = data[list(common_columns)]\n    sheets_data[sheet]['year'] = sheet\n\n# 合并所有 sheet 的数据\nmerged_data = pd.concat(sheets_data.values(), ignore_index=True)\n\n# 输出合并后的数据到指定路径\nmerged_data.to_excel(output_path, index=False)\n\n# 重新导入合并后的数据\nmerged_data = pd.read_excel(output_path)\n\n# 剔除键值全部为 null 或 0 的变量，同时保持原始变量的顺序\nnon_null_columns = merged_data.dropna(axis=1, how='all').columns\nnon_zero_columns = merged_data.loc[:, (merged_data != 0).any(axis=0)].columns\nvalid_columns = [col for col in merged_data.columns if col in non_null_columns and col in non_zero_columns]\n\ncleaned_data = merged_data.loc[:, valid_columns]\n\n# 输出清理后的数据到指定路径\ncleaned_data.to_excel(final_output_path, index=False)\n\nprint(f\"清理后的数据已输出到 {final_output_path}\")\n\n# 展示部分数据\n\n# 随机抽取10个样本数据\nsample_data = cleaned_data.sample(n=10)\n\n# 打印样本数据\nprint(sample_data)\n\n清理后的数据已输出到 C:/Users/asus/Desktop/test/stata/data/cleaned-merge-data.xlsx\n      目的 断脐后预防性使用抗菌药物给药时间1 6麻醉方式  1愈合 死亡患者尸检 入院途径  2.4临床诊断项目费  \\\n14170  -               NaN   NaN  NaN    NaN   急诊      1553.0   \n20116  -               NaN   NaN  NaN    NaN   门诊      1272.5   \n37053  -               NaN   NaN  NaN    NaN   门诊         0.0   \n43824  -               NaN   NaN    甲      否   门诊      1106.0   \n32348  -               NaN   NaN  NaN    NaN   门诊       398.5   \n37403  -               NaN   NaN  NaN    NaN   门诊        29.0   \n31026  -               NaN   NaN    甲    NaN   门诊      1814.0   \n12348  -               NaN   NaN    乙    NaN   门诊       592.0   \n28312  -               NaN   NaN   其他    NaN   门诊      2022.5   \n35851  -               NaN   NaN   其他    NaN   门诊      1417.5   \n\n       清洁手术预防使用抗菌药物总天数              麻醉结束时间1 麻醉结束时间2  ...  2.3影像学诊断费 麻醉方式2  \\\n14170              NaN                  NaN     NaN  ...          0   NaN   \n20116              NaN                  NaN     NaN  ...          0   NaN   \n37053              NaN                  NaN     NaN  ...          0   NaN   \n43824              1.0  2023-07-07 12:05:33     NaN  ...        256   NaN   \n32348              NaN                  NaN     NaN  ...          0   NaN   \n37403              NaN                  NaN     NaN  ...          0   NaN   \n31026              NaN                  NaN     NaN  ...          0   NaN   \n12348              NaN                  NaN     NaN  ...          0   NaN   \n28312              NaN                  NaN     NaN  ...        640   NaN   \n35851              NaN  2022-07-07 16:25:49     NaN  ...          0   NaN   \n\n       4手术时间 入院后颅脑损伤昏迷时间  级别2  细菌名称1 是否药物过敏 细菌名称3  级别1  year  \n14170    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n20116    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n37053    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n43824    NaN      -天-时-分  NaN      -      无     -  4.0  2023  \n32348    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n37403    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n31026    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2021  \n12348    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n28312    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2021  \n35851    NaN      -天-时-分  NaN      -      无     -  2.0  2022  \n\n[10 rows x 160 columns]",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#筛选变量",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#筛选变量",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "经过上述筛选后的变量依然还有很多，其中不乏无用信息变量或无效信息变量，对变量做进一步筛选。\n对样本进行筛选，需要满足在当年收治且出院，满足常规医保使用条件，关键变量含有缺失值的样本。\n使用 pandas 进行处理（根据Excel视图挑选的缺失数据的变量或含有较多缺失值的变量）：\n\nimport pandas as pd\n\n# 文件路径\ninput_file = r\"C:\\Users\\asus\\Desktop\\test\\stata\\data\\cleaned-merge-data.xlsx\"\noutput_file = r\"C:\\Users\\asus\\Desktop\\test\\stata\\data\\allclean.xlsx\"\n\n# 读取 Excel 文件\ndf = pd.read_excel(input_file)\n\n# 要删除的列列表\ncolumns_to_drop = [\n    \"新生儿出生体重\", \"新生儿入院体重\", \"国籍\", \"籍贯\", \"病室\", \"病室.1\", \"是否有出院31天内再住院计划\",\n    \"病理诊断\", \"院内感染\", \"药物过敏\", \"死亡患者尸检\", \"血型\", \"RH\",\n    \"1手术编码\", \"1手术时间\", \"1级别\", \"1切口\", \"1愈合\", \"1麻醉方式\",\n    \"2手术名称\", \"2手术编码\", \"2手术时间\", \"2级别\", \"2切口\", \"2愈合\", \"2麻醉方式\",\n    \"3手术名称\", \"3手术编码\", \"3手术时间\", \"3级别\", \"3切口\", \"3愈合\", \"3麻醉方式\",\n    \"4手术名称\", \"4手术编码\", \"4手术时间\", \"4级别\", \"4切口\", \"4愈合\", \"4麻醉方式\",\n    \"5手术名称\", \"5手术编码\", \"5手术时间\", \"5级别\", \"5切口\", \"5愈合\", \"5麻醉方式\",\n    \"6手术名称\", \"6手术编码\", \"6手术时间\", \"6级别\", \"6切口\", \"6愈合\", \"6麻醉方式\",\n    \"7手术名称\", \"7手术编码\", \"7手术时间\", \"7级别\", \"7切口\", \"7愈合\", \"7麻醉方式\",\n    \"8手术名称\", \"8手术编码\", \"8手术时间\", \"8级别\", \"8切口\", \"8愈合\", \"8麻醉方式\",\n    \"目的\", \"入院前颅脑损伤昏迷时间\", \"入院后颅脑损伤昏迷时间\",\n    \"抗菌药物使用天数\", \"清洁手术预防使用抗菌药物品种数\", \"是否临床路径\", \"清洁手术预防使用抗菌药物总天数\",\n    \"患者入住重症监护室（ICU）情况\", \"感染情况\", \"输血反应\", \"输血反应次数\", \"输液反应\", \"输液反应次数\",\n    \"细菌名称1\", \"细菌名称2\", \"细菌名称3\", \"细菌名称4\", \"血管介入治疗\", \"血管介入治疗抗菌药物使用天数\",\n    \"手术次数1\", \"手术操作名称1\", \"手术操作编码1\", \"手术开始时间1\", \"手术结束时间1\", \"择期手术1\",\n    \"麻醉开始时间1\", \"麻醉结束时间1\", \"麻醉方式1\", \"麻醉分级1\", \"切口部位1\", \"切口等级1\", \"NNIS分级1\",\n    \"手术部位感染1\", \"术前住院天数1\", \"手术持续时间1\", \"是否非计划重返手术室病例1\", \"术前使用预防性抗菌药物1\",\n    \"术前预防性抗菌药物给药时间1\", \"是否在术后使用预防性抗菌药物1\", \"术后预防性抗菌药物结束时间1\",\n    \"手术预防性使用抗菌药物天数1\", \"是否有使用抗菌药物1\", \"预防性抗菌药物使用时机1\",\n    \"断脐后预防性使用抗菌药物给药时间1\", \"本次住院期间有无重返手术室的计划1\", \"腔镜手术名称1\", \"级别1\", \"愈合1\",\n    \"是否微创手术1\", \"手术次数2\", \"手术操作名称2\", \"手术操作编码2\", \"手术开始时间2\", \"手术结束时间2\",\n    \"择期手术2\", \"麻醉开始时间2\", \"麻醉结束时间2\", \"麻醉方式2\", \"麻醉分级2\", \"切口部位2\", \"切口等级2\",\n    \"NNIS分级2\", \"手术部位感染2\", \"术前住院天数2\", \"手术持续时间2\", \"是否非计划重返手术室病例2\",\n    \"术前使用预防性抗菌药物2\", \"术前预防性抗菌药物给药时间2\", \"是否在术后使用预防性抗菌药物2\",\n    \"术后预防性抗菌药物结束时间2\", \"手术预防性使用抗菌药物天数2\", \"是否有使用抗菌药物2\",\n    \"预防性抗菌药物使用时机2\", \"断脐后预防性使用抗菌药物给药时间2\", \"本次住院期间有无重返手术室的计划2\",\n    \"腔镜手术名称2\", \"级别2\", \"愈合2\", \"是否微创手术2\", \"手术次数3\", \"手术操作名称3\", \"手术操作编码3\", \"手术开始时间3\",\n    \"手术结束时间3\", \"择期手术3\", \"麻醉开始时间3\", \"麻醉结束时间3\", \"麻醉方式3\", \"麻醉分级3\", \"切口部位3\", \"切口等级3\",\n    \"NNIS分级3\", \"手术部位感染3\", \"术前住院天数3\", \"手术持续时间3\", \"4麻醉医师\", \"出生地\", \"籍贯\"\n]\n\n# 删除指定的列\ndf = df.drop(columns=columns_to_drop, errors='ignore')\n\n# 过滤掉 '公安病区'\nif '入院科别' in df.columns and '出院科别' in df.columns:\n    df = df[~df['入院科别'].isin(['公安病区'])]\n    df = df[~df['出院科别'].isin(['公安病区'])]\n\n# 打印随机 10 个样本\nprint(\"随机 10 个样本：\")\nprint(df.sample(10))\n\n# 将处理后的 DataFrame 写入新的 Excel 文件\n# df.to_excel(output_file, index=False)\n\n# print(f\"数据清洗完成，已保存到 {output_file}\")\n\n随机 10 个样本：\n      入院途径  2.4临床诊断项目费   10.其他费  7.2中草药费 性别    总药品费  \\\n45540   门诊       225.0    28.98      0.0  女   22.17   \n27023   门诊      1105.0  9923.46      0.0  男   77.87   \n10102   门诊       178.3  3442.00      0.0  女  168.30   \n29291   门诊       977.5   511.57      0.0  女  626.99   \n18117   门诊       624.0   139.90      0.0  女  526.41   \n48037   门诊       264.9  3286.00      0.0  男   88.97   \n35602   门诊       201.0   336.92      0.0  男  723.32   \n17413   门诊      1278.5   357.29      0.0  女  702.13   \n966     门诊         0.0    24.75      0.0  女    0.00   \n14589   门诊       147.3  4724.00      0.0  女  196.45   \n\n                                                    入院诊断     职业  1.3护理费  \\\n45540                       老年核性白内障|H25.100,翼状胬肉|H11.000     农民    25.0   \n27023              结肠恶性肿瘤个人史|Z85.006,手术后恶性肿瘤化学治疗|Z51.102     农民    75.0   \n10102                       老年性白内障|H25.900,玻璃体混浊|H43.300     农民    50.0   \n29291                           结肠息肉|K63.500,胃息肉|K31.703  自由职业者   196.0   \n18117  大疱性类天疱疮|L12.000,冠状动脉粥样硬化性心脏病|I25.103,心功能Ⅲ级|I50...     居民    85.0   \n48037                                    老年核性白内障|H25.100     农民    50.0   \n35602                       节肢动物咬伤|T63.402,过敏性皮炎|L23.901     农民   200.6   \n17413                                         腹痛|R10.400     居民   125.0   \n966                                      脑外伤后综合征|F07.201      -   156.0   \n14589  老年性白内障|H25.900,翼状胬肉|H11.000,玻璃体混浊|H43.300,特指手术...     农民    50.0   \n\n       1.1一般医疗服务费  ...  婚姻  3.5手术费   年龄  民族 2.2实验室诊断费  住院天数       总费用  \\\n45540        32.0  ...  已婚     0.0  61岁  汉族     372.0     1    716.25   \n27023       105.0  ...  已婚     0.0  29岁  汉族     499.0     3  11888.53   \n10102        52.0  ...  已婚  1976.0  76岁  汉族     387.0     2   6314.60   \n29291       245.0  ...  已婚     0.0  67岁  汉族     589.0     7  11239.16   \n18117       294.0  ...  已婚     0.0  92岁  汉族    1095.0     3   3133.31   \n48037        32.0  ...  已婚  1976.0  77岁  汉族     381.0     1   6164.87   \n35602       140.0  ...  已婚     0.0  56岁  汉族     731.0     4   2475.84   \n17413       160.0  ...  已婚     0.0  60岁  汉族     608.0     5   4195.02   \n966         414.0  ...  未婚     0.0   8岁  汉族       0.0    13   7004.75   \n14589        52.0  ...  已婚  1976.0  78岁  汉族       0.0     2   7166.75   \n\n      2.3影像学诊断费 是否药物过敏  year  \n45540         0      无  2023  \n27023         0      无  2021  \n10102        36      无  2019  \n29291         0      无  2021  \n18117         0      无  2020  \n48037        36      无  2023  \n35602         0      无  2022  \n17413         0      无  2020  \n966           0      无  2018  \n14589         0      无  2020  \n\n[10 rows x 39 columns]",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#使用-stata-赋值和分析",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#使用-stata-赋值和分析",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "因为需要对疾病进行分类与根据诊断信息确定来生成共病信息，根据出院诊断来对疾病进行分类与赋值，这里使用 Stata 来完成。\n**************\n* 1. 清理环境并导入数据\n**************\nclear all\n\n* 读取 Excel 文件，假设第一行为列名\nimport excel \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\allclean.xlsx\", ///\n    firstrow case(lower) clear\n\n* 注意：\n*  - firstrow 表示将第一行作为变量名\n*  - case(lower) 将变量名转换为小写，避免中文或大小写冲突\n*  - 如果您的表格存在中文列名，可能需要手动 rename\n\n************************\n* 2. 处理、提取与分类: 以“出院诊断”列为例\n************************\n\n*------------------\ngen disease = 出院诊断\n*------------------\n\n* 假设您已经将\"出院诊断\"重命名为了 \"disease\"\n* 现在要从 disease 里提取 ICD 编码到 icd10 列。\n* 如果原数据已包含 icd10 这列，可跳过此步。\n* 这里只是示例，具体提取逻辑需根据实际字符串格式做 parsing:\n* 例如： 出院诊断 字符串为 \"急性化脓性阑尾炎|K35.902|有,高血压病|I10.x00|有\"\n\n*（示例）如果 disease 形如 \"XXX|K35.902|有,YYY|I10.x00|有\"\n* 可以先把逗号换成某种分隔，然后再拆分，这里仅给示例逻辑\n* 注意：以下只是思路示例，可能需正则表达式、substr、split 等更复杂处理\n\n// 对disease进行拆分\n* 1. 按 | 分隔 disease 列，生成多个新变量\nsplit disease, parse(\"|\") generate(disease_part)\n \n// 提取第一个 ICD 编码\n* 2. 提取第二部分（part2）作为 icd10，使用正则表达式剔除多余编码\n* 保留 disease_part2 的前7个字符作为 icd10，形如 C15.900\ngen icd10 = substr(disease_part2, 1, 7)\ngen icd_com = substr(disease_part4, 1, 7)\n* 去除前后的空格\nreplace icd10 = trim(icd10)\n\n* 3. 删除所有拆分部分\ndrop disease_part1-disease_part55\n\n* 4. 检查结果\nlist disease icd10 in 1/10\n\n***************************************\n* 按 ICD 数量判断是否共病\n***************************************\ngen comorbidity = 0  // 初始值为 0\nreplace comorbidity = 1 if !missing(icd10) & !missing(icd_com) \n// 如果ICD10和ICD_com都不为空，则赋值为1\n\n// 查看前10行的数据\nlist icd10 icd_com comorbidity in 1/10\n\n***************************************\n* 筛除部分变量\n***************************************\n\ndrop 入院日期 入院科别 出院日期 出院科别 出院诊断 disease 离院方式 病案质量\n\n***************************************\n* 按 ICD 编码生成截取变量\n***************************************\n\n* 如果 icd10 是数值型，转换为字符串型\ntostring icd10, replace  \n\n* 检查并创建 icd_3c 变量\ngen icd_3c = \"\"   // 如果 icd_3c 不存在，创建一个空的字符串变量\n\n* 截取 icd10 的前三位并赋值给 icd_3c\nreplace icd_3c = substr(icd10, 1, 3)  \n\n* 如果 icd_3c 是数值型，转换为字符串型\ntostring icd_3c, replace \n\n* 创建 icd_str1 变量\ngen icd_str1 = \"\"\n\n* icd_str1: ICD 编码首位\nreplace icd_str1 = substr(icd10,1,1)\n\n* 如果 icd_str1 是数值型，转换为字符串型\ntostring icd_str1, replace \n\n* 使用 trim() 来去除空格\nreplace icd_str1 = trim(icd_str1)\n\n* 查看 icd_str1 的数据类型\ndescribe icd_str1\n\n* 查看是否有空值或特殊字符\n* list icd_str1 if missing(icd_str1)\n\n************************\n* 按照ICD编码归为22类\n************************\n\n* 创建icd分类变量：icd_chapter\ngen icd_chapter = \"\" \n\n* 字符转换为数值\ndestring icd_chapter,replace\n\n// 为icd_chapter赋值\n\nreplace icd_chapter=1 if icd_str1==\"A\"|icd_str1==\"B\"\nreplace icd_chapter=2 if icd_str1==\"C\"|(icd_3c&gt;=\"D00\"&icd_3c&lt;=\"D48\")\nreplace icd_chapter=3 if icd_3c&gt;=\"D50\"&icd_3c&lt;=\"D89\"\nreplace icd_chapter=4 if icd_3c&gt;=\"E00\"&icd_3c&lt;=\"E90\"\nreplace icd_chapter=5 if icd_3c&gt;=\"F00\"&icd_3c&lt;=\"F99\"\nreplace icd_chapter=6 if icd_3c&gt;=\"G00\"&icd_3c&lt;=\"G99\"\nreplace icd_chapter=7 if icd_3c&gt;=\"H00\"&icd_3c&lt;=\"H59\"\nreplace icd_chapter=8 if icd_3c&gt;=\"H60\"&icd_3c&lt;=\"H99\"\nreplace icd_chapter=9 if icd_str1==\"I\"\nreplace icd_chapter=10 if icd_str1==\"J\"\nreplace icd_chapter=11 if icd_str1==\"K\"\nreplace icd_chapter=12 if icd_str1==\"L\"\nreplace icd_chapter=13 if icd_str1==\"M\"\nreplace icd_chapter=14 if icd_str1==\"N\"\nreplace icd_chapter=15 if icd_str1==\"O\"\nreplace icd_chapter=16 if icd_str1==\"P\"\nreplace icd_chapter=17 if icd_str1==\"Q\"\nreplace icd_chapter=18 if icd_str1==\"R\"\nreplace icd_chapter=19 if icd_str1==\"S\"| icd_str1==\"T\"\nreplace icd_chapter=20 if icd_str1==\"V\"| icd_str1==\"Y\"\nreplace icd_chapter=21 if icd_str1==\"Z\"\nreplace icd_chapter=22 if icd_str1==\"U\"\nreplace icd_chapter=20 if icd_str1==\"V\"| icd_str1==\"Y\"|icd_str1==\"X\"|icd_str1==\"W\"\nreplace icd_chapter=21 if icd_str1==\"Z\"| substr(trim(icd10),1,2)==\"WW\"\nreplace icd_chapter=22 if icd_str1==\"U\"\n\n***************************************\n* 检查分类缺失\n***************************************\ntab icd_3c if icd_chapter==.\n\n****************************\n* 3. 导出处理后的数据\n****************************\n\n* 导出为 Stata 格式\nsave \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\ICD-result.dta\", replace\n\n\n\n// 数据处理\nclear all\nuse \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\ICD-result.dta\",clear\n\ncapture drop Cost  // 捕获可能发生的错误，如果变量不存在则继续执行\n\n* 创建 id 变量并赋值\ngen id = _n\n\nsort id year\nxtset id year\n\n*- 次均费用，需要查看总费用是否和各项目费用加总一致，此处不一致\n* gen Cost = 总费用 / 次数 \n\n*- 住院天数\ngen Day = 住院天数\n\n*- DIP政策\ngen DIP = .\nreplace DIP = 0 if year == 2018\nreplace DIP = 0 if year == 2019\nreplace DIP = 0 if year == 2020\nreplace DIP = 0 if year == 2021\nreplace DIP = 1 if year == 2022\nreplace DIP = 1 if year == 2023\n\n*- 控制变量序列\n*- 年龄\ngen Age = 年龄\n\n*- 性别（虚拟变量；当受访者性别为女性时，赋值为\"0\"，否则为\"1\"）\ngen Gender = 0 if 性别 == \"女\"\nrecode Gender .= 1\n\n*- 婚姻（虚拟变量；当受访者已婚时，赋值为\"1\"，否则为\"0\"）\ngen Marriage = 1 if 婚姻 == \"已婚\"\nrecode Marriage .= 0\n\n*- 药物过敏\ngen Sensitive = 1 if 是否药物过敏 == \"有\"\nrecode Sensitive .= 0\n\n*- 是否手术\ngen Opera = 1 if 手术费 &gt; 0\nreplace Cmedicine = 0 if 手术费 == 0\n\n*- 职业\ngen Career = 1 if strmatch(职业, \"*农*\")\nreplace Career = 2 if strmatch(职业, \"*职*\")\nreplace Career = 3 if strmatch(职业, \"*无业*\")\nreplace Career = 4 if Career == .\n\n*- 是否共病\ngen Comorbidity = 1 if comorbidity == 1\nreplace Comorbidity = 0 if comorbidity == 0\n\n* 疾病类型按照 ICD-10 划分\ngen Disease = icd_chapter\n\ngen Insurance = 1 if strmatch(医疗付费方式, \"*自费*\")\nreplace Insurance = 2 if strmatch(医疗付费方式, \"*商业*\")\nreplace Insurance = 3 if strmatch(医疗付费方式, \"*城乡居民*\")\nreplace Insurance = 3 if strmatch(医疗付费方式, \"*城镇居民*\")\nreplace Insurance = 4 if strmatch(医疗付费方式, \"*城镇职工*\")\nreplace Insurance = 5 if strmatch(医疗付费方式, \"*贫困救助*\")\nreplace Insurance = 6 if strmatch(医疗付费方式, \"*新型农村合作*\")\nreplace Insurance = 7 if strmatch(医疗付费方式, \"*全公费*\")\nreplace Insurance = 2 if Insurance == .\n\n*- 是否使用中药\ngen Cmedicine = 1 if 中成药费 &gt; 0\nreplace Cmedicine = 1 if 中草药费 &gt; 0\nreplace Cmedicine = 0 if Cmedicine == .\n\n*- 入院途径\ngen category = 1 if 入院途径 == \"门诊\"\nreplace category = 0 if 入院途径 == \"急诊\"\n\n*- 自付金额\ngen SelfCost = 自付金额 / 次数\n\n*- 除去空值变量\ndrop 其他费用 病理诊断费 临床物理治疗费 手术治疗费 康复费 中医治疗费 抗菌药物费用 白蛋白类制品费 球蛋白类制品费 凝血因子类制品费 细胞因子类制品费 检查用一次性医用材料费 治疗用一次性医用材料费 手术用一次性医用材料费\n\n*- 费用变量数据\ngen GService = 一般医疗服务费 / 次数\n* gen GOperate = 一般治疗操作费 / 次数\ngen GSurgery = 手术费 / 次数\ngen GNurse = 护理费 / 次数\ngen GNonoperate = 非手术治疗项目费 / 次数\ngen GNarcosis = 麻醉费 / 次数\ngen GDrug = 西药费 / 次数\ngen GBlood = 血费 / 次数\ngen Others = 其他费 / 次数\n// 计算行总和\negen temp_total_diagnose = rowtotal(实验室诊断费 影像学诊断费 临床诊断项目费)\n\n// 进行除法运算\ngen GDiagnose = temp_total_diagnose / 次数\n\n// 删除临时变量\ndrop temp_total_diagnose\n\n// 计算行总和\negen temp_total_cdrug = rowtotal(中成药费 中草药费)\n\n// 进行除法运算\ngen GCDrug = temp_total_cdrug / 次数\n\n// 删除临时变量\ndrop temp_total_cdrug\n\n*- 次均费用\negen Cost = rowtotal(GService GSurgery GNurse GNonoperate GNarcosis GDrug GBlood GDiagnose GCDrug Others) \n\n*- 对变量进行排序\norder id year Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#数据分析",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#数据分析",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "// 描述性统计 保留小数点后两位\nestpost summarize Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose, detail\nesttab, cells(\"count mean(fmt(2)) sd(fmt(2)) min(fmt(2)) p50(fmt(2)) max(fmt(2))\") noobs compress replace title(Descriptive statistics)\nesttab using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result\\描述性统计0218.rtf\", cells(\"count mean(fmt(2)) sd(fmt(2)) min(fmt(2)) p50(fmt(2)) max(fmt(2))\") noobs compress replace title(Descriptive statistics)\n\n\n// 全样本费用指标\ntabstat Cost SelfCost Day GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others, s(mean) by(year)\n\n// 变量指标\ntabstat Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category, s(mean) by(year)\n\n* 计算所有年份的均值\nsummarize Cost SelfCost Day GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others\n\n\n\n为了稳健性，对因变量进行缩尾处理。\n*- 缩尾处理（目的：剔除异常值；剔除的比例根据研究而定）\nwinsor2 Cost Day Age, replace cuts(1, 99)\n\n// 全局暂元\nglobal Control Age Gender Career Marriage category Disease Opera Comorbidity Cmedicine Insurance \n\n// 基准模型\nreg Cost DIP $Control, r\nest store m1\nreg SelfCost DIP $Control, r\nest store m2\nreg Day DIP $Control Sensitive, r // Sensitive 只在Day的模型中出现\nest store m3\nreg Cost DIP $Control Day, r\nest store m4\nreg SelfCost DIP $Control Day, r\nest store m5\n\n* 输出基准模型结果\nesttab m1 m2 m3 m4 m5 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\基准模型结果.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\n// 调节效应分析\nglobal Control Age Gender Career Marriage Disease Opera Comorbidity Cmedicine Insurance\n\nreg Cost DIP $Control Day if category == 1, r\nest store m1\nreg Cost DIP $Control Day if category == 0, r\nest store m2\n\n\n*- esttab m1 m2 m3 using \nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-cost&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\nreg SelfCost DIP $Control Day if category==1, r\nest store m1\nreg SelfCost DIP $Control Day if category==0, r\nest store m2\n\n\nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-self&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\nreg Day DIP $Control Sensitive if category==1, r\nest store m1\nreg Day DIP $Control Sensitive if category==0, r\nest store m2\n\n\nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-day&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html",
    "href": "Guide/R/2025-02-22-CLHLS.html",
    "title": "CLHLS Data Analysis by R",
    "section": "",
    "text": "这是一个使用 R 语言对 CLHLS 数据进行清洗和分析的工作文档。\n\n本文数据源来自北大开放研究数据平台。DVN/WBO7LK_2020\n使用 SAS 逐渐让我失去的耐心，极其臃肿和笨重，Vintage Car，交互页面也很糟糕，用起来很令人烦躁，遂改用 R 对数据进行分析。\n2025-03-06 R 也没那么好用，反而觉得 Stata 的简便也是一种优势所在。",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html#加载必要的包",
    "href": "Guide/R/2025-02-22-CLHLS.html#加载必要的包",
    "title": "CLHLS Data Analysis by R",
    "section": "",
    "text": "本文数据源来自北大开放研究数据平台。DVN/WBO7LK_2020\n使用 SAS 逐渐让我失去的耐心，极其臃肿和笨重，Vintage Car，交互页面也很糟糕，用起来很令人烦躁，遂改用 R 对数据进行分析。\n2025-03-06 R 也没那么好用，反而觉得 Stata 的简便也是一种优势所在。",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html#数据导出",
    "href": "Guide/R/2025-02-22-CLHLS.html#数据导出",
    "title": "CLHLS Data Analysis by R",
    "section": "\n2.1 数据导出",
    "text": "2.1 数据导出\n\nlibrary(writexl)  \n# 12. 保存描述性统计表格  \nwrite_xlsx(final_summary, \"C:/Users/asus/Desktop/test/CLHLS/Analysis-0214/Rsummary0223.xlsx\")  \n# 13. 保存结果  \nwrite_xlsx(final_data, \"C:/Users/asus/Desktop/test/CLHLS/Analysis-0214/final_data0223.xlsx\")",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html#描述性统计",
    "href": "Guide/R/2025-02-22-CLHLS.html#描述性统计",
    "title": "CLHLS Data Analysis by R",
    "section": "\n2.2 描述性统计",
    "text": "2.2 描述性统计",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html#构建logistic回归方程",
    "href": "Guide/R/2025-02-22-CLHLS.html#构建logistic回归方程",
    "title": "CLHLS Data Analysis by R",
    "section": "\n3.1 构建Logistic回归方程",
    "text": "3.1 构建Logistic回归方程",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "R introduction",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/R/R-intro.html",
    "href": "Guide/R/R-intro.html",
    "title": "01-R 相关介绍与记录",
    "section": "",
    "text": "R语言是一种自由软件编程语言与操作环境，主要用于统计分析、绘图以及数据挖掘。R由新西兰奥克兰大学的统计学家罗斯·伊哈卡和罗伯特·杰特曼开发，现在由R核心小组负责开发，同时也有其他用户编写了诸多外挂的软件包。R以S语言为基础，其词法作用域语义来自Scheme。R的后台程序大多由C语言、FORTRAN语言和R自己写成。\nR 语言是为数学研究工作者设计的一种数学编程语言，主要用于统计分析、绘图、数据挖掘。\n如果你是一个计算机程序的初学者并且急切地想了解计算机的通用编程，R 语言不是一个很理想的选择，可以选择 Python、C 或 Java。\nR 语言与 C 语言都是贝尔实验室的研究成果，但两者有不同的侧重领域，R 语言是一种解释型的面向数学理论研究工作者的语言，而 C 语言是为计算机软件工程师设计的。\nR 语言是解释运行的语言（与 C 语言的编译运行不同），它的执行速度比 C 语言慢得多，不利于优化。但它在语法层面提供了更加丰富的数据结构操作并且能够十分方便地输出文字和图形信息，所以它广泛应用于数学尤其是统计学领域https://www.runoob.com/r/r-tutorial.html\n近些年 R 的发展也是极为迅速，在 Rstudio 改名为 Posit 后，R 的生态在快速发展。\n本网站就是其中一个分支的成果： Quarto\n\nAn open-source scientific and technical publishing system\n\n其他的还有诸如：\n\n0.1 Posit Workbench（数据科学家协同开发平台）\n\nJupyter, RStudio, and VS Code environments centrally maintained and ready to use\n\n\n\n0.2 MLOps（机器学习模型部署）\n\nMachine learning operations, or MLOps, is a set of practices to deploy and maintain machine learning models in production reliably and efficiently. The vetiver framework is for MLOps tasks in Python and R.\n\n\n\n0.3 Shiny for python\n\nEffortless Python web applications with the power of reactive programming.",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "01-R 相关介绍与记录"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html",
    "href": "Guide/SAS/SAS-install.html",
    "title": "SAS Install",
    "section": "",
    "text": "SAS Install",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "SAS introduction",
      "SAS Install"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "医学统计学习笔记",
    "section": "",
    "text": "本书的前身是开始于23年夏天的Rmarkdown笔记的制作，在考研的过程中，一些无聊又不想学习的时间被用来整理这些笔记和制作文档与网页，由于Quarto的出现和发展，感觉到它的强大与便利，遂用Quarto重制。\n现在重制这本学习笔记，按照初步构想，会继续完善卫生/医学统计学部分的内容，从低阶往高阶完善；再就是记录相关程序的使用技巧和学习记录（R、Python、SAS etc.）；还有有一部分的项目分享（如果顺利的话）和读书报告。\nEpidemiologists, in response to a health emergency or as a result of systematic surveillance, first obtain and analyze observed data. They use data, observations, science, and theory as they work at identifying a pathogen (when unknown) behind an observed disease outbreak or as they proceed to plan or implement policies that ameliorates its impact.\n按照大类再细分的话会有如下：\n\n统计学\n\n医学统计学（基础篇）\n高级医学统计学\n\n统计软件的使用\n\nR\nPython\nSAS\nStata\nQuarto\n\n项目记录\n\nKaggle\n\n读书笔记\n\n\n\n引用格式BibTeX@online{zhou2025,\n  author = {ZHOU, LB},\n  title = {医学统计学习笔记},\n  date = {2025-07-25},\n  url = {https://github.com/pumc-zhou/Med-Stat-Notes},\n  langid = {zh}\n}\n请按如下格式引用：\nZHOU, LB. 2025. “医学统计学习笔记.” July 25, 2025. https://github.com/pumc-zhou/Med-Stat-Notes.",
    "crumbs": [
      "Home",
      "主页"
    ]
  },
  {
    "objectID": "Learn/Basic/01-preview.html",
    "href": "Learn/Basic/01-preview.html",
    "title": "关于卫生统计学",
    "section": "",
    "text": "卫生统计学是一门致力于收集、分析和解释与健康相关的数据的学科。它的目标是通过统计方法来评估和改善人群的健康状况，从而提高公共卫生水平。卫生统计学将统计学原理应用于医学和公共卫生领域，以支持健康决策的制定和实施。\n\n\n\n卫生统计学在以下几个方面发挥着重要作用：\n\n疾病监测与控制： 通过收集和分析疾病发病率、死亡率和流行病学数据，卫生统计学帮助识别疾病的流行趋势，并制定相应的预防和控制策略。\n健康政策制定： 卫生统计学提供了评估不同健康政策和干预措施效果的方法，为政策制定者提供了决策支持。\n卫生服务评估： 通过分析卫生服务的覆盖范围、质量和效率，卫生统计学评估卫生系统的运作情况，并提供改进建议。\n流行病学研究： 卫生统计学在研究人群健康与疾病之间的关系方面发挥着关键作用，帮助揭示疾病的发病机制和影响因素。\n\n\n\n\n随着数据科学和人工智能技术的发展，卫生统计学将迎来新的机遇和挑战：\n\n大数据与人工智能： 大数据技术使得收集、整合和分析海量的健康数据成为可能，而人工智能技术则提供了更高效、精确的数据处理和预测能力，为卫生统计学研究提供了新的方法和工具。\n个性化医疗： 基于个体遗传信息和生活方式数据的个性化医疗将成为未来的发展趋势，卫生统计学将在个体化医疗决策和健康管理中发挥更加重要的作用。\n跨学科合作： 卫生统计学将与流行病学、遗传学、生物信息学等学科交叉融合，形成跨学科合作的新模式，共同解决健康领域的复杂问题。\n公众参与与健康促进： 卫生统计学将更加注重公众参与和社区健康促进，通过社会行为和环境因素的分析，促进健康政策的制定和实施，推动社会健康公平。\n\n随着社会的发展和健康需求的不断变化，卫生统计学将继续发挥着重要的作用，为促进全民健康、预防疾病、改善医疗服务质量和推动卫生政策提供支持和指导。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "关于卫生统计学"
    ]
  },
  {
    "objectID": "Learn/Basic/01-preview.html#什么是卫生统计学",
    "href": "Learn/Basic/01-preview.html#什么是卫生统计学",
    "title": "关于卫生统计学",
    "section": "",
    "text": "卫生统计学是一门致力于收集、分析和解释与健康相关的数据的学科。它的目标是通过统计方法来评估和改善人群的健康状况，从而提高公共卫生水平。卫生统计学将统计学原理应用于医学和公共卫生领域，以支持健康决策的制定和实施。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "关于卫生统计学"
    ]
  },
  {
    "objectID": "Learn/Basic/01-preview.html#卫生统计学的重要性",
    "href": "Learn/Basic/01-preview.html#卫生统计学的重要性",
    "title": "关于卫生统计学",
    "section": "",
    "text": "卫生统计学在以下几个方面发挥着重要作用：\n\n疾病监测与控制： 通过收集和分析疾病发病率、死亡率和流行病学数据，卫生统计学帮助识别疾病的流行趋势，并制定相应的预防和控制策略。\n健康政策制定： 卫生统计学提供了评估不同健康政策和干预措施效果的方法，为政策制定者提供了决策支持。\n卫生服务评估： 通过分析卫生服务的覆盖范围、质量和效率，卫生统计学评估卫生系统的运作情况，并提供改进建议。\n流行病学研究： 卫生统计学在研究人群健康与疾病之间的关系方面发挥着关键作用，帮助揭示疾病的发病机制和影响因素。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "关于卫生统计学"
    ]
  },
  {
    "objectID": "Learn/Basic/01-preview.html#卫生统计学的未来展望",
    "href": "Learn/Basic/01-preview.html#卫生统计学的未来展望",
    "title": "关于卫生统计学",
    "section": "",
    "text": "随着数据科学和人工智能技术的发展，卫生统计学将迎来新的机遇和挑战：\n\n大数据与人工智能： 大数据技术使得收集、整合和分析海量的健康数据成为可能，而人工智能技术则提供了更高效、精确的数据处理和预测能力，为卫生统计学研究提供了新的方法和工具。\n个性化医疗： 基于个体遗传信息和生活方式数据的个性化医疗将成为未来的发展趋势，卫生统计学将在个体化医疗决策和健康管理中发挥更加重要的作用。\n跨学科合作： 卫生统计学将与流行病学、遗传学、生物信息学等学科交叉融合，形成跨学科合作的新模式，共同解决健康领域的复杂问题。\n公众参与与健康促进： 卫生统计学将更加注重公众参与和社区健康促进，通过社会行为和环境因素的分析，促进健康政策的制定和实施，推动社会健康公平。\n\n随着社会的发展和健康需求的不断变化，卫生统计学将继续发挥着重要的作用，为促进全民健康、预防疾病、改善医疗服务质量和推动卫生政策提供支持和指导。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "关于卫生统计学"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html",
    "href": "Learn/Basic/03-Random-events-probabilities.html",
    "title": "随机事件的概率",
    "section": "",
    "text": "不确定的知识+所含不确定性度量的知识=可用的知识\n\n\n\n确定性现象（deterministic phenomenon）：在一定条件下必然会发生的现象；\n随机现象（random phenomenon）：在同一条件下具有不确定结果的现象。\n\n对随机现象获得一个观察或进行一次测量的过程称为随机试验，简称试验，用\\(E\\)表示。\n用\\(\\omega\\)表示试验\\(E\\)的一个可能的结果，则称\\(\\omega\\)为\\(E\\)的一个基本事件（elementary event）；基本事件是指不能在分解为更简单结果的事件。\n\n\n\n\n包含关系：对任意两事件A和B，如果事件B发生，则事件A必发生，则称事件A包含事件B，记作\\(A\\supseteq B\\)或\\(B\\subseteq A\\)，符号\\(\\supseteq\\)和 \\(\\subseteq\\)分别表示包含与被包含。\n相等关系：如果事件A和事件B满足以下关系：\\(A\\supseteq B 且B\\supseteq A\\)，则称A和B相等，记作\\(A=B\\)\n事件的和：在一次试验中，对任意两事件A和B，“A和B中至少有一个发生”也是一个事件，称此事件为A和B的“和或并”，记作\\(A\\cup B\\)，也可以表示为\\(A+B\\)。\n事件的交：在一次试验中，对任意两事件A和B，“A和B同时发生”也是一个事件，称此事件为A和B的“交”，记作\\(A\\cap B\\)，也可以表示为\\(AB\\)。\n事件的差：在一次试验中，“A发生且B不发生”也是一个事件，称此事件为A和B的“差”，记作\\(A-B\\)。\n互不相容事件：在一次试验中，“A与B不能同时发生”，即\\(A\\cup B=\\emptyset\\)，则称此事件A和事件B为互不相容事件，也称互斥事件。\n对立事件：是一种特殊的互不相容事件，若“事件A与事件B不能同时发生，且他们的和组成样本空间”，即\\(A\\cap B=\\emptyset 且A\\cup B=\\Omega\\)，则称A和B互为对立事件（complementary events）。\n\n\n\n\n\n频率（frequency）：设E为一随机试验，A是其中一事件，在同样条件下把E重复的做n次，以m表示事件A在这n次试验中发生的次数，则称比值\\(m/n\\)为事件A发生的频率，记为F(A)： \\[F(A)=\\frac{m}{n}\\]\n概率（probability）：设在同一条件下，重复进行n次试验，随机事件A发生m次，若试验次数n无限大时，频率\\(m/n\\)将在某一确定值p的附近摆动，则称p为事件A的概率，记为P(A)： \\[P(A)=p \\approx \\frac{m}{n}\\]\n概率有时也被称为相对频率方法；概率是事物固有的属性，不以人的主观意志为转移。\n概率的描述性定义\n概率的公理化定义：\n\n\n非负性:对于任意事件A，有\\(P(A)\\geq 0\\)\n规范性:\\(P(\\Omega)=1\\)\n可加可列性:\\(P(\\bigcup \\limits_{i=1}^{\\infty}A_i)=\\sum \\limits_{i=1}^{\\infty}P(A_i)\\)\n\n\n\n\n若A和B是样本空间\\(\\Omega\\)中两个互不相容的事件，则事件和的概率等于两事件概率之和，即：\n\\[P(A+B)=P(A)+P(B)\\] 这称之为概率的加法公式。此公式要求事件A和事件B互不相容。\n易知，A的对立事件\\(\\bar A\\)的概率\\(P(\\bar A)=1-P(A)\\)，加法公式可自然推广到多个事件的情形。\n对于任意事件A和B，即事件A和不是互不相容，更一般有： \\[P(A+B)=P(A)+P(B)-P(AB)\\] 因为当A和B互不相容时，有\\(P(AB)=0\\)。\n\n\n\n当存在某些可能影响结果的条件时，事件发生的概率可能会改变，我们称这种情况下的概率为条件概率。\n\n条件概率\n\n在已知事件A发生的条件下，事件B发生的概率成为条件概率（conditional probability），记为\\(P(B\\mid A)\\)，公式为： \\[P(B\\mid A)=\\frac{P(AB)}{P(A)},P(A)&gt;0\\] 即条件概率等于事件A和B同时发生的概率除以事件A发生的概率。\n\n概率乘法公式\n\n\\[P(AB)=P(A)P(B\\mid A),P(A)&gt;0\\\\\nP(AB)=P(B)P(A\\mid B),P(B)&gt;0\\]\n\n独立事件\n\n如果事件B发生的概率不受事件A发生概率的影响，即\\(P(B\\mid A)=P(B)\\)，则称事件B对事件A独立。由于两事件间的独立总是相互的，故也有\\(P(A\\mid B)=P(A)\\)。\n根据\\(P(AB)=P(B)P(A\\mid B),P(B)&gt;0\\)，若事件A、B独立，则有 \\[P(AB)=P(A)P(B)\\]\n在医学研究中，可以根据试验条件及生物学知识判断事物之间的独立性。\n\n\n\n\n全概率公式\n\n如果事件组\\(A_1,A_2,\\dots,A_n\\)满足以下两个条件：\n\n\\(A_1,A_2,\\dots,A_n\\)互不相容，且\\(P(A_i)&gt;0(i=1,2,\\dots,n)\\);\n\\(A_1+A_2+\\dots+A_n=\\Omega\\)\n\n那么对于任意事件B，都有\n\\[P(B)=\\sum_\\limits{i=1}^{n}P(A_i)P(B\\mid A_i)\\]\n此公式成为全概率公式（total probability formula），即B的概率可以表示为在给定\\(A_i\\)发生条件下B发生的条件概率的加权平均。\n\nBayes公式\n\n对于n个互不相容的事件\\(A_1,A_2,\\dots,A_n\\)，且他们的和为必然事件，则在时间B发生的前提下事件\\(A_k(k=1,2,\\dots,n)\\)发生的概率为：\n\\[P(A_k\\mid B)=\\frac{P(A_k)P(B\\mid A_k)}{\\sum_\\limits{i=1}^{n}P(A_i)P(B\\mid A_i)},(k=1,2,\\dots,n),P(B)&gt;0\\]\nBayes公式的意义在于，它可以改变条件概率结论的方向，即在知道结果的情况下来推断原因，用式子\\(P(A_k\\mid B)\\)表示，称为后验概率(posterior probability)；而\\(P(A_k)\\)表示各种原因出现可能性的大小，一般是过去经验的总结，称为先验概率(prior probability)。\n\n\n在流行病学领域，研究人员一直在寻找提高分析准确性和可靠性的方法。一种越来越流行的方法是使用贝叶斯方法，该方法为处理复杂数据和整合先验知识提供了独特的视角。\n\n为什么使用贝叶斯方法？\n\n贝叶斯方法允许流行病学家纳入关于该疾病的现有知识或信念或风险因素纳入分析，从而获得更准确、更翔实的结果。\n\n处理复杂模型\n\n贝叶斯方法非常适合分析复杂的流行病学模型具有许多参数和相互作用，因为它们可以解释这些参数的不确定性。\n\n处理缺失数据\n\n贝叶斯方法可以将缺失数据视为另一个需要估计的未知参数来处理，与排除或归咎于缺失数据的传统方法相比，这可以产生更准确的结果。\n\n提供概率解释\n\n贝叶斯方法以概率分布的形式提供结果，比频率学派方法获得的点估计和置信区间更直观、更具信息量。\n\n\n\n在人卫版《流行病学》第8版中，第七章-筛检，关于预测值的计算，该指标反映了筛检试验实际应用到人群筛查后，获得的首医收益大小。\n在医院开展的基于病例-非病例设计的筛查试验研究，病例组和非病例组的构成比不能代表目标人群的现患与未患比例，因此不能直接计算预测值。此时，可以根据灵敏度、特异度、现患率与预测值的关系式（Bayes公式）来估算预测值。\n[PPV=] [NPV=]\n公式中，PPV是阳性预测值，NPV是阴性预测值；SE是灵敏度，SP是特异度，P是患病率。\n流行病学中的贝叶斯方法:简介\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#随机事件与样本空间",
    "href": "Learn/Basic/03-Random-events-probabilities.html#随机事件与样本空间",
    "title": "随机事件的概率",
    "section": "",
    "text": "确定性现象（deterministic phenomenon）：在一定条件下必然会发生的现象；\n随机现象（random phenomenon）：在同一条件下具有不确定结果的现象。\n\n对随机现象获得一个观察或进行一次测量的过程称为随机试验，简称试验，用\\(E\\)表示。\n用\\(\\omega\\)表示试验\\(E\\)的一个可能的结果，则称\\(\\omega\\)为\\(E\\)的一个基本事件（elementary event）；基本事件是指不能在分解为更简单结果的事件。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#事件的运算",
    "href": "Learn/Basic/03-Random-events-probabilities.html#事件的运算",
    "title": "随机事件的概率",
    "section": "",
    "text": "包含关系：对任意两事件A和B，如果事件B发生，则事件A必发生，则称事件A包含事件B，记作\\(A\\supseteq B\\)或\\(B\\subseteq A\\)，符号\\(\\supseteq\\)和 \\(\\subseteq\\)分别表示包含与被包含。\n相等关系：如果事件A和事件B满足以下关系：\\(A\\supseteq B 且B\\supseteq A\\)，则称A和B相等，记作\\(A=B\\)\n事件的和：在一次试验中，对任意两事件A和B，“A和B中至少有一个发生”也是一个事件，称此事件为A和B的“和或并”，记作\\(A\\cup B\\)，也可以表示为\\(A+B\\)。\n事件的交：在一次试验中，对任意两事件A和B，“A和B同时发生”也是一个事件，称此事件为A和B的“交”，记作\\(A\\cap B\\)，也可以表示为\\(AB\\)。\n事件的差：在一次试验中，“A发生且B不发生”也是一个事件，称此事件为A和B的“差”，记作\\(A-B\\)。\n互不相容事件：在一次试验中，“A与B不能同时发生”，即\\(A\\cup B=\\emptyset\\)，则称此事件A和事件B为互不相容事件，也称互斥事件。\n对立事件：是一种特殊的互不相容事件，若“事件A与事件B不能同时发生，且他们的和组成样本空间”，即\\(A\\cap B=\\emptyset 且A\\cup B=\\Omega\\)，则称A和B互为对立事件（complementary events）。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#概率的定义",
    "href": "Learn/Basic/03-Random-events-probabilities.html#概率的定义",
    "title": "随机事件的概率",
    "section": "",
    "text": "频率（frequency）：设E为一随机试验，A是其中一事件，在同样条件下把E重复的做n次，以m表示事件A在这n次试验中发生的次数，则称比值\\(m/n\\)为事件A发生的频率，记为F(A)： \\[F(A)=\\frac{m}{n}\\]\n概率（probability）：设在同一条件下，重复进行n次试验，随机事件A发生m次，若试验次数n无限大时，频率\\(m/n\\)将在某一确定值p的附近摆动，则称p为事件A的概率，记为P(A)： \\[P(A)=p \\approx \\frac{m}{n}\\]\n概率有时也被称为相对频率方法；概率是事物固有的属性，不以人的主观意志为转移。\n概率的描述性定义\n概率的公理化定义：\n\n\n非负性:对于任意事件A，有\\(P(A)\\geq 0\\)\n规范性:\\(P(\\Omega)=1\\)\n可加可列性:\\(P(\\bigcup \\limits_{i=1}^{\\infty}A_i)=\\sum \\limits_{i=1}^{\\infty}P(A_i)\\)",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#概率的加法公式",
    "href": "Learn/Basic/03-Random-events-probabilities.html#概率的加法公式",
    "title": "随机事件的概率",
    "section": "",
    "text": "若A和B是样本空间\\(\\Omega\\)中两个互不相容的事件，则事件和的概率等于两事件概率之和，即：\n\\[P(A+B)=P(A)+P(B)\\] 这称之为概率的加法公式。此公式要求事件A和事件B互不相容。\n易知，A的对立事件\\(\\bar A\\)的概率\\(P(\\bar A)=1-P(A)\\)，加法公式可自然推广到多个事件的情形。\n对于任意事件A和B，即事件A和不是互不相容，更一般有： \\[P(A+B)=P(A)+P(B)-P(AB)\\] 因为当A和B互不相容时，有\\(P(AB)=0\\)。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#概率的乘法公式",
    "href": "Learn/Basic/03-Random-events-probabilities.html#概率的乘法公式",
    "title": "随机事件的概率",
    "section": "",
    "text": "当存在某些可能影响结果的条件时，事件发生的概率可能会改变，我们称这种情况下的概率为条件概率。\n\n条件概率\n\n在已知事件A发生的条件下，事件B发生的概率成为条件概率（conditional probability），记为\\(P(B\\mid A)\\)，公式为： \\[P(B\\mid A)=\\frac{P(AB)}{P(A)},P(A)&gt;0\\] 即条件概率等于事件A和B同时发生的概率除以事件A发生的概率。\n\n概率乘法公式\n\n\\[P(AB)=P(A)P(B\\mid A),P(A)&gt;0\\\\\nP(AB)=P(B)P(A\\mid B),P(B)&gt;0\\]\n\n独立事件\n\n如果事件B发生的概率不受事件A发生概率的影响，即\\(P(B\\mid A)=P(B)\\)，则称事件B对事件A独立。由于两事件间的独立总是相互的，故也有\\(P(A\\mid B)=P(A)\\)。\n根据\\(P(AB)=P(B)P(A\\mid B),P(B)&gt;0\\)，若事件A、B独立，则有 \\[P(AB)=P(A)P(B)\\]\n在医学研究中，可以根据试验条件及生物学知识判断事物之间的独立性。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#全概率公式与bayes公式",
    "href": "Learn/Basic/03-Random-events-probabilities.html#全概率公式与bayes公式",
    "title": "随机事件的概率",
    "section": "",
    "text": "全概率公式\n\n如果事件组\\(A_1,A_2,\\dots,A_n\\)满足以下两个条件：\n\n\\(A_1,A_2,\\dots,A_n\\)互不相容，且\\(P(A_i)&gt;0(i=1,2,\\dots,n)\\);\n\\(A_1+A_2+\\dots+A_n=\\Omega\\)\n\n那么对于任意事件B，都有\n\\[P(B)=\\sum_\\limits{i=1}^{n}P(A_i)P(B\\mid A_i)\\]\n此公式成为全概率公式（total probability formula），即B的概率可以表示为在给定\\(A_i\\)发生条件下B发生的条件概率的加权平均。\n\nBayes公式\n\n对于n个互不相容的事件\\(A_1,A_2,\\dots,A_n\\)，且他们的和为必然事件，则在时间B发生的前提下事件\\(A_k(k=1,2,\\dots,n)\\)发生的概率为：\n\\[P(A_k\\mid B)=\\frac{P(A_k)P(B\\mid A_k)}{\\sum_\\limits{i=1}^{n}P(A_i)P(B\\mid A_i)},(k=1,2,\\dots,n),P(B)&gt;0\\]\nBayes公式的意义在于，它可以改变条件概率结论的方向，即在知道结果的情况下来推断原因，用式子\\(P(A_k\\mid B)\\)表示，称为后验概率(posterior probability)；而\\(P(A_k)\\)表示各种原因出现可能性的大小，一般是过去经验的总结，称为先验概率(prior probability)。\n\n\n在流行病学领域，研究人员一直在寻找提高分析准确性和可靠性的方法。一种越来越流行的方法是使用贝叶斯方法，该方法为处理复杂数据和整合先验知识提供了独特的视角。\n\n为什么使用贝叶斯方法？\n\n贝叶斯方法允许流行病学家纳入关于该疾病的现有知识或信念或风险因素纳入分析，从而获得更准确、更翔实的结果。\n\n处理复杂模型\n\n贝叶斯方法非常适合分析复杂的流行病学模型具有许多参数和相互作用，因为它们可以解释这些参数的不确定性。\n\n处理缺失数据\n\n贝叶斯方法可以将缺失数据视为另一个需要估计的未知参数来处理，与排除或归咎于缺失数据的传统方法相比，这可以产生更准确的结果。\n\n提供概率解释\n\n贝叶斯方法以概率分布的形式提供结果，比频率学派方法获得的点估计和置信区间更直观、更具信息量。\n\n\n\n在人卫版《流行病学》第8版中，第七章-筛检，关于预测值的计算，该指标反映了筛检试验实际应用到人群筛查后，获得的首医收益大小。\n在医院开展的基于病例-非病例设计的筛查试验研究，病例组和非病例组的构成比不能代表目标人群的现患与未患比例，因此不能直接计算预测值。此时，可以根据灵敏度、特异度、现患率与预测值的关系式（Bayes公式）来估算预测值。\n[PPV=] [NPV=]\n公式中，PPV是阳性预测值，NPV是阴性预测值；SE是灵敏度，SP是特异度，P是患病率。\n流行病学中的贝叶斯方法:简介\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html",
    "title": "连续型随机变量的概率分布",
    "section": "",
    "text": "若随机变量X的密度函数是\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, (-\\infty&lt;x&lt;+\\infty)\\] 则称X服从正态分布，记为\\(X\\sim N(\\mu,\\sigma^2)\\)。\n\n\n\n\nNormal Curve comparsion\n\n\n\n\n正态分布(Normal Distribution)：正态分布是最重要的连续型分布，随机变量\\(X\\)服从均数为\\(\\mu\\)，标准差为\\(\\sigma\\)的正态分布，记为\\(X\\sim N(\\mu,\\sigma^{2})\\)。\n正态曲线（Normal curve）：即正态分布曲线，\\(\\mu\\)和\\(\\sigma\\)是正态分布的两个参数。\n\n\n\n\n\nNormal Curve\n\n\n\n\n性质\n\n\n正态曲线在横轴上方均数处最高\n\n正态分布以均数为中心，左右对称\n\n正态分布有两个参数，即位置参数\\(\\mu\\)和形态参数\\(\\sigma\\)\n\n固定\\(\\sigma\\)，改变\\(\\mu\\)值，形态不变，曲线沿着\\(X\\)轴平行移动\n固定\\(\\mu\\)，改变\\(\\sigma\\)值，中心在\\(X\\)轴的位置不变\n\n\n\\(\\sigma\\)越小，曲线越陡峭\\(\\to\\)瘦高\n\n\\(\\sigma\\)越大，曲线越低平\\(\\to\\)矮胖\n\n\n正态分布的可加性，当随机变量X服从正态分布\\(N(\\mu_1,\\sigma_1^2)\\)，Y服从正态分布\\(N(\\mu_2,\\sigma_2^2)\\)，X与Y独立，则\\(X-Y\\)服从\\(N(\\mu_1-\\mu_2,\\sigma_1^2+\\sigma_2^2)\\)的正态分布\n\n\n\n\n\n\n\nDifferent Normal Curve\n\n\n\n\n标准正态随机变量U的密度函数用\\(\\varphi(u)\\)表示，为： \\[\\varphi(u)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{u^2}{2}},(-\\infty&lt;x&lt;+\\infty)\\]\n\n标准正态分布（Standard normal distribution）：是一种特殊的正态分布，通常用\\(U\\)或\\(Z\\)表示服从标准正态分布的变量，此时称随机变量\\(X\\)服从均数为0，标准差为1的标准正态分布，记为\\(X \\sim N(0,1)\\)\n\n\n\n正态分布：一簇曲线\n标准正态分布：一条曲线\n\n\n标准正态变换：Z变换、U变换\n\n\n疑难1：Z值到底表达什么意思？\n\n个体值到均值的距离，有多少个标准差 \\(Z = \\frac{X-\\mu}{\\sigma}\\)\n\n只有正态分布的资料才能通过Z变换变成标准正态分布\n\n\n疑难2：标准化变换的公式如何理解？\n\n个体值减去均值，除以标准差，均数和标准差由\\(\\mu,\\sigma\\)变为\\(0,1\\)\n\n\n\n\n\n\nTableGrob (1 x 2) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (1-1,2-2) arrange gtable[layout]\n\n\n\n\nNormalized Transformation\n\n\n\n\n\n正态分布的68-95-99.7法则\n\n\n\n\n\nNormal\n\n\n\n\n标准化转换，涉及到以下两个互逆计算\n\n\n估计某个随机变量在一定取值范围内的观测值个数占全部观测值数量的百分比\n通过已知的百分比，估计总体变量值的分布范围（本质同医学参考值范围的计算）\n\n\n运用正态近似法计算医学参考值范围\n\n\n\nMedical reference range\n\n\n运用正态近似法计算置信区间\n正态分布是很多统计学分析方法的理论基础\n\nnotice:\n\n正态曲线上的拐点所对应的横坐标为\\(\\mu ±\\sigma\\)。\n设随机变量\\(X\\)的概率密度曲线为\\(f(x)=\\frac{1}{2\\sqrt{p}}e^{\\frac{(x+2)^2}{4}}\\)，若要将\\(X\\)转化为服从标准正态分布的变量\\(\\mu\\)，则所采用的标准化变换为：\\(\\frac{X-2}{\\sqrt{2}}\\)（其原式为：\\(f(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\)，题目和原式中：\\(p=\\pi\\)）\n\n\n\n\n\nSkewed Curves\n\n\n\nnotice：\n\n左偏，左边尾长，平均数靠近左侧，平均数小于中位数小于众数，负偏态；\n右偏，右边尾长，平均数靠近右侧，平均数大于中位数大于众数，正偏态。\n\n\n\nconversion relationship",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#正态分布normal-distribution",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#正态分布normal-distribution",
    "title": "连续型随机变量的概率分布",
    "section": "",
    "text": "若随机变量X的密度函数是\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, (-\\infty&lt;x&lt;+\\infty)\\] 则称X服从正态分布，记为\\(X\\sim N(\\mu,\\sigma^2)\\)。\n\n\n\n\nNormal Curve comparsion\n\n\n\n\n正态分布(Normal Distribution)：正态分布是最重要的连续型分布，随机变量\\(X\\)服从均数为\\(\\mu\\)，标准差为\\(\\sigma\\)的正态分布，记为\\(X\\sim N(\\mu,\\sigma^{2})\\)。\n正态曲线（Normal curve）：即正态分布曲线，\\(\\mu\\)和\\(\\sigma\\)是正态分布的两个参数。\n\n\n\n\n\nNormal Curve\n\n\n\n\n性质\n\n\n正态曲线在横轴上方均数处最高\n\n正态分布以均数为中心，左右对称\n\n正态分布有两个参数，即位置参数\\(\\mu\\)和形态参数\\(\\sigma\\)\n\n固定\\(\\sigma\\)，改变\\(\\mu\\)值，形态不变，曲线沿着\\(X\\)轴平行移动\n固定\\(\\mu\\)，改变\\(\\sigma\\)值，中心在\\(X\\)轴的位置不变\n\n\n\\(\\sigma\\)越小，曲线越陡峭\\(\\to\\)瘦高\n\n\\(\\sigma\\)越大，曲线越低平\\(\\to\\)矮胖\n\n\n正态分布的可加性，当随机变量X服从正态分布\\(N(\\mu_1,\\sigma_1^2)\\)，Y服从正态分布\\(N(\\mu_2,\\sigma_2^2)\\)，X与Y独立，则\\(X-Y\\)服从\\(N(\\mu_1-\\mu_2,\\sigma_1^2+\\sigma_2^2)\\)的正态分布\n\n\n\n\n\n\n\nDifferent Normal Curve",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#标准正态分布",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#标准正态分布",
    "title": "连续型随机变量的概率分布",
    "section": "",
    "text": "标准正态随机变量U的密度函数用\\(\\varphi(u)\\)表示，为： \\[\\varphi(u)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{u^2}{2}},(-\\infty&lt;x&lt;+\\infty)\\]\n\n标准正态分布（Standard normal distribution）：是一种特殊的正态分布，通常用\\(U\\)或\\(Z\\)表示服从标准正态分布的变量，此时称随机变量\\(X\\)服从均数为0，标准差为1的标准正态分布，记为\\(X \\sim N(0,1)\\)\n\n\n\n正态分布：一簇曲线\n标准正态分布：一条曲线\n\n\n标准正态变换：Z变换、U变换\n\n\n疑难1：Z值到底表达什么意思？\n\n个体值到均值的距离，有多少个标准差 \\(Z = \\frac{X-\\mu}{\\sigma}\\)\n\n只有正态分布的资料才能通过Z变换变成标准正态分布\n\n\n疑难2：标准化变换的公式如何理解？\n\n个体值减去均值，除以标准差，均数和标准差由\\(\\mu,\\sigma\\)变为\\(0,1\\)\n\n\n\n\n\n\nTableGrob (1 x 2) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (1-1,2-2) arrange gtable[layout]\n\n\n\n\nNormalized Transformation\n\n\n\n\n\n正态分布的68-95-99.7法则\n\n\n\n\n\nNormal\n\n\n\n\n标准化转换，涉及到以下两个互逆计算\n\n\n估计某个随机变量在一定取值范围内的观测值个数占全部观测值数量的百分比\n通过已知的百分比，估计总体变量值的分布范围（本质同医学参考值范围的计算）\n\n\n运用正态近似法计算医学参考值范围\n\n\n\nMedical reference range\n\n\n运用正态近似法计算置信区间\n正态分布是很多统计学分析方法的理论基础\n\nnotice:\n\n正态曲线上的拐点所对应的横坐标为\\(\\mu ±\\sigma\\)。\n设随机变量\\(X\\)的概率密度曲线为\\(f(x)=\\frac{1}{2\\sqrt{p}}e^{\\frac{(x+2)^2}{4}}\\)，若要将\\(X\\)转化为服从标准正态分布的变量\\(\\mu\\)，则所采用的标准化变换为：\\(\\frac{X-2}{\\sqrt{2}}\\)（其原式为：\\(f(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\)，题目和原式中：\\(p=\\pi\\)）\n\n\n\n\n\nSkewed Curves\n\n\n\nnotice：\n\n左偏，左边尾长，平均数靠近左侧，平均数小于中位数小于众数，负偏态；\n右偏，右边尾长，平均数靠近右侧，平均数大于中位数大于众数，正偏态。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#小结",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#小结",
    "title": "连续型随机变量的概率分布",
    "section": "",
    "text": "conversion relationship",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#t分布",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#t分布",
    "title": "连续型随机变量的概率分布",
    "section": "\n2.1 t分布",
    "text": "2.1 t分布\n说起t分布，首先要提一句u分布，正态分布（Normal Distribution）是许多统计方法的理论基础。\n正态分布的两个参数\\(\\mu\\)和\\(\\sigma\\)决定了正态分布的位置和形态。为了应用方便，常将一般的正态变量X通过u变换\\([(X-\\mu)/\\sigma]\\)转化成标准正态变量u，以使原来各种形态的正态分布都转换为\\(\\mu=0,\\sigma=1\\)的标准正态分布(Standard Normal Distribution)，亦称u分布。\n根据中心极限定理，通过抽样模拟试验表明，在正态分布总体中以固定 n 抽取若干个样本时，样本均数的分布仍服从正态分布，即\\(N(\\mu,\\sigma)\\)。所以，对样本均数的分布进行u变换，也可变换为标准正态分布\\(N(0,1)\\)。\n由于在实际工作中，往往\\(\\sigma^2\\)(总体方差)是未知的，常用\\(s^2\\)(样本方差)作为\\(\\sigma^2\\)的估计值，为了与u变换区别，称为 t 变换，统计量 t 值的分布称为 t 分布。\n\n\n\n\nt-distribution Curves\n\n\n\n\nt 分布是英国统计学家 W.S. Gosset 在 1908 年以笔名 Student发表的论文中提出的, 故后人称为 “学生氏 (Student) 分布” 或 “t 分 布”。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#f分布",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#f分布",
    "title": "连续型随机变量的概率分布",
    "section": "\n2.2 F分布",
    "text": "2.2 F分布\n\n\n\n\nF-distribution Curves\n\n\n\n\n2.2.1 F分布的应用\n\n方差的同质性检验 组与组之间的差异称组间变异（variation between classes），反映在各组的平均数不同。同一组内部被试（个体）之间的差异称组内变异（variation within class），反映在每一个个体之间的差异。\n总变异的分解：\n\n\n总变异 = 组间变异+组内变异\n组间变异 = 实验条件 + 随机误差\n组内变异 = 个体差异 + 实验误差 。组内误差都是随机误差。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#chi2分布",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#chi2分布",
    "title": "连续型随机变量的概率分布",
    "section": "\n2.3 \\(\\chi^2\\)分布",
    "text": "2.3 \\(\\chi^2\\)分布\n\n\n\n\nChi-square Distribution Curves\n\n\n\n\n2.3.1 卡方检验应用\n\n检验连续变量的分布是否与某种理论分布一致。\n检验某个分类变量各类的出现概率是否等于指定概率。\n检验某两种方法的结果是否一致。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/07-parameter-test.html",
    "href": "Learn/Basic/07-parameter-test.html",
    "title": "参数检验",
    "section": "",
    "text": "维度\n参数检验（Parameter test）\n非参数检验（Non-parameter tests）\n\n\n\n定义\n以特定的总体分布为前提\\(\\rightarrow\\)?\n不依赖于总体分布特征\\(\\rightarrow\\)?\n\n\n举例\n\n\\(Z\\)检验、\\(t\\)分布、\\(F\\)检验\n秩和检验（Rank sum test）、卡方检验\n\n\n优点\n1. 直接利用原始观测值计算统计量，检验效能高；2.可对总体参数做出估计\n1. 适用范围广、收集资料方便；2. 多数非参数检验方法比较简便、易于掌握\n\n\n缺点\n对数据分布有特定要求，适用范围窄\n1. 没有充分利用原始数据，检验效能低；2. 不能对总体参数做出推断\n\n\n适用范围\n必须符合相应的要求，如两样本t检验要求：独立、正态、方差齐\n1. 总体分布形式未知、分布类型不明确、偏态分布数据；2. 等级资料；3. 不满足参数检验条件的数据；4. 数据一段或两端为无法测量的数值等。\n\n\n选用原则\n1. 如果数据符合参数检验条件，或经过变换后符合参数检验的条件，最好用参数检验；2. 参数检验误用为非参数检验，会导致检验效能降低。\n\n\n\n\n\n\n\n\n\n\n类目\n\n\\(t\\)分布\n\n\n\n概念\n设从正态分布\\(N(\\mu,\\sigma^2)\\)随机抽取含量为n的样本，样本均数为\\(\\bar x\\)、标准差为\\(s\\)、则\\(t=\\frac{\\bar x-\\mu}{s_{\\bar x}}=\\frac{\\bar x-\\mu}{s/\\sqrt{n}}\\)，自由度为\\(n-1\\)。\n\n\n图形特点\n一簇以0为中心，左右对称的单峰曲线； 但随着自由度的增加，\\(t\\)分布曲线将越来越接近于标准正态分布曲线\n\n\n统计量值\n\n\\(t\\)的取值范围\\(-\\infty \\sim +\\infty\\)\n\n\n\n自由度\n\\(v=n-1\\)\n\n\n\n\n\n\n\nt-Distribution Curves vs. Standard Normal Curve\n\n\n\n\n\n\n\n\n正态（或正态近似法）\nt分布法\n\n\n\n\n样本均数的中心极限定理。从任意均数等于\\(\\mu\\)，方差等于\\(\\sigma^2\\)的一个总体中抽取样本量为\\(n\\)的简单随机样本，当样本量\\(n\\)很大时，无论总体分布形态如何，样本均数的抽样分布近似服从正态分布。\n样本率的中心极限定理。从“成功”率为\\(\\pi\\)的总体中随机抽取样本量为\\(n\\)的样本，其样本“成功”率用\\(p\\)表示，当\\(n\\pi&gt;5\\)且\\(n(1-\\pi)&gt;5\\)时，样本率\\(p\\)近似服从正态分布。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/07-parameter-test.html#参数检验和非参数检验的区别",
    "href": "Learn/Basic/07-parameter-test.html#参数检验和非参数检验的区别",
    "title": "参数检验",
    "section": "",
    "text": "维度\n参数检验（Parameter test）\n非参数检验（Non-parameter tests）\n\n\n\n定义\n以特定的总体分布为前提\\(\\rightarrow\\)?\n不依赖于总体分布特征\\(\\rightarrow\\)?\n\n\n举例\n\n\\(Z\\)检验、\\(t\\)分布、\\(F\\)检验\n秩和检验（Rank sum test）、卡方检验\n\n\n优点\n1. 直接利用原始观测值计算统计量，检验效能高；2.可对总体参数做出估计\n1. 适用范围广、收集资料方便；2. 多数非参数检验方法比较简便、易于掌握\n\n\n缺点\n对数据分布有特定要求，适用范围窄\n1. 没有充分利用原始数据，检验效能低；2. 不能对总体参数做出推断\n\n\n适用范围\n必须符合相应的要求，如两样本t检验要求：独立、正态、方差齐\n1. 总体分布形式未知、分布类型不明确、偏态分布数据；2. 等级资料；3. 不满足参数检验条件的数据；4. 数据一段或两端为无法测量的数值等。\n\n\n选用原则\n1. 如果数据符合参数检验条件，或经过变换后符合参数检验的条件，最好用参数检验；2. 参数检验误用为非参数检验，会导致检验效能降低。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/07-parameter-test.html#t分布",
    "href": "Learn/Basic/07-parameter-test.html#t分布",
    "title": "参数检验",
    "section": "",
    "text": "类目\n\n\\(t\\)分布\n\n\n\n概念\n设从正态分布\\(N(\\mu,\\sigma^2)\\)随机抽取含量为n的样本，样本均数为\\(\\bar x\\)、标准差为\\(s\\)、则\\(t=\\frac{\\bar x-\\mu}{s_{\\bar x}}=\\frac{\\bar x-\\mu}{s/\\sqrt{n}}\\)，自由度为\\(n-1\\)。\n\n\n图形特点\n一簇以0为中心，左右对称的单峰曲线； 但随着自由度的增加，\\(t\\)分布曲线将越来越接近于标准正态分布曲线\n\n\n统计量值\n\n\\(t\\)的取值范围\\(-\\infty \\sim +\\infty\\)\n\n\n\n自由度\n\\(v=n-1\\)\n\n\n\n\n\n\n\nt-Distribution Curves vs. Standard Normal Curve",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/07-parameter-test.html#一个正态总体参数的估计",
    "href": "Learn/Basic/07-parameter-test.html#一个正态总体参数的估计",
    "title": "参数检验",
    "section": "",
    "text": "正态（或正态近似法）\nt分布法",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/07-parameter-test.html#小结",
    "href": "Learn/Basic/07-parameter-test.html#小结",
    "title": "参数检验",
    "section": "",
    "text": "样本均数的中心极限定理。从任意均数等于\\(\\mu\\)，方差等于\\(\\sigma^2\\)的一个总体中抽取样本量为\\(n\\)的简单随机样本，当样本量\\(n\\)很大时，无论总体分布形态如何，样本均数的抽样分布近似服从正态分布。\n样本率的中心极限定理。从“成功”率为\\(\\pi\\)的总体中随机抽取样本量为\\(n\\)的样本，其样本“成功”率用\\(p\\)表示，当\\(n\\pi&gt;5\\)且\\(n(1-\\pi)&gt;5\\)时，样本率\\(p\\)近似服从正态分布。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html",
    "href": "Learn/Basic/09-non-parameter-test.html",
    "title": "非参数检验",
    "section": "",
    "text": "资料特征\n数据特征\n\n完全随机设计\n\n配对设计\n随机区组\n\n\n\n\n\n\n单组\n两组\n多组\n\n\n\n\n分类资料\n无序分类资料\n二项分布直接计算概率法、正态近似法（Z检验）、率的正态近似\n独立四格表\\(\\chi^2\\)检验、Fisher确切概率法\nR×C交叉表\\(\\chi^2\\)检验、Fisher确切概率法\n配对四格表\\(\\chi^2\\)检验，配对R×R列联表\\(\\chi^2\\)检验\n/\n\n\n\n等级资料\nWilcoxon符合秩和检验\nwilcoxon秩和检验\nKruskal-Wallis H检验\nWilcoxon符合秩和检验\nFriedman M秩和检验\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n内容\n\n\n\n\n确切概率法\n1. 适用情形：样本量较小或\\(\\pi_0\\)不靠近0.5时作单侧检验的情形。2. 计算公式：(1)最多有k例阳性的概率：\\(Pr(X\\le k)\\)(2)最少有k例阳性的概率：\\(Pr(X\\ge k)\\)\n\n\n正态近似法\n1. 适用情形：样本量较大时，\\(n\\pi,n(1-\\pi)\\)均大于5；2. 计算公式：分子为\\(p-\\pi_0\\)，分母为率的标准误\n\n\n\nnotice：上式中p为样本率，\\(\\pi_0\\)为给的总体率（常为理论值或标准值），n为样本含量。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n独立四格表卡方检验\n\\(n\\ge 40\\)且所有的\\(T\\ge 5\\)\\(n\\ge 40\\)且任一理论频数有\\(1\\le T&lt; 5\\)当\\(n&lt;40\\)，或任一一个格子理论频数\\(T&lt;1\\)时\n卡方基本公式、独立四格表专用公式同上、但是需要校正用四格表资料的Fisher确切概率法\n\n\n正态近似法\n\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)均大于5\n分子为样本率之差，分母为样本率差的标准误\\(S_{p1-p2}\\)为两个样本率之差的标准误，\\(p_c=\\frac{x_1+x_2}{n_1+n_2}\\)为两样本的合并率\n\n\n校正样本率的正态近似法\n当\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)不太大时\n同上，但是需要对样本率实施“分子+2、分母+4”的校正\n\n\n\nnotice：\n\n正态近似法与卡方检验结果是很接近的。在日常计算时，因为计算简便，故常用卡方检验公式。\n四格表的自由度为1。\n四格表实际频数变动时，若周边合计数保持不变，则理论频数将不会产生变化。\n用\\(n_R\\)和\\(n_C\\)和n分别表示行合计、列合计和总合计，则计算每格理论数的公式为：\\(T_{RC}=\\frac{n_R×n_C}{n}\\)。\n\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}\\)。\n校正的\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|A-T|-0.5)^2}{T}\\)。\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n配对四格表卡方检验\n当\\((b+c)\\ge 40\\)时当\\((b+c)&lt;40\\)时\n配对卡方检验专用公式校正配对卡方检验专用公式\n\n\n配对R×R交叉表数据的\\(\\chi^2\\)检验\nR（\\(R\\ge2\\)）\n\\(T=\\frac{k-1}{k}\\sum_{i=1}^{k}\\frac{(n_i-m_i)^2}{n_i+m_i-2A_{ii}}\\)\n\n\n\nnotice：\n\n配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}=\\frac{(b-c)^2}{b+c}\\)。\n若b+c&lt;40,使用校正的配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|b-c|-1)^2}{b+c}\\)。\n\n\n\n\n\n\n\n\n建立假设检验，确定检验水准 \\(H_0\\):两变量之间相互独立 \\(H_1\\):两变量之间相互独立 \\(\\alpha=0.05\\)\n计算检验统计量 [^2=_{i,j} ]\n确定P值，做出推断\n关联系数的计算 [r=]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n内容\n\n\n\n\n假设检验\n\\(H_0\\)：各组总体率（或构成比）相同。\\(H_1\\)：各组总体率（或构成比）不同（不全相同）。\n\n\n计算公式\n卡方检验基本公式，自由度为：\\(v=(R-1)(C-1)\\)\n\n\n数据要求\n1. 应用条件：不能有理论频数小于1的格子，或者不能有1/5以上的理论频数大于等于1且小于5 2. 不能进行卡方检验时的解决办法：①增加样本量；②合并或删除理论频数比较小的行或列；③采用Fisher确切概率法\n\n\n卡方分割\n多个率或多个频率分布比较的卡方检验，当结论为拒绝\\(H_0\\)时，仅表示多组之间是有差别的。若需要明确研究是那两组之间存在差别，可做率的多重比较，将R×C表分割为若干个小的四格表进行检验，并且需要根据比较的次数合理地修正检验水准\\(\\alpha\\)，否则将人为地增大犯第一类错误的概率\n\n\n\nnotice:\n\n多个独立样本率的比较，根据R个独立样本的频率分布，是检验R个二项分布总体的概率是否相同，。假设对四个样本率进行比较，进行\\(\\chi^2\\)检验，则它的行数为4，列数为2，其自由度为\\(v=(R-1)×(C-1)=(4-1)(2-1)=3\\)。\n针对行列表资料的\\(\\chi^2\\)检验，若有\\(1/5\\)格子以上的理论频数小于5，即\\(1\\le T\\le5\\)时，应考虑增加样本量，或结合专业知识对行或列进行合并。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html#卡方检验",
    "href": "Learn/Basic/09-non-parameter-test.html#卡方检验",
    "title": "非参数检验",
    "section": "",
    "text": "资料特征\n数据特征\n\n完全随机设计\n\n配对设计\n随机区组\n\n\n\n\n\n\n单组\n两组\n多组\n\n\n\n\n分类资料\n无序分类资料\n二项分布直接计算概率法、正态近似法（Z检验）、率的正态近似\n独立四格表\\(\\chi^2\\)检验、Fisher确切概率法\nR×C交叉表\\(\\chi^2\\)检验、Fisher确切概率法\n配对四格表\\(\\chi^2\\)检验，配对R×R列联表\\(\\chi^2\\)检验\n/\n\n\n\n等级资料\nWilcoxon符合秩和检验\nwilcoxon秩和检验\nKruskal-Wallis H检验\nWilcoxon符合秩和检验\nFriedman M秩和检验\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n内容\n\n\n\n\n确切概率法\n1. 适用情形：样本量较小或\\(\\pi_0\\)不靠近0.5时作单侧检验的情形。2. 计算公式：(1)最多有k例阳性的概率：\\(Pr(X\\le k)\\)(2)最少有k例阳性的概率：\\(Pr(X\\ge k)\\)\n\n\n正态近似法\n1. 适用情形：样本量较大时，\\(n\\pi,n(1-\\pi)\\)均大于5；2. 计算公式：分子为\\(p-\\pi_0\\)，分母为率的标准误\n\n\n\nnotice：上式中p为样本率，\\(\\pi_0\\)为给的总体率（常为理论值或标准值），n为样本含量。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html#率的比较",
    "href": "Learn/Basic/09-non-parameter-test.html#率的比较",
    "title": "非参数检验",
    "section": "",
    "text": "方法\n情形\n计算公式\n\n\n\n\n独立四格表卡方检验\n\\(n\\ge 40\\)且所有的\\(T\\ge 5\\)\\(n\\ge 40\\)且任一理论频数有\\(1\\le T&lt; 5\\)当\\(n&lt;40\\)，或任一一个格子理论频数\\(T&lt;1\\)时\n卡方基本公式、独立四格表专用公式同上、但是需要校正用四格表资料的Fisher确切概率法\n\n\n正态近似法\n\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)均大于5\n分子为样本率之差，分母为样本率差的标准误\\(S_{p1-p2}\\)为两个样本率之差的标准误，\\(p_c=\\frac{x_1+x_2}{n_1+n_2}\\)为两样本的合并率\n\n\n校正样本率的正态近似法\n当\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)不太大时\n同上，但是需要对样本率实施“分子+2、分母+4”的校正\n\n\n\nnotice：\n\n正态近似法与卡方检验结果是很接近的。在日常计算时，因为计算简便，故常用卡方检验公式。\n四格表的自由度为1。\n四格表实际频数变动时，若周边合计数保持不变，则理论频数将不会产生变化。\n用\\(n_R\\)和\\(n_C\\)和n分别表示行合计、列合计和总合计，则计算每格理论数的公式为：\\(T_{RC}=\\frac{n_R×n_C}{n}\\)。\n\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}\\)。\n校正的\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|A-T|-0.5)^2}{T}\\)。\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n配对四格表卡方检验\n当\\((b+c)\\ge 40\\)时当\\((b+c)&lt;40\\)时\n配对卡方检验专用公式校正配对卡方检验专用公式\n\n\n配对R×R交叉表数据的\\(\\chi^2\\)检验\nR（\\(R\\ge2\\)）\n\\(T=\\frac{k-1}{k}\\sum_{i=1}^{k}\\frac{(n_i-m_i)^2}{n_i+m_i-2A_{ii}}\\)\n\n\n\nnotice：\n\n配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}=\\frac{(b-c)^2}{b+c}\\)。\n若b+c&lt;40,使用校正的配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|b-c|-1)^2}{b+c}\\)。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html#独立性检验",
    "href": "Learn/Basic/09-non-parameter-test.html#独立性检验",
    "title": "非参数检验",
    "section": "",
    "text": "建立假设检验，确定检验水准 \\(H_0\\):两变量之间相互独立 \\(H_1\\):两变量之间相互独立 \\(\\alpha=0.05\\)\n计算检验统计量 [^2=_{i,j} ]\n确定P值，做出推断\n关联系数的计算 [r=]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n内容\n\n\n\n\n假设检验\n\\(H_0\\)：各组总体率（或构成比）相同。\\(H_1\\)：各组总体率（或构成比）不同（不全相同）。\n\n\n计算公式\n卡方检验基本公式，自由度为：\\(v=(R-1)(C-1)\\)\n\n\n数据要求\n1. 应用条件：不能有理论频数小于1的格子，或者不能有1/5以上的理论频数大于等于1且小于5 2. 不能进行卡方检验时的解决办法：①增加样本量；②合并或删除理论频数比较小的行或列；③采用Fisher确切概率法\n\n\n卡方分割\n多个率或多个频率分布比较的卡方检验，当结论为拒绝\\(H_0\\)时，仅表示多组之间是有差别的。若需要明确研究是那两组之间存在差别，可做率的多重比较，将R×C表分割为若干个小的四格表进行检验，并且需要根据比较的次数合理地修正检验水准\\(\\alpha\\)，否则将人为地增大犯第一类错误的概率\n\n\n\nnotice:\n\n多个独立样本率的比较，根据R个独立样本的频率分布，是检验R个二项分布总体的概率是否相同，。假设对四个样本率进行比较，进行\\(\\chi^2\\)检验，则它的行数为4，列数为2，其自由度为\\(v=(R-1)×(C-1)=(4-1)(2-1)=3\\)。\n针对行列表资料的\\(\\chi^2\\)检验，若有\\(1/5\\)格子以上的理论频数小于5，即\\(1\\le T\\le5\\)时，应考虑增加样本量，或结合专业知识对行或列进行合并。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html#秩和检验",
    "href": "Learn/Basic/09-non-parameter-test.html#秩和检验",
    "title": "非参数检验",
    "section": "2.1 秩和检验",
    "text": "2.1 秩和检验\n秩和检验（Rank-Sum Test）是一种非参数检验方法，用于比较两个独立样本的分布是否存在显著差异。它无需对数据分布作正态性假设，适用于数据偏离正态分布、样本量较小或数据为序数型变量的场景。\n常见的秩和检验包括：\n\nMann-Whitney U 检验（也称Wilcoxon秩和检验）：用于比较两个独立样本的中位数是否相等。\nWilcoxon 符号秩检验：用于两个配对样本的比较（类似配对t检验，但无需正态性假设）。\n\n\n2.1.1 秩和检验的公式\n\nMann-Whitney U 检验公式\n\n假设两组独立样本分别为 \\(X\\) 和 \\(Y\\)，样本量分别为 \\(n_1\\) 和 \\(n_2\\)。\n对两组样本合并并按大小排序，赋予秩次。计算两组的秩次和 \\(R_1\\) 和 \\(R_2\\)（分别为 $ X$ 和 \\(Y\\) 的秩次总和）。\n\n确定统计量T值：\n\n假设两组样本量 \\(n_1&lt;n_2\\)，一般情况下以样本量较小者\\(n_1\\)对应的秩和\\(T_1\\)为检验统计量\\(T\\)，当样本相等时可以选择任一组的秩和为\\(T\\)。1\n当两组中样本量较小者不低于10时，在\\(H_0\\)成立假设下，统计量\\(T\\)的抽样分布近似于正态分布，有\n\\[T\\approx N\\left(\\frac{n_1(n+1)}{2},\\frac{n_1 n_2(n+1)}{12} \\right)\\] 此时，Wilcoxon 秩和统计量在\\(H_0\\)下关于\\(\\mu=\\frac{n_1(n+1)}{2}\\)对称。\n如果没有或存在较少的“结”，将\\(T\\)标准化后为：\n\\[U=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2(n+1)}{12}}}\\approx N(0,1)\\]\n其中，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n若“结”的比例较多（&gt;25%），则用以下公式校正：\n\\[U_c=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2}{12}[(n+1)-\\sum_\\limits{i=1}^{g}\\frac{t_i^3-t_i}{n(n-1)}]}}\\approx N(0,1)\\]\n\nWilcoxon 符号秩检验公式\n\n对配对样本 \\((X_i, Y_i)\\)，计算差值 \\(D_i = X_i - Y_i\\)，取非零差值的绝对值并排序（若差值为0则舍去不计，且减去相应的个数），赋予秩次 \\(R_i\\)。再根据差值的符号计算符号秩次和 \\(W\\)：\n\\[W = \\sum R_i \\cdot \\text{sign}(D_i)\\]\n检验统计量 \\(T\\) 是 \\(W\\) 的绝对值，依据表或正态分布计算显著性。\n正态近似法：\n当\\(n\\ge 30\\)时，有中心极限定理可知，当\\(H_0\\)成立时统计量\\(T\\)的抽样分布近似正态分布，有\n\\[T\\approx N \\left(\\frac{n(n+1)}{4},\\frac{n(n+1)(2n+1)}{24}\\right)\\] 其中，均数\\(\\mu=\\frac{n(n+1)}{4}\\)，方差\\(\\sigma^2=\\frac{n(n+1)(2n+1)}{24}\\)。 将T标准化后，近似服从标准正态分布，有\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}}}\\approx N(0,1)\\] 其中，n是差值不为0的对子数，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n当N较大时，样本中可能存在较多的“结”，（如“结”所占比例大于25%），此时需要使用校正公式：\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}-\\frac{\\sum_\\limits{i=1}^g(t_i^3-t_i)}{48}}}\\approx N(0,1)\\] 其中，\\(t_i\\)为\\(i\\)个“结”中有相同秩次的个数，\\(g\\)是“结”的个数。\nWilcoxon符号秩检验的前提条件为数据是连续的且差值分布是对称的。\nnotice：秩和秩和的区别：秩是指全部观察值按某种顺序排列的位序，在一定程度上反映了等级的高低；而秩和则表示同组秩次之和，在一定程度上反映了等级的分布。2\n\n\n2.1.2 应用场景\n\nMann-Whitney U 检验：\n\n比较两个独立样本的中位数是否存在显著差异。\n\n适用于非正态分布数据或含有极端值的样本。\n\n示例：比较两种治疗方法的疗效（不同受试者组）。\n\nWilcoxon 符号秩检验：\n\n比较两个配对样本的中位数差异。\n\n适用于重复测量数据或实验设计中存在配对关系的场景。\n\n示例：同一批受试者在治疗前后血压的变化。\n\n\n\n\n2.1.3 注意事项\n\n秩和检验是非参数方法，对数据分布假设少，但效率可能低于参数方法（如t检验）在满足条件时的效果，如果满足参数检验的条件，应优先考虑使用参数检验的方法，否则会增加犯二类错误的概率。\n数据需要满足独立性假设，否则检验结果可能不准确。\n\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html#footnotes",
    "href": "Learn/Basic/09-non-parameter-test.html#footnotes",
    "title": "非参数检验",
    "section": "脚注",
    "text": "脚注\n\n\n不是说一定要选择样本量较小者对应的秩和作为检验统计量，只是长期的使用习惯，造成了这一惯例。如果取较小的秩和计算后得到的\\(U&lt;u_{\\alpha/2}\\)，则表示拒绝\\(H_0\\)；相反，如果取较大的秩和计算后得到的\\(U&gt;u_{1-\\alpha/2}\\)，也会表示拒绝\\(H_0\\)，他们都表示检验统计量落在了拒绝域中。↩︎\n尽管非参数方法对总体分布形式未做要求，但如果我们知道总体的一些性质而不去利用，就会浪费许多有用的信息，最常见的就是分布的对称性，配对设计的 Wilcoxon 符号秩检验充分利用了差值分布对称性这一信息，这与尽可能地采用有效方法，利用尽可能多的信息进行统计分析的大原则相一致。↩︎",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Bayes/00-About-Bayes.html",
    "href": "Learn/Bayes/00-About-Bayes.html",
    "title": "贝叶斯与概率推理",
    "section": "",
    "text": "Bayes系列的笔记主要基于Learning-Probabilistic-Graphical-Models-in-R.\n皮埃尔-西蒙·拉普拉斯(Pierre-Simon Laplace,1749-1827)，法国数学家，也是有史以来最伟大的科学家之被认为是第一批理解数据收集重要性的人:他发现了数 -，不据不可靠，有不确定性，也就是今天说的有噪声。他也是第一个研究使用概率来处理不确定性等问题，并表示事件或信息信念度的人。\n在他的论文《概率的哲学》​（Essai philosophique sur lesprobabilités,1814）中，拉普拉斯给出了最初的支持新老数据推理的数学系统，其中的用户信念会在新数据可用的时候得到更新和改进。\n概率是表示和处理不确定性的严密的数学方法。\n概率是一种量化常识推理和信念程度的工具。\n概率图模型，从数学的角度看，是一种表示几个变量概率分布的方法，也叫作联合概率分布。换句话说，它是一种表示几个变量共同出现的数值信念的工具。基于这种理解，虽然概率图模型看起来很简单，但是概率图模型强调的是对于许多变量概率分布的表示。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理"
    ]
  },
  {
    "objectID": "Learn/Bayes/00-About-Bayes.html#联合概率分布",
    "href": "Learn/Bayes/00-About-Bayes.html#联合概率分布",
    "title": "贝叶斯与概率推理",
    "section": "\n1 联合概率分布",
    "text": "1 联合概率分布\n当我们同时考虑两个试验（投掷硬币2次和投掷一个骰子）的时候，我们对同时获得0、1或2的概率以及1、2、3、4、5或6的点数概率更感兴趣。这两个同时考虑的随机变量的概率分布写作 \\(P(N, D)\\) ，称作联合概率分布。\n一个概率图模型就是一个联合概率分布。除此之外，别无他物。\n联合概率分布的最后一个重要概念是边缘化（marginalization）。\n联合分布\\(P(X,Y)\\)的边缘分布\\(P(X)\\)可以通过下列操作获得：\n\\[P(X)=\\sum_y P(X,Y)\\]\n当Y值是连续值是，边缘化可以写作：\n\\(P(X)=\\int_{y} P(X,y) \\mathrm{d}y\\)。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理"
    ]
  },
  {
    "objectID": "Learn/Bayes/00-About-Bayes.html#贝叶斯规则",
    "href": "Learn/Bayes/00-About-Bayes.html#贝叶斯规则",
    "title": "贝叶斯与概率推理",
    "section": "\n2 贝叶斯规则",
    "text": "2 贝叶斯规则\n\n2.1 条件概率\n条件概率是指在知道其他时间发生的条件下当前事件的概率。很明显，两个事件必须某种程度的依赖，否则一个事件的发生不会影响另一个事件。\n条件概率转化为公式如下：\n\\(P(X|Y)=\\frac{P(X,Y)}{P(Y)}\\)和\\(P(Y|X)=\\frac{P(X,Y)}{P(X)}\\)\n\n2.2 贝叶斯公式\n从上述两个公式推导出贝叶斯公式：\n\\(P(X|Y)=\\frac{{P(Y|X)}·P(X)}{P(Y)}\\)\n在这个公式中，我们把 \\(P(X|Y)\\) 叫做是给定\\(Y\\)下\\(X\\)的后验分布，因此，\\(P(X)\\) 叫做后验分布。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理"
    ]
  },
  {
    "objectID": "Learn/Bayes/00-About-Bayes.html#例子机器与灯泡",
    "href": "Learn/Bayes/00-About-Bayes.html#例子机器与灯泡",
    "title": "贝叶斯与概率推理",
    "section": "\n3 例子：机器与灯泡",
    "text": "3 例子：机器与灯泡\n在构建贝叶斯统计的时候，我们总是需要建立两个部件：\n\n先验分布\n似然率\n\n先验分布使我们关于机器工作状态的初试信念。我们确定了第一个刻画机器状态的随机变量 \\(M\\) 。这个随机变量有两个状态 \\({working,broken}\\) 。我们相信机器是好的，是可以正常工作的，所以先验分布如下：\n\n\\(P(M=working)=0.99\\)\n\\(P(M=broken)=0.01\\)\n\n\n3.1 R代码的实现\n先验分布、似然率和数据序列\n\nprior &lt;- c(working=0.99, broken=0.01) \nlikelihood &lt;- rbind( \n    working = c(good = 0.99, bad = 0.01),broken = c(good = 0.6, bad = 0.4) \n) \ndata &lt;- c(\"bad\", \"bad\", \"bad\", \"bad\") \n\n贝叶斯更新函数\n\nbayes &lt;-function(prior, likelihood, data)\n{\nposterior &lt;-matrix(0, nrow =length(data), ncol =length(prior))\ndimnames(posterior) &lt;-list(data, names(prior))\ninitial_prior &lt;-prior\nfor (i in 1:length(data))\n{\nposterior[i, ] &lt;-\nprior *likelihood[, data[i]]/\nsum(prior *likelihood[, data[i]])\nprior &lt;-posterior[i, ]\n}\nreturn(rbind(initial_prior, posterior))\n}\n\n\n创建一个矩阵，存储后验分布的连续计算结果。\n然后对于每一个数据，给定当前先验概率计算后验概率：和之前的一样，你可以看到贝叶斯公式的R代码。\n最后，新的先验概率是当前的后验概率，而且同样的过程可以迭代。\n分布的演化情况\n\nmatplot(bayes(prior, likelihood, data), t ='b', \n        lty =1, pch =20,\n        col =c(3, 2))\n\n\n\n\n\n\n\n随着坏灯泡的增多，机器正常的概率快速下降（绿色线）。\n如果我们换一个先验分布，假设我们不知道机器是否可以正常工作，即好坏参半，我们给定如下概率：\n\nprior &lt;- c(working = 0.5, broken = 0.5)\nmatplot(bayes(prior, likelihood, data), t ='b', \n        lty =1, pch =20,\n        col =c(3, 2))\n\n\n\n\n\n\n\n这个曲线快速收敛，机器有问题的概率很高。\n再对数据变换一下，假设机器正常工作的概率是99%，我们观察10个灯泡，其中一个灯泡是坏的：\n\nprior =c(working =0.99, broken =0.01)\ndata =c(\"bad\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\",\"good\", \"good\", \"good\")\nmatplot(bayes(prior, likelihood, data), t ='b', pch =20, col =c(3, 2))\n\n\n\n\n\n\n\n算法在第一个灯泡处犹豫了一下，因为这么好的机器不太可能生产出一个坏灯泡。然后它又收敛到很高的概率，因为好灯泡的序列不会预示任何问题。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理"
    ]
  },
  {
    "objectID": "navbar/book.html",
    "href": "navbar/book.html",
    "title": "公共卫生相关的书籍",
    "section": "",
    "text": "读的书有点乱，主要都在微信读书看，部分是纸质书，介绍主要以我看过或在看的书为主。\n\n1 Maybe you will be interested\n对于刚接触统计的朋友，可以先看看《女士品茶：统计学如何变革了科学和生活》这本书，也会他能激发你的一些对于统计学（家们）的兴趣，通过一些有趣的小故事，带你走进统计学的世界。\n2025-03-30，出于对 THE LADY TASTING TEA 的兴趣，后续将会整理一下出现的主要统计学家和他的主要成果（公式）。\n\n\n2 Basic\n考研的话，除了人卫第八版是一个通用教材，其他的院校各有不同，比如说赵耐青、方积乾和贺佳，根据院校给的参考书目来选择，这几个人的书都买了看过一下，个人主要用过人卫版和姜晶梅的《医学统计学》，各有见长。\n\n\n3 Advance\n提升部分可以看看 Springer 出版的 Mathematical Models in Epidemiology ，2023年科学出版社出版了中文版 《流行病学中的数学模型》\n\nThe book is a comprehensive, self-contained introduction to the mathematical modeling and analysis of disease transmission models. It includes (i) an introduction to the main concepts of compartmental models including models with heterogeneous mixing of individuals and models for vector-transmitted diseases, (ii) a detailed analysis of models for important specific diseases, including tuberculosis, HIV/AIDS, influenza, Ebola virus disease, malaria, dengue fever and the Zika virus, (iii) an introduction to more advanced mathematical topics, including age structure, spatial structure, and mobility, and (iv) some challenges and opportunities for the future.\nThere are exercises of varying degrees of difficulty, and projects leading to new research directions. For the benefit of public health professionals whose contact with mathematics may not be recent, there is an appendix covering the necessary mathematical background. There are indications which sections require a strong mathematical background so that the book can be useful for both mathematical modelers and public health professionals.\n\nend."
  },
  {
    "objectID": "Quarto/quarto-intro.html",
    "href": "Quarto/quarto-intro.html",
    "title": "Quarto introduction",
    "section": "",
    "text": "Quarto introduction\n\nAn open-source scientific and technical publishing system\n\nAuthor using Jupyter notebooks or with plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nShare knowledge and insights organization-wide by publishing to Posit Connect, Confluence, or other publishing systems.\nWrite using Pandoc markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\n\n\nAnalyze. Share. Reproduce. You have a story to tell with data—tell it with Quarto.\nFrom https://quarto.org/\nend.",
    "crumbs": [
      "Home",
      "Quarto introduction"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html#构建logistic回归模型",
    "href": "Guide/R/2025-02-22-CLHLS.html#构建logistic回归模型",
    "title": "CLHLS Data Analysis by R",
    "section": "\n3.1 构建Logistic回归模型",
    "text": "3.1 构建Logistic回归模型\nLogistic 回归分析最大的一个优势可能就是广泛涉及优势比（？），在这里介绍一下优势比的概念。\n\n3.1.1 优势比\n优势（Odds）是指某事件的发生概率 \\(\\pi\\) 与该事件不发生的概率 \\(1-\\pi\\) 之比，亦称为比，记为 Odds ，某事件在两种不同条件下的优势之比称为优势比（Odds Ratio，OR）。优势比在流行病学中的病例对照研究中被普遍应用（当然不仅限于病例对照研究）。\n设某事件的两种不同暴露的发生概率分别为 \\(\\pi_1\\) 和 \\(\\pi_0\\) ，对应的 \\(Odds_1= \\pi_1 /（1-\\pi_1）\\)，\\(Odds_0 =\\pi_0 /（1-\\pi_0）\\) ，两个 \\(Odds\\) 之比定义为 \\(OR=Odds_1/Odds_0\\)。\n由于 \\(Odds=\\pi /(1-\\pi)=(\\pi-1+1)/(1-\\pi)=－1+1 /(1-\\pi)\\) 以及 \\(0＜ \\pi ＜1\\)，所以 \\(\\pi\\) 越大，\\(Odds\\) 就越大，反之 \\(\\pi\\) 越小，\\(Odds\\) 就越小，特別当 \\(\\pi\\) 越接近 0时，\\(Odds\\) 也越接近 0，因此优势和优势比具有下列性质。\n\n如果 \\(\\pi_1 = \\pi_0\\)。，对应有 \\(Odds_1=Odds_0\\) , \\(OR=1\\) 。\n如果 \\(\\pi_1&gt;\\pi_0\\) ，则 \\(Odds_1&gt;Odds_o\\) ，相应有 $ OR&gt;1$ 。同理如果 \\(\\pi_1&lt;\\pi_0\\)，则 \\(Odds_1&lt;Odds_0\\) ，相应有 \\(OR &lt; 1\\)。\n\n由于概率 \\(\\pi_1\\) 也可以用 \\(Odds\\) 表示，\\(\\pi=\\frac{Odds}{1+Odds}\\) ，所以也可以通过比较 \\(Odds_1\\) 与 \\(Odds_0\\) 的大小来推断 \\(\\pi_1\\) 和 \\(\\pi_0\\) 的大小关系，即可以用 \\(OR&gt;1\\), \\(OR=1\\) 或 \\(OR&lt;1\\) 推断 \\(\\pi_1\\) 和 \\(\\pi_0\\) 的大小关系。\n\n3.1.2 Logistic 回归模型\nLogistic 回归模型的因变量必须为分类变量，主要有三种：二分类 Logistic 回归模型、有序分类 Logistic 回归模型、无序分类 Logistic 回归模型。其中二分类 Logistic 回归模型最为常用。自变量则无要求，可以是定量变量、有序分类变量和无序分类变量。\n假如研究所关注的事件（如死亡或痊愈等）是否发生用因变量 \\(Y\\) 表示，\\(Y=1\\) 表示该结局事件发生，反之，\\(Y=0\\) 表示该结局事件未发生。\n那么可构建如下方程式：\n\\[logit(\\pi(Y=1))=ln\\frac{\\pi(Y=1)}{\\pi(Y=0)}=ln\\frac{\\pi(Y=1)}{1-\\pi(Y=1)}=\\beta_0 + \\beta_1 X_1 + \\cdot \\beta_p X_p\\]\n\n3.1.3 共线性检验\n共线性(Colinearity)指的是自变量之间存在高度相关性。这种情况会导致回归系数的不稳定，并使得对模型参数 的估计变得不可靠。在 Logistic 回归中，严重的共线性可能导致模型性能下降，甚至可能导致预测结果难以解释\n方差膨胀因子是统计学中用于衡量多元线性回归模型中自变量之间共线性程度的指标，提供了一种定量的方式来评估自变量之间的共线性。\n方差膨胀因子的解释标准通常如下：如果VIF值小于5，表示自变量之间的共线性程度较低，可以接受。如果VIF值在5到10之间， 表示自变量之间存在一定程度的共线性，但尚可接受。如果VIF值大于10，表示自变量之间存在严重的共线性问题， 需要考虑进行变量选择或者采取其他方法来处理共线性。\n自评健康作为 因变量 ，自变量是 经济支持、生活支持和情绪支持，在前序的处理过程，只有原始的的经济支持变量（加总金额）是定量变量，其他两个都是做的二分类变量处理，如何做共线性检验，看起来并不明朗，因为三个变量看起来没有太多的关系，但是控制变量有些可能存在相关性，但是，是否有对控制变量做共线性检验的必要？\n\n3.1.4 2025-03-07 共线性检验\n使用car包对共线性进行检验。\n理论上，共线性检验较为简单，但是在处理此数据时，碰壁较多。\n首先核查数据的完整性，删除全NA和单一值变量，再将分类变量转换为因子类型，经济变量这里也是用“经济分组”这一变量处理，即所有的变量都是分类变量。（全部为分类变量在这里处理其实可能有些问题，我暂未找到合适的处理办法，寻找相关的信息也没有较为清晰地答案）\n\n# library(car)\n# 数据完整性检查，删除全NA和单一值变量\nvalid_vars &lt;- all_predictors[sapply(final_data[, all_predictors, drop = FALSE], function(x) !(all(is.na(x)) || length(unique(na.omit(x))) == 1))]\nfinal_data &lt;- final_data[, c(outcome, valid_vars)]\n\n# 仅将二分类变量转换为因子类型\nbinary_vars &lt;- valid_vars[sapply(final_data[, valid_vars], function(x) length(unique(na.omit(x))) == 2)]\nfinal_data &lt;- final_data %&gt;% mutate(across(all_of(binary_vars), as.factor))\n\n# VIF计算\nif (length(valid_vars) &gt; 1) {\n  formula_vif &lt;- as.formula(paste(outcome, \"~\", paste(valid_vars, collapse = \" + \")))\n  lm_model &lt;- lm(formula_vif, data = final_data)\n  vif_values &lt;- vif(lm_model)\n  \n  vif_df &lt;- data.frame(\n    Variable = names(vif_values),\n    VIF = round(vif_values, 3)\n  )\n  mean_vif &lt;- mean(vif_values, na.rm = TRUE)\n  # 结果保留两位小数\n  mean_vif &lt;- round(mean_vif, 3)\n  \n  # 保存结果\n  writeLines(c(\"=== 多重共线性检验 (VIF Results) ===\", \n               paste(\"平均 VIF 值:\", round(mean_vif, 3)), \"\",\n               capture.output(print(vif_df))), con = output_vif_file)\n  \n  write_xlsx(list(VIF_Results = vif_df, Mean_VIF = data.frame(Statistic = \"平均 VIF 值\", Value = round(mean_vif, 3))), \n             output_vif_excel)\n  \n  cat(\"VIF 检验完成，结果已保存至:\", output_vif_file, \"和\", output_vif_excel, \"\\n\")\n} else {\n  cat(\"错误：自变量数量不足，无法计算 VIF\\n\")\n}\n\n\n3.1.5 logistic 回归\n\n# 5. Logistic 回归分析\nlogistic_results &lt;- list()\n\nfor (outcome in outcomes) {\n  formula &lt;- as.formula(\n    paste(outcome, \"~\", paste(independents, collapse = \" + \"), \"+\", paste(controls, collapse = \" + \"))\n  )\n  \n  model &lt;- glm(formula, data = final_data, family = binomial(link = \"logit\"))\n  \n  # 提取回归结果，保留三位小数\n  model_summary &lt;- summary(model)\n  coef_table &lt;- as.data.frame(coef(model_summary)) %&gt;%\n    mutate(\n      Variable = rownames(.),\n      OR = round(exp(Estimate), 3),\n      OR_Lower = round(exp(Estimate - 1.96 * `Std. Error`), 3),\n      OR_Upper = round(exp(Estimate + 1.96 * `Std. Error`), 3),\n      `Wald χ²` = round((Estimate / `Std. Error`)^2, 3),\n      df = 1\n    ) %&gt;%\n    select(Variable, df, Estimate, `Std. Error`, `Wald χ²`, `Pr(&gt;|z|)`, OR, OR_Lower, OR_Upper) %&gt;%\n    rename(\n      `回归系数` = Estimate,\n      `标准误` = `Std. Error`,\n      `P 值` = `Pr(&gt;|z|)`\n    ) %&gt;%\n    mutate(across(c(`回归系数`, `标准误`, `P 值`), ~ round(.x, 3)))\n  \n  logistic_results[[outcome]] &lt;- coef_table\n}\n\n# 6. 生成 Word 文档\ndoc &lt;- read_docx()\n\nfor (outcome in outcomes) {\n  doc &lt;- doc %&gt;%\n    body_add_par(value = paste(\"Logistic 回归分析结果：因变量 =\", outcome), style = \"heading 1\") %&gt;%\n    body_add_flextable(flextable(logistic_results[[outcome]]) %&gt;%\n                         set_table_properties(width = 1, layout = \"autofit\"))\n}\n\nprint(doc, target = output_logistic_file)\ncat(\"Logistic 回归分析结果已保存至:\", output_logistic_file, \"\\n\")",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html",
    "href": "Guide/Python/25-03-09-cos-reg.html",
    "title": "用Python做生存分析和COX回归",
    "section": "",
    "text": "生存分析（survival analysis）是一种统计方法，用于分析时间数据， 主要研究生存时间和结局的分布及其影响因素的统计方法。在生存分析中，每个研究对象的结局变量由 “time”（生存时间） 和 “status”（生存状态）组成。生存时间是指从某个特定时间点开始，到某个事件的节点时，事件数据是指某个事件是否发生。 生存时间是一个非负实数，生存状态是一个二元变量，通常用1表示事件发生，0表示事件未发生。\n生存分析的主要应用领域是医学、生物学、工程学、经济学等。\n\n\n生存函数（survival function）是生存分析的基本概念之一，它是一个函数，用于刻画研究对象在某个时刻 t 内存存活的概率。 生存函数通常用 \\(S(t)\\) 表示。\n风险函数（hazard function）是生存分析的另一个基本概念，用于刻画研究对象在某个时刻 t 还存活但是极短的时间内死亡的风险。 风险函数通常用 \\(h(t)\\) 表示。\n如果记寿命分布的密度为 \\(f(t)\\)，则有： \\(h(t) = f(t) / S(t)\\) 。\n\n\n\n这里使用 R 语言 survival 包中的 ovarian 数据集，该数据集来自一项比较卵巢癌患者在两种治疗方式下的生存率比较的随机对照试验。\n首先找到 ovarian 数据集，你可以从互联网上寻找相关资源；或者从 R 的 survival 包中导出这一数据集，操作如下：\n# install.packages(\"survival\")\nlibrary(survival)\novarian\ndata(cancer, package=\"survival\")\n\ndf &lt;- ovarian\nwrite.csv(df, \"your-file-path\\\\ovarian.csv\", row.names = FALSE)\n将数据集下载到你的工作目录，然后使用 Pandas 导入与读取：\n\nimport pandas as pd\novarian = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\R\\quarto\\Med-Stat-Notes\\Data\\ovarian.csv\")\novarian.head()\n\n\n\n\n\n\n\n\nfutime\nfustat\nage\nresid.ds\nrx\necog.ps\n\n\n\n\n0\n59\n1\n72.3315\n2\n1\n1\n\n\n1\n115\n1\n74.4932\n2\n1\n1\n\n\n2\n156\n1\n66.4658\n2\n1\n2\n\n\n3\n421\n0\n53.3644\n2\n2\n1\n\n\n4\n431\n1\n50.3397\n2\n1\n1\n\n\n\n\n\n\n\n数据集包括 26 个观测值，6 个变量。变量如下：futime（随访时间），fustat（研究结束时的状态：0 表示存活，1表示死亡），age（患者的年龄），resid.ds（疾病残留情况：1 表示有残留，2 表示没有残留），rx（治疗方式：1 表示环磷酰胺，2 表示环磷酰胺+阿霉素）和 ecog.ps（患者的 ECOG 评分：1 表示较好，2 表示较差）。\n对年龄进行分组，分为 &lt;50 和 &gt;50 两组，并将其他三个变量的各水平加上相应的标签。\n\n# 查看 ovarian(DataFrame)的变量名，因为不同的渠道下载的 Dataset 可能会有区别\nprint(ovarian.columns)\n\n# 将 age 列转换为数值类型\novarian['age'] = pd.to_numeric(ovarian['age'], errors='coerce')\n\novarian.age = pd.cut(ovarian.age, [0,50,75], labels = ['&lt;=50','&gt;50'])\n\novarian['resid.ds'] = ovarian['resid.ds'].map({1: \"NO\", 2: \"Yes\"})\novarian['rx'] = ovarian['rx'].map({1: \"A\", 2: \"B\"})\novarian['ecog.ps'] = ovarian['ecog.ps'].map({1: \"Good\", 2: \"Bad\"})\n\nIndex(['futime', 'fustat', 'age', 'resid.ds', 'rx', 'ecog.ps'], dtype='object')\n\n\n\n\n\n生存率的 Kaplan-Meier 估计的计算可以调用 lifelines 库中的 KaplanMeierFitter 函数实现。\npip 安装：pip install lifelines\nconda 安装： conda install lifelines\n拟合 fit :\n\nfrom lifelines import KaplanMeierFitter\nkmf = KaplanMeierFitter()\nfit = kmf.fit(ovarian.futime,ovarian.fustat)\nfit\n\n&lt;lifelines.KaplanMeierFitter:\"KM_estimate\", fitted with 26 total observations, 14 right-censored observations&gt;\n\n\n拟合结果 fit 包含了很多属性，我们可以通过点操作符单独提取其中的属性。例如，查看中位生存时间：\n\nfit.median_survival_time_\n\n638.0\n\n\n中位数生存时间表示，有 50% 的患者生存时间达到了 638 天。还可以提取寿命表和生存函数等属性，通过以下方式实现合并查看：\n\npd.concat([fit.event_table,fit.survival_function_], axis = 1)\n\n\n\n\n\n\n\n\nremoved\nobserved\ncensored\nentrance\nat_risk\nKM_estimate\n\n\n\n\n0.0\n0\n0\n0\n26\n26\n1.000000\n\n\n59.0\n1\n1\n0\n0\n26\n0.961538\n\n\n115.0\n1\n1\n0\n0\n25\n0.923077\n\n\n156.0\n1\n1\n0\n0\n24\n0.884615\n\n\n268.0\n1\n1\n0\n0\n23\n0.846154\n\n\n329.0\n1\n1\n0\n0\n22\n0.807692\n\n\n353.0\n1\n1\n0\n0\n21\n0.769231\n\n\n365.0\n1\n1\n0\n0\n20\n0.730769\n\n\n377.0\n1\n0\n1\n0\n19\n0.730769\n\n\n421.0\n1\n0\n1\n0\n18\n0.730769\n\n\n431.0\n1\n1\n0\n0\n17\n0.687783\n\n\n448.0\n1\n0\n1\n0\n16\n0.687783\n\n\n464.0\n1\n1\n0\n0\n15\n0.641931\n\n\n475.0\n1\n1\n0\n0\n14\n0.596078\n\n\n477.0\n1\n0\n1\n0\n13\n0.596078\n\n\n563.0\n1\n1\n0\n0\n12\n0.546405\n\n\n638.0\n1\n1\n0\n0\n11\n0.496732\n\n\n744.0\n1\n0\n1\n0\n10\n0.496732\n\n\n769.0\n1\n0\n1\n0\n9\n0.496732\n\n\n770.0\n1\n0\n1\n0\n8\n0.496732\n\n\n803.0\n1\n0\n1\n0\n7\n0.496732\n\n\n855.0\n1\n0\n1\n0\n6\n0.496732\n\n\n1040.0\n1\n0\n1\n0\n5\n0.496732\n\n\n1106.0\n1\n0\n1\n0\n4\n0.496732\n\n\n1129.0\n1\n0\n1\n0\n3\n0.496732\n\n\n1206.0\n1\n0\n1\n0\n2\n0.496732\n\n\n1227.0\n1\n0\n1\n0\n1\n0.496732\n\n\n\n\n\n\n\n\n\nKaplan-Meier 法估计的生存率是一个阶梯状的函数，其阶梯跳跃点是给定的时间点，我们可以通过调用 plot 方法绘制生存曲线，如图所示：\n\nimport matplotlib.pyplot as plt\nfit.plot(show_censors = True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n在生存分析中，经常需要比较不同情形下的生存率。在本例中，想要比较不同治疗方式下生存率，可以输入下面的命令：\n\ng1 = ovarian.rx == \"A\"\ng2 = ovarian.rx == \"B\"\nkmf_A = KaplanMeierFitter()\nkmf_A.fit(ovarian.futime[g1],ovarian.fustat[g1],label = \"Treatmeat A\")\nkmf_B = KaplanMeierFitter()\nkmf_B.fit(ovarian.futime[g2],ovarian.fustat[g2],label = \"Treatmeat B\")\n\n&lt;lifelines.KaplanMeierFitter:\"Treatmeat B\", fitted with 13 total observations, 8 right-censored observations&gt;\n\n\n\n\n可以单独提取两组的生存函数进行比较，但在同一个图中显示多条生存曲线更有助于生存率的比较。\n\nfig, axes = plt.subplots()\nkmf_A.plot(ax = axes,show_censors = True)\nkmf_B.plot(ax = axes,show_censors = True)\nplt.show()\n\n\n\n\n\n\n\n\n从上图中可以看出，治疗方式 “B” 的生存率高于治疗方式 “A” 的生存率，但是这种差异是由随机误差引起还是真是的治疗方式的不同所造成的差异，需要做进一步的统计学检验。\n\n\n\n因果关联的推断步骤\n\n\n\n\n\n生存分析中常用的统计学检验是 时序检验（log rank test），其基本思想是先计算出不同时间两种治疗方式的暴露人数和死亡人数，并由此在两种治疗方式效果相同的假设下计算出预期死亡人数，如果不拒绝零假设（ \\(H_0\\) ：两种治疗方式的效果相同，即预期死亡人数一致），则实际观测值和期望值的差异不会很大，如果差异过大则不能认为该差异是由随机误差引起的。\n对此，用 \\(\\chi^2\\) 检验做判断。时序检验可以用 lifetimes 库的函数 logrank_test 实现。\n\nfrom lifelines.statistics import logrank_test\nlr = logrank_test(ovarian.futime[g1], ovarian.futime[g2],\n                  ovarian.fustat[g1], ovarian.fustat[g2])\nlr.p_value\n\n0.3025911169890923\n\n\n这里得到的结果为 \\(P&gt;0.05\\) ，在一般情况下，我们会认为这是没有统计学意义的，即无法排除差异是由随机误差引起的。\n这种结果不显著的情况下，我们可以做一些思考，即是否有其他因素干预了结果的显著性，以及是否是样本量过小，导致差异不显著。（样本量的大小会影响检验效能，如果检验效能太低，即使有差异，也很难被检验方法发现）\n这里我们无法改变样本量的大小，但是可以考虑其他混杂因素的影响，并且使用更全面的模型进行检验。",
    "crumbs": [
      "Home",
      "Guide",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#生存函数",
    "href": "Guide/Python/25-03-09-cos-reg.html#生存函数",
    "title": "用Python做生存分析和COX回归",
    "section": "",
    "text": "生存函数（survival function）是生存分析的基本概念之一，它是一个函数，用于刻画研究对象在某个时刻 t 内存存活的概率。 生存函数通常用 \\(S(t)\\) 表示。\n风险函数（hazard function）是生存分析的另一个基本概念，用于刻画研究对象在某个时刻 t 还存活但是极短的时间内死亡的风险。 风险函数通常用 \\(h(t)\\) 表示。\n如果记寿命分布的密度为 \\(f(t)\\)，则有： \\(h(t) = f(t) / S(t)\\) 。",
    "crumbs": [
      "Home",
      "Guide",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#数据集及来源",
    "href": "Guide/Python/25-03-09-cos-reg.html#数据集及来源",
    "title": "用Python做生存分析和COX回归",
    "section": "",
    "text": "这里使用 R 语言 survival 包中的 ovarian 数据集，该数据集来自一项比较卵巢癌患者在两种治疗方式下的生存率比较的随机对照试验。\n首先找到 ovarian 数据集，你可以从互联网上寻找相关资源；或者从 R 的 survival 包中导出这一数据集，操作如下：\n# install.packages(\"survival\")\nlibrary(survival)\novarian\ndata(cancer, package=\"survival\")\n\ndf &lt;- ovarian\nwrite.csv(df, \"your-file-path\\\\ovarian.csv\", row.names = FALSE)\n将数据集下载到你的工作目录，然后使用 Pandas 导入与读取：\n\nimport pandas as pd\novarian = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\R\\quarto\\Med-Stat-Notes\\Data\\ovarian.csv\")\novarian.head()\n\n\n\n\n\n\n\n\nfutime\nfustat\nage\nresid.ds\nrx\necog.ps\n\n\n\n\n0\n59\n1\n72.3315\n2\n1\n1\n\n\n1\n115\n1\n74.4932\n2\n1\n1\n\n\n2\n156\n1\n66.4658\n2\n1\n2\n\n\n3\n421\n0\n53.3644\n2\n2\n1\n\n\n4\n431\n1\n50.3397\n2\n1\n1\n\n\n\n\n\n\n\n数据集包括 26 个观测值，6 个变量。变量如下：futime（随访时间），fustat（研究结束时的状态：0 表示存活，1表示死亡），age（患者的年龄），resid.ds（疾病残留情况：1 表示有残留，2 表示没有残留），rx（治疗方式：1 表示环磷酰胺，2 表示环磷酰胺+阿霉素）和 ecog.ps（患者的 ECOG 评分：1 表示较好，2 表示较差）。\n对年龄进行分组，分为 &lt;50 和 &gt;50 两组，并将其他三个变量的各水平加上相应的标签。\n\n# 查看 ovarian(DataFrame)的变量名，因为不同的渠道下载的 Dataset 可能会有区别\nprint(ovarian.columns)\n\n# 将 age 列转换为数值类型\novarian['age'] = pd.to_numeric(ovarian['age'], errors='coerce')\n\novarian.age = pd.cut(ovarian.age, [0,50,75], labels = ['&lt;=50','&gt;50'])\n\novarian['resid.ds'] = ovarian['resid.ds'].map({1: \"NO\", 2: \"Yes\"})\novarian['rx'] = ovarian['rx'].map({1: \"A\", 2: \"B\"})\novarian['ecog.ps'] = ovarian['ecog.ps'].map({1: \"Good\", 2: \"Bad\"})\n\nIndex(['futime', 'fustat', 'age', 'resid.ds', 'rx', 'ecog.ps'], dtype='object')",
    "crumbs": [
      "Home",
      "Guide",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#生存率的-kaplan-meier-估计",
    "href": "Guide/Python/25-03-09-cos-reg.html#生存率的-kaplan-meier-估计",
    "title": "用Python做生存分析和COX回归",
    "section": "",
    "text": "生存率的 Kaplan-Meier 估计的计算可以调用 lifelines 库中的 KaplanMeierFitter 函数实现。\npip 安装：pip install lifelines\nconda 安装： conda install lifelines\n拟合 fit :\n\nfrom lifelines import KaplanMeierFitter\nkmf = KaplanMeierFitter()\nfit = kmf.fit(ovarian.futime,ovarian.fustat)\nfit\n\n&lt;lifelines.KaplanMeierFitter:\"KM_estimate\", fitted with 26 total observations, 14 right-censored observations&gt;\n\n\n拟合结果 fit 包含了很多属性，我们可以通过点操作符单独提取其中的属性。例如，查看中位生存时间：\n\nfit.median_survival_time_\n\n638.0\n\n\n中位数生存时间表示，有 50% 的患者生存时间达到了 638 天。还可以提取寿命表和生存函数等属性，通过以下方式实现合并查看：\n\npd.concat([fit.event_table,fit.survival_function_], axis = 1)\n\n\n\n\n\n\n\n\nremoved\nobserved\ncensored\nentrance\nat_risk\nKM_estimate\n\n\n\n\n0.0\n0\n0\n0\n26\n26\n1.000000\n\n\n59.0\n1\n1\n0\n0\n26\n0.961538\n\n\n115.0\n1\n1\n0\n0\n25\n0.923077\n\n\n156.0\n1\n1\n0\n0\n24\n0.884615\n\n\n268.0\n1\n1\n0\n0\n23\n0.846154\n\n\n329.0\n1\n1\n0\n0\n22\n0.807692\n\n\n353.0\n1\n1\n0\n0\n21\n0.769231\n\n\n365.0\n1\n1\n0\n0\n20\n0.730769\n\n\n377.0\n1\n0\n1\n0\n19\n0.730769\n\n\n421.0\n1\n0\n1\n0\n18\n0.730769\n\n\n431.0\n1\n1\n0\n0\n17\n0.687783\n\n\n448.0\n1\n0\n1\n0\n16\n0.687783\n\n\n464.0\n1\n1\n0\n0\n15\n0.641931\n\n\n475.0\n1\n1\n0\n0\n14\n0.596078\n\n\n477.0\n1\n0\n1\n0\n13\n0.596078\n\n\n563.0\n1\n1\n0\n0\n12\n0.546405\n\n\n638.0\n1\n1\n0\n0\n11\n0.496732\n\n\n744.0\n1\n0\n1\n0\n10\n0.496732\n\n\n769.0\n1\n0\n1\n0\n9\n0.496732\n\n\n770.0\n1\n0\n1\n0\n8\n0.496732\n\n\n803.0\n1\n0\n1\n0\n7\n0.496732\n\n\n855.0\n1\n0\n1\n0\n6\n0.496732\n\n\n1040.0\n1\n0\n1\n0\n5\n0.496732\n\n\n1106.0\n1\n0\n1\n0\n4\n0.496732\n\n\n1129.0\n1\n0\n1\n0\n3\n0.496732\n\n\n1206.0\n1\n0\n1\n0\n2\n0.496732\n\n\n1227.0\n1\n0\n1\n0\n1\n0.496732\n\n\n\n\n\n\n\n\n\nKaplan-Meier 法估计的生存率是一个阶梯状的函数，其阶梯跳跃点是给定的时间点，我们可以通过调用 plot 方法绘制生存曲线，如图所示：\n\nimport matplotlib.pyplot as plt\nfit.plot(show_censors = True)\nplt.show()",
    "crumbs": [
      "Home",
      "Guide",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#生存率的比较",
    "href": "Guide/Python/25-03-09-cos-reg.html#生存率的比较",
    "title": "用Python做生存分析和COX回归",
    "section": "",
    "text": "在生存分析中，经常需要比较不同情形下的生存率。在本例中，想要比较不同治疗方式下生存率，可以输入下面的命令：\n\ng1 = ovarian.rx == \"A\"\ng2 = ovarian.rx == \"B\"\nkmf_A = KaplanMeierFitter()\nkmf_A.fit(ovarian.futime[g1],ovarian.fustat[g1],label = \"Treatmeat A\")\nkmf_B = KaplanMeierFitter()\nkmf_B.fit(ovarian.futime[g2],ovarian.fustat[g2],label = \"Treatmeat B\")\n\n&lt;lifelines.KaplanMeierFitter:\"Treatmeat B\", fitted with 13 total observations, 8 right-censored observations&gt;\n\n\n\n\n可以单独提取两组的生存函数进行比较，但在同一个图中显示多条生存曲线更有助于生存率的比较。\n\nfig, axes = plt.subplots()\nkmf_A.plot(ax = axes,show_censors = True)\nkmf_B.plot(ax = axes,show_censors = True)\nplt.show()\n\n\n\n\n\n\n\n\n从上图中可以看出，治疗方式 “B” 的生存率高于治疗方式 “A” 的生存率，但是这种差异是由随机误差引起还是真是的治疗方式的不同所造成的差异，需要做进一步的统计学检验。\n\n\n\n因果关联的推断步骤\n\n\n\n\n\n生存分析中常用的统计学检验是 时序检验（log rank test），其基本思想是先计算出不同时间两种治疗方式的暴露人数和死亡人数，并由此在两种治疗方式效果相同的假设下计算出预期死亡人数，如果不拒绝零假设（ \\(H_0\\) ：两种治疗方式的效果相同，即预期死亡人数一致），则实际观测值和期望值的差异不会很大，如果差异过大则不能认为该差异是由随机误差引起的。\n对此，用 \\(\\chi^2\\) 检验做判断。时序检验可以用 lifetimes 库的函数 logrank_test 实现。\n\nfrom lifelines.statistics import logrank_test\nlr = logrank_test(ovarian.futime[g1], ovarian.futime[g2],\n                  ovarian.fustat[g1], ovarian.fustat[g2])\nlr.p_value\n\n0.3025911169890923\n\n\n这里得到的结果为 \\(P&gt;0.05\\) ，在一般情况下，我们会认为这是没有统计学意义的，即无法排除差异是由随机误差引起的。\n这种结果不显著的情况下，我们可以做一些思考，即是否有其他因素干预了结果的显著性，以及是否是样本量过小，导致差异不显著。（样本量的大小会影响检验效能，如果检验效能太低，即使有差异，也很难被检验方法发现）\n这里我们无法改变样本量的大小，但是可以考虑其他混杂因素的影响，并且使用更全面的模型进行检验。",
    "crumbs": [
      "Home",
      "Guide",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#建立模型",
    "href": "Guide/Python/25-03-09-cos-reg.html#建立模型",
    "title": "用Python做生存分析和COX回归",
    "section": "2.1 建立模型",
    "text": "2.1 建立模型\n在建立模型前，需要对分类变量进行哑变量处理：\n\ndf_dummy = pd.get_dummies(ovarian,drop_first = True)\ndf_dummy.head()\n\n\n\n\n\n\n\n\nfutime\nfustat\nage_&gt;50\nresid.ds_Yes\nrx_B\necog.ps_Good\n\n\n\n\n0\n59\n1\nTrue\nTrue\nFalse\nTrue\n\n\n1\n115\n1\nTrue\nTrue\nFalse\nTrue\n\n\n2\n156\n1\nTrue\nTrue\nFalse\nFalse\n\n\n3\n421\n0\nTrue\nTrue\nTrue\nTrue\n\n\n4\n431\n1\nTrue\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n使用 drop_first = True 参数是为了去掉各个参考类别。\n下面将所有的协变量都纳入，建立 Cox 回归模型：\n\nfrom lifelines import CoxPHFitter\ncox = CoxPHFitter()\ncox.fit(df_dummy,duration_col = 'futime', event_col = 'fustat')\ncox.print_summary()\n\n\n\n\n\n\n\nmodel\nlifelines.CoxPHFitter\n\n\nduration col\n'futime'\n\n\nevent col\n'fustat'\n\n\nbaseline estimation\nbreslow\n\n\nnumber of observations\n26\n\n\nnumber of events observed\n12\n\n\npartial log-likelihood\n-28.89\n\n\ntime fit was run\n2025-03-10 05:46:54 UTC\n\n\n\n\n\n\n\n\n\ncoef\nexp(coef)\nse(coef)\ncoef lower 95%\ncoef upper 95%\nexp(coef) lower 95%\nexp(coef) upper 95%\ncmp to\nz\np\n-log2(p)\n\n\n\n\nage_&gt;50\n2.20\n9.04\n1.11\n0.03\n4.37\n1.03\n79.10\n0.00\n1.99\n0.05\n4.42\n\n\nresid.ds_Yes\n1.45\n4.25\n0.73\n0.02\n2.88\n1.02\n17.75\n0.00\n1.98\n0.05\n4.40\n\n\nrx_B\n-1.38\n0.25\n0.64\n-2.65\n-0.12\n0.07\n0.89\n0.00\n-2.14\n0.03\n4.96\n\n\necog.ps_Good\n-0.59\n0.56\n0.63\n-1.83\n0.65\n0.16\n1.92\n0.00\n-0.93\n0.35\n1.50\n\n\n\n\n\n\n\n\n\nConcordance\n0.79\n\n\nPartial AIC\n65.78\n\n\nlog-likelihood ratio test\n12.19 on 4 df\n\n\n-log2(p) of ll-ratio test\n5.97\n\n\n\n\n\n\n\n结果显示，在调整了协变量后，两种治疗方式的死亡风险的差异具有统计学意义（P&lt;0.05）。\n模型的回归系数及其置信区间可以通过 plot 方法进行直观展示：\n\ncox.plot()\n\n\n\n\n\n\n\n\nCox 回归是一种半参数回归模型，也像多元线性回归一样，存在变量选择的问题。通常可以用 AIC 进行变量选择。1\n查看当前模型的 AIC 值：\n\ncox.AIC_partial_\n\n65.77513405570859\n\n\n根据前序的结果，变量 ecog.ps 对应的 P 值最大，对其进行剔除后再次拟合模型：\n\ncox1 = CoxPHFitter()\ndf_dummy_sub = df_dummy.drop('ecog.ps_Good', axis = 1)\ncox1.fit(df_dummy_sub, duration_col = 'futime', event_col = 'fustat')\ncox1.print_summary()\ncox1.AIC_partial_\n\n\n\n\n\n\n\nmodel\nlifelines.CoxPHFitter\n\n\nduration col\n'futime'\n\n\nevent col\n'fustat'\n\n\nbaseline estimation\nbreslow\n\n\nnumber of observations\n26\n\n\nnumber of events observed\n12\n\n\npartial log-likelihood\n-29.33\n\n\ntime fit was run\n2025-03-10 05:46:54 UTC\n\n\n\n\n\n\n\n\n\ncoef\nexp(coef)\nse(coef)\ncoef lower 95%\ncoef upper 95%\nexp(coef) lower 95%\nexp(coef) upper 95%\ncmp to\nz\np\n-log2(p)\n\n\n\n\nage_&gt;50\n2.11\n8.29\n1.09\n-0.02\n4.25\n0.98\n70.32\n0.00\n1.94\n0.05\n4.25\n\n\nresid.ds_Yes\n1.25\n3.50\n0.69\n-0.10\n2.61\n0.90\n13.58\n0.00\n1.81\n0.07\n3.83\n\n\nrx_B\n-1.28\n0.28\n0.62\n-2.50\n-0.06\n0.08\n0.94\n0.00\n-2.06\n0.04\n4.67\n\n\n\n\n\n\n\n\n\nConcordance\n0.77\n\n\nPartial AIC\n64.66\n\n\nlog-likelihood ratio test\n11.31 on 3 df\n\n\n-log2(p) of ll-ratio test\n6.62\n\n\n\n\n\n\n\n64.65793573545281\n\n\n剔除 ecog_ps 后的模型， AIC 值有所下降，但是不多（谨慎对待），能认为新模型优于原有模型。",
    "crumbs": [
      "Home",
      "Guide",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#footnotes",
    "href": "Guide/Python/25-03-09-cos-reg.html#footnotes",
    "title": "用Python做生存分析和COX回归",
    "section": "脚注",
    "text": "脚注\n\n\n赤池信息量准则，即Akaike information criterion，简称AIC，是衡量统计模型拟合优良性的一种标准，是由日本统计学家赤池弘次创立和发展的。赤池信息量准则建立在熵的概念基础上。\nAIC越小，模型越好，通常选择AIC最小的模型↩︎",
    "crumbs": [
      "Home",
      "Guide",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Stata/Stata-intro.html",
    "href": "Guide/Stata/Stata-intro.html",
    "title": "Stata",
    "section": "",
    "text": "关于 Stata 大二上统计学课的时候，老师在课上提了一嘴，说：“等你们以后读研了，就不用 SPSS 这种工具了，就会开始用 Stata、R 这些工具了“。也确实，本科期间确实就是一个 SPSS 管了四年，因为实在脱离他的应用场景，加之，老师就只会 SPSS ，那就没办法咯。\n想起那时候，专业两个班的 SPSS 软件基本上都是我去装的，老师弄不会，同学们更不会，我比较喜欢摸索，所以摸索出了这些，找到了安装包和密钥，然后拷在 U盘 里，课前课后课中就是给他们装软件，有时候，有些系统还装不上，某为就是，同学的某为一直装不上，当时看是因为缺 Java 环境，但是装了 Java JDK 还是不行，遂放弃。\n等到毕业的时候，想着看能不能用 Stata 做一下毕业论文的数据分析，最后太忙，没时间也没精力，用了 SPSS 结束。\n老师也是到了我大四的时候在哪里自学 Stata ，不过要说的是，在 AI 成熟以前，没有 AI 的辅助情况下，从0开始去学一门技能或程序，没有捷径，耗时耗力。现在逐渐理解，因为自己当时抱着 Python 的几本书，看了两三年也没有啥进展，等到 AI 出来了，不懂的就问 AI，节省了很多时间和精力；也和理解力的提升有关，进展迅速。\n回归正题，Stata 是一款用于数据科学的统计软件，其功能强大，但是对比 Python、R、MATLAB等程序或软件，还是略显不足，但是对于一般情况的数据分析，Stata 是够用的，其主要的优点是语法简洁和有诸多可以拿来即用的包，同时作为一款商业软件，其价格相较于 SAS 是很低的（但是换算RMB仍然很高），其支持相较于 R 等也可以说是较为丰富的（庞大的社区），还有跨平台使用等优势，这里不一一列举。\nStata 的安装很简单，互联网上有很多教程，但是关键的一点是获得 Key (许可证和激活秘钥)，当然互联网亦有诸多的资源可供选择。\n然后根据研究目的，选择合适的模型，找到前人写的代码，拿来增删改查，AI 大法，跑通，分析。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html",
    "href": "Guide/Stata/25-03-11-ITSA.html",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "Interrupted Time Series Analysis (ITSA) 是一种常用的时间序列分析方法，用于评估某个干预措施对某个事件或趋势的影响。\n\n\nITSA 模型的基本形式如下：\n\\[\nY_t = \\beta_0 + \\beta_1 \\cdot T_t + \\beta_2 \\cdot X_t + \\beta_3 \\cdot T_t \\cdot X_t + \\epsilon_t\n\\]\n公式中各代码的含义分别为：\n\n\\(Y_t\\)：因变量，时间序列的观测值\n\\(T_t\\)：时间变量（序列），表示时间点 \\(t\\) 距离干预前的时间长度\n\\(X_t\\)：干预变量（哑变量），表示干预措施的状态，通常为 0 或 1\n\\(\\beta_0\\)：截距，即常数项\n\\(\\beta_1\\)：时间变量的系数，表示时间的趋势（改革前的变化趋势）\n\\(\\beta_2\\)：干预变量的系数，表示干预的效应\n\\(\\beta_3\\)：交互项系数，表示改革后与改革前斜率的差值，故改革后的斜率值为 \\(\\beta_1 + \\beta_3\\)\n\\(\\epsilon_t\\)：误差项\n\n\n\n\n这里使用 Stata 中的 nlswork 数据集，该数据集包含了 1987 年和 1988 年的 个体数据。 首先找到 nlswork 数据集，你可以从互联网上寻找相关资源；或者从 Stata 的 nlswork 包中导出这一数据集，操作如下：\nsysuse nlswork, clear\nsave \"your-file-path\\nlswork.dta\", replace\n\n\n\n\n\nssc install itsa\nssc install actest\n\n\n\n\n需要分析的变量很多，但是我们可以首先从次均住院费用开始，这是最直接的一组数据，根据 DIP政策 实施的时间点来划分时间段。\n这里是2018-2023年六年的费用数据，我们首先对其按年进行处理，得到一个费用均数，实际上大多数论文都是按照月份进行处理，这样更合理也更详细一些，这里也可以按月份来，但是需要重新清洗数据，还是按照年份先试一下。\n但是也有个问题就是，2022年开始DIP改革，数据只截止到2023，因此2022-2023无法进行回归，只能用截距代替一下（数据不稳定，谨慎对待）。\n操作代码如下：\nclear all\nuse \"C:\\Users\\asus\\ITSA\\ITSA-PRE.dta\", replace\n\n// 按年份聚合数据，取平均值\ncollapse (mean) Cost, by(year)\n\n// 设置时间变量\ntsset year\n\n// 定义时间变量和干预变量\ngen time = year - 2017\ngen DIP = (year &gt;= 2022) // 2022年及以后为1，之前为0\ngen time_post = (time - 5) * DIP // 干预后时间变量\n\n// 计算政策前的趋势（2018-2021）\nregress Cost time if year &lt;= 2021\nlocal beta0 = _b[_cons]\nlocal beta1 = _b[time]\n\n// 计算 2022 年的预测值\nlocal cost_2022_pred = `beta0' + `beta1' * 5\ngen Cost_pred_2022 = `cost_2022_pred' if year == 2022\n\n// 计算政策前趋势线（2018-2022）\ngenerate Cost_pred_2018_2022 = `beta0' + `beta1' * time if year &lt;= 2022\n\n// 计算 2022 和 2023 年的真实值\ngen cost_2022_actual = Cost if year == 2022\ngen cost_2023_actual = Cost if year == 2023\negen cost_2022_real = max(cost_2022_actual)\negen cost_2023_real = max(cost_2023_actual)\nlocal cost_2022_actual = cost_2022_real\nlocal cost_2023_actual = cost_2023_real\n\n// 计算政策后趋势斜率\nlocal beta_post_1 = (`cost_2023_actual' - `cost_2022_actual') / (6 - 5)\n\n// 计算政策后趋势线（从 2022 真实值开始）\ngenerate Cost_pred_post = `cost_2022_actual' + `beta_post_1' * (time - 5) if year &gt;= 2022\n\n// 显示关键信息\ndisplay \"政策前趋势 β1: `beta1'\"\ndisplay \"2022 预测值: `cost_2022_pred'\"\ndisplay \"2022 真实值: `cost_2022_actual'\"\ndisplay \"2023 真实值: `cost_2023_actual'\"\ndisplay \"政策后斜率 β_post_1: `beta_post_1'\"\n\n// 画图\ntwoway (scatter Cost year, msize(small)) /// 观察值\n       (line Cost_pred_2018_2022 year if year &lt;= 2022, lcolor(blue)) /// 政策前趋势\n       (line Cost_pred_post year if year &gt;= 2022, lcolor(red)) /// 政策后趋势\n       (scatter Cost_pred_2022 year if year == 2022, mcolor(green) msize(medium)) /// 2022预测值\n       (pcarrow Cost_pred_2022 year Cost year if year == 2022, lcolor(green) mcolor(green)), /// \n       xline(2022, lpattern(dash)) ///\n       title(\"Cost Time Series Analysis\") ///\n       subtitle(\"Intervention at 2022\") ///\n       xlabel(2018(1)2023) ///\n       legend(label(1 \"Observed\") label(2 \"Pre-intervention trend\") ///\n              label(3 \"Post-intervention trend\") label(4 \"2022 Prediction\") label(5 \"Level change at 2022\"))\n\n// 保存图形\ngraph save \"C:\\Users\\asus\\test\\cost_time_series_graph.gph\", replace\n\n// 计算回归系数表\nregress Cost time DIP time_post\noutreg2 using \"C:\\Users\\asus\\test\\itsa_results.doc\", replace word\n这里用\n\n2018年的数据作为起始数据（ITSA方程的截距，即 \\(\\beta_0\\) ）；\n2018-2021年的数据拟合政策前 次均费用 随时间变化的趋势，即 \\(\\beta_1\\) ；\n然后用 \\(\\beta_1\\) 计算2022年的预测值(\\(cost_{2022pred} = \\beta_0 +\\beta_1 * 5\\) )；\n2022年到2023年的数据直接使用真实值，得到政策后的趋势直线，如果数据足够多（≥3），则可以对政策后的数据进行回归得到 \\(\\beta_3\\)；\n使用2022年的真实值减去2022年的预测值，得到政策变化对 cost 造成的水平影响，即 \\(\\beta_2\\) 。\n\n\n\n\n上述代码运行后输出 中断时间序列分析 趋势图：\n\n\n\nITSA-TREND\n\n\n\n\n\n代码的最后，做回归系数表，得到如下结果：\n\n\n\n回归系数表\n\n\n\n政策前的时间趋势为：\\(\\beta_1=331.6185\\)；\n政策实施时的瞬时变化为：\\(\\beta_2=-219.9033\\);\n政策实施后的变化趋势为：\\(\\beta_3=-1628.044\\) 。\n\n其他变量也就可以按照此种模式进行一一计算，当然也可以用循环的模式计算，以后再论。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#stsa-公式",
    "href": "Guide/Stata/25-03-11-ITSA.html#stsa-公式",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "ITSA 模型的基本形式如下：\n\\[\nY_t = \\beta_0 + \\beta_1 \\cdot T_t + \\beta_2 \\cdot X_t + \\beta_3 \\cdot T_t \\cdot X_t + \\epsilon_t\n\\]\n公式中各代码的含义分别为：\n\n\\(Y_t\\)：因变量，时间序列的观测值\n\\(T_t\\)：时间变量（序列），表示时间点 \\(t\\) 距离干预前的时间长度\n\\(X_t\\)：干预变量（哑变量），表示干预措施的状态，通常为 0 或 1\n\\(\\beta_0\\)：截距，即常数项\n\\(\\beta_1\\)：时间变量的系数，表示时间的趋势（改革前的变化趋势）\n\\(\\beta_2\\)：干预变量的系数，表示干预的效应\n\\(\\beta_3\\)：交互项系数，表示改革后与改革前斜率的差值，故改革后的斜率值为 \\(\\beta_1 + \\beta_3\\)\n\\(\\epsilon_t\\)：误差项",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#数据集及来源",
    "href": "Guide/Stata/25-03-11-ITSA.html#数据集及来源",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "这里使用 Stata 中的 nlswork 数据集，该数据集包含了 1987 年和 1988 年的 个体数据。 首先找到 nlswork 数据集，你可以从互联网上寻找相关资源；或者从 Stata 的 nlswork 包中导出这一数据集，操作如下：\nsysuse nlswork, clear\nsave \"your-file-path\\nlswork.dta\", replace",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#stata-准备",
    "href": "Guide/Stata/25-03-11-ITSA.html#stata-准备",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "ssc install itsa\nssc install actest",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#对次均住院费用进行分析",
    "href": "Guide/Stata/25-03-11-ITSA.html#对次均住院费用进行分析",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "需要分析的变量很多，但是我们可以首先从次均住院费用开始，这是最直接的一组数据，根据 DIP政策 实施的时间点来划分时间段。\n这里是2018-2023年六年的费用数据，我们首先对其按年进行处理，得到一个费用均数，实际上大多数论文都是按照月份进行处理，这样更合理也更详细一些，这里也可以按月份来，但是需要重新清洗数据，还是按照年份先试一下。\n但是也有个问题就是，2022年开始DIP改革，数据只截止到2023，因此2022-2023无法进行回归，只能用截距代替一下（数据不稳定，谨慎对待）。\n操作代码如下：\nclear all\nuse \"C:\\Users\\asus\\ITSA\\ITSA-PRE.dta\", replace\n\n// 按年份聚合数据，取平均值\ncollapse (mean) Cost, by(year)\n\n// 设置时间变量\ntsset year\n\n// 定义时间变量和干预变量\ngen time = year - 2017\ngen DIP = (year &gt;= 2022) // 2022年及以后为1，之前为0\ngen time_post = (time - 5) * DIP // 干预后时间变量\n\n// 计算政策前的趋势（2018-2021）\nregress Cost time if year &lt;= 2021\nlocal beta0 = _b[_cons]\nlocal beta1 = _b[time]\n\n// 计算 2022 年的预测值\nlocal cost_2022_pred = `beta0' + `beta1' * 5\ngen Cost_pred_2022 = `cost_2022_pred' if year == 2022\n\n// 计算政策前趋势线（2018-2022）\ngenerate Cost_pred_2018_2022 = `beta0' + `beta1' * time if year &lt;= 2022\n\n// 计算 2022 和 2023 年的真实值\ngen cost_2022_actual = Cost if year == 2022\ngen cost_2023_actual = Cost if year == 2023\negen cost_2022_real = max(cost_2022_actual)\negen cost_2023_real = max(cost_2023_actual)\nlocal cost_2022_actual = cost_2022_real\nlocal cost_2023_actual = cost_2023_real\n\n// 计算政策后趋势斜率\nlocal beta_post_1 = (`cost_2023_actual' - `cost_2022_actual') / (6 - 5)\n\n// 计算政策后趋势线（从 2022 真实值开始）\ngenerate Cost_pred_post = `cost_2022_actual' + `beta_post_1' * (time - 5) if year &gt;= 2022\n\n// 显示关键信息\ndisplay \"政策前趋势 β1: `beta1'\"\ndisplay \"2022 预测值: `cost_2022_pred'\"\ndisplay \"2022 真实值: `cost_2022_actual'\"\ndisplay \"2023 真实值: `cost_2023_actual'\"\ndisplay \"政策后斜率 β_post_1: `beta_post_1'\"\n\n// 画图\ntwoway (scatter Cost year, msize(small)) /// 观察值\n       (line Cost_pred_2018_2022 year if year &lt;= 2022, lcolor(blue)) /// 政策前趋势\n       (line Cost_pred_post year if year &gt;= 2022, lcolor(red)) /// 政策后趋势\n       (scatter Cost_pred_2022 year if year == 2022, mcolor(green) msize(medium)) /// 2022预测值\n       (pcarrow Cost_pred_2022 year Cost year if year == 2022, lcolor(green) mcolor(green)), /// \n       xline(2022, lpattern(dash)) ///\n       title(\"Cost Time Series Analysis\") ///\n       subtitle(\"Intervention at 2022\") ///\n       xlabel(2018(1)2023) ///\n       legend(label(1 \"Observed\") label(2 \"Pre-intervention trend\") ///\n              label(3 \"Post-intervention trend\") label(4 \"2022 Prediction\") label(5 \"Level change at 2022\"))\n\n// 保存图形\ngraph save \"C:\\Users\\asus\\test\\cost_time_series_graph.gph\", replace\n\n// 计算回归系数表\nregress Cost time DIP time_post\noutreg2 using \"C:\\Users\\asus\\test\\itsa_results.doc\", replace word\n这里用\n\n2018年的数据作为起始数据（ITSA方程的截距，即 \\(\\beta_0\\) ）；\n2018-2021年的数据拟合政策前 次均费用 随时间变化的趋势，即 \\(\\beta_1\\) ；\n然后用 \\(\\beta_1\\) 计算2022年的预测值(\\(cost_{2022pred} = \\beta_0 +\\beta_1 * 5\\) )；\n2022年到2023年的数据直接使用真实值，得到政策后的趋势直线，如果数据足够多（≥3），则可以对政策后的数据进行回归得到 \\(\\beta_3\\)；\n使用2022年的真实值减去2022年的预测值，得到政策变化对 cost 造成的水平影响，即 \\(\\beta_2\\) 。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#绘制itsa趋势图",
    "href": "Guide/Stata/25-03-11-ITSA.html#绘制itsa趋势图",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "上述代码运行后输出 中断时间序列分析 趋势图：\n\n\n\nITSA-TREND",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#输出统计结果",
    "href": "Guide/Stata/25-03-11-ITSA.html#输出统计结果",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "代码的最后，做回归系数表，得到如下结果：\n\n\n\n回归系数表\n\n\n\n政策前的时间趋势为：\\(\\beta_1=331.6185\\)；\n政策实施时的瞬时变化为：\\(\\beta_2=-219.9033\\);\n政策实施后的变化趋势为：\\(\\beta_3=-1628.044\\) 。\n\n其他变量也就可以按照此种模式进行一一计算，当然也可以用循环的模式计算，以后再论。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html",
    "title": "用Python做桑基图",
    "section": "",
    "text": "桑基图（Sankey diagram），即桑基能量分流图，也叫桑基能量平衡图。它是一种特定类型的流程图，概述图中延伸的分支的宽度对应数据流量的大小，通常应用于能源、材料成分、金融等数据的可视化分析。因1898年Matthew Henry Phineas Riall Sankey绘制的“蒸汽机的能源效率图”而闻名，此后便以其名字命名为“桑基图”。\nSankey diagrams are a data visualisation technique or flow diagram that emphasizes flow/movement/change from one state to another or one time to another, in which the width of the arrows is proportional to the flow rate of the depicted extensive property. The arrows being connected are called nodes and the connections are called links.\nSankey diagrams can also visualize the energy accounts, material flow accounts on a regional or national level, and cost breakdowns.The diagrams are often used in the visualization of material flow analysis.\nSankey diagrams emphasize the major transfers or flows within a system. They help locate the most important contributions to a flow. They often show conserved quantities within defined system boundaries.(Wikipedia contributors 2025)\n\n\n桑基图常用于可持续能源、物流、人口流动、资源分配等领域的数据可视化。它可以帮助用户直观地理解和分析复杂的流动和关系，从而支持决策和策划过程。\n\n\n\n\n节点：桑基图由一系列节点组成，每个节点代表一个特定的实体或类别。例如，节点可以代表不同的时间、地点和部门等。\n箭头：箭头表示流动的路径，从一个节点流向另一个节点。箭头的宽度通常表示流量或数量的大小。\n流量量级：桑基图可以显示不同节点之间的流量量级，通过箭头的宽度来表示。宽度越大，表示流量或数量越大。\n路径：桑基图可以显示多个节点之间的复杂路径，通过连接不同的节点和箭头来表示。\n颜色编码：桑基图可以使用颜色来编码不同的节点或流动路径，以帮助用户更好地理解和区分不同的实体或类别。\n\n\n\n\n在设计桑基图图表时，以下是一些需要注意的事项：\n\n数据准备：确保数据准备充分，包括节点和流量的数据。节点应该清晰明确，流量数据应该准确可靠。\n简洁明了：桑基图应该保持简洁明了，避免过多的节点和复杂的路径。过多的节点和路径可能会导致图表混乱不清晰，难以理解。\n良好的布局：选择合适的布局方式，使得节点和箭头的排列有一定的逻辑性。可以按照流动的方向或重要性进行布局。\n色彩选择：选择合适的色彩来区分不同的节点和流动路径。颜色应该鲜明对比，以便用户能够清晰地区分不同的实体或类别。\n箭头宽度控制：根据流量的大小，合理调整箭头的宽度。宽度应该能够直观地反映流量的差异，但也不能过于夸张。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html#桑基图的主要应用场景",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html#桑基图的主要应用场景",
    "title": "用Python做桑基图",
    "section": "",
    "text": "桑基图常用于可持续能源、物流、人口流动、资源分配等领域的数据可视化。它可以帮助用户直观地理解和分析复杂的流动和关系，从而支持决策和策划过程。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html#桑基图的特点",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html#桑基图的特点",
    "title": "用Python做桑基图",
    "section": "",
    "text": "节点：桑基图由一系列节点组成，每个节点代表一个特定的实体或类别。例如，节点可以代表不同的时间、地点和部门等。\n箭头：箭头表示流动的路径，从一个节点流向另一个节点。箭头的宽度通常表示流量或数量的大小。\n流量量级：桑基图可以显示不同节点之间的流量量级，通过箭头的宽度来表示。宽度越大，表示流量或数量越大。\n路径：桑基图可以显示多个节点之间的复杂路径，通过连接不同的节点和箭头来表示。\n颜色编码：桑基图可以使用颜色来编码不同的节点或流动路径，以帮助用户更好地理解和区分不同的实体或类别。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html#设计的注意事项",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html#设计的注意事项",
    "title": "用Python做桑基图",
    "section": "",
    "text": "在设计桑基图图表时，以下是一些需要注意的事项：\n\n数据准备：确保数据准备充分，包括节点和流量的数据。节点应该清晰明确，流量数据应该准确可靠。\n简洁明了：桑基图应该保持简洁明了，避免过多的节点和复杂的路径。过多的节点和路径可能会导致图表混乱不清晰，难以理解。\n良好的布局：选择合适的布局方式，使得节点和箭头的排列有一定的逻辑性。可以按照流动的方向或重要性进行布局。\n色彩选择：选择合适的色彩来区分不同的节点和流动路径。颜色应该鲜明对比，以便用户能够清晰地区分不同的实体或类别。\n箭头宽度控制：根据流量的大小，合理调整箭头的宽度。宽度应该能够直观地反映流量的差异，但也不能过于夸张。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html#本地python",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html#本地python",
    "title": "用Python做桑基图",
    "section": "2.1 本地Python",
    "text": "2.1 本地Python\n安装相关的包和库：\n\npip install dash\npip install numpy\n\n\n2.1.1 本地运行示例\n在 Python 终端或编辑器运行后，可以在浏览器中输入 http://127.0.0.1:8051/ 进行查看。\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Mar 14 17:07:53 2025\n\n@author: asus\n\"\"\"\n\nimport dash\nfrom dash import html, dcc, Input, Output\nimport plotly.graph_objects as go\nimport dash_bootstrap_components as dbc\nimport numpy as np\nimport plotly.express as px\n \n \ndef create_complex_sankey():\n    # 示例数据\n    labels = [\"能源\", \"电力\", \"运输\", \"工业\", \"住宅\", \"商业\", \"损失\", \"可再生\", \"化石燃料\", \"核能\"]\n    sources = [0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6]\n    targets = [1, 2, 3, 4, 5, 6, 4, 5, 6, 7, 7, 8, 8, 9, 9, 7]\n    values = [8, 4, 2, 8, 4, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1]\n \n    # 创建桑基图\n    sankey_fig = go.Figure(data=[go.Sankey(\n        node=dict(\n            pad=15,\n            thickness=20,\n            line=dict(color=\"black\", width=0.5),\n            label=labels,\n            color=[\"#FF9999\", \"#66B3FF\", \"#99FF99\", \"#FFCC99\", \"#FF6666\", \"#66FF66\", \"#6666FF\", \"#FF66FF\", \"#66FFFF\", \"#FFFF66\"]\n        ),\n        link=dict(\n            source=sources,\n            target=targets,\n            value=values,\n            color=[\"rgba(255, 153, 153, 0.6)\", \"rgba(102, 179, 255, 0.6)\", \"rgba(153, 255, 153, 0.6)\", \"rgba(255, 204, 153, 0.6)\",\n                   \"rgba(255, 102, 102, 0.6)\", \"rgba(102, 255, 102, 0.6)\", \"rgba(102, 102, 255, 0.6)\", \"rgba(255, 102, 255, 0.6)\",\n                   \"rgba(102, 255, 255, 0.6)\", \"rgba(255, 255, 102, 0.6)\", \"rgba(255, 153, 153, 0.6)\", \"rgba(102, 179, 255, 0.6)\",\n                   \"rgba(153, 255, 153, 0.6)\", \"rgba(255, 204, 153, 0.6)\", \"rgba(255, 102, 102, 0.6)\", \"rgba(102, 255, 102, 0.6)\"]\n        )\n    )])\n \n    # 更新布局\n    sankey_fig.update_layout(\n        title='复杂桑基图示例',\n        font_size=10,\n        template='plotly_white'\n    )\n \n    return sankey_fig\n \napp = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n \napp.layout = html.Div([\n    html.H3(\"桑基图展示\", className=\"text-center mt-4 mb-3\"),\n    dcc.Graph(figure=create_complex_sankey())\n])\n \nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=8051)",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html#jupyter-实现",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html#jupyter-实现",
    "title": "用Python做桑基图",
    "section": "2.2 Jupyter 实现",
    "text": "2.2 Jupyter 实现\n为了能在本网页中显示桑基图，需要在 jupyter 中运行该程序，那么除了相关的 jupyter 包需要被安装外，需要更换 dash 包为 jupyter-dash 。\n\n2.2.1 安装 jupyter-dash\n\npip install jupyter-dash\n\n\n\n2.2.2 运行程序\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Mar 14 17:07:53 2025\n\n@author: asus\n\"\"\"\n\nfrom jupyter_dash import JupyterDash  # 替换 dash\nfrom dash import html, dcc, Input, Output\nimport plotly.graph_objects as go\nimport dash_bootstrap_components as dbc\nimport numpy as np\nimport plotly.express as px\n\ndef create_complex_sankey():\n    labels = [\"能源\", \"电力\", \"运输\", \"工业\", \"住宅\", \"商业\", \"损失\", \"可再生\", \"化石燃料\", \"核能\"]\n    sources = [0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6]\n    targets = [1, 2, 3, 4, 5, 6, 4, 5, 6, 7, 7, 8, 8, 9, 9, 7]\n    values = [8, 4, 2, 8, 4, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1]\n\n    sankey_fig = go.Figure(data=[go.Sankey(\n        node=dict(\n            pad=15,\n            thickness=20,\n            line=dict(color=\"black\", width=0.5),\n            label=labels,\n            color=[\"#FF9999\", \"#66B3FF\", \"#99FF99\", \"#FFCC99\", \"#FF6666\", \"#66FF66\", \"#6666FF\", \"#FF66FF\", \"#66FFFF\", \"#FFFF66\"]\n        ),\n        link=dict(\n            source=sources,\n            target=targets,\n            value=values,\n            color=[\"rgba(255, 153, 153, 0.6)\", \"rgba(102, 179, 255, 0.6)\", \"rgba(153, 255, 153, 0.6)\", \"rgba(255, 204, 153, 0.6)\",\n                   \"rgba(255, 102, 102, 0.6)\", \"rgba(102, 255, 102, 0.6)\", \"rgba(102, 102, 255, 0.6)\", \"rgba(255, 102, 255, 0.6)\",\n                   \"rgba(102, 255, 255, 0.6)\", \"rgba(255, 255, 102, 0.6)\", \"rgba(255, 153, 153, 0.6)\", \"rgba(102, 179, 255, 0.6)\",\n                   \"rgba(153, 255, 153, 0.6)\", \"rgba(255, 204, 153, 0.6)\", \"rgba(255, 102, 102, 0.6)\", \"rgba(102, 255, 102, 0.6)\"]\n        )\n    )])\n\n    sankey_fig.update_layout(\n        title='复杂桑基图示例',\n        font_size=10,\n        template='plotly_white'\n    )\n\n    return sankey_fig\n\n# 使用 JupyterDash 替代 Dash\napp = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n\napp.layout = html.Div([\n    html.H3(\"桑基图展示\", className=\"text-center mt-4 mb-3\"),\n    dcc.Graph(figure=create_complex_sankey())\n])\n\n# 在 Jupyter 中运行，mode='inline' 将图形嵌入笔记本\napp.run_server(mode='inline')\n\n\n        \n        \n\n\n\n        \n        \n\n\n\n\n2.2.3 网页无法正确显示\n因为 GitHub 只支持静态网页，但是 桑基图 是一个通过 Flask 生成的动态 Web 应用，因为当本地生成后托管到 GitHub 后，是无法正确显示该图形。\n解决办法：\n\n将网页托管到支持动态图像的服务器上，如 Render 等\n改用静态图形\n\n\nimport plotly.graph_objects as go\n\ndef create_complex_sankey():\n    labels = [\"能源\", \"电力\", \"运输\", \"工业\", \"住宅\", \"商业\", \"损失\", \"可再生\", \"化石燃料\", \"核能\"]\n    sources = [0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6]\n    targets = [1, 2, 3, 4, 5, 6, 4, 5, 6, 7, 7, 8, 8, 9, 9, 7]\n    values = [8, 4, 2, 8, 4, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1]\n    sankey_fig = go.Figure(data=[go.Sankey(\n        node=dict(pad=15, thickness=20, line=dict(color=\"black\", width=0.5), label=labels),\n        link=dict(source=sources, target=targets, value=values)\n    )])\n    sankey_fig.update_layout(title='复杂桑基图示例', font_size=10, template='plotly_white')\n    return sankey_fig\n\n# 保存为静态 HTML 文件\nfig = create_complex_sankey()\nfig.write_html(\"sankey.html\", include_plotlyjs='cdn')",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#假设检验",
    "href": "Learn/Basic/10-regression-correlation.html#假设检验",
    "title": "简单线性相关和回归",
    "section": "\n2.1 假设检验",
    "text": "2.1 假设检验\n\n2.1.1 F检验\n\\(y_i\\)的总离均差平方和为：\n\\[SS_{yy}=\\sum_{i}(y_i-\\bar y)^2\\] 对其做分解，得到等式：\n\\[SS_{yy}=\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\] \\(\\sum_{i}^{n}(\\hat y_i-\\bar y)^2\\)为回归平方和（regression sum of squares），记为\\(SS_R\\)，表示回归估计值\\(\\hat y_i\\)与均数\\(\\bar y\\)的离差平方和，其公式为：\n\\[\n\\begin{align}\nSS_{yy} &= \\sum_{i=1}^{n}(\\hat y_i - \\bar y)^2 \\\\\n        &= \\sum_{i=1}^{n}[a + bx_i - (a + b\\bar x)]^2 \\\\\n        &= SS_{xx}b^2 \\\\\n        &= SS_{xy}b\n\\end{align}\n\\] 显然，回归平方和\\(SS_{R}\\)反映的是在y的总变异中由x与y的直线回归关系解释的那部分变异。\\(SS_R\\)值越大，说明回归直线的拟合效果就越好。\n\\(\\sum_{i}^{n}(y_i-\\hat y_i)^2\\)为残差平方和（residual sum of squares），记为\\(SS_E\\)，表示观测值\\(y_i\\)与回归估计值\\(\\hat y_i\\)的离差平方和，其公式为： \\[SS_E=\\sum_{i=1}^{n}(y_i-\\hat y_i)^2\\] \\(SS_E\\)反映了在总变异中扣除自变量x对因变量y的线性影响以后的其他因素（包括x对y的非线性影响和随机误差等）对y变异的影响，也就是在总平方和中无法用y和x线性回归关系解释的部分。\\(SS_E\\)值越小，说明回归直线的拟合效果就越好。\n对公式进行简化： \\[\\begin{align}\nSS_{yy}=&\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\\\\n=&SS_R+SS_E\n\\end{align}\\] 上述三个平方和，各有其相应的自由度\\(v\\)，并有如下关系： \\[v_{yy}=v_R+v_E\\\\\nv_{yy}=n-1,v_R=1,v_E=n-2\\]\n在\\(H_0\\)成立的条件下，有： \\[\\frac{SS_R}{\\sigma^2}\\sim \\chi^2(v_R),\\frac{SS_E}{\\sigma^2}\\sim \\chi^2(v_E)\\] 且\\(SS_R\\)和\\(SS_E\\)相互独立。\n检验统计量：\n\\[F=\\frac{SS_R/v_R}{SS_E/v_E}\\] 服从自由度\\(v_R=1,v_E=n-2\\)的F分布。如果y和x确实存在直线回归关系，那么回归所解释的变异\\(SS_R\\)应大于其他因素所解释的变异\\(SS_E\\)。由此可见，F检验正是建立在这个基础上的。\n对于给定的检验水准\\(\\alpha\\)， 如果\\(F&gt;F_{(v_R,v_E),1-\\alpha}\\)，则拒绝\\(H_0\\)，认为直线回归方程有统计学显著性；\n如果\\(F\\leq F_{(v_R,v_E),1-\\alpha}\\)，则不拒绝\\(H_0\\)，尚不能认为直线回归方程有统计学显著性。\n\n2.1.2 t检验法\n回归直线方程的稳定性程度取决于 \\(b\\) 的波动大小，即 \\(S_b\\) 的大小，这里的 \\(S_b\\) 为样本回归系数 \\(b\\) 的标准误的估计值。由于统计量 \\(b\\) 来自正态总体，故可从 \\(b\\) 的抽样分布出发构造 \\(t\\) 统计量对其进行假设检验。\n当 \\(H_0\\) ：\\(\\beta = 0\\) 成立时，检验统计量服从自由度 \\(v_E=n-2\\) 的 \\(t\\) 分布。\n\\[\nt=\\frac{b-0}{S_b}\\sim t(v_E)\n\\]\n其中：\n\\[\nS_b=\\frac{S_{yx}}{\\sqrt{SS_{xx}}}=\\sqrt{\\frac{S_{yx}/(n-2)}{SS_{xx}}}\n\\]\n\\(S_{yx}=\\sqrt{SS_E/(n-2)}\\) 为剩余标准差（residual standard deviation），是指扣除 \\(x\\) 对 \\(y\\) 的线性影响后，衡量观测值 \\(y\\) 对回归直线的平均离散程度，即回归直线 \\(\\hat y\\) 估计的精度。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#直线回归方程的区间估计",
    "href": "Learn/Basic/10-regression-correlation.html#直线回归方程的区间估计",
    "title": "简单线性相关和回归",
    "section": "\n2.2 直线回归方程的区间估计",
    "text": "2.2 直线回归方程的区间估计\n\n2.2.1 总体回归系数的置信区间\n\\[\nt = \\frac{b-\\beta}{S_b}\\sim t(v_E)\n\\]",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#数学定义",
    "href": "Learn/Basic/10-regression-correlation.html#数学定义",
    "title": "简单线性相关和回归",
    "section": "\n3.1 数学定义",
    "text": "3.1 数学定义\n均值向量：对于双元正态变量 \\((X, Y)\\)，均值向量为：\n\\[\\mu = \\begin{pmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{pmatrix}\\]\n其中 \\(\\mu_X\\) 和 \\(\\mu_Y\\) 分别是随机变量 \\(X\\) 和 \\(Y\\) 的均值。 2. 协方差矩阵：协方差矩阵为：\n\\[\\Sigma = \\begin{pmatrix}\n\\sigma_X^2 & \\sigma_{XY} \\\\\n\\sigma_{XY} & \\sigma_Y^2\n\\end{pmatrix}\\]\n其中 \\(\\sigma_X^2\\) 和 \\(\\sigma_Y^2\\) 是 \\(X\\) 和 \\(Y\\) 的方差，\\(\\sigma_{XY}\\) 是它们之间的协方差。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#性质",
    "href": "Learn/Basic/10-regression-correlation.html#性质",
    "title": "简单线性相关和回归",
    "section": "\n3.2 性质",
    "text": "3.2 性质\n\n边缘分布：\\(X\\) 和 \\(Y\\) 的边缘分布也是正态分布。\n条件分布：给定 \\(X\\) 的值，\\(Y\\) 的条件分布也是正态分布。\n相关性：协方差矩阵的值可以用来判断 X 和 Y 之间的相关性。如果 \\(\\sigma_{XY} &gt; 0\\)，则两者正相关；如果 \\(\\sigma_{XY} &lt; 0\\)，则负相关。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#示例",
    "href": "Learn/Basic/10-regression-correlation.html#示例",
    "title": "简单线性相关和回归",
    "section": "\n3.3 示例",
    "text": "3.3 示例\n假设有一组数据描述学生的身高 \\(X\\) 和体重 \\(Y\\)，并且假设 \\((X, Y)\\) 服从双元正态分布：\n\n3.3.1 均值向量：\n\\[\\mu = \\begin{pmatrix}\n170 \\\\\n65\n\\end{pmatrix}\\]\n表示身高的均值为 170 厘米，体重的均值为 65 千克。 •\n\n3.3.2 协方差矩阵：\n\\[\\Sigma = \\begin{pmatrix}\n100 & 20 \\\\\n20 & 25\n\\end{pmatrix}\\]\n这里，身高的方差为 100，体重的方差为 25，协方差为 20，表示身高和体重之间存在正相关关系。\n在这个示例中，如果我们知道某个学生的身高为 180 厘米，我们可以利用条件分布来预测他的体重，这个体重的预测值也是正态分布。\n我们用一个图形来展示：\n\n## 安装和加载所需的包\n#install.packages(\"plotly\")\n#install.packages(\"mvtnorm\")\nlibrary(plotly)\nlibrary(mvtnorm)\nlibrary(webshot2)\n\n# 创建网格数据\nx &lt;- seq(150, 190, length.out = 100)\n#身高150-190，等距的100个值\ny &lt;- seq(50, 80, length.out = 100)\n#体重50-80，等距的100个值\ngrid &lt;- expand.grid(X = x, Y = y)\n#生成 x 和 y 的所有组合，用于构建一个网格数据框，以便计算多元正态分布的概率密度。\n\n# 设置均值和协方差矩阵\nmu &lt;- c(170, 65)\n#设置双元正态分布的均值向量，表示均值分别为身高 170 cm 和体重 65 kg\n\nsigma &lt;- matrix(c(100, 20, 20, 25), nrow = 2)\n#设置协方差矩阵，表示身高的方差为 100，体重的方差为 25，身高和体重之间的协方差为 20\n\n# 计算概率密度\nz &lt;- dmvnorm(as.matrix(grid), mean = mu, sigma = sigma)\n#计算每个网格点上双元正态分布的概率密度。\n\n# 将概率密度矩阵转换为适合绘图的形状\nz_matrix &lt;- matrix(z, nrow = 100, ncol = 100)\n\n# 绘制三维表面图\nplot_ly(x = x, y = y, z = z_matrix, type = \"surface\") %&gt;%\n  layout(title = list(text = \"双元正态分布的三维概率密度图\", y=0.95),\n         scene = list(xaxis = list(title = \"身高 (cm)\"),\n                      yaxis = list(title = \"体重 (kg)\"),\n                      zaxis = list(title = \"概率密度\")))\n\n\nBinary normal distribution\n\n\n注：上述图像在被转换为PDF文件时，会发生报错：Quarto 文档中包含了一些生成 HTML 输出的函数（比如交互式图表或其他 HTML 小部件），但你当前的目标输出格式是 PDF。由于 PDF 是静态格式，无法直接渲染 HTML 内容，Quarto 会报错并停止执行。\n解决方案，此章节不转换为PDF格式，或者：\n如果你仍想输出 PDF，但希望将 HTML 小部件作为静态截图嵌入，可以安装 R 的 webshot 或 webshot2 包。Quarto 会利用它们将 HTML 内容转换为图片。\n需要安装：\n\ninstall.packages(\"webshot2\")\n\n然后在这段程序的前部导入该包：library(webshot2)。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-non-parameter-test.html",
    "href": "Learn/Basic/10-non-parameter-test.html",
    "title": "非参数检验",
    "section": "",
    "text": "秩和检验（Rank-Sum Test）是一种非参数检验方法，用于比较两个独立样本的分布是否存在显著差异。它无需对数据分布作正态性假设，适用于数据偏离正态分布、样本量较小或数据为序数型变量的场景。\n常见的秩和检验包括：\n\nMann-Whitney U 检验（也称Wilcoxon秩和检验）：用于比较两个独立样本的中位数是否相等。\nWilcoxon 符号秩检验：用于两个配对样本的比较（类似配对t检验，但无需正态性假设）。\n\n\n\n\nMann-Whitney U 检验公式\n\n假设两组独立样本分别为 \\(X\\) 和 \\(Y\\)，样本量分别为 \\(n_1\\) 和 \\(n_2\\)。\n对两组样本合并并按大小排序，赋予秩次。计算两组的秩次和 \\(R_1\\) 和 \\(R_2\\)（分别为 $ X$ 和 \\(Y\\) 的秩次总和）。\n\n确定统计量T值：\n\n假设两组样本量 \\(n_1&lt;n_2\\)，一般情况下以样本量较小者\\(n_1\\)对应的秩和\\(T_1\\)为检验统计量\\(T\\)，当样本相等时可以选择任一组的秩和为\\(T\\)。1\n当两组中样本量较小者不低于10时，在\\(H_0\\)成立假设下，统计量\\(T\\)的抽样分布近似于正态分布，有\n\\[T\\approx N\\left(\\frac{n_1(n+1)}{2},\\frac{n_1 n_2(n+1)}{12} \\right)\\] 此时，Wilcoxon 秩和统计量在\\(H_0\\)下关于\\(\\mu=\\frac{n_1(n+1)}{2}\\)对称。\n如果没有或存在较少的“结”，将\\(T\\)标准化后为：\n\\[U=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2(n+1)}{12}}}\\approx N(0,1)\\]\n其中，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n若“结”的比例较多（&gt;25%），则用以下公式校正：\n\\[U_c=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2}{12}[(n+1)-\\sum_\\limits{i=1}^{g}\\frac{t_i^3-t_i}{n(n-1)}]}}\\approx N(0,1)\\]\n\nWilcoxon 符号秩检验公式\n\n对配对样本 \\((X_i, Y_i)\\)，计算差值 \\(D_i = X_i - Y_i\\)，取非零差值的绝对值并排序（若差值为0则舍去不计，且减去相应的个数），赋予秩次 \\(R_i\\)。再根据差值的符号计算符号秩次和 \\(W\\)：\n\\[W = \\sum R_i \\cdot \\text{sign}(D_i)\\]\n检验统计量 \\(T\\) 是 \\(W\\) 的绝对值，依据表或正态分布计算显著性。\n正态近似法：\n当\\(n\\ge 30\\)时，有中心极限定理可知，当\\(H_0\\)成立时统计量\\(T\\)的抽样分布近似正态分布，有\n\\[T\\approx N \\left(\\frac{n(n+1)}{4},\\frac{n(n+1)(2n+1)}{24}\\right)\\] 其中，均数\\(\\mu=\\frac{n(n+1)}{4}\\)，方差\\(\\sigma^2=\\frac{n(n+1)(2n+1)}{24}\\)。 将T标准化后，近似服从标准正态分布，有\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}}}\\approx N(0,1)\\] 其中，n是差值不为0的对子数，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n当N较大时，样本中可能存在较多的“结”，（如“结”所占比例大于25%），此时需要使用校正公式：\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}-\\frac{\\sum_\\limits{i=1}^g(t_i^3-t_i)}{48}}}\\approx N(0,1)\\] 其中，\\(t_i\\)为\\(i\\)个“结”中有相同秩次的个数，\\(g\\)是“结”的个数。\nWilcoxon符号秩检验的前提条件为数据是连续的且差值分布是对称的。\nnotice：秩和秩和的区别：秩是指全部观察值按某种顺序排列的位序，在一定程度上反映了等级的高低；而秩和则表示同组秩次之和，在一定程度上反映了等级的分布。2\n\n\n\n\nMann-Whitney U 检验：\n\n比较两个独立样本的中位数是否存在显著差异。\n\n适用于非正态分布数据或含有极端值的样本。\n\n示例：比较两种治疗方法的疗效（不同受试者组）。\n\nWilcoxon 符号秩检验：\n\n比较两个配对样本的中位数差异。\n\n适用于重复测量数据或实验设计中存在配对关系的场景。\n\n示例：同一批受试者在治疗前后血压的变化。\n\n\n\n\n\n\n秩和检验是非参数方法，对数据分布假设少，但效率可能低于参数方法（如t检验）在满足条件时的效果，如果满足参数检验的条件，应优先考虑使用参数检验的方法，否则会增加犯二类错误的概率。\n数据需要满足独立性假设，否则检验结果可能不准确。\n\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/10-non-parameter-test.html#秩和检验",
    "href": "Learn/Basic/10-non-parameter-test.html#秩和检验",
    "title": "非参数检验",
    "section": "",
    "text": "秩和检验（Rank-Sum Test）是一种非参数检验方法，用于比较两个独立样本的分布是否存在显著差异。它无需对数据分布作正态性假设，适用于数据偏离正态分布、样本量较小或数据为序数型变量的场景。\n常见的秩和检验包括：\n\nMann-Whitney U 检验（也称Wilcoxon秩和检验）：用于比较两个独立样本的中位数是否相等。\nWilcoxon 符号秩检验：用于两个配对样本的比较（类似配对t检验，但无需正态性假设）。\n\n\n\n\nMann-Whitney U 检验公式\n\n假设两组独立样本分别为 \\(X\\) 和 \\(Y\\)，样本量分别为 \\(n_1\\) 和 \\(n_2\\)。\n对两组样本合并并按大小排序，赋予秩次。计算两组的秩次和 \\(R_1\\) 和 \\(R_2\\)（分别为 $ X$ 和 \\(Y\\) 的秩次总和）。\n\n确定统计量T值：\n\n假设两组样本量 \\(n_1&lt;n_2\\)，一般情况下以样本量较小者\\(n_1\\)对应的秩和\\(T_1\\)为检验统计量\\(T\\)，当样本相等时可以选择任一组的秩和为\\(T\\)。1\n当两组中样本量较小者不低于10时，在\\(H_0\\)成立假设下，统计量\\(T\\)的抽样分布近似于正态分布，有\n\\[T\\approx N\\left(\\frac{n_1(n+1)}{2},\\frac{n_1 n_2(n+1)}{12} \\right)\\] 此时，Wilcoxon 秩和统计量在\\(H_0\\)下关于\\(\\mu=\\frac{n_1(n+1)}{2}\\)对称。\n如果没有或存在较少的“结”，将\\(T\\)标准化后为：\n\\[U=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2(n+1)}{12}}}\\approx N(0,1)\\]\n其中，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n若“结”的比例较多（&gt;25%），则用以下公式校正：\n\\[U_c=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2}{12}[(n+1)-\\sum_\\limits{i=1}^{g}\\frac{t_i^3-t_i}{n(n-1)}]}}\\approx N(0,1)\\]\n\nWilcoxon 符号秩检验公式\n\n对配对样本 \\((X_i, Y_i)\\)，计算差值 \\(D_i = X_i - Y_i\\)，取非零差值的绝对值并排序（若差值为0则舍去不计，且减去相应的个数），赋予秩次 \\(R_i\\)。再根据差值的符号计算符号秩次和 \\(W\\)：\n\\[W = \\sum R_i \\cdot \\text{sign}(D_i)\\]\n检验统计量 \\(T\\) 是 \\(W\\) 的绝对值，依据表或正态分布计算显著性。\n正态近似法：\n当\\(n\\ge 30\\)时，有中心极限定理可知，当\\(H_0\\)成立时统计量\\(T\\)的抽样分布近似正态分布，有\n\\[T\\approx N \\left(\\frac{n(n+1)}{4},\\frac{n(n+1)(2n+1)}{24}\\right)\\] 其中，均数\\(\\mu=\\frac{n(n+1)}{4}\\)，方差\\(\\sigma^2=\\frac{n(n+1)(2n+1)}{24}\\)。 将T标准化后，近似服从标准正态分布，有\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}}}\\approx N(0,1)\\] 其中，n是差值不为0的对子数，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n当N较大时，样本中可能存在较多的“结”，（如“结”所占比例大于25%），此时需要使用校正公式：\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}-\\frac{\\sum_\\limits{i=1}^g(t_i^3-t_i)}{48}}}\\approx N(0,1)\\] 其中，\\(t_i\\)为\\(i\\)个“结”中有相同秩次的个数，\\(g\\)是“结”的个数。\nWilcoxon符号秩检验的前提条件为数据是连续的且差值分布是对称的。\nnotice：秩和秩和的区别：秩是指全部观察值按某种顺序排列的位序，在一定程度上反映了等级的高低；而秩和则表示同组秩次之和，在一定程度上反映了等级的分布。2\n\n\n\n\nMann-Whitney U 检验：\n\n比较两个独立样本的中位数是否存在显著差异。\n\n适用于非正态分布数据或含有极端值的样本。\n\n示例：比较两种治疗方法的疗效（不同受试者组）。\n\nWilcoxon 符号秩检验：\n\n比较两个配对样本的中位数差异。\n\n适用于重复测量数据或实验设计中存在配对关系的场景。\n\n示例：同一批受试者在治疗前后血压的变化。\n\n\n\n\n\n\n秩和检验是非参数方法，对数据分布假设少，但效率可能低于参数方法（如t检验）在满足条件时的效果，如果满足参数检验的条件，应优先考虑使用参数检验的方法，否则会增加犯二类错误的概率。\n数据需要满足独立性假设，否则检验结果可能不准确。\n\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/10-non-parameter-test.html#footnotes",
    "href": "Learn/Basic/10-non-parameter-test.html#footnotes",
    "title": "非参数检验",
    "section": "脚注",
    "text": "脚注\n\n\n不是说一定要选择样本量较小者对应的秩和作为检验统计量，只是长期的使用习惯，造成了这一惯例。如果取较小的秩和计算后得到的\\(U&lt;u_{\\alpha/2}\\)，则表示拒绝\\(H_0\\)；相反，如果取较大的秩和计算后得到的\\(U&gt;u_{1-\\alpha/2}\\)，也会表示拒绝\\(H_0\\)，他们都表示检验统计量落在了拒绝域中。↩︎\n尽管非参数方法对总体分布形式未做要求，但如果我们知道总体的一些性质而不去利用，就会浪费许多有用的信息，最常见的就是分布的对称性，配对设计的 Wilcoxon 符号秩检验充分利用了差值分布对称性这一信息，这与尽可能地采用有效方法，利用尽可能多的信息进行统计分析的大原则相一致。↩︎",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-chi2-test.html",
    "href": "Learn/Basic/09-chi2-test.html",
    "title": "卡方检验",
    "section": "",
    "text": "资料特征\n数据特征\n\n完全随机设计\n\n配对设计\n随机区组\n\n\n\n\n\n\n单组\n两组\n多组\n\n\n\n\n分类资料\n无序分类资料\n二项分布直接计算概率法、正态近似法（Z检验）、率的正态近似\n独立四格表\\(\\chi^2\\)检验、Fisher确切概率法\nR×C交叉表\\(\\chi^2\\)检验、Fisher确切概率法\n配对四格表\\(\\chi^2\\)检验，配对R×R列联表\\(\\chi^2\\)检验\n/\n\n\n\n等级资料\nWilcoxon符合秩和检验\nwilcoxon秩和检验\nKruskal-Wallis H检验\nWilcoxon符合秩和检验\nFriedman M秩和检验\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n内容\n\n\n\n\n确切概率法\n1. 适用情形：样本量较小或\\(\\pi_0\\)不靠近0.5时作单侧检验的情形。2. 计算公式：(1)最多有k例阳性的概率：\\(Pr(X\\le k)\\)(2)最少有k例阳性的概率：\\(Pr(X\\ge k)\\)\n\n\n正态近似法\n1. 适用情形：样本量较大时，\\(n\\pi,n(1-\\pi)\\)均大于5；2. 计算公式：分子为\\(p-\\pi_0\\)，分母为率的标准误\n\n\n\nnotice：上式中p为样本率，\\(\\pi_0\\)为给的总体率（常为理论值或标准值），n为样本含量。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n独立四格表卡方检验\n\\(n\\ge 40\\)且所有的\\(T\\ge 5\\)\\(n\\ge 40\\)且任一理论频数有\\(1\\le T&lt; 5\\)当\\(n&lt;40\\)，或任一一个格子理论频数\\(T&lt;1\\)时\n卡方基本公式、独立四格表专用公式同上、但是需要校正用四格表资料的Fisher确切概率法\n\n\n正态近似法\n\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)均大于5\n分子为样本率之差，分母为样本率差的标准误\\(S_{p1-p2}\\)为两个样本率之差的标准误，\\(p_c=\\frac{x_1+x_2}{n_1+n_2}\\)为两样本的合并率\n\n\n校正样本率的正态近似法\n当\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)不太大时\n同上，但是需要对样本率实施“分子+2、分母+4”的校正\n\n\n\nnotice：\n\n正态近似法与卡方检验结果是很接近的。在日常计算时，因为计算简便，故常用卡方检验公式。\n四格表的自由度为1。\n四格表实际频数变动时，若周边合计数保持不变，则理论频数将不会产生变化。\n用\\(n_R\\)和\\(n_C\\)和n分别表示行合计、列合计和总合计，则计算每格理论数的公式为：\\(T_{RC}=\\frac{n_R×n_C}{n}\\)。\n\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}\\)。\n校正的\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|A-T|-0.5)^2}{T}\\)。\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n配对四格表卡方检验\n当\\((b+c)\\ge 40\\)时当\\((b+c)&lt;40\\)时\n配对卡方检验专用公式校正配对卡方检验专用公式\n\n\n配对R×R交叉表数据的\\(\\chi^2\\)检验\nR（\\(R\\ge2\\)）\n\\(T=\\frac{k-1}{k}\\sum_{i=1}^{k}\\frac{(n_i-m_i)^2}{n_i+m_i-2A_{ii}}\\)\n\n\n\nnotice：\n\n配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}=\\frac{(b-c)^2}{b+c}\\)。\n若b+c&lt;40,使用校正的配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|b-c|-1)^2}{b+c}\\)。\n\n\n\n\n\n\n\n\n建立假设检验，确定检验水准 \\(H_0\\):两变量之间相互独立 \\(H_1\\):两变量之间相互独立 \\(\\alpha=0.05\\)\n计算检验统计量 [^2=_{i,j} ]\n确定P值，做出推断\n关联系数的计算 [r=]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n内容\n\n\n\n\n假设检验\n\\(H_0\\)：各组总体率（或构成比）相同。\\(H_1\\)：各组总体率（或构成比）不同（不全相同）。\n\n\n计算公式\n卡方检验基本公式，自由度为：\\(v=(R-1)(C-1)\\)\n\n\n数据要求\n1. 应用条件：不能有理论频数小于1的格子，或者不能有1/5以上的理论频数大于等于1且小于5 2. 不能进行卡方检验时的解决办法：①增加样本量；②合并或删除理论频数比较小的行或列；③采用Fisher确切概率法\n\n\n卡方分割\n多个率或多个频率分布比较的卡方检验，当结论为拒绝\\(H_0\\)时，仅表示多组之间是有差别的。若需要明确研究是那两组之间存在差别，可做率的多重比较，将R×C表分割为若干个小的四格表进行检验，并且需要根据比较的次数合理地修正检验水准\\(\\alpha\\)，否则将人为地增大犯第一类错误的概率\n\n\n\nnotice:\n\n多个独立样本率的比较，根据R个独立样本的频率分布，是检验R个二项分布总体的概率是否相同，。假设对四个样本率进行比较，进行\\(\\chi^2\\)检验，则它的行数为4，列数为2，其自由度为\\(v=(R-1)×(C-1)=(4-1)(2-1)=3\\)。\n针对行列表资料的\\(\\chi^2\\)检验，若有\\(1/5\\)格子以上的理论频数小于5，即\\(1\\le T\\le5\\)时，应考虑增加样本量，或结合专业知识对行或列进行合并。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "卡方检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-chi2-test.html#卡方检验",
    "href": "Learn/Basic/09-chi2-test.html#卡方检验",
    "title": "卡方检验",
    "section": "",
    "text": "资料特征\n数据特征\n\n完全随机设计\n\n配对设计\n随机区组\n\n\n\n\n\n\n单组\n两组\n多组\n\n\n\n\n分类资料\n无序分类资料\n二项分布直接计算概率法、正态近似法（Z检验）、率的正态近似\n独立四格表\\(\\chi^2\\)检验、Fisher确切概率法\nR×C交叉表\\(\\chi^2\\)检验、Fisher确切概率法\n配对四格表\\(\\chi^2\\)检验，配对R×R列联表\\(\\chi^2\\)检验\n/\n\n\n\n等级资料\nWilcoxon符合秩和检验\nwilcoxon秩和检验\nKruskal-Wallis H检验\nWilcoxon符合秩和检验\nFriedman M秩和检验\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n内容\n\n\n\n\n确切概率法\n1. 适用情形：样本量较小或\\(\\pi_0\\)不靠近0.5时作单侧检验的情形。2. 计算公式：(1)最多有k例阳性的概率：\\(Pr(X\\le k)\\)(2)最少有k例阳性的概率：\\(Pr(X\\ge k)\\)\n\n\n正态近似法\n1. 适用情形：样本量较大时，\\(n\\pi,n(1-\\pi)\\)均大于5；2. 计算公式：分子为\\(p-\\pi_0\\)，分母为率的标准误\n\n\n\nnotice：上式中p为样本率，\\(\\pi_0\\)为给的总体率（常为理论值或标准值），n为样本含量。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "卡方检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-chi2-test.html#率的比较",
    "href": "Learn/Basic/09-chi2-test.html#率的比较",
    "title": "卡方检验",
    "section": "",
    "text": "方法\n情形\n计算公式\n\n\n\n\n独立四格表卡方检验\n\\(n\\ge 40\\)且所有的\\(T\\ge 5\\)\\(n\\ge 40\\)且任一理论频数有\\(1\\le T&lt; 5\\)当\\(n&lt;40\\)，或任一一个格子理论频数\\(T&lt;1\\)时\n卡方基本公式、独立四格表专用公式同上、但是需要校正用四格表资料的Fisher确切概率法\n\n\n正态近似法\n\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)均大于5\n分子为样本率之差，分母为样本率差的标准误\\(S_{p1-p2}\\)为两个样本率之差的标准误，\\(p_c=\\frac{x_1+x_2}{n_1+n_2}\\)为两样本的合并率\n\n\n校正样本率的正态近似法\n当\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)不太大时\n同上，但是需要对样本率实施“分子+2、分母+4”的校正\n\n\n\nnotice：\n\n正态近似法与卡方检验结果是很接近的。在日常计算时，因为计算简便，故常用卡方检验公式。\n四格表的自由度为1。\n四格表实际频数变动时，若周边合计数保持不变，则理论频数将不会产生变化。\n用\\(n_R\\)和\\(n_C\\)和n分别表示行合计、列合计和总合计，则计算每格理论数的公式为：\\(T_{RC}=\\frac{n_R×n_C}{n}\\)。\n\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}\\)。\n校正的\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|A-T|-0.5)^2}{T}\\)。\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n配对四格表卡方检验\n当\\((b+c)\\ge 40\\)时当\\((b+c)&lt;40\\)时\n配对卡方检验专用公式校正配对卡方检验专用公式\n\n\n配对R×R交叉表数据的\\(\\chi^2\\)检验\nR（\\(R\\ge2\\)）\n\\(T=\\frac{k-1}{k}\\sum_{i=1}^{k}\\frac{(n_i-m_i)^2}{n_i+m_i-2A_{ii}}\\)\n\n\n\nnotice：\n\n配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}=\\frac{(b-c)^2}{b+c}\\)。\n若b+c&lt;40,使用校正的配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|b-c|-1)^2}{b+c}\\)。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "卡方检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-chi2-test.html#独立性检验",
    "href": "Learn/Basic/09-chi2-test.html#独立性检验",
    "title": "卡方检验",
    "section": "",
    "text": "建立假设检验，确定检验水准 \\(H_0\\):两变量之间相互独立 \\(H_1\\):两变量之间相互独立 \\(\\alpha=0.05\\)\n计算检验统计量 [^2=_{i,j} ]\n确定P值，做出推断\n关联系数的计算 [r=]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n内容\n\n\n\n\n假设检验\n\\(H_0\\)：各组总体率（或构成比）相同。\\(H_1\\)：各组总体率（或构成比）不同（不全相同）。\n\n\n计算公式\n卡方检验基本公式，自由度为：\\(v=(R-1)(C-1)\\)\n\n\n数据要求\n1. 应用条件：不能有理论频数小于1的格子，或者不能有1/5以上的理论频数大于等于1且小于5 2. 不能进行卡方检验时的解决办法：①增加样本量；②合并或删除理论频数比较小的行或列；③采用Fisher确切概率法\n\n\n卡方分割\n多个率或多个频率分布比较的卡方检验，当结论为拒绝\\(H_0\\)时，仅表示多组之间是有差别的。若需要明确研究是那两组之间存在差别，可做率的多重比较，将R×C表分割为若干个小的四格表进行检验，并且需要根据比较的次数合理地修正检验水准\\(\\alpha\\)，否则将人为地增大犯第一类错误的概率\n\n\n\nnotice:\n\n多个独立样本率的比较，根据R个独立样本的频率分布，是检验R个二项分布总体的概率是否相同，。假设对四个样本率进行比较，进行\\(\\chi^2\\)检验，则它的行数为4，列数为2，其自由度为\\(v=(R-1)×(C-1)=(4-1)(2-1)=3\\)。\n针对行列表资料的\\(\\chi^2\\)检验，若有\\(1/5\\)格子以上的理论频数小于5，即\\(1\\le T\\le5\\)时，应考虑增加样本量，或结合专业知识对行或列进行合并。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "卡方检验"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html",
    "href": "Learn/Basic/11-regression-correlation.html",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "two variables relationship\n\n\n\n\n\nThe basic process of straight-line regression analysis\n\n\n\n\n\n\n\n\n名称\n适用条件\n\n\n\nPearson直线相关系数\n双变量正态分布的资料\\(\\rightarrow\\)定量\\(\\rightarrow\\)类比t检验、方差分析\n\n\n列联系数\n非等级资料\\(\\rightarrow\\)分类\\(\\rightarrow\\)类比卡方检验\n\n\nSpearman秩相关系数\n不满足双变量正态分布、分布未知、等级资料\\(\\rightarrow\\)定量+分类\\(\\rightarrow\\)类比秩和检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#两变量关系分析",
    "href": "Learn/Basic/11-regression-correlation.html#两变量关系分析",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "two variables relationship",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#常见相关系数",
    "href": "Learn/Basic/11-regression-correlation.html#常见相关系数",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "The basic process of straight-line regression analysis\n\n\n\n\n\n\n\n\n名称\n适用条件\n\n\n\nPearson直线相关系数\n双变量正态分布的资料\\(\\rightarrow\\)定量\\(\\rightarrow\\)类比t检验、方差分析\n\n\n列联系数\n非等级资料\\(\\rightarrow\\)分类\\(\\rightarrow\\)类比卡方检验\n\n\nSpearman秩相关系数\n不满足双变量正态分布、分布未知、等级资料\\(\\rightarrow\\)定量+分类\\(\\rightarrow\\)类比秩和检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#假设检验",
    "href": "Learn/Basic/11-regression-correlation.html#假设检验",
    "title": "简单线性相关和回归",
    "section": "\n2.1 假设检验",
    "text": "2.1 假设检验\n\n2.1.1 F检验\n\\(y_i\\)的总离均差平方和为：\n\\[SS_{yy}=\\sum_{i}(y_i-\\bar y)^2\\] 对其做分解，得到等式：\n\\[SS_{yy}=\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\] \\(\\sum_{i}^{n}(\\hat y_i-\\bar y)^2\\)为回归平方和（regression sum of squares），记为\\(SS_R\\)，表示回归估计值\\(\\hat y_i\\)与均数\\(\\bar y\\)的离差平方和，其公式为：\n\\[\n\\begin{align}\nSS_{yy} &= \\sum_{i=1}^{n}(\\hat y_i - \\bar y)^2 \\\\\n        &= \\sum_{i=1}^{n}[a + bx_i - (a + b\\bar x)]^2 \\\\\n        &= SS_{xx}b^2 \\\\\n        &= SS_{xy}b\n\\end{align}\n\\] 显然，回归平方和\\(SS_{R}\\)反映的是在y的总变异中由x与y的直线回归关系解释的那部分变异。\\(SS_R\\)值越大，说明回归直线的拟合效果就越好。\n\\(\\sum_{i}^{n}(y_i-\\hat y_i)^2\\)为残差平方和（residual sum of squares），记为\\(SS_E\\)，表示观测值\\(y_i\\)与回归估计值\\(\\hat y_i\\)的离差平方和，其公式为： \\[SS_E=\\sum_{i=1}^{n}(y_i-\\hat y_i)^2\\] \\(SS_E\\)反映了在总变异中扣除自变量x对因变量y的线性影响以后的其他因素（包括x对y的非线性影响和随机误差等）对y变异的影响，也就是在总平方和中无法用y和x线性回归关系解释的部分。\\(SS_E\\)值越小，说明回归直线的拟合效果就越好。\n对公式进行简化： \\[\\begin{align}\nSS_{yy}=&\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\\\\n=&SS_R+SS_E\n\\end{align}\\] 上述三个平方和，各有其相应的自由度\\(v\\)，并有如下关系： \\[v_{yy}=v_R+v_E\\\\\nv_{yy}=n-1,v_R=1,v_E=n-2\\]\n在\\(H_0\\)成立的条件下，有： \\[\\frac{SS_R}{\\sigma^2}\\sim \\chi^2(v_R),\\frac{SS_E}{\\sigma^2}\\sim \\chi^2(v_E)\\] 且\\(SS_R\\)和\\(SS_E\\)相互独立。\n检验统计量：\n\\[F=\\frac{SS_R/v_R}{SS_E/v_E}\\] 服从自由度\\(v_R=1,v_E=n-2\\)的F分布。如果y和x确实存在直线回归关系，那么回归所解释的变异\\(SS_R\\)应大于其他因素所解释的变异\\(SS_E\\)。由此可见，F检验正是建立在这个基础上的。\n对于给定的检验水准\\(\\alpha\\)， 如果\\(F&gt;F_{(v_R,v_E),1-\\alpha}\\)，则拒绝\\(H_0\\)，认为直线回归方程有统计学显著性；\n如果\\(F\\leq F_{(v_R,v_E),1-\\alpha}\\)，则不拒绝\\(H_0\\)，尚不能认为直线回归方程有统计学显著性。\n\n2.1.2 t检验法\n回归直线方程的稳定性程度取决于 \\(b\\) 的波动大小，即 \\(S_b\\) 的大小，这里的 \\(S_b\\) 为样本回归系数 \\(b\\) 的标准误的估计值。由于统计量 \\(b\\) 来自正态总体，故可从 \\(b\\) 的抽样分布出发构造 \\(t\\) 统计量对其进行假设检验。\n当 \\(H_0\\) ：\\(\\beta = 0\\) 成立时，检验统计量服从自由度 \\(v_E=n-2\\) 的 \\(t\\) 分布。\n\\[\nt=\\frac{b-0}{S_b}\\sim t(v_E)\n\\]\n其中：\n\\[\nS_b=\\frac{S_{yx}}{\\sqrt{SS_{xx}}}=\\sqrt{\\frac{S_{yx}/(n-2)}{SS_{xx}}}\n\\]\n\\(S_{yx}=\\sqrt{SS_E/(n-2)}\\) 为剩余标准差（residual standard deviation），是指扣除 \\(x\\) 对 \\(y\\) 的线性影响后，衡量观测值 \\(y\\) 对回归直线的平均离散程度，即回归直线 \\(\\hat y\\) 估计的精度。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#直线回归方程的区间估计",
    "href": "Learn/Basic/11-regression-correlation.html#直线回归方程的区间估计",
    "title": "简单线性相关和回归",
    "section": "\n2.2 直线回归方程的区间估计",
    "text": "2.2 直线回归方程的区间估计\n\n2.2.1 总体回归系数的置信区间\n\\[\nt = \\frac{b-\\beta}{S_b}\\sim t(v_E)\n\\]",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#直线相关与直线回归的比较",
    "href": "Learn/Basic/11-regression-correlation.html#直线相关与直线回归的比较",
    "title": "简单线性相关和回归",
    "section": "\n2.3 直线相关与直线回归的比较",
    "text": "2.3 直线相关与直线回归的比较\n\n\n\n\n\n\n\n区别与联系\n类目\n内容\n\n\n\n区别\n资料要求\n1. 线性相关要求X,Y服从双变量正态分布，对这种资料进行回归分析称为\\(\\textrm{II}\\)型回归，即可以把X当自变量，也可以当因变量，反之亦然。2. 线性回归要求Y在给定X值时服从正态分布，X可以是精确测量和严格控制的变量，这时的回归称为型回归，即不可以把X当因变量，Y当自变量进行回归分析。\n\n\n\n\n应用\n1. 线性相关用来表达两个变量间的互依关系，两个变量的研究地位是相等的，谁做X，谁做Y都可以；2. 线性回归用来表达两个变量间的依存变化的数量关系，即一个变量（为因变量Y）如何依存于另一个变量（为自变量X）而变化，两个变量的研究地位是不相等的。\n\n\n\n意义\n1. 相关系数r说明具有线性关系的两个变量之间的密切程度和相关方向；2. 回归系数b表示X每变化一个单位所导致的Y的平均变化量。\n\n\n\nr和b的取值范围\nr没有单位，而b有单位（其单位是：Y的单位/X的单位），所以导致两者的取值范围不同；\\(-1 \\le r \\le 1\\),\\(-\\infty&lt;b&lt;+\\infty\\)\n\n\n\n\nr和b的计算公式不同\n\n\\(r=\\frac{l_{xy}}{\\sqrt{l_{xx}l_{yy}}}\\),\\(b=\\frac{SS_{xy}}{SS_{xx}}\\)\n\n\n\n联系\n符号\n对于既可以做相关又可作回归的同一组资料，计算出r与b的正负号相同\n\n\n\n假设检验\n对于同一组资料，相关系数和回归系数的假设检验等价。即有：\\(t_b=t_r\\)\n\n\n\n\n相互换算\n对于同一组资料，相关系数和回归系数可通过下式换算：\\(b=r\\frac{S_Y}{S_X}\\)，式中的\\(S_X,S_Y\\)分别是\\(X,Y\\)的标准差\n\n\n\n用回归解释相关\n又决定系数\\(R^2=\\frac{SS_{回}}{SS_{总}}\\in [0,1]\\)当总平方和的大小决定了相关的密切程度，回归平方和越接近总平方和，则\\(R^2\\)越接近1，相关的效果越好，说明回归效果越好，相关的密切程度也越高。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#数学定义",
    "href": "Learn/Basic/11-regression-correlation.html#数学定义",
    "title": "简单线性相关和回归",
    "section": "\n3.1 数学定义",
    "text": "3.1 数学定义\n均值向量：对于双元正态变量 \\((X, Y)\\)，均值向量为：\n\\[\\mu = \\begin{pmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{pmatrix}\\]\n其中 \\(\\mu_X\\) 和 \\(\\mu_Y\\) 分别是随机变量 \\(X\\) 和 \\(Y\\) 的均值。 2. 协方差矩阵：协方差矩阵为：\n\\[\\Sigma = \\begin{pmatrix}\n\\sigma_X^2 & \\sigma_{XY} \\\\\n\\sigma_{XY} & \\sigma_Y^2\n\\end{pmatrix}\\]\n其中 \\(\\sigma_X^2\\) 和 \\(\\sigma_Y^2\\) 是 \\(X\\) 和 \\(Y\\) 的方差，\\(\\sigma_{XY}\\) 是它们之间的协方差。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#性质",
    "href": "Learn/Basic/11-regression-correlation.html#性质",
    "title": "简单线性相关和回归",
    "section": "\n3.2 性质",
    "text": "3.2 性质\n\n边缘分布：\\(X\\) 和 \\(Y\\) 的边缘分布也是正态分布。\n条件分布：给定 \\(X\\) 的值，\\(Y\\) 的条件分布也是正态分布。\n相关性：协方差矩阵的值可以用来判断 X 和 Y 之间的相关性。如果 \\(\\sigma_{XY} &gt; 0\\)，则两者正相关；如果 \\(\\sigma_{XY} &lt; 0\\)，则负相关。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#示例",
    "href": "Learn/Basic/11-regression-correlation.html#示例",
    "title": "简单线性相关和回归",
    "section": "\n3.3 示例",
    "text": "3.3 示例\n假设有一组数据描述学生的身高 \\(X\\) 和体重 \\(Y\\)，并且假设 \\((X, Y)\\) 服从双元正态分布：\n\n3.3.1 均值向量：\n\\[\\mu = \\begin{pmatrix}\n170 \\\\\n65\n\\end{pmatrix}\\]\n表示身高的均值为 170 厘米，体重的均值为 65 千克。 •\n\n3.3.2 协方差矩阵：\n\\[\\Sigma = \\begin{pmatrix}\n100 & 20 \\\\\n20 & 25\n\\end{pmatrix}\\]\n这里，身高的方差为 100，体重的方差为 25，协方差为 20，表示身高和体重之间存在正相关关系。\n在这个示例中，如果我们知道某个学生的身高为 180 厘米，我们可以利用条件分布来预测他的体重，这个体重的预测值也是正态分布。\n我们用一个图形来展示：\n\n## 安装和加载所需的包\n#install.packages(\"plotly\")\n#install.packages(\"mvtnorm\")\nlibrary(plotly)\nlibrary(mvtnorm)\nlibrary(webshot2)\n\n# 创建网格数据\nx &lt;- seq(150, 190, length.out = 100)\n#身高150-190，等距的100个值\ny &lt;- seq(50, 80, length.out = 100)\n#体重50-80，等距的100个值\ngrid &lt;- expand.grid(X = x, Y = y)\n#生成 x 和 y 的所有组合，用于构建一个网格数据框，以便计算多元正态分布的概率密度。\n\n# 设置均值和协方差矩阵\nmu &lt;- c(170, 65)\n#设置双元正态分布的均值向量，表示均值分别为身高 170 cm 和体重 65 kg\n\nsigma &lt;- matrix(c(100, 20, 20, 25), nrow = 2)\n#设置协方差矩阵，表示身高的方差为 100，体重的方差为 25，身高和体重之间的协方差为 20\n\n# 计算概率密度\nz &lt;- dmvnorm(as.matrix(grid), mean = mu, sigma = sigma)\n#计算每个网格点上双元正态分布的概率密度。\n\n# 将概率密度矩阵转换为适合绘图的形状\nz_matrix &lt;- matrix(z, nrow = 100, ncol = 100)\n\n# 绘制三维表面图\nplot_ly(x = x, y = y, z = z_matrix, type = \"surface\") %&gt;%\n  layout(title = list(text = \"双元正态分布的三维概率密度图\", y=0.95),\n         scene = list(xaxis = list(title = \"身高 (cm)\"),\n                      yaxis = list(title = \"体重 (kg)\"),\n                      zaxis = list(title = \"概率密度\")))\n\n\nBinary normal distribution\n\n\n注：上述图像在被转换为PDF文件时，会发生报错：Quarto 文档中包含了一些生成 HTML 输出的函数（比如交互式图表或其他 HTML 小部件），但你当前的目标输出格式是 PDF。由于 PDF 是静态格式，无法直接渲染 HTML 内容，Quarto 会报错并停止执行。\n解决方案，此章节不转换为PDF格式，或者：\n如果你仍想输出 PDF，但希望将 HTML 小部件作为静态截图嵌入，可以安装 R 的 webshot 或 webshot2 包。Quarto 会利用它们将 HTML 内容转换为图片。\n需要安装：\n\ninstall.packages(\"webshot2\")\n\n然后在这段程序的前部导入该包：library(webshot2)。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html#灯泡机的简单概率图模型",
    "href": "Learn/Bayes/01-Bayes-PGM.html#灯泡机的简单概率图模型",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "",
    "text": "首先，为每一个节点定义取值：\n\nmachine_val &lt;- c(\"working\", \"broken\")\nlight_bulb_val &lt;- c(\"good\", \"bad\")\n\n为两个随机变量定义百分比数值：\n\nmachine_val &lt;- c(99,1)\nlight_bulb_val &lt;- c(99,1,60,40)\n\n使用 gRain 定义随机变量：\n\nlibrary(gRain)\nM &lt;- cptable(~machine, values = machine_prob,\n            levels = machine_val)\nL &lt;- cptable(~light_bulb | machine,\n            values = light_bulb_prob,\n            levels = light_bulb_val)\n\n这里的 cptable 表示条件概率表1：它是离散型随机变量概率分布的内存表示2。\n\n\nplist &lt;- compileCPT(list(M,L))\nplist\n\n输出结果如上，这里可以清楚地看到之前定义的概率分布\n2025.04.05 再次尝试复现程序，失败，暂时停止概率图R程序的复现工作。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html#footnotes",
    "href": "Learn/Bayes/01-Bayes-PGM.html#footnotes",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "脚注",
    "text": "脚注\n\n条件概率表（Conditional Probability Table, CPT），是一种表格形式的数据结构，用来描述离散型随机变量之间的概率关系。通常用于表示一个变量（或一组变量）的概率分布，可能依赖于其他变量（条件变量）。↩︎\n“内存表示”指的是在计算机程序中，这种概率分布被组织和存储为一种数据结构（例如表格、数组或矩阵），以便程序可以高效地访问和操作这些概率值。具体来说，cptable 函数会根据你提供的参数（如 values 和 levels）生成一个对象，这个对象在内存中以某种形式存储了变量的概率分布。↩︎",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID-test.html",
    "href": "Guide/Python/25-04-28-DID-test.html",
    "title": "How to Use the DID in Python",
    "section": "",
    "text": "双重差分回归 (DID) 用于评估一个事件的因果效应，其方法是比较事件发生的单元集合（处理组）与事件未发生的单元集合（控制组）。\nDID 背后的逻辑是，如果事件从未发生，处理组和控制组之间的差异应该随着时间的推移保持不变。\nDID 通过比较处理组和控制组在事件发生前后的差异来估计事件的因果效应。\nDID 法是一种无法随机分配样本情况下的替代方法，主要应用于区域行的策略评估问题。\n目标：获取相对同质的策略组和控制组，这个“相对”是指除策略影响外，策略组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n对于相对同质的策略组和控制组，DID法通过第一次的差分消除这个基本固定的差异，通过第二次的差分消除时间趋势的影响，评估策略带来的实际效应。\n从DID 法的目标中可知，该方法面对的实验数据是面板数据（多个时间点的截面数据组成面板数据），即在策略干预时间点前，至少有两个时间点的数据。\n\\[\ny = \\alpha_0 +\\alpha_1g +\\alpha_2T + \\alpha_3gT + \\epsilon\n\\] \\(\\alpha_0\\)为常数项，\\(\\alpha_1\\)为处理组和控制组的差异，\\(\\alpha_2\\)为时间效应，\\(\\epsilon\\)为误差项。 \\(\\alpha_3\\)为交互项的系数，表示处理组和控制组在事件发生前后的差异。\n其中，\\(y\\)为结果变量，\\(g\\)为处理组和控制组的虚拟变量，\\(T\\)为时间虚拟变量，\\(gT\\)为交互项。 \\(\\alpha_3\\)为DID估计量，表示处理组和控制组在事件发生前后的差异。\nDID 模型的有效性检验\n为了保证该模型的有效性，在试验设计时需要满足平行趋势假设：在事件发生前，处理组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n平行趋势，即策略组和控制组在干预前保持相同的变化趋势。\n3种常见的平行趋势的检验方法：\n\n画图法：画出处理组和控制组在事件发生前后的结果变量的变化趋势图，观察两组的变化趋势是否平行。\n统计检验法：使用t检验或F检验等统计方法，检验处理组和控制组在事件发生前的结果变量的差异是否显著。\n伪DID法：在事件发生前，随机选择一个时间点，将处理组和控制组的结果变量进行差分，检验差分后的结果变量是否显著。",
    "crumbs": [
      "Home",
      "Guide",
      "Python",
      "How to Use the DID in Python"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID-test.html#did双重差分回归",
    "href": "Guide/Python/25-04-28-DID-test.html#did双重差分回归",
    "title": "How to Use the DID in Python",
    "section": "",
    "text": "双重差分回归 (DID) 用于评估一个事件的因果效应，其方法是比较事件发生的单元集合（处理组）与事件未发生的单元集合（控制组）。\nDID 背后的逻辑是，如果事件从未发生，处理组和控制组之间的差异应该随着时间的推移保持不变。\nDID 通过比较处理组和控制组在事件发生前后的差异来估计事件的因果效应。\nDID 法是一种无法随机分配样本情况下的替代方法，主要应用于区域行的策略评估问题。\n目标：获取相对同质的策略组和控制组，这个“相对”是指除策略影响外，策略组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n对于相对同质的策略组和控制组，DID法通过第一次的差分消除这个基本固定的差异，通过第二次的差分消除时间趋势的影响，评估策略带来的实际效应。\n从DID 法的目标中可知，该方法面对的实验数据是面板数据（多个时间点的截面数据组成面板数据），即在策略干预时间点前，至少有两个时间点的数据。\n\\[\ny = \\alpha_0 +\\alpha_1g +\\alpha_2T + \\alpha_3gT + \\epsilon\n\\] \\(\\alpha_0\\)为常数项，\\(\\alpha_1\\)为处理组和控制组的差异，\\(\\alpha_2\\)为时间效应，\\(\\epsilon\\)为误差项。 \\(\\alpha_3\\)为交互项的系数，表示处理组和控制组在事件发生前后的差异。\n其中，\\(y\\)为结果变量，\\(g\\)为处理组和控制组的虚拟变量，\\(T\\)为时间虚拟变量，\\(gT\\)为交互项。 \\(\\alpha_3\\)为DID估计量，表示处理组和控制组在事件发生前后的差异。\nDID 模型的有效性检验\n为了保证该模型的有效性，在试验设计时需要满足平行趋势假设：在事件发生前，处理组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n平行趋势，即策略组和控制组在干预前保持相同的变化趋势。\n3种常见的平行趋势的检验方法：\n\n画图法：画出处理组和控制组在事件发生前后的结果变量的变化趋势图，观察两组的变化趋势是否平行。\n统计检验法：使用t检验或F检验等统计方法，检验处理组和控制组在事件发生前的结果变量的差异是否显著。\n伪DID法：在事件发生前，随机选择一个时间点，将处理组和控制组的结果变量进行差分，检验差分后的结果变量是否显著。",
    "crumbs": [
      "Home",
      "Guide",
      "Python",
      "How to Use the DID in Python"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID.html",
    "href": "Guide/Python/25-04-28-DID.html",
    "title": "DID 双重差分模型",
    "section": "",
    "text": "双重差分回归 (DID) 用于评估一个事件的因果效应，其方法是比较事件发生的单元集合（处理组）与事件未发生的单元集合（控制组）。\nDID 背后的逻辑是，如果事件从未发生，处理组和控制组之间的差异应该随着时间的推移保持不变。\nDID 通过比较处理组和控制组在事件发生前后的差异来估计事件的因果效应。\nDID 法是一种无法随机分配样本情况下的替代方法，主要应用于区域行的策略评估问题。\n目标：获取相对同质的策略组和控制组，这个“相对”是指除策略影响外，策略组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n对于相对同质的策略组和控制组，DID法通过第一次的差分消除这个基本固定的差异，通过第二次的差分消除时间趋势的影响，评估策略带来的实际效应。\n从DID 法的目标中可知，该方法面对的实验数据是面板数据（多个时间点的截面数据组成面板数据），即在策略干预时间点前，至少有两个时间点的数据。\n\n\n\n\n\n\\[\ny = \\alpha_0 +\\alpha_1g +\\alpha_2T + \\alpha_3gT + \\epsilon\n\\]\n其中，\\(\\alpha_0\\)为常数项，\\(\\alpha_1\\)为处理组和控制组的差异，\\(\\alpha_2\\)为时间效应，\\(\\epsilon\\)为误差项。\n\\(y\\)为结果变量，\\(g\\)为处理组和控制组的虚拟变量，\\(T\\)为时间虚拟变量，\\(gT\\)为交互项。\n\\(\\alpha_3\\)为交互项的系数，也就是DID的估计量，当交互项 \\(gT\\) 与结果变量 \\(y\\) 显著相关时，\\(\\alpha_3\\) 为评估的实际的策略效应，表示处理组和控制组在事件发生前后的差异。\n\n\n\n\n\n\nDID模型示意图\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n组别\n干预前（T=0）\n干预后（T=1)\n差分（干预后非策略差异）\n\n\n\n\n策略组(g=1)\n\\(\\alpha_0+\\alpha_1\\)\n\\(\\alpha_0+\\alpha_1+\\alpha_2+\\alpha_3\\)\n\\(\\alpha_2+\\alpha_3\\)\n\n\n控制组(g=0)\n\\(\\alpha_0\\)（基准值）\n\\(\\alpha_0+\\alpha_2\\)\n\\(\\alpha_2\\)\n\n\n差分(组间差异)\n\\(\\alpha_1\\)\n\\(\\alpha_1+\\alpha_3\\)\n\\(\\alpha_3\\)（策略效应）\n\n\n\nDID模型的原理很清晰，在无法获取完全同质的策略组和控制组的情况下，替代地获取存在固定组间差异的试验数据以抵抗混杂的影响，最后通过回归系数的显著性检验来评估策略实施的净效应。但是，理论很简洁，现实很残酷，获取 @ref(DID-models) 所示的理想数据并不简单，为了保证DID模型可以有效地评估策略效应，需要一些必要的前提检验。下面介绍这些关于DID模型的有效性检验。\n\n\n\n\n为了保证该模型的有效性，在试验设计时需要满足平行趋势假设：在事件发生前，处理组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n平行趋势，即策略组和控制组在干预前保持相同的变化趋势。\n3种常见的平行趋势的检验方法：\n\n画图法：画出处理组和控制组在事件发生前后的结果变量的变化趋势图，观察两组的变化趋势是否平行。\n统计检验法（差异性检验）：使用t检验或F检验等统计方法，检验处理组和控制组在事件发生前的结果变量的差异是否显著。\n伪DID法（交互项显著性检验）：在事件发生前，随机选择一个时间点，将处理组和控制组的结果变量进行差分，检验差分后的结果变量是否显著。\n\n\n\n\n数据来自 princeton 的 Oscar Torres-Reyna 教授构建的虚拟数据集。\n案例数据示例如下表所示：",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "DID 双重差分模型"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID.html#did-定义与目标",
    "href": "Guide/Python/25-04-28-DID.html#did-定义与目标",
    "title": "DID 双重差分模型",
    "section": "",
    "text": "双重差分回归 (DID) 用于评估一个事件的因果效应，其方法是比较事件发生的单元集合（处理组）与事件未发生的单元集合（控制组）。\nDID 背后的逻辑是，如果事件从未发生，处理组和控制组之间的差异应该随着时间的推移保持不变。\nDID 通过比较处理组和控制组在事件发生前后的差异来估计事件的因果效应。\nDID 法是一种无法随机分配样本情况下的替代方法，主要应用于区域行的策略评估问题。\n目标：获取相对同质的策略组和控制组，这个“相对”是指除策略影响外，策略组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n对于相对同质的策略组和控制组，DID法通过第一次的差分消除这个基本固定的差异，通过第二次的差分消除时间趋势的影响，评估策略带来的实际效应。\n从DID 法的目标中可知，该方法面对的实验数据是面板数据（多个时间点的截面数据组成面板数据），即在策略干预时间点前，至少有两个时间点的数据。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "DID 双重差分模型"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID.html#did-模型的原理",
    "href": "Guide/Python/25-04-28-DID.html#did-模型的原理",
    "title": "DID 双重差分模型",
    "section": "",
    "text": "\\[\ny = \\alpha_0 +\\alpha_1g +\\alpha_2T + \\alpha_3gT + \\epsilon\n\\]\n其中，\\(\\alpha_0\\)为常数项，\\(\\alpha_1\\)为处理组和控制组的差异，\\(\\alpha_2\\)为时间效应，\\(\\epsilon\\)为误差项。\n\\(y\\)为结果变量，\\(g\\)为处理组和控制组的虚拟变量，\\(T\\)为时间虚拟变量，\\(gT\\)为交互项。\n\\(\\alpha_3\\)为交互项的系数，也就是DID的估计量，当交互项 \\(gT\\) 与结果变量 \\(y\\) 显著相关时，\\(\\alpha_3\\) 为评估的实际的策略效应，表示处理组和控制组在事件发生前后的差异。\n\n\n\n\n\n\nDID模型示意图\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n组别\n干预前（T=0）\n干预后（T=1)\n差分（干预后非策略差异）\n\n\n\n\n策略组(g=1)\n\\(\\alpha_0+\\alpha_1\\)\n\\(\\alpha_0+\\alpha_1+\\alpha_2+\\alpha_3\\)\n\\(\\alpha_2+\\alpha_3\\)\n\n\n控制组(g=0)\n\\(\\alpha_0\\)（基准值）\n\\(\\alpha_0+\\alpha_2\\)\n\\(\\alpha_2\\)\n\n\n差分(组间差异)\n\\(\\alpha_1\\)\n\\(\\alpha_1+\\alpha_3\\)\n\\(\\alpha_3\\)（策略效应）\n\n\n\nDID模型的原理很清晰，在无法获取完全同质的策略组和控制组的情况下，替代地获取存在固定组间差异的试验数据以抵抗混杂的影响，最后通过回归系数的显著性检验来评估策略实施的净效应。但是，理论很简洁，现实很残酷，获取 @ref(DID-models) 所示的理想数据并不简单，为了保证DID模型可以有效地评估策略效应，需要一些必要的前提检验。下面介绍这些关于DID模型的有效性检验。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "DID 双重差分模型"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID.html#did-模型的有效性检验",
    "href": "Guide/Python/25-04-28-DID.html#did-模型的有效性检验",
    "title": "DID 双重差分模型",
    "section": "",
    "text": "为了保证该模型的有效性，在试验设计时需要满足平行趋势假设：在事件发生前，处理组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n平行趋势，即策略组和控制组在干预前保持相同的变化趋势。\n3种常见的平行趋势的检验方法：\n\n画图法：画出处理组和控制组在事件发生前后的结果变量的变化趋势图，观察两组的变化趋势是否平行。\n统计检验法（差异性检验）：使用t检验或F检验等统计方法，检验处理组和控制组在事件发生前的结果变量的差异是否显著。\n伪DID法（交互项显著性检验）：在事件发生前，随机选择一个时间点，将处理组和控制组的结果变量进行差分，检验差分后的结果变量是否显著。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "DID 双重差分模型"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID.html#did-案例分析",
    "href": "Guide/Python/25-04-28-DID.html#did-案例分析",
    "title": "DID 双重差分模型",
    "section": "",
    "text": "数据来自 princeton 的 Oscar Torres-Reyna 教授构建的虚拟数据集。\n案例数据示例如下表所示：",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "DID 双重差分模型"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html",
    "href": "Guide/Stata/25-04-29-Stata-PY.html",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "在Stata官网冲浪的时候，看到他们推出了Python与Stata联合使用的功能，于是就想试试。\nStata是一个强大的统计分析软件，广泛应用于社会科学、经济学、医学等领域。它提供了丰富的统计分析功能和数据处理工具，适合进行复杂的数据分析和建模工作。\n在使用Stata的时候，我们可能会需要在Python中调用Stata的功能。PyStata就是一个可以让我们在Python中使用Stata的工具。它允许我们在Python中运行Stata命令，并且可以将结果返回到Python中进行进一步的分析和处理。\n\n\n\nStata 官网关于 pystata 的介绍\nPyStata 是 Stata17 中引入的一个新概念，它涵盖了所有 Stata 和 Python 的交互方式。\n事实上从 Stata16 开始，我们就可以在 Stata 中调用 Python 代码，并通过 Stata 函数接口（ sfi 模块）实现 Python 与 Stata 核心功能的交互；但 Stata17 通过允许我们通过导入一个新的 Python 包（pystata）从一个独立的 Python 环境中调用 Stata ，这大大扩展二者的交互功能，使我们可以在基于或支持 IPython 内核的环境中（例如：Jupyter Notebook 、Jupyter Lab 、Spyder 、PyCharm 、VScode 等）更加方便地调用 Stata 和 Mata。\n\n\n自 Stata17 之后，官方推出了一种 StataS与 Python 的全新交互方式，而 stata_kernel 是一个第三方项目。\n即通过在 Python 环境中直接安装 pystata 模块，便能在 Python 环境中直接调用 Stata17 的命令。而 stata_kernel 是通过在 Jupyter Notebook 中安装 stata_kernel 模块，来实现 Python 与 Stata 的交互。\n在使用Quarto制作本网站时，可以编译 .qmd 和 .ipynb 文件，生成 .html 和 .pdf 文件，就想是否可以 Quarto 中利用 .ipynb 文件来调用 Stata 然后编译成 .html 文件再在网站中展示出来。\n于是有了这篇笔记。\n步骤主要分为四部分：\n\n将 Stata 添加到系统环境变量中\n在 Python 中安装 PyStata\n在 Jupyter Notebook 中使用PyStata\n在 Quarto 中使用 PyStata\n\n\n\n\n\n\n请先安装好 Stata 17 或更高版本，且最好是无限制的版本，因为有很多网上的资源是破解的，可能会有一些限制。\nStata17/18/19 软件必须具备有效的许可证，否则无法调用\n拥有基于或支持 IPython 内核的 Python 环境（建议使用 Jupyter Lab 或 VScode）\nPython 3.7 或更高版本（建议使用 Anaconda 进行安装和管理）\n\n\n\n\n要使用 pystata 包的完整功能，需要安装以下 Python 包：\n\nNumPy 1.9 或更高版本\npandas 0.15 或更高版本\n\n如果您仅计划通过调用 stata 模块中的 run() 方法执行 Stata 命令，则无需安装 NumPy 和 pandas 包。\n然而，如果需要调用 stata 模块中用于在 Stata 和 Python 之间传递数据和结果的方法，则必须安装这些包。\n\nIPython 5.0 或更高版本\n\n如果您想使用魔法命令，则需要安装 IPython 包。\n\n\n\n\n\n\n\n\n找到 Stata 的安装目录，通常在 C:\\Program Files\\Stata18 或 C:\\Program Files (x86)\\Stata18。\n复制该目录的路径。\n右键点击“此电脑”或“计算机”，选择“属性”。\n点击“高级系统设置”。\n在“系统属性”窗口中，点击“环境变量”。\n在“系统变量”部分，找到名为“Path”的变量，选中它并点击“编辑”。\n在“编辑环境变量”窗口中，点击“新建”，然后粘贴 Stata 的安装目录路径。\n点击“确定”保存更改，关闭所有窗口。\n重新启动命令提示符或 PowerShell，以使更改生效。\n在命令提示符中输入 stata，如果 Stata 启动，则说明添加成功。\n\n\n\n\n\n打开命令提示符或 PowerShell。\n输入以下命令，将 C:\\Program Files\\Stata18 替换为 Stata 的安装目录：\n\nsetx PATH \"%PATH%;C:\\Program Files\\Stata17\"\n\n按下回车键执行命令。\n关闭命令提示符或 PowerShell。\n重新打开命令提示符或 PowerShell，以使更改生效。\n在命令提示符中输入 stata，如果 Stata 启动，则说明添加成功。\n\n\n\n\n这个办法也有一定的普及程度，但是可能不太好用，对于新手来说会有些难以理解。可以参考连玉君的珠联璧合：Jupyter Notebook 和 Stata 之融合。\n主要步骤如下：\n\n找到 Stata 的安装目录，通常在 C:\\Program Files\\Stata18 或 C:\\Program Files (x86)\\Stata18。\n在该目录下找到 StataMP.exe 或 StataSE.exe 文件。\n将该文件的路径复制下来。\n以管理员身份打开电脑的 Windows Powershell 。\n在 PS C:\\WINDOWS\\system32&gt; 后输入以下命令（将路径转到 Stata 安装目录下）：\n\n在 Windows PowerShell 执行 cd 命令，以进入 stata 程序安装的路径。cd 命令后接上步所获取的 stata 安装路径。根据个人电脑安装路径不同有所差异。路径请以英文引号包围，这样可以避免路径文件夹名称中包含空格导致无法顺利进入目标路径。\ncd \"C:\\Program Files\\Stata18\"\n实际效果应该如下：\nPS C:\\WINDOWS\\system32&gt; cd 'C:\\Program Files\\Stata18'\nPS C:\\Program Files\\Stata18&gt;\n\n输入以下命令来将 stata 添加到命令行注册表中：\n\n.\\StataMP-64.exe /Register\n需要注意的是： .\\StataMP-64.exe /Register 中的 .\\StataMP-64.exe 部分，根据个人电脑安装 Stata17+ 版本有所差异。我电脑安装的是 MP 版，所以为 .\\StataMP-64.exe。如果安装的是 SE 版，应该为 .\\StataSE-64.exe。\n这里实测效果不太好，不知道为什么，注册成功但是命令行输入 stata 还是无法打开 Stata。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#前言",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#前言",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "在Stata官网冲浪的时候，看到他们推出了Python与Stata联合使用的功能，于是就想试试。\nStata是一个强大的统计分析软件，广泛应用于社会科学、经济学、医学等领域。它提供了丰富的统计分析功能和数据处理工具，适合进行复杂的数据分析和建模工作。\n在使用Stata的时候，我们可能会需要在Python中调用Stata的功能。PyStata就是一个可以让我们在Python中使用Stata的工具。它允许我们在Python中运行Stata命令，并且可以将结果返回到Python中进行进一步的分析和处理。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#关于pystata",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#关于pystata",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "Stata 官网关于 pystata 的介绍\nPyStata 是 Stata17 中引入的一个新概念，它涵盖了所有 Stata 和 Python 的交互方式。\n事实上从 Stata16 开始，我们就可以在 Stata 中调用 Python 代码，并通过 Stata 函数接口（ sfi 模块）实现 Python 与 Stata 核心功能的交互；但 Stata17 通过允许我们通过导入一个新的 Python 包（pystata）从一个独立的 Python 环境中调用 Stata ，这大大扩展二者的交互功能，使我们可以在基于或支持 IPython 内核的环境中（例如：Jupyter Notebook 、Jupyter Lab 、Spyder 、PyCharm 、VScode 等）更加方便地调用 Stata 和 Mata。\n\n\n自 Stata17 之后，官方推出了一种 StataS与 Python 的全新交互方式，而 stata_kernel 是一个第三方项目。\n即通过在 Python 环境中直接安装 pystata 模块，便能在 Python 环境中直接调用 Stata17 的命令。而 stata_kernel 是通过在 Jupyter Notebook 中安装 stata_kernel 模块，来实现 Python 与 Stata 的交互。\n在使用Quarto制作本网站时，可以编译 .qmd 和 .ipynb 文件，生成 .html 和 .pdf 文件，就想是否可以 Quarto 中利用 .ipynb 文件来调用 Stata 然后编译成 .html 文件再在网站中展示出来。\n于是有了这篇笔记。\n步骤主要分为四部分：\n\n将 Stata 添加到系统环境变量中\n在 Python 中安装 PyStata\n在 Jupyter Notebook 中使用PyStata\n在 Quarto 中使用 PyStata",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#前置要求",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#前置要求",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "请先安装好 Stata 17 或更高版本，且最好是无限制的版本，因为有很多网上的资源是破解的，可能会有一些限制。\nStata17/18/19 软件必须具备有效的许可证，否则无法调用\n拥有基于或支持 IPython 内核的 Python 环境（建议使用 Jupyter Lab 或 VScode）\nPython 3.7 或更高版本（建议使用 Anaconda 进行安装和管理）",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#依赖项",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#依赖项",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "要使用 pystata 包的完整功能，需要安装以下 Python 包：\n\nNumPy 1.9 或更高版本\npandas 0.15 或更高版本\n\n如果您仅计划通过调用 stata 模块中的 run() 方法执行 Stata 命令，则无需安装 NumPy 和 pandas 包。\n然而，如果需要调用 stata 模块中用于在 Stata 和 Python 之间传递数据和结果的方法，则必须安装这些包。\n\nIPython 5.0 或更高版本\n\n如果您想使用魔法命令，则需要安装 IPython 包。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#stata-添加到系统环境变量中",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#stata-添加到系统环境变量中",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "找到 Stata 的安装目录，通常在 C:\\Program Files\\Stata18 或 C:\\Program Files (x86)\\Stata18。\n复制该目录的路径。\n右键点击“此电脑”或“计算机”，选择“属性”。\n点击“高级系统设置”。\n在“系统属性”窗口中，点击“环境变量”。\n在“系统变量”部分，找到名为“Path”的变量，选中它并点击“编辑”。\n在“编辑环境变量”窗口中，点击“新建”，然后粘贴 Stata 的安装目录路径。\n点击“确定”保存更改，关闭所有窗口。\n重新启动命令提示符或 PowerShell，以使更改生效。\n在命令提示符中输入 stata，如果 Stata 启动，则说明添加成功。\n\n\n\n\n\n打开命令提示符或 PowerShell。\n输入以下命令，将 C:\\Program Files\\Stata18 替换为 Stata 的安装目录：\n\nsetx PATH \"%PATH%;C:\\Program Files\\Stata17\"\n\n按下回车键执行命令。\n关闭命令提示符或 PowerShell。\n重新打开命令提示符或 PowerShell，以使更改生效。\n在命令提示符中输入 stata，如果 Stata 启动，则说明添加成功。\n\n\n\n\n这个办法也有一定的普及程度，但是可能不太好用，对于新手来说会有些难以理解。可以参考连玉君的珠联璧合：Jupyter Notebook 和 Stata 之融合。\n主要步骤如下：\n\n找到 Stata 的安装目录，通常在 C:\\Program Files\\Stata18 或 C:\\Program Files (x86)\\Stata18。\n在该目录下找到 StataMP.exe 或 StataSE.exe 文件。\n将该文件的路径复制下来。\n以管理员身份打开电脑的 Windows Powershell 。\n在 PS C:\\WINDOWS\\system32&gt; 后输入以下命令（将路径转到 Stata 安装目录下）：\n\n在 Windows PowerShell 执行 cd 命令，以进入 stata 程序安装的路径。cd 命令后接上步所获取的 stata 安装路径。根据个人电脑安装路径不同有所差异。路径请以英文引号包围，这样可以避免路径文件夹名称中包含空格导致无法顺利进入目标路径。\ncd \"C:\\Program Files\\Stata18\"\n实际效果应该如下：\nPS C:\\WINDOWS\\system32&gt; cd 'C:\\Program Files\\Stata18'\nPS C:\\Program Files\\Stata18&gt;\n\n输入以下命令来将 stata 添加到命令行注册表中：\n\n.\\StataMP-64.exe /Register\n需要注意的是： .\\StataMP-64.exe /Register 中的 .\\StataMP-64.exe 部分，根据个人电脑安装 Stata17+ 版本有所差异。我电脑安装的是 MP 版，所以为 .\\StataMP-64.exe。如果安装的是 SE 版，应该为 .\\StataSE-64.exe。\n这里实测效果不太好，不知道为什么，注册成功但是命令行输入 stata 还是无法打开 Stata。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#方法一使用-pip-配置-pystata",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#方法一使用-pip-配置-pystata",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "2.1 方法一：使用 pip 配置 pystata",
    "text": "2.1 方法一：使用 pip 配置 pystata\nPyStata 可以通过 pip 安装。Windows 可以使用以下命令进行安装：\npip install --upgrade --user stata_setup\nmacOS 或 Unix 系统可以使用以下命令进行安装：\n$ pip install --upgrade --user stata_setup\n\n2.1.1 配置 Stata\n假设你的 Stata 安装在 STATA_SYSDIR 目录下，并且使用的是 Stata/MP 版本。你可以在 Python 环境中按如下方式配置 Stata：\n如果 Stata 配置正确，stata_setup.config() 将返回如下的启动画面，其中包含 Stata 的徽标和初始化消息。\n\n\n代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp')\n\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       MP—Parallel Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user 2-core network, expiring  8 Apr 2026\nSerial number: 501809376090\n  Licensed to: ausa\n               NJU\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. More than 2 billion observations are allowed; see help obs_advice.\n      3. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\n如果你不想看到初始化的信息，可以使用如下命令将其隐藏：\nstata_setup.config('YOUR_STATA_SYSDIR', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#方法二将-pystata-添加到-sys.path",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#方法二将-pystata-添加到-sys.path",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "2.2 方法二：将 pystata 添加到 sys.path",
    "text": "2.2 方法二：将 pystata 添加到 sys.path\n找到 pystata 包的最直接方法是将 pystata 子目录的位置添加到 Python 的模块搜索路径中。在你的 Python 环境中，你可以输入\nimport sys\nsys.path.append('STATA_SYSDIR/utilities')\nfrom pystata import config\nconfig.init('mp')\n如果配置正确，config.init() 应该返回无错误，并显示同上面一样的的启动画面，其中包含 Stata 的徽标和初始化消息。如果您想隐藏这些消息，可以将 splash 参数设置为 False。\n更多的安装和配置信息可以访问：pystata",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html",
    "href": "Guide/Stata/25-05-02-stata-obs.html",
    "title": "02-数据的初步观测",
    "section": "",
    "text": "describe  // 描述数据集的基本信息\nsummarize  // 描述变量的基本信息\nlist  // 列出数据集的所有观测值\nbrowse  // 浏览数据集的所有观测值\ninspect  // 检查变量的分布情况\ntabulate  // 生成频数表\nhistogram  // 生成直方图\nscatter  // 生成散点图",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#数据集的基本信息",
    "href": "Guide/Stata/25-05-02-stata-obs.html#数据集的基本信息",
    "title": "01-数据的初步观测",
    "section": "",
    "text": "Stata内置的 1978 automobile data（数据集名为 auto）是一个经典的示例数据集，常用于演示统计分析、回归建模等操作。\n\n\n代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)\n\n\n\n\n代码\n%%stata\nsysuse auto, clear  \n// 加载内置数据集\n\n\n\n. sysuse auto, clear  \n(1978 automobile data)\n\n. // 加载内置数据集\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "01-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#describe",
    "href": "Guide/Stata/25-05-02-stata-obs.html#describe",
    "title": "02-数据的初步观测",
    "section": "2.2 describe",
    "text": "2.2 describe\n展示变量类型、格式、和任何的赋值/变量标签\n语法: describe [varlist]\n注意: [] 意味着[varlist]是可选的，varlist 是变量列表，可以指定一个或多个变量。如果不指定，Stata将显示数据集中所有变量的信息。\n\n\n代码\n%%stata\ndescribe\n// 描述数据集的变量信息\n\n\n\n. describe\n\nContains data from C:\\Program Files\\Stata18/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2022 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n. // 描述数据集的变量信息\n. \n\n\n\n\n代码\n%%stata\n// 简要描述数据集的变量信息\ndescribe ,short\n\n\n\n. // 简要描述数据集的变量信息\n. describe ,short\n\nContains data from C:\\Program Files\\Stata18/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2022 17:45\nSorted by: foreign\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#count",
    "href": "Guide/Stata/25-05-02-stata-obs.html#count",
    "title": "02-数据的初步观测",
    "section": "2.3 count",
    "text": "2.3 count\n计算数据集中观测值的数量\n语法: count [if] [in]\n\n\n代码\n%%stata\n// 计算数据集中的观测值数量\ncount\n// 计算数据集中观测值价格大于4000的数量\ncount if price &gt; 4000\n\n\n\n. // 计算数据集中的观测值数量\n. count\n  74\n\n. // 计算数据集中观测值价格大于4000的数量\n. count if price &gt; 4000\n  63\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#isid",
    "href": "Guide/Stata/25-05-02-stata-obs.html#isid",
    "title": "02-数据的初步观测",
    "section": "2.4 isid",
    "text": "2.4 isid\n检查数据集中是否有重复的观测值（是否唯一标识符）\n语法: isid varlist\n如果报错就说明不是，如果没有报错就说明是唯一标识符。\n\n\n代码\n%%stata\n// 检查变量mpg是否是唯一标识符\nisid mpg\n\n\n\n---------------------------------------------------------------------------\nSystemError                               Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 get_ipython().run_cell_magic('stata', '', '// 检查变量mpg是否是唯一标识符\\nisid mpg\\n')\n\nFile c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2543, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2541 with self.builtin_trap:\n   2542     args = (magic_arg_s, cell)\n-&gt; 2543     result = fn(*args, **kwargs)\n   2545 # The code below prevents the output from being displayed\n   2546 # when using magics with decorator @output_can_be_silenced\n   2547 # when the last Python token in the expression is a ';'.\n   2548 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile C:\\Program Files/Stata18\\utilities\\pystata\\ipython\\stpymagic.py:276, in PyStataMagic.stata(self, line, cell, local_ns)\n    274     _stata.run(cell, quietly=True, inline=_config.stconfig['grshow'])\n    275 else:\n--&gt; 276     _stata.run(cell, quietly=False, inline=_config.stconfig['grshow'])\n    278 if '-gw' in args or '-gh' in args:\n    279     _config.set_graph_size(gwidth, gheight)\n\nFile C:\\Program Files/Stata18\\utilities\\pystata\\stata.py:325, in run(cmd, quietly, echo, inline)\n    323         _stata_wrk2(\"qui include \" + tmpf, None, False, 1)\n    324     else:\n--&gt; 325         _stata_wrk2(\"include \" + tmpf, None, False, 1)\n    327 if inline:\n    328     if config.get_stipython()&gt;=3:\n\nFile C:\\Program Files/Stata18\\utilities\\pystata\\stata.py:116, in _stata_wrk2(cmd, real_cmd, colon, mode)\n    114         err = callback[0]\n    115         callback.clear()\n--&gt; 116         raise SystemError(err)\n    117 except KeyboardInterrupt:\n    118     outputter.done()\n\nSystemError: \n. // 检查变量mpg是否是唯一标识符\n. isid mpg\nvariable mpg does not uniquely identify the observations\nr(459);\nr(459);\n\n\n\n\n\n\n代码\n%%stata\n// 检查变量price是否是唯一标识符\nisid price\n// 检查变量mpg和price的组合是否是唯一标识符\nisid mpg price\n\n\n\n. isid price\n\n. // 检查变量price是否是唯一标识符\n. isid mpg price\n\n. // 检查变量mpg和price的组合是否是唯一标识符\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#unique",
    "href": "Guide/Stata/25-05-02-stata-obs.html#unique",
    "title": "02-数据的初步观测",
    "section": "2.5 unique",
    "text": "2.5 unique\nunique 是一个用户自定义命令，用于检查数据集中变量的唯一值，它不是自带的，需要用户进行安装。\n语法: unique varlist\n它和 isid 的区别在于，isid 只检查唯一标识符，而 unique 可以检查任意变量的唯一值。\nisid 在遇到重复值时会报错，而 unique 会返回一个包含唯一值的列表。\n\n\n代码\n%%stata\nssc install unique\n\n\nchecking unique consistency and verifying not already installed...\ninstalling into C:\\Users\\asus\\ado\\plus\\...\ninstallation complete.\n\n\n\n\n代码\n%%stata\nunique mpg\nunique weight\nunique price\nunique mpg weight\n\n\n\n. unique mpg\nNumber of unique values of mpg is  21\nNumber of records is  74\n\n. unique weight\nNumber of unique values of weight is  64\nNumber of records is  74\n\n. unique price\nNumber of unique values of price is  74\nNumber of records is  74\n\n. unique mpg weight\nNumber of unique values of mpg weight is  74\nNumber of records is  74\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#summarize",
    "href": "Guide/Stata/25-05-02-stata-obs.html#summarize",
    "title": "02-数据的初步观测",
    "section": "2.6 summarize",
    "text": "2.6 summarize\n展示变量的基本统计信息，包括均值、标准差、最小值、最大值等。\n语法: summarize [varlist]\n\n\n代码\n%%stata\n// 只查看 `auto` 数据集中的 `price` 变量\nsummarize price\n// 查看 `auto` 数据集中所有的变量信息\nsummarize\n\n\n\n. // 只查看 `auto` 数据集中的 `price` 变量\n. summarize price\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. // 查看 `auto` 数据集中所有的变量信息\n. summarize\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        make |          0\n       price |         74    6165.257    2949.496       3291      15906\n         mpg |         74     21.2973    5.785503         12         41\n       rep78 |         69    3.405797    .9899323          1          5\n    headroom |         74    2.993243    .8459948        1.5          5\n-------------+---------------------------------------------------------\n       trunk |         74    13.75676    4.277404          5         23\n      weight |         74    3019.459    777.1936       1760       4840\n      length |         74    187.9324    22.26634        142        233\n        turn |         74    39.64865    4.399354         31         51\ndisplacement |         74    197.2973    91.83722         79        425\n-------------+---------------------------------------------------------\n  gear_ratio |         74    3.014865    .4562871       2.19       3.89\n     foreign |         74    .2972973    .4601885          0          1\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#list",
    "href": "Guide/Stata/25-05-02-stata-obs.html#list",
    "title": "02-数据的初步观测",
    "section": "2.7 list",
    "text": "2.7 list\n列出数据集的所有观测值\n语法: list [varlist] [if] [in]\n如果不指定 varlist，Stata 将列出数据集中的所有变量。 如果不指定 if 和 in，Stata 将列出数据集中的所有观测值。\n如果指定了 if 和 in，Stata 将只列出满足条件的观测值。\n\n\n代码\n%%stata\n// 只查看 `auto` 数据集中的 `price` 变量，显示详细信息\nsummarize price, detail\n\n// 查看 `auto` 数据集中所有的变量信息，限制显示前10行\nlist in 1/10\n\n\n\n. // 只查看 `auto` 数据集中的 `price` 变量，显示详细信息\n. summarize price, detail\n\n                            Price\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%         3291           3291\n 5%         3748           3299\n10%         3895           3667       Obs                  74\n25%         4195           3748       Sum of wgt.          74\n\n50%       5006.5                      Mean           6165.257\n                        Largest       Std. dev.      2949.496\n75%         6342          13466\n90%        11385          13594       Variance        8699526\n95%        13466          14500       Skewness       1.653434\n99%        15906          15906       Kurtosis       4.819188\n\n. \n. // 查看 `auto` 数据集中所有的变量信息，限制显示前10行\n. list in 10\n\n     +-----------------------------------------------------------------+\n 10. | make          | price | mpg | rep78 | headroom | trunk | weight |\n     | Buick Skylark | 4,082 |  19 |     3 |      3.5 |    13 |  3,400 |\n     |-----------------------------------------------------------------|\n     |  length   |  turn   |  displa~t   |   gear_r~o   |    foreign   |\n     |     200   |    42   |       231   |       3.08   |   Domestic   |\n     +-----------------------------------------------------------------+\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#tabulate",
    "href": "Guide/Stata/25-05-02-stata-obs.html#tabulate",
    "title": "02-数据的初步观测",
    "section": "2.8 tabulate",
    "text": "2.8 tabulate\n用于生成频数表\n语法: tabulate varlist [if] [in]\n命令可以简写为 tab。\n如果不指定 if 和 in，Stata 将列出数据集中的所有观测值。 如果指定了 if 和 in，Stata 将只列出满足条件的观测值。\n\n\n代码\n%%stata\n// 对 `auto` 数据集生成频数表\ntabulate foreign\n\n// 对 `auto` 数据集生成二维交叉表\ntab foreign rep78\n\n\n\n. // 对 `auto` 数据集生成频数表\n. tabulate foreign\n\n Car origin |      Freq.     Percent        Cum.\n------------+-----------------------------------\n   Domestic |         52       70.27       70.27\n    Foreign |         22       29.73      100.00\n------------+-----------------------------------\n      Total |         74      100.00\n\n. \n. // 对 `auto` 数据集生成二维交叉表\n. tab foreign rep78\n\n           |                   Repair record 1978\nCar origin |         1          2          3          4          5 |     Total\n-----------+-------------------------------------------------------+----------\n  Domestic |         2          8         27          9          2 |        48 \n   Foreign |         0          0          3          9          9 |        21 \n-----------+-------------------------------------------------------+----------\n     Total |         2          8         30         18         11 |        69 \n\n. \n\n\n\n\n代码\n%%stata\n// 将缺失值定位某一类别, missing 可以简写为 m\ntab foreign, m\n\n// 不显示频率结果, nofreq 可以简写为 nof\ntab foreign, nofreq\n\n// 不显示标签值, nolabel 可以简写为 nol\ntab foreign, nolabel\n\n\n\n. // 将缺失值定位某一类别, missing 可以简写为 m\n. tab foreign, m\n\n Car origin |      Freq.     Percent        Cum.\n------------+-----------------------------------\n   Domestic |         52       70.27       70.27\n    Foreign |         22       29.73      100.00\n------------+-----------------------------------\n      Total |         74      100.00\n\n. \n. // 不显示频率结果, nofreq 可以简写为 nof\n. tab foreign, nofreq\n\n. \n. // 不显示标签值, nolabel 可以简写为 nol\n. tab foreign, nolabel\n\n Car origin |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          0 |         52       70.27       70.27\n          1 |         22       29.73      100.00\n------------+-----------------------------------\n      Total |         74      100.00\n\n. \n\n\n\n\n代码\n%%stata\n// 生成相对频率的条形图\ntab foreign, p\n\n// 按照频率数对 `auto` 数据集进行排序\ntabulate foreign, sort\n\n\n\n. // 生成相对频率的条形图\n. tab foreign, p\n\n Car origin |      Freq.\n------------+------------+-----------------------------------------------------\n   Domestic |         52 |****************************************************\n    Foreign |         22 |**********************\n------------+------------+-----------------------------------------------------\n      Total |         74 \n\n. \n. // 按照频率数对 `auto` 数据集进行排序\n. tabulate foreign, sort\n\n Car origin |      Freq.     Percent        Cum.\n------------+-----------------------------------\n   Domestic |         52       70.27       70.27\n    Foreign |         22       29.73      100.00\n------------+-----------------------------------\n      Total |         74      100.00\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#histogram",
    "href": "Guide/Stata/25-05-02-stata-obs.html#histogram",
    "title": "02-数据的初步观测",
    "section": "2.9 histogram",
    "text": "2.9 histogram\n生成直方图，用于展示变量的分布情况\n语法: histogram varname [if] [in]\n如果不指定 if 和 in，Stata 将列出数据集中的所有观测值。\n\n\n代码\n%%stata\n// 生成 `price` 变量的直方图，并叠加正态分布曲线\nhistogram price, normal\n\n\n\n. // 生成 `price` 变量的直方图，并叠加正态分布曲线\n. histogram price, normal\n(bin=8, start=3291, width=1576.875)\n\n. \n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\n// 生成 `price` 变量的直方图，并叠加频率分布曲线\nhistogram price, normal frequency\n\n\n\n. // 生成 `price` 变量的直方图，并叠加频率分布曲线\n. histogram price, normal frequency\n(bin=8, start=3291, width=1576.875)\n\n. \n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\n// 生成 `price` 变量的直方图，并叠加频率分布曲线，设置标题和坐标轴标签\nhistogram price, normal frequency ytitle(\"Frequency\") xtitle(\"Price\") title(\"Histogram of Price\")\n\n\n\n. // 生成 `price` 变量的直方图，并叠加频率分布曲线，设置标题和坐标轴标签\n. histogram price, normal frequency ytitle(\"Frequency\") xtitle(\"Price\") title(\"\n&gt; Histogram of Price\")\n(bin=8, start=3291, width=1576.875)\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#scatter",
    "href": "Guide/Stata/25-05-02-stata-obs.html#scatter",
    "title": "02-数据的初步观测",
    "section": "2.10 scatter",
    "text": "2.10 scatter\n生成散点图，用于展示两个变量之间的关系\n语法: scatter yvar xvar [if] [in]\n\n\n代码\n%%stata\n// 生成 `mpg` 和 `weight` 变量的散点图\nscatter mpg weight\n\n\n\n. // 生成 `mpg` 和 `weight` 变量的散点图\n. scatter mpg weight\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html",
    "href": "Guide/Stata/25-05-03-describe-index.html",
    "title": "03-统计描述指标",
    "section": "",
    "text": "代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#codebook",
    "href": "Guide/Stata/25-05-03-describe-index.html#codebook",
    "title": "03-统计描述指标",
    "section": "1.1 codebook",
    "text": "1.1 codebook\n数据字典，可以用来描述数据集的基本信息，包括变量名称、变量类型、缺失值、变量描述等。\n语法： codebook [varlist] [if] [in] [, options]\n\n[] 表示可选项，不是必须的。\nvarlist 表示变量列表，可以指定一个或多个变量。\nif 和 in 是条件语句，可以用来筛选数据。\noptions 是可选项，可以用来指定其他参数，一些可以自定义的选项。\n\n\n\n代码\n%%stata\n// 载入数据集，使用 Stata 的内置数据集 auto.dta\n// 该数据集包含汽车的各种属性，如价格、重量、马力等\nsysuse auto, clear\ncodebook price\n\n\n\n. // 载入数据集，使用 Stata 的内置数据集 auto.dta\n. // 该数据集包含汽车的各种属性，如价格、重量、马力等\n. sysuse auto, clear\n(1978 automobile data)\n\n. codebook price\n\n-------------------------------------------------------------------------------\nprice                                                                     Price\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [3291,15906]                  Units: 1\n         Unique values: 74                        Missing .: 0/74\n\n                  Mean: 6165.26\n             Std. dev.:  2949.5\n\n           Percentiles:     10%       25%       50%       75%       90%\n                           3895      4195    5006.5      6342     11385\n\n. \n\n\n\n\n代码\n%%stata\ncodebook price if price &gt; 5000\n\n\n\n-------------------------------------------------------------------------------\nprice                                                                     Price\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [5079,15906]                  Units: 1\n         Unique values: 37                        Missing .: 0/37\n\n                  Mean: 8086.95\n             Std. dev.: 3142.58\n\n           Percentiles:     10%       25%       50%       75%       90%\n                           5189      5788      6342     10371     13466\n\n\n\n\n代码\n%%stata\ncodebook price in 10/20\n\n\n\n-------------------------------------------------------------------------------\nprice                                                                     Price\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [3299,15906]                  Units: 1\n         Unique values: 11                        Missing .: 0/11\n\n                  Mean: 6917.36\n             Std. dev.: 4668.09\n\n           Percentiles:     10%       25%       50%       75%       90%\n                           3667      3955      4504     11385     14500\n\n\n\n\n代码\n%%stata\nhelp codebook\n\n\n\n[D] codebook -- Describe data contents\n                (View complete PDF manual entry)\n\n\nSyntax\n------\n\n        codebook [varlist] [if] [in] [, options]\n\n    options                  Description\n    -------------------------------------------------------------------------\n    Options\n      all                    print complete report without missing values\n      header                 print dataset name and last saved date\n      notes                  print any notes attached to variables\n      mv                     report pattern of missing values\n      tabulate(#)            set tables/summary statistics threshold; default\n                               is tabulate(9)\n      problems               report potential problems in dataset\n      detail                 display detailed report on the variables; only\n                               with problems\n      compact                display compact report on the variables\n      dots                   display a dot for each variable processed; only\n                               with compact\n\n    Languages\n      languages[(namelist)]  use with multilingual datasets; see [D] label\n                               language for details\n    -------------------------------------------------------------------------\n    collect is allowed; see prefix.\n\n\nMenu\n----\n\n    Data &gt; Describe data &gt; Describe data contents (codebook)\n\n\nDescription\n-----------\n\n    codebook examines the variable names, labels, and data to produce a\n    codebook describing the dataset.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Quick start\n\n        Remarks and examples\n\n    The above sections are not included in this help file.\n\n\nOptions\n-------\n\n        +---------+\n    ----+ Options +----------------------------------------------------------\n\n    all is equivalent to specifying the header and notes options.  It\n        provides a complete report, which excludes only performing mv.\n\n    header adds to the top of the output a header that lists the dataset\n        name, the date that the dataset was last saved, etc.\n\n    notes lists any notes attached to the variables; see [D] notes.\n\n    mv specifies that codebook search the data to determine the pattern of\n        missing values.  This is a CPU-intensive task.\n\n    tabulate(#) specifies the number of unique values of the variables to use\n        to determine whether a variable is categorical or continuous.\n        Missing values are not included in this count.  The default is 9;\n        when there are more than nine unique values, the variable is\n        classified as continuous.  Extended missing values will be included\n        in the tabulation.\n\n    problems specifies that a summary report is produced describing potential\n        problems that have been diagnosed:\n\n        - Variables that are labeled with an undefined value label\n        - Incompletely value-labeled variables\n        - Variables that are constant, including always missing\n        - Leading, trailing, and embedded spaces in string variables\n        - Embedded binary 0 (\\0) in string variables\n        - Noninteger-valued date variables\n\n        See codebook problems for a discussion of these problems and advice\n        on overcoming them.\n\n    detail may be specified only with the problems option.  It specifies that\n        the detailed report on the variables not be suppressed.\n\n    compact specifies that a compact report on the variables be displayed.\n        compact may not be specified with any options other than dots.\n\n    dots specifies that a dot be displayed for every variable processed.\n        dots may be specified only with compact.\n\n        +-----------+\n    ----+ Languages +--------------------------------------------------------\n\n    languages[(namelist)] is for use with multilingual datasets; see [D]\n        label language.  It indicates that the codebook pertains to the\n        languages in namelist or to all defined languages if no such list is\n        specified as an argument to languages().  The output of codebook\n        lists the data label and variable labels in these languages and which\n        value labels are attached to variables in these languages.\n\n        Problems are diagnosed in all of these languages, as well.  The\n        problem report does not provide details in which language problems\n        occur.  We advise you to rerun codebook for problematic variables;\n        specify detail to produce the problem report again.\n\n        If you have a multilingual dataset but do not specify languages(),\n        all output, including the problem report, is shown in the \"active\"\n        language.\n\n\nExamples\n--------\n\n    With standard (monolingual) datasets,\n\n        -----------------------------------------------------------------------\n        Setup\n            . sysuse auto\n            . note rep78: \"investigate missing values\"\n            . label values rep78 repairlbl\n\n        Display codebook for all variables in dataset\n            . codebook\n\n        Same as above command\n            . codebook _all\n\n        Same as above command, but print dataset name, date last saved,\n        dataset label, number of variables and of observations, and dataset\n        size\n            . codebook, header\n\n        Display codebook for rep78 variable\n            . codebook rep78\n\n        Display codebook for rep78 variable, including notes attached to\n        rep78\n            . codebook rep78, notes\n\n        Report potential problems with dataset\n            . codebook, problems\n\n        Display compact report for all variables in dataset\n            . codebook, compact\n\n        -----------------------------------------------------------------------\n        Setup\n            . webuse citytemp, clear\n\n        Display codebook for cooldd, heatdd, tempjan, and tempjuly, and\n        report pattern of missing values\n            . codebook cooldd heatdd tempjan tempjuly, mv\n        -----------------------------------------------------------------------\n\n\n    With multilingual datasets, with languages en and es, and with active\n    language en,\n\n        Setup\n            . webuse autom\n\n        Display codebook for foreign in language en\n            . codebook foreign\n\n        Display codebook for foreign in language es\n            . codebook foreign, language(es)\n\n        Display codebook for foreign in both en and es\n            . codebook foreign, languages\n\n\nStored results\n--------------\n\n    codebook stores the following lists of variables with potential problems\n    in r():\n\n    Macros             \n      r(cons)                 constant (or missing)\n      r(labelnotfound)        undefined value labeled\n      r(notlabeled)           value labeled but with unlabeled categories\n      r(str_type)             compressible\n      r(str_leading)          leading blanks\n      r(str_trailing)         trailing blanks\n      r(str_embedded)         embedded blanks\n      r(str_embedded0)        embedded binary 0 (\\0)\n      r(realdate)             noninteger dates",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#summarize",
    "href": "Guide/Stata/25-05-03-describe-index.html#summarize",
    "title": "03-统计描述指标",
    "section": "1.2 summarize",
    "text": "1.2 summarize\n打印数据集的基本统计描述指标，包括均值、标准差、最小值、最大值等。 语法： summarize [varlist] [if] [in] [, options] 可以使用 sum 或 summ 来代替 summarize。\n\n\n代码\n%%stata\nsum price\nsumm price\n\n\n\n. sum price\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. summ price\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. \n\n\n\n1.2.1 codebook 与 summarize 的区别\n最大的区别就是 summarize 有一个 detail 选项，可以打印出更多的统计描述指标，比如四分位数、偏度、峰度等。\n\n\n代码\n%%stata\nhelp summarize\n\n\n\n[R] summarize -- Summary statistics\n                 (View complete PDF manual entry)\n\n\nSyntax\n------\n\n        summarize [varlist] [if] [in] [weight] [, options]\n\n    options           Description\n    -------------------------------------------------------------------------\n    Main\n      detail          display additional statistics\n      meanonly        suppress the display; calculate only the mean;\n                        programmer's option\n      format          use variable's display format\n      separator(#)    draw separator line after every # variables; default is\n                        separator(5)\n      display_options control spacing, line width, and base and empty cells\n\n    -------------------------------------------------------------------------\n    varlist may contain factor variables; see fvvarlist.\n    varlist may contain time-series operators; see tsvarlist.\n    by, collect, rolling, and statsby are allowed; see prefix.\n  \n    aweights, fweights, and iweights are allowed.  However, iweights may not\n      be used with the detail option; see weight.\n\n\nMenu\n----\n\n    Statistics &gt; Summaries, tables, and tests &gt; Summary and descriptive\n        statistics &gt; Summary statistics\n\n\nDescription\n-----------\n\n    summarize calculates and displays a variety of univariate summary\n    statistics.  If no varlist is specified, summary statistics are\n    calculated for all the variables in the dataset.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Quick start\n\n        Remarks and examples\n\n        Methods and formulas\n\n    The above sections are not included in this help file.\n\n\nOptions\n-------\n\n        +------+\n    ----+ Main +-------------------------------------------------------------\n\n    detail produces additional statistics, including skewness, kurtosis, the\n        four smallest and four largest values, and various percentiles.\n\n    meanonly, which is allowed only when detail is not specified, suppresses\n        the display of results and calculation of the variance.  Ado-file\n        writers will find this useful for fast calls.\n\n    format requests that the summary statistics be displayed using the\n        display formats associated with the variables rather than the default\n        g display format; see [D] format.\n\n    separator(#) specifies how often to insert separation lines into the\n        output.  The default is separator(5), meaning that a line is drawn\n        after every five variables.  separator(10) would draw a line after\n        every 10 variables.  separator(0) suppresses the separation line.\n\n    display_options:  vsquish, noemptycells, baselevels, allbaselevels,\n        nofvlabel, fvwrap(#), and fvwrapon(style); see [R] Estimation\n        options.\n\n\nExamples\n--------\n\n    . sysuse auto\n    . summarize\n    . summarize mpg weight\n    . summarize mpg weight if foreign\n    . summarize mpg weight if foreign, detail\n    . summarize i.rep78\n\n\nVideo example\n-------------\n\n    Descriptive statistics in Stata\n\n\nStored results\n--------------\n\n    summarize stores the following in r():\n\n    Scalars   \n      r(N)           number of observations\n      r(mean)        mean\n      r(skewness)    skewness (detail only)\n      r(min)         minimum\n      r(max)         maximum\n      r(sum_w)       sum of the weights\n      r(p1)          1st percentile (detail only)\n      r(p5)          5th percentile (detail only)\n      r(p10)         10th percentile (detail only)\n      r(p25)         25th percentile (detail only)\n      r(p50)         50th percentile (detail only)\n      r(p75)         75th percentile (detail only)\n      r(p90)         90th percentile (detail only)\n      r(p95)         95th percentile (detail only)\n      r(p99)         99th percentile (detail only)\n      r(Var)         variance\n      r(kurtosis)    kurtosis (detail only)\n      r(sum)         sum of variable\n      r(sd)          standard deviation\n\n\n\n\n代码\n%%stata\nsum price, detail\n\n\n\n                            Price\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%         3291           3291\n 5%         3748           3299\n10%         3895           3667       Obs                  74\n25%         4195           3748       Sum of wgt.          74\n\n50%       5006.5                      Mean           6165.257\n                        Largest       Std. dev.      2949.496\n75%         6342          13466\n90%        11385          13594       Variance        8699526\n95%        13466          14500       Skewness       1.653434\n99%        15906          15906       Kurtosis       4.819188",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#histogram",
    "href": "Guide/Stata/25-05-03-describe-index.html#histogram",
    "title": "03-统计描述指标",
    "section": "1.3 histogram",
    "text": "1.3 histogram\n绘制直方图，可以用来查看数据的分布情况。\n语法： histogram varname [if] [in] [weight] [,[continuous_opts][discrete_opts] options]\n\nvarname 是变量名称，可以指定一个变量。\nif 和 in 是条件语句，可以用来筛选数据。\noptions 是可选项，可以用来指定其他参数，比如直方图的颜色、边框、标题等。\nbin() 选项可以用来指定直方图的分组数，比如 bin(20) 表示将数据分成 20 组。\nnormal 选项可以用来绘制正态分布曲线，可以用来查看数据是否服从正态分布。\nfreq 选项可以用来绘制频率直方图，可以用来查看数据的频率分布情况。\npercent 选项可以用来绘制百分比直方图，可以用来查看数据的百分比分布情况。\ndensity 选项可以用来绘制密度直方图，可以用来查看数据的密度分布情况。\nstart() 选项可以用来指定直方图的起始值，比如 start(0) 表示从 0 开始。\nwidth() 选项可以用来指定直方图的宽度，比如 width(1) 表示每组的宽度为 1。\ngap() 选项可以用来指定直方图的间隔，比如 gap(0) 表示没有间隔。\nbarwidth() 选项可以用来指定直方图的条形宽度，比如 barwidth(0.5) 表示条形宽度为 0.5。\nbarcolor() 选项可以用来指定直方图的条形颜色，比如 barcolor(red) 表示条形颜色为红色。\nbarlabel() 选项可以用来指定直方图的条形标签，比如 barlabel(1) 表示条形标签为 1。\nbarlabelcolor() 选项可以用来指定直方图的条形标签颜色，比如 barlabelcolor(blue) 表示条形标签颜色为蓝色。\nbarlabelsize() 选项可以用来指定直方图的条形标签大小，比如 barlabelsize(10) 表示条形标签大小为 10。\nbarlabelposition() 选项可以用来指定直方图的条形标签位置，比如 barlabelposition(inside) 表示条形标签在条形内部， barlabelposition(outside) 表示条形标签在条形外部。\n\nhistogram 可以简写为 histo，也可以简写为 hist。\n\n\n代码\n%%stata\nhist price\n\n\n(bin=8, start=3291, width=1576.875)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nhist price, freq\n\n\n(bin=8, start=3291, width=1576.875)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nhist price, percent\n\n\n(bin=8, start=3291, width=1576.875)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nhist price, frac\n\n\n(bin=8, start=3291, width=1576.875)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nhist price, freq bin(5)\n\n\n(bin=5, start=3291, width=2523)\n\n\n\n\n\n\n\n\n\n\n1.3.1 添加密度曲线\n\nnormal 选项可以用来绘制正态分布曲线，可以用来查看数据是否服从正态分布。\nnormopts(line_options) 选项可以用来指定正态分布曲线的线条属性，比如 normopts(lcolor(red)) 表示正态分布曲线的颜色为红色。\nkdensity 选项可以用来绘制核密度曲线，可以用来查看数据的密度分布情况。\nkdenopts(line_options) 选项可以用来指定核密度曲线的线条属性，比如 kdenopts(lcolor(blue)) 表示核密度曲线的颜色为蓝色。\n\n\n\n代码\n%%stata\nhist price, freq bin(5) normal\n\n\n(bin=5, start=3291, width=2523)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nhist price, by(foreign)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#graph-box",
    "href": "Guide/Stata/25-05-03-describe-index.html#graph-box",
    "title": "03-统计描述指标",
    "section": "1.4 graph box",
    "text": "1.4 graph box\n绘制箱线图，可以用来查看数据的分布情况和异常值。\n箱线图的中位数、四分位数、最大值、最小值、异常值等信息。\n箱线图的中位数是箱子的中间线，四分位数是箱子的上下边界，最大值和最小值是箱子的上下须，异常值是箱子外的点。\n箱线图的优点是可以清晰地显示数据的分布情况和异常值，缺点是不能显示数据的具体数值。\n语法： graph box yvars [if] [in] [weight] [, options]\ngraph hbox yvars [if] [in] [weight] [, options]\n他们的区别在于 graph box 是绘制垂直箱线图，graph hbox 是绘制水平箱线图。\n\n[] 表示可选项，不是必须的。\nvarlist 表示变量列表，可以指定一个或多个变量。\nif 和 in 是条件语句，可以用来筛选数据。\noptions 是可选项，可以用来指定其他参数，一些可以自定义的选项。\n\n\n\n代码\n%%stata\ngraph box price\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\ngraph hbox price\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\ngraph box price, over(foreign)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#biolin-plot",
    "href": "Guide/Stata/25-05-03-describe-index.html#biolin-plot",
    "title": "03-统计描述指标",
    "section": "1.5 biolin plot",
    "text": "1.5 biolin plot\n小提琴图，类似于箱线图，可以用来查看数据的分布情况和异常值。\n语法： biolin [varlist] [if] [in] [, options] - [] 表示可选项，不是必须的。 - varlist 表示变量列表，可以指定一个或多个变量。 - if 和 in 是条件语句，可以用来筛选数据。 - options 是可选项，可以用来指定其他参数，一些可以自定义的选项。\n\n\n代码\n%%stata\n// 不是自带的命令，需要下载安装\nssc install vioplot\n\n\n\n. // 不是自带的命令，需要下载安装\n. ssc install vioplot\nchecking vioplot consistency and verifying not already installed...\ninstalling into C:\\Users\\asus\\ado\\plus\\...\ninstallation complete.\n\n. \n\n\n\n\n代码\n%%stata\nvioplot price\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nvioplot price, over(foreign)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#以下数据集的基本信息",
    "href": "Guide/Stata/25-05-02-stata-obs.html#以下数据集的基本信息",
    "title": "02-数据的初步观测",
    "section": "2.1 以下数据集的基本信息",
    "text": "2.1 以下数据集的基本信息\nStata内置的 1978 automobile data（数据集名为 auto）是一个经典的示例数据集，常用于演示统计分析、回归建模等操作。\n\n\n代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)\n\n\n\n\n代码\n%%stata\nsysuse auto, clear  \n// 加载内置数据集\n\n\n\n. sysuse auto, clear  \n(1978 automobile data)\n\n. // 加载内置数据集\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html",
    "href": "Guide/Stata/25-05-04-CI.html",
    "title": "04-置信区间（CI）",
    "section": "",
    "text": "代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html#置信区间的定义",
    "href": "Guide/Stata/25-05-04-CI.html#置信区间的定义",
    "title": "04-置信区间（CI）",
    "section": "1 置信区间的定义",
    "text": "1 置信区间的定义\n置信区间是一个范围，用于估计总体参数的可能值。它是基于样本数据计算得出的，并且在一定的置信水平下，包含了总体参数的真实值。 置信区间通常用以下形式表示：\n\\[ CI = (\\hat{\\theta} - E, \\hat{\\theta} + E) \\]\n其中，\\(\\hat{\\theta}\\) 是样本统计量的估计值，\\(E\\) 是误差范围（也称为边际误差）。\n置信区间计算的一般形式为： \\[ CI = (\\hat{\\theta} - z_{\\alpha/2} \\cdot SE, \\hat{\\theta} + z_{\\alpha/2} \\cdot SE) \\]\n置信区间的宽度取决于样本大小、样本标准差和所选的置信水平。较大的样本通常会导致更窄的置信区间，而较高的置信水平则会导致更宽的置信区间。\n置信区间的计算通常涉及以下步骤：\n\n选择一个置信水平（例如，95%或99%）。\n计算样本统计量（例如，样本均值或样本比例）。\n计算样本标准差或标准误差。\n根据所选的置信水平，查找相应的临界值（例如，Z值或t值）。\n计算置信区间的边际误差。\n构建置信区间。\n解释置信区间的含义。\n报告置信区间的结果。\n进行假设检验时，置信区间可以用来判断是否拒绝原假设。\n在进行回归分析时，置信区间可以用来评估回归系数的显著性和可靠性。\n在进行方差分析时，置信区间可以用来评估组间差异的显著性和可靠性。\n\n\n\n代码\n%%stata\n// 载入数据集，使用 Stata 的内置数据集 auto.dta\nsysuse auto, clear\n\n\n\n. // 载入数据集，使用 Stata 的内置数据集 auto.dta\n. sysuse auto, clear\n(1978 automobile data)\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html#ci-mean",
    "href": "Guide/Stata/25-05-04-CI.html#ci-mean",
    "title": "04-置信区间（CI）",
    "section": "2 ci mean",
    "text": "2 ci mean\n连续变量 mean 的标准误（SE）和置信区间（CI）\n语法：\nci mean varname [if] [in] [weight] [,options]\n或者\ncii means #obs #mean #sd [,level(#)]\n默认置信水平为95%，可以通过 options 或 level(#) 选项更改。\n\n\n代码\n%%stata\nci mean mpg price,level(95)\n\n\n\n    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n-------------+---------------------------------------------------------------\n         mpg |         74     21.2973    .6725511         19.9569    22.63769\n       price |         74    6165.257    342.8719        5481.914      6848.6\n\n\n\n\n代码\n%%stata\ncii mean 144 19599 4389,level(95)\n\n\n\n    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n-------------+---------------------------------------------------------------\n             |        166       19599    340.6525         18926.4     20271.6",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html#分类变量的置信区间",
    "href": "Guide/Stata/25-05-04-CI.html#分类变量的置信区间",
    "title": "04-置信区间（CI）",
    "section": "3 分类变量的置信区间",
    "text": "3 分类变量的置信区间\n\n3.1 继续使用 ci 命令\nci 命令可以用于计算分类变量的置信区间。对于分类变量，通常使用比例（proportion）来表示其分布情况。ci 命令可以计算样本比例的置信区间。\nci priportion varname [if] [in] [weight] [,options]\n缺点：ci prop 只能用于计算 binary 变量的置信区间，不能用于多分类变量。\n\n\n代码\n%%stata\nci prop foreign\n\n\n\n                                                            Binomial exact   \n    Variable |        Obs  Proportion    Std. err.       [95% conf. interval]\n-------------+---------------------------------------------------------------\n     foreign |         74    .2972973    .0531331         .196584    .4148353\n\n\n\n\n代码\n%%stata\nci prop rep78\n\n\nno binary (0/1) variables found; nothing to compute\n\n\n\n\n3.2 proportion\nproportion 命令可以用于计算分类变量的置信区间。它可以处理多分类变量，并且可以计算每个类别的比例和置信区间。\n语法形式：\nproportion varname [if] [in] [weight] [,options]\n默认置信水平为95%，可以通过 options 选项更改。\n\n\n代码\n%%stata\nprop foreign\n\n\n\nProportion estimation                       Number of obs = 74\n\n--------------------------------------------------------------\n             |                                   Logit\n             | Proportion   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n     foreign |\n   Domestic  |   .7027027   .0531331      .5874215     .796909\n    Foreign  |   .2972973   .0531331       .203091    .4125785\n--------------------------------------------------------------\n\n\n\n\n代码\n%%stata\nprop rep78\n\n\n\nProportion estimation                       Number of obs = 69\n\n--------------------------------------------------------------\n             |                                   Logit\n             | Proportion   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n       rep78 |\n          1  |   .0289855   .0201966      .0070794    .1110924\n          2  |    .115942   .0385422       .058317    .2173648\n          3  |   .4347826   .0596787      .3214848    .5553295\n          4  |   .2608696   .0528625      .1695907    .3788629\n          5  |   .1594203   .0440694      .0895793     .267702\n--------------------------------------------------------------\n\n\n\n\n代码\n%%stata\nprop foreign rep78\n\n\n\nProportion estimation                       Number of obs = 69\n\n--------------------------------------------------------------\n             |                                   Logit\n             | Proportion   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n     foreign |\n   Domestic  |   .6956522   .0553932      .5755656     .793927\n    Foreign  |   .3043478   .0553932       .206073    .4244344\n             |\n       rep78 |\n          1  |   .0289855   .0201966      .0070794    .1110924\n          2  |    .115942   .0385422       .058317    .2173648\n          3  |   .4347826   .0596787      .3214848    .5553295\n          4  |   .2608696   .0528625      .1695907    .3788629\n          5  |   .1594203   .0440694      .0895793     .267702\n--------------------------------------------------------------\n\n\nprop 多个变量时，Stata 会默认去除缺失值。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html#pwcorr",
    "href": "Guide/Stata/25-05-04-CI.html#pwcorr",
    "title": "04-置信区间（CI）",
    "section": "4 pwcorr",
    "text": "4 pwcorr\npwcorr 命令用于计算变量之间的成对相关系数。它可以处理连续变量和分类变量，并且可以计算每对变量之间的相关系数和置信区间。\npwcorr [varlist] [if] [in] [weight] [,options]\n\n\n代码\n%%stata\npwcorr price headroom mpg displacement\n\n\n\n             |    price headroom      mpg displa~t\n-------------+------------------------------------\n       price |   1.0000 \n    headroom |   0.1145   1.0000 \n         mpg |  -0.4686  -0.4138   1.0000 \ndisplacement |   0.4949   0.4745  -0.7056   1.0000 \n\n\n\n\n代码\n%%stata\n// 展示P值版，并且用星号标记显著性水平为0.05的相关系数\npwcorr price headroom mpg displacement, sig star(0.05)\n\n\n\n. // 展示P值版\n. pwcorr price headroom mpg displacement, sig star(0.05)\n\n             |    price headroom      mpg displa~t\n-------------+------------------------------------\n       price |   1.0000 \n             |\n             |\n    headroom |   0.1145   1.0000 \n             |   0.3313\n             |\n         mpg |  -0.4686* -0.4138*  1.0000 \n             |   0.0000   0.0002\n             |\ndisplacement |   0.4949*  0.4745* -0.7056*  1.0000 \n             |   0.0000   0.0000   0.0000\n             |\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html#graph-matrix",
    "href": "Guide/Stata/25-05-04-CI.html#graph-matrix",
    "title": "04-置信区间（CI）",
    "section": "5 graph matrix",
    "text": "5 graph matrix\ngraph matrix 命令用于绘制变量之间的散点图矩阵。它可以处理连续变量和分类变量，并且可以计算每对变量之间的相关系数和置信区间。\ngraph matrix [varlist] [if] [in] [weight] [,options]\n\n\n代码\n%%stata\ngraph matrix price headroom mpg displacement\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\ngraph matrix price headroom mpg displacement, half",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-ttest.html",
    "href": "Guide/Stata/25-05-05-ttest.html",
    "title": "07-t检验（t-test）",
    "section": "",
    "text": "代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "07-t检验（t-test）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-ttest.html#数据导入",
    "href": "Guide/Stata/25-05-05-ttest.html#数据导入",
    "title": "07-t检验（t-test）",
    "section": "0.1 数据导入",
    "text": "0.1 数据导入\n\n\n代码\n%%stata\nwebuse auto.dta, clear\n\n\n(1978 automobile data)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "07-t检验（t-test）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-ttest.html#独立样本-t-检验-two-samples-t-test",
    "href": "Guide/Stata/25-05-05-ttest.html#独立样本-t-检验-two-samples-t-test",
    "title": "07-t检验（t-test）",
    "section": "1.1 独立样本 t 检验 (two-samples t-test)",
    "text": "1.1 独立样本 t 检验 (two-samples t-test)\n语法：\nttest varname, by(groupvar) [if] [in] [,level(#)]\n\n\n代码\n%%stata\nttest price, by(foreign)\n\n\n\nTwo-sample t test with equal variances\n------------------------------------------------------------------------------\n   Group |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\nDomestic |      52    6072.423    429.4911    3097.104    5210.184    6934.662\n Foreign |      22    6384.682    558.9942    2621.915     5222.19    7547.174\n---------+--------------------------------------------------------------------\nCombined |      74    6165.257    342.8719    2949.496    5481.914      6848.6\n---------+--------------------------------------------------------------------\n    diff |           -312.2587    754.4488               -1816.225    1191.708\n------------------------------------------------------------------------------\n    diff = mean(Domestic) - mean(Foreign)                         t =  -0.4139\nH0: diff = 0                                     Degrees of freedom =       72\n\n    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0\n Pr(T &lt; t) = 0.3401         Pr(|T| &gt; |t|) = 0.6802          Pr(T &gt; t) = 0.6599",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "07-t检验（t-test）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-ttest.html#两个变量的比较",
    "href": "Guide/Stata/25-05-05-ttest.html#两个变量的比较",
    "title": "07-t检验（t-test）",
    "section": "1.2 两个变量的比较",
    "text": "1.2 两个变量的比较\n\n1.2.1 非配对样本\nttest var1 == var2 [if] [in] ,unpaired [level(#)]\n\n\n代码\n%%stata\nwebuse fuel.dta, clear\nttest mpg1 == mpg2, unpaired\n\n\n\n. webuse fuel.dta, clear\n\n. ttest mpg1 == mpg2, unpaired\n\nTwo-sample t test with equal variances\n------------------------------------------------------------------------------\nVariable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\n    mpg1 |      12          21    .7881701    2.730301    19.26525    22.73475\n    mpg2 |      12       22.75    .9384465    3.250874    20.68449    24.81551\n---------+--------------------------------------------------------------------\nCombined |      24      21.875    .6264476    3.068954    20.57909    23.17091\n---------+--------------------------------------------------------------------\n    diff |               -1.75    1.225518               -4.291568    .7915684\n------------------------------------------------------------------------------\n    diff = mean(mpg1) - mean(mpg2)                                t =  -1.4280\nH0: diff = 0                                     Degrees of freedom =       22\n\n    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0\n Pr(T &lt; t) = 0.0837         Pr(|T| &gt; |t|) = 0.1673          Pr(T &gt; t) = 0.9163\n\n. \n\n\n\n\n1.2.2 配对样本 t 检验 (paired t-test)\n语法：\nttest var1 == var2 [if] [in] ,[level(#)]\n\n\n代码\n%%stata\nwebuse fuel.dta, clear\nttest mpg1 == mpg2\n\n\n\n. webuse fuel.dta, clear\n\n. ttest mpg1 == mpg2\n\nPaired t test\n------------------------------------------------------------------------------\nVariable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\n    mpg1 |      12          21    .7881701    2.730301    19.26525    22.73475\n    mpg2 |      12       22.75    .9384465    3.250874    20.68449    24.81551\n---------+--------------------------------------------------------------------\n    diff |      12       -1.75    .7797144     2.70101    -3.46614   -.0338602\n------------------------------------------------------------------------------\n     mean(diff) = mean(mpg1 - mpg2)                               t =  -2.2444\n H0: mean(diff) = 0                              Degrees of freedom =       11\n\n Ha: mean(diff) &lt; 0           Ha: mean(diff) != 0           Ha: mean(diff) &gt; 0\n Pr(T &lt; t) = 0.0232         Pr(|T| &gt; |t|) = 0.0463          Pr(T &gt; t) = 0.9768\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "07-t检验（t-test）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html",
    "href": "Guide/Stata/25-05-05-log-do.html",
    "title": "06-代码的可复现性与文件管理",
    "section": "",
    "text": "代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)\n代码\n%%stata\n// 载入数据集，使用 Stata 的内置数据集 auto.dta\nsysuse auto, clear\n\n\n\n. // 载入数据集，使用 Stata 的内置数据集 auto.dta\n. sysuse auto, clear\n(1978 automobile data)\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#代码规范",
    "href": "Guide/Stata/25-05-05-log-do.html#代码规范",
    "title": "06-代码的可复现性与文件管理",
    "section": "2.1 代码规范",
    "text": "2.1 代码规范\n/* README: \n   1. 创建时间：2025-05-05\n   2. 上次修改时间：2025-05-08\n   3. 本文档的目的：用于探索数据&绘图&制表&建模\n   4. 本文档的依赖项：\n      - Stata &gt; 17.0\n      - Data: data.dta or other.files\n      - packages: estout, outreg2, etc.\n   5. 输入数据：output.dta or other.files\n   6. 输出结果：results.txt\n   7. 如有问题，请联系作者或查看文档。\nNotes: \n   1. 本文档的内容仅供参考，作者不对其准确性和完整性负责。\n   2. 本文档的内容可能会随时更新，作者不保证其及时性和有效性。\n   3. 本文档的内容可能会受到版权保护，未经授权不得转载或引用。\n   4. 本文档的内容仅代表作者个人观点，不代表任何机构或组织的观点。\n   5. 其他信息：\n      - 作者：Simon Zhou\n      - 邮箱：\n*/\n\n* 代码如下：\n\n// 载入数据集，使用 Stata 的内置数据集 auto.dta\nsysuse auto, clear\n\n* 1. 数据探索\ndescribe\nsummarize\n\n* 2. 绘制散点图\ntwoway scatter mpg weight",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#append-和-replace-的区别",
    "href": "Guide/Stata/25-05-05-log-do.html#append-和-replace-的区别",
    "title": "06-代码的可复现性与文件管理",
    "section": "3.1 append 和 replace 的区别",
    "text": "3.1 append 和 replace 的区别\n\nappend：将新的输出追加到已有的 log file 中。\nreplace：替换已有的 log file，创建一个新的 log file。\n如果文件不存在，append 和 replace 的效果是一样的，都会创建一个新的 log file。\n如果文件已经存在，而用户没有指定 append 或 replace，Stata 会报错来询问用户如何选择。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#text-和-smcl-的区别",
    "href": "Guide/Stata/25-05-05-log-do.html#text-和-smcl-的区别",
    "title": "06-代码的可复现性与文件管理",
    "section": "3.2 text 和 smcl 的区别",
    "text": "3.2 text 和 smcl 的区别\n\ntext：创建一个纯文本格式的 log file，适合于在文本编辑器中查看和编辑，体积较小，只有单色。\nsmcl：创建一个 Stata 默认格式的 log file，适合于在 Stata 中查看和编辑，可以保存各种颜色。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#namelogname-的作用",
    "href": "Guide/Stata/25-05-05-log-do.html#namelogname-的作用",
    "title": "06-代码的可复现性与文件管理",
    "section": "3.3 name(logname) 的作用",
    "text": "3.3 name(logname) 的作用\n\nname(logname)：给 log file 起一个名字，方便后续的引用和管理。\n如果不指定，Stata 会自动生成一个默认的名字，通常是以日期和时间为基础的字符串。\n如果用户给 log file 起了名字（不是 filename），那么用户就可以同时打开好几个 log file。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#结束-log-file",
    "href": "Guide/Stata/25-05-05-log-do.html#结束-log-file",
    "title": "06-代码的可复现性与文件管理",
    "section": "3.4 结束 log file",
    "text": "3.4 结束 log file\n\n如果没有起名字，可以世界使用 log close 命令来结束 log file。\n如果用户没有结束 log file，Stata 会在退出时自动结束 log file。\n如果用户在 Stata 中打开了多个 log file，用户可以使用 log close logfile-name 命令来结束指定的 log file，或者使用 log close _all 命令来结束所有的 log file。\n\n\n\n代码\n%%stata\nlog close // 关闭现有的日志文件 \nlog using test,append\nlog close _all // 关闭所有日志文件\n\n\n\n. log close // 关闭现有的日志文件 \n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\asus\\Desktop\\test\\Stata-test\\Statatest.smcl\n  log type:  smcl\n closed on:   5 May 2025, 12:18:32\n-------------------------------------------------------------------------------\n\n. log using test,append\n(file C:\\Users\\asus\\Desktop\\test\\Stata-test\\test.smcl not found)\n-------------------------------------------------------------------------------\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\asus\\Desktop\\test\\Stata-test\\test.smcl\n  log type:  smcl\n opened on:   5 May 2025, 12:18:32\n\n. log close _all // 关闭所有日志文件\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\asus\\Desktop\\test\\Stata-test\\test.smcl\n  log type:  smcl\n closed on:   5 May 2025, 12:18:32\n-------------------------------------------------------------------------------\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#do-file-的基本语法",
    "href": "Guide/Stata/25-05-05-log-do.html#do-file-的基本语法",
    "title": "06-代码的可复现性与文件管理",
    "section": "4.1 Do file 的基本语法",
    "text": "4.1 Do file 的基本语法\n\n4.1.1 注释\n\n使用 * 或 // 来添加单行注释，* 作为一行开头时，这一行都是注释。\n\n在 Command 后添加注释使用 // 较好，并在 // 添加一些文字注释，以提醒或解释本行代码用途。\n\n使用 /* ... */ 来添加多行注释。\n注释可以放在代码的前面或后面，也可以单独成行。\n\n\n\n代码\n%%stata\nclear all // 清除所有变量和数据集\n\npwd // 显示当前工作目录\ncd \"C:\\Users\\asus\\Desktop\\test\\Stata-test\" // 更改工作目录到指定路径\n// 注意：请将 \"YourUsername\" 替换为你的实际用户名\n\n\n\n. clear all // 清除所有变量和数据集\n\n. \n. pwd // 显示当前工作目录\nC:\\Users\\asus\\Desktop\\test\\Stata-test\n\n. cd \"C:\\Users\\asus\\Desktop\\test\\Stata-test\" // 更改工作目录到指定路径\nC:\\Users\\asus\\Desktop\\test\\Stata-test\n\n. // 注意：请将 \"YourUsername\" 替换为你的实际用户名\n. \n\n\n\n\n代码\n%%stata\n*log close // 关闭现有的日志文件\nlog using Statatest, replace \n// 创建一个新的日志文件，替换现有的文件\n\n\n\n. *log close // 关闭现有的日志文件\n. log using Statatest, replace \n-------------------------------------------------------------------------------\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\asus\\Desktop\\test\\Stata-test\\Statatest.smcl\n  log type:  smcl\n opened on:   5 May 2025, 12:18:53\n\n. // 创建一个新的日志文件，替换现有的文件\n. \n\n\n\n\n代码\n%%stata\n// 载入数据集，使用 Stata 的内置数据集 auto.dta\nsysuse auto.dta, clear \n\n** 数据分析的探索性描述\nsum price\ncodebook mpg\n\n\n\n. // 载入数据集，使用 Stata 的内置数据集 auto.dta\n. sysuse auto.dta, clear \n(1978 automobile data)\n\n. \n. ** 数据分析的探索性描述\n. sum price\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. codebook mpg\n\n-------------------------------------------------------------------------------\nmpg                                                               Mileage (mpg)\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [12,41]                       Units: 1\n         Unique values: 21                        Missing .: 0/74\n\n                  Mean: 21.2973\n             Std. dev.:  5.7855\n\n           Percentiles:     10%       25%       50%       75%       90%\n                             14        18        20        25        29\n\n. \n\n\n\n\n代码\n%%stata\nci mean rep78\ncorr weight length\n\n\n\n. ci mean rep78\n\n    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n-------------+---------------------------------------------------------------\n       rep78 |         69    3.405797    .1191738        3.167989    3.643605\n\n. corr weight length\n(obs=74)\n\n             |   weight   length\n-------------+------------------\n      weight |   1.0000\n      length |   0.9460   1.0000\n\n\n. \n\n\n\n\n4.1.2 copy the “Command”\n如果用户使用交互页面来选择相关的操作，Stata 会在命令窗口中显示用户操作所对应的命令。\n用户可以直接复制这些命令到 Do file 中，方便后续的修改和复现。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html",
    "href": "Guide/Stata/25-05-05-scatter-plot.html",
    "title": "05-双变量作图",
    "section": "",
    "text": "代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#stata的scheme主题",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#stata的scheme主题",
    "title": "05-双变量作图",
    "section": "1.1 Stata的scheme（主题）",
    "text": "1.1 Stata的scheme（主题）\nStata提供了多种 scheme （style）来美化图形。可以使用set scheme命令来设置主题。以下是一些常用的主题：\n\ns1color：适用于需要强调数据点的情况，具有鲜艳的颜色。\ns2color：默认主题，适用于需要强调数据点的情况。\ns1mono：单色主题，适用于打印或黑白显示。\ns2mono：单色主题，适用于强调数据点的情况。\neconomist：适用于经济学和社会科学领域的主题。\njournal：适用于学术期刊的主题，具有简洁和专业的外观。\ns1manual：手动主题，适用于需要自定义颜色和样式的情况。\ns2manual：手动主题，适用于强调数据点的情况。\ns2color8：适用于需要强调数据点的情况，具有8种颜色的主题。\nplotplain：适用于需要强调数据点的情况，具有简单和清晰的外观。\n\n更好的主题？\nsimono：适用于需要强调数据点的情况，具有简洁和专业的外观。\n\n1.1.1 scheme 的设置\nset scheme s1mono : 当前会话中设置主题为s1mono。\nset scheme s1mono, perm : 永久设置主题为s1mono, perm 为 permanent 的缩写。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#scatter命令",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#scatter命令",
    "title": "05-双变量作图",
    "section": "1.2 scatter命令",
    "text": "1.2 scatter命令\nscatter命令用于绘制散点图。基本语法如下：\n[twoway] scatter yvar xvar [if] [in] [weight] [, options]\n最基础的形式：\ntwoway scatter yvar xvar\n进阶形式：\ntwoway scatter y1 y2 y3 x1 x2 x3 [if] [in] [weight], options\n\n\n代码\n%%stata\ntwoway scatter mpg weight\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\ntwoway scatter weight length price\n\n\n\n\n\n\n\n\n\n\n1.2.1 改变散点图的样式\ntwoway scatter y x,msymbol(oh) mcolor(red) msize(medium) scheme(s1mono)\n\nmsymbol:改变形状(help symbolstyle)\nmcolor:改变颜色(help colorstyle)\nmsize:改变大小(help markerstyle)\n\n\n\n代码\n%%stata\nhelp symbolstyle\n\n\n\n[G-4] symbolstyle -- Choices for the shape of markers\n                     (View complete PDF manual entry)\n\n\nSyntax\n------\n\n                        Synonym\n        symbolstyle     (if any)     Description\n        -------------------------------------------------------\n        circle             O         solid\n        diamond            D         solid\n        triangle           T         solid\n        square             S         solid\n        plus               +\n        X                  X\n        arrowf             A         filled arrow head\n        arrow              a\n        pipe               |\n        V                  V\n\n        smcircle           o         solid\n        smdiamond          d         solid\n        smsquare           s         solid\n        smtriangle         t         solid\n        smplus\n        smx                x\n        smv                v\n\n        circle_hollow      Oh        hollow\n        diamond_hollow     Dh        hollow\n        triangle_hollow    Th        hollow\n        square_hollow      Sh        hollow\n\n        smcircle_hollow    oh        hollow\n        smdiamond_hollow   dh        hollow\n        smtriangle_hollow  th        hollow\n        smsquare_hollow    sh        hollow\n\n        point              p         a small dot\n        none               i         a symbol that is invisible\n        -------------------------------------------------------\n\n        For a symbol palette displaying each of the above symbols, type\n\n            . palette symbolpalette [, scheme(schemename)]\n\n        Other symbolstyles may be available; type\n\n            . graph query symbolstyle\n\n        to obtain the complete list of symbolstyles installed on your\n        computer.\n\n\nDescription\n-----------\n\n    Markers are the ink used to mark where points are on a plot; see [G-3]\n    marker_options.  symbolstyle specifies the shape of the marker.\n\n    You specify the symbolstyle inside the msymbol() option allowed with many\n    of the graph commands:\n\n        . graph twoway ..., msymbol(symbolstyle) ...\n\n    Sometimes you will see that a symbolstylelist is allowed:\n\n        . scatter ..., msymbol(symbolstylelist) ...\n\n    A symbolstylelist is a sequence of symbolstyles separated by spaces.\n    Shorthands are allowed to make specifying the list easier; see [G-4]\n    stylelists.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Remarks and examples\n\n    The above sections are not included in this help file.\n\n\nRemarks\n-------\n\n    Remarks are presented under the following headings:\n\n        Typical use\n        Filled and hollow symbols\n        Size of symbols\n\n\nTypical use\n-----------\n\n    msymbol(symbolstyle) is one of the more commonly specified options.  For\n    instance, you may not be satisfied with the default rendition of\n\n        . scatter mpg weight if foreign ||\n          scatter mpg weight if !foreign\n\n    and prefer\n\n        . scatter mpg weight if foreign, msymbol(oh) ||\n          scatter mpg weight if !foreign, msymbol(x)\n\n    When you are graphing multiple y variables in the same plot, you can\n    specify a list of symbolstyles inside the msymbol() option:\n\n        . scatter mpg1 mpg2 weight, msymbol(oh x)\n\n    The result is the same as typing\n\n        . scatter mpg1 weight, msymbol(oh) ||\n          scatter mpg2 weight, msymbol(x)\n\n    Also, in the above, we specified the symbol-style synonyms.  Whether you\n    type\n\n        . scatter mpg1 weight, msymbol(oh) ||\n          scatter mpg2 weight, msymbol(x)\n\n    or\n\n        . scatter mpg1 weight, msymbol(smcircle_hollow) ||\n          scatter mpg2 weight, msymbol(smx)\n\n    makes no difference.\n\n\nFilled and hollow symbols\n-------------------------\n\n    The symbolstyle specifies the shape of the symbol, and in that sense, one\n    of the styles circle and hcircle -- and diamond and hdiamond, etc. -- is\n    unnecessary in that each is a different rendition of the same shape.  The\n    option mfcolor(colorstyle) (see [G-3] marker_options) specifies how the\n    inside of the symbol is to be filled.  hcircle(), hdiamond, etc., are\n    included for convenience and are equivalent to specifying\n\n        msymbol(Oh): msymbol(O) mfcolor(none)\n\n        msymbol(dh): msymbol(d) mfcolor(none)\n\n        etc.\n\n    Using mfcolor() to fill the inside of a symbol with different colors\n    sometimes creates what are effectively new symbols.  For instance, if you\n    take msymbol(O) and fill its interior with a lighter shade of the same\n    color used to outline the shape, you obtain a pleasing result.  For\n    instance, you might try\n\n        msymbol(O) mlcolor(yellow) mfcolor(.5*yellow)\n\n    or\n\n        msymbol(O) mlcolor(gs5) mfcolor(gs12)\n\n    as in\n\n        . scatter mpg weight, msymbol(O) mlcolor(gs5) mfcolor(gs14)\n          (click to run)\n\n\nSize of symbols\n---------------\n\n    Just as msymbol(O) and msymbol(Oh) differ only in mfcolor(), msymbol(O)\n    and msymbol(o) -- symbols circle and smcircle -- differ only in msize().\n    In particular,\n\n        msymbol(O): msymbol(O) msize(medium)\n\n        msymbol(o): msymbol(O) msize(small)\n\n    and the same is true for all the other large and small symbol pairs.\n\n    msize() is interpreted as being relative to the size of the graph region\n    (see [G-3] region_options), so the same symbol size will in fact be a\n    little different in\n\n        . scatter mpg weight\n\n    and\n\n        . scatter mpg weight, by(foreign total)\n\n\n\n\n代码\n%%stata\nhelp colorstyle\n\n\n\n[G-4] colorstyle -- Choices for color\n                    (View complete PDF manual entry)\n\n\nSyntax\n------\n\n    Set color of &lt;object&gt; to colorstyle\n\n        &lt;object&gt;color(colorstyle)\n\n\n    Set color of all affected objects to colorstyle\n\n        color(colorstyle)\n\n\n    Set opacity of &lt;object&gt; to #, where # is a percentage of 100% opacity\n\n        &lt;object&gt;color(%#)\n\n\n    Set opacity for all affected objects colors to #\n\n        color(%#)\n\n\n    Set both color and opacity of &lt;object&gt;\n\n        &lt;object&gt;color(colorstyle%#)\n\n\n    Set both color and opacity of all affected objects\n\n        &lt;object&gt;color(colorstyle%#)\n\n\n    colorstyle            Description\n    -------------------------------------------------------------------------\n    black                 \n\n    stc1                  color used by scheme stcolor\n    stc2                  color used by scheme stcolor\n    .                     \n    .                     \n    stc15                 color used by scheme stcolor\n    stblue                blue used by scheme stcolor\n    stgreen               green used by scheme stcolor\n    stred                 red used by scheme stcolor\n    styellow              yellow used by scheme stcolor\n\n    gs0                   gray scale: 0 = black\n    gs1                   gray scale: very dark gray\n    gs2                   \n    .                     \n    .                     \n    gs15                  gray scale: very light gray\n    gs16                  gray scale: 16 = white\n\n    white                 \n\n    blue                  \n    bluishgray            \n    brown                 \n    cranberry             \n    cyan                  \n    dimgray               between gs14 and gs15\n    dkgreen               dark green\n    dknavy                dark navy blue\n    dkorange              dark orange\n    eggshell              \n    emerald               \n    forest_green          \n    gold                  \n    gray                  equivalent to gs8\n    green                 \n    khaki                 \n    lavender              \n    lime                  \n    ltblue                light blue\n    ltbluishgray          light blue-gray, used by scheme s2color\n    ltkhaki               light khaki\n    magenta               \n    maroon                \n    midblue               \n    midgreen              \n    mint                  \n    navy                  \n    olive                 \n    olive_teal            \n    orange                \n    orange_red            \n    pink                  \n    purple                \n    red                   \n    sand                  \n    sandb                 bright sand\n    sienna                \n    stone                 \n    teal                  \n    yellow                \n\n                          colors used by The Economist magazine:\n    ebg                           background color\n    ebblue                        bright blue\n    edkblue                       dark blue\n    eltblue                       light blue\n    eltgreen                      light green\n    emidblue                      midblue\n    erose                         rose\n\n    none                  no color; invisible; draws nothing\n    background or bg      same color as background\n    foreground or fg      same color as foreground\n\n    \"# # #\"               RGB value; white = \"255 255 255\"\n\n    \"# # # #\"             CMYK value; yellow = \"0 0 255 0\"\n\n    \"hsv # # #\"           HSV value; white = \"hsv 0 0 1\"\n\n    colorstyle*#          color with adjusted intensity; #'s range from 0 to\n                            255\n\n    colorstyle%#          color with adjusted opacity; #s range from 0 to 100\n\n    *#                    default color with adjusted intensity\n    %#                    default color with adjusted opacity\n    -------------------------------------------------------------------------\n    When specifying RGB, CMYK, or HSV values, it is best to enclose the\n      values in quotes; type \"128 128 128\" not 128 128 128.\n\n\nDescription\n-----------\n\n    colorstyle sets the color and opacity of graph components such as lines,\n    backgrounds, and bars.  Some options allow a sequence of colorstyles with\n    colorstylelist; see [G-4] stylelists.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Remarks and examples\n\n    The above sections are not included in this help file.\n\n\nRemarks\n-------\n\n    colorstyle sets the color and opacity of graph components such as lines,\n    backgrounds, and bars.  Colors can be specified with a named color, such\n    as black, olive, and yellow, or with a color value in the RGB, CMYK, or\n    HSV format.  colorstyle can also set a component to match the background\n    color or foreground color.  Additionally, colorstyle can modify color\n    intensity, making the color lighter or darker.  Some options allow a\n    sequence of colorstyles with colorstylelist; see [G-4] stylelists.\n\n    To see a list of named colors, use graph query colorstyle.  See [G-2]\n    graph query.  For a color palette showing an individual color or\n    comparing two colors, use palette color.  See [G-2] palette.\n\n    Remarks are presented under the following headings:\n\n        Adjust opacity\n        Adjust intensity\n        Specify RGB values\n        Specify CMYK values\n        Specify HSV values\n        Export custom colors\n\n\nAdjust opacity\n--------------\n\n    Opacity is the percentage of a color that covers the background color.\n    That is, 100% means that the color fully hides the background, and 0%\n    means that the color has no coverage and is fully transparent.  If you\n    prefer to think about transparency, opacity is the inverse of\n    transparency.  Adjust opacity with the % modifier.  For example, type\n\n        green%50\n        \"0 255 0%50\"\n        %30\n\n    Omitting the color specification in the command adjusts the opacity of\n    the object while retaining the default color.  For instance, specify\n    mcolor(%30) with graph twoway scatter to use the default fill color at\n    30% opacity.\n\n    Specifying color%0 makes the object completely transparent and is\n    equivalent to color none.\n\n\nAdjust intensity\n----------------\n\n    Color intensity (brightness) can be modified by specifying a color, *,\n    and a multiplier value.  For example, type\n\n        green*.8\n        purple*1.5\n        \"0 255 255*1.2\"\n        \"hsv 240 1 1*.5\"\n\n    A value of 1 leaves the color unchanged, a value greater than 1 makes the\n    color darker, and a value less than 1 makes the color lighter.  Note that\n    there is no space between color and *, even when color is a numerical\n    value for RGB or CMYK.\n\n    Omitting the color specification in the command adjusts the intensity of\n    the object's default color.  For instance, specify bcolor(*.7) with graph\n    twoway bar to use the default fill color at reduced brightness, or\n    specify bcolor(*2) to increase the brightness of the default color.\n\n    Specifying color*0 makes the color as light as possible, but it is not\n    equivalent to color none.  color*255 makes the color as dark as possible,\n    although values much smaller than 255 usually achieve the same result.\n\n    For an example using the intensity adjustment, see Typical use in [G-2]\n    graph twoway kdensity.\n\n\nRGB values\n----------\n\n    In addition to specifying named colors such as yellow, you can specify\n    colors with RGB values.  An RGB value is a triplet of numbers ranging\n    from 0 to 255 that describes the level of red, green, and blue light that\n    must be emitted to produce a given color.  RGB is used to define colors\n    for on-screen display and in nonprofessional printing.  Examples of RGB\n    values are\n\n        red     =   255    0    0\n        green   =     0  255    0\n        blue    =     0    0  255\n        white   =   255  255  255\n        black   =     0    0    0\n        gray    =   128  128  128\n        navy    =    26   71  111\n\n\nCMYK values\n-----------\n\n    You can specify colors using CMYK values.  You will probably only use\n    CMYK values when they are provided by a journal or publisher.  You can\n    specify CMYK values either as integers from 0 to 255 or as proportions of\n    ink using real numbers from 0.0 to 1.0.  If all four values are 1 or\n    less, the numbers are taken to be proportions of ink.  For example,\n\n        red     =     0  255  255    0   or, equivalently,     0     1  1     0\n        green   =   255    0  255    0   or, equivalently,     1     0  1     0\n        blue    =   255  255    0    0   or, equivalently,     1     1  0     0\n        white   =     0    0    0    0   or, equivalently,     0     0  0     0\n        black   =     0    0    0  255   or, equivalently,     0     0  0     1\n        gray    =     0    0    0  128   or, equivalently,     0     0  0    .5\n        navy    =    85   40    0  144   or, equivalently,  .334  .157  0  .565\n\n\nHSV values\n----------\n\n    You can specify colors with HSV (hue, saturation, and value), also called\n    HSL (hue, saturation, and luminance) and HSB (hue, saturation, and\n    brightness).  HSV is often used in image editing software.  An HSV value\n    is a triplet of numbers.  So that Stata can differentiate them from RGB\n    values, HSV colors must be prefaced with hsv.  The first number specifies\n    the hue from 0 to 360, the second number specifies the saturation from 0\n    to 1, and the third number specifies the value (luminance or brightness)\n    from 0 to 1.  For example,\n\n        red     =   hsv   0     1     1\n        green   =   hsv 120     1  .502\n        blue    =   hsv 240     1     1\n        white   =   hsv   0     0     1\n        black   =   hsv   0     0     0\n        navy    =   hsv 209  .766  .435\n\n\nExport custom colors\n--------------------\n\n    graph export stores all colors as RGB+opacity values, that is, RGB values\n    0-255 and opacity values 0-1.  If you need color values from Stata in\n    CMYK format, use the graph export command with the cmyk(on) option, and\n    save the graph in one of the following formats: PostScript, Encapsulated\n    PostScript, or PDF.\n\n    You can set Stata to permanently use CMYK colors for PostScript export\n    files by typing translator set Graph2ps cmyk on and for EPS export files\n    by typing translator set Graph2eps cmyk on.\n\n    The CMYK values returned in graph export may differ from the CMYK values\n    that you entered.  This is because Stata normalizes CMYK values by\n    reducing all CMY values until one value is 0.  The difference is added to\n    the K (black) value.  For example, Stata normalizes the CMYK value 10 10\n    5 0 to 5 5 0 5.  Stata subtracts 5 from the CMY values so that Y is 0 and\n    then adds 5 to K.\n\n\nVideo example\n-------------\n\n        Transparency in Stata graphs\n\n\n\n\n代码\n%%stata\ntwoway scatter mpg weight,msymbol(D) mcolor(red) msize(medium)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#change-the-scheme-of-the-scatter-plot",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#change-the-scheme-of-the-scatter-plot",
    "title": "04-双变量作图",
    "section": "1.3 change the scheme of the scatter plot",
    "text": "1.3 change the scheme of the scatter plot\ntwoway scatter y x,msymbol(oh) mcolor(red) msize(medium) scheme(s1mono)\n\nmsymbol:改变形状(help symbolstyle)\nmcolor:改变颜色(help colorstyle)\nmsize:改变大小(help markerstyle)\n\n\n\n代码\n%%stata\nhelp symbolstyle\n\n\n\n[G-4] symbolstyle -- Choices for the shape of markers\n                     (View complete PDF manual entry)\n\n\nSyntax\n------\n\n                        Synonym\n        symbolstyle     (if any)     Description\n        -------------------------------------------------------\n        circle             O         solid\n        diamond            D         solid\n        triangle           T         solid\n        square             S         solid\n        plus               +\n        X                  X\n        arrowf             A         filled arrow head\n        arrow              a\n        pipe               |\n        V                  V\n\n        smcircle           o         solid\n        smdiamond          d         solid\n        smsquare           s         solid\n        smtriangle         t         solid\n        smplus\n        smx                x\n        smv                v\n\n        circle_hollow      Oh        hollow\n        diamond_hollow     Dh        hollow\n        triangle_hollow    Th        hollow\n        square_hollow      Sh        hollow\n\n        smcircle_hollow    oh        hollow\n        smdiamond_hollow   dh        hollow\n        smtriangle_hollow  th        hollow\n        smsquare_hollow    sh        hollow\n\n        point              p         a small dot\n        none               i         a symbol that is invisible\n        -------------------------------------------------------\n\n        For a symbol palette displaying each of the above symbols, type\n\n            . palette symbolpalette [, scheme(schemename)]\n\n        Other symbolstyles may be available; type\n\n            . graph query symbolstyle\n\n        to obtain the complete list of symbolstyles installed on your\n        computer.\n\n\nDescription\n-----------\n\n    Markers are the ink used to mark where points are on a plot; see [G-3]\n    marker_options.  symbolstyle specifies the shape of the marker.\n\n    You specify the symbolstyle inside the msymbol() option allowed with many\n    of the graph commands:\n\n        . graph twoway ..., msymbol(symbolstyle) ...\n\n    Sometimes you will see that a symbolstylelist is allowed:\n\n        . scatter ..., msymbol(symbolstylelist) ...\n\n    A symbolstylelist is a sequence of symbolstyles separated by spaces.\n    Shorthands are allowed to make specifying the list easier; see [G-4]\n    stylelists.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Remarks and examples\n\n    The above sections are not included in this help file.\n\n\nRemarks\n-------\n\n    Remarks are presented under the following headings:\n\n        Typical use\n        Filled and hollow symbols\n        Size of symbols\n\n\nTypical use\n-----------\n\n    msymbol(symbolstyle) is one of the more commonly specified options.  For\n    instance, you may not be satisfied with the default rendition of\n\n        . scatter mpg weight if foreign ||\n          scatter mpg weight if !foreign\n\n    and prefer\n\n        . scatter mpg weight if foreign, msymbol(oh) ||\n          scatter mpg weight if !foreign, msymbol(x)\n\n    When you are graphing multiple y variables in the same plot, you can\n    specify a list of symbolstyles inside the msymbol() option:\n\n        . scatter mpg1 mpg2 weight, msymbol(oh x)\n\n    The result is the same as typing\n\n        . scatter mpg1 weight, msymbol(oh) ||\n          scatter mpg2 weight, msymbol(x)\n\n    Also, in the above, we specified the symbol-style synonyms.  Whether you\n    type\n\n        . scatter mpg1 weight, msymbol(oh) ||\n          scatter mpg2 weight, msymbol(x)\n\n    or\n\n        . scatter mpg1 weight, msymbol(smcircle_hollow) ||\n          scatter mpg2 weight, msymbol(smx)\n\n    makes no difference.\n\n\nFilled and hollow symbols\n-------------------------\n\n    The symbolstyle specifies the shape of the symbol, and in that sense, one\n    of the styles circle and hcircle -- and diamond and hdiamond, etc. -- is\n    unnecessary in that each is a different rendition of the same shape.  The\n    option mfcolor(colorstyle) (see [G-3] marker_options) specifies how the\n    inside of the symbol is to be filled.  hcircle(), hdiamond, etc., are\n    included for convenience and are equivalent to specifying\n\n        msymbol(Oh): msymbol(O) mfcolor(none)\n\n        msymbol(dh): msymbol(d) mfcolor(none)\n\n        etc.\n\n    Using mfcolor() to fill the inside of a symbol with different colors\n    sometimes creates what are effectively new symbols.  For instance, if you\n    take msymbol(O) and fill its interior with a lighter shade of the same\n    color used to outline the shape, you obtain a pleasing result.  For\n    instance, you might try\n\n        msymbol(O) mlcolor(yellow) mfcolor(.5*yellow)\n\n    or\n\n        msymbol(O) mlcolor(gs5) mfcolor(gs12)\n\n    as in\n\n        . scatter mpg weight, msymbol(O) mlcolor(gs5) mfcolor(gs14)\n          (click to run)\n\n\nSize of symbols\n---------------\n\n    Just as msymbol(O) and msymbol(Oh) differ only in mfcolor(), msymbol(O)\n    and msymbol(o) -- symbols circle and smcircle -- differ only in msize().\n    In particular,\n\n        msymbol(O): msymbol(O) msize(medium)\n\n        msymbol(o): msymbol(O) msize(small)\n\n    and the same is true for all the other large and small symbol pairs.\n\n    msize() is interpreted as being relative to the size of the graph region\n    (see [G-3] region_options), so the same symbol size will in fact be a\n    little different in\n\n        . scatter mpg weight\n\n    and\n\n        . scatter mpg weight, by(foreign total)\n\n\n\n\n代码\n%%stata\nhelp colorstyle\n\n\n\n[G-4] colorstyle -- Choices for color\n                    (View complete PDF manual entry)\n\n\nSyntax\n------\n\n    Set color of &lt;object&gt; to colorstyle\n\n        &lt;object&gt;color(colorstyle)\n\n\n    Set color of all affected objects to colorstyle\n\n        color(colorstyle)\n\n\n    Set opacity of &lt;object&gt; to #, where # is a percentage of 100% opacity\n\n        &lt;object&gt;color(%#)\n\n\n    Set opacity for all affected objects colors to #\n\n        color(%#)\n\n\n    Set both color and opacity of &lt;object&gt;\n\n        &lt;object&gt;color(colorstyle%#)\n\n\n    Set both color and opacity of all affected objects\n\n        &lt;object&gt;color(colorstyle%#)\n\n\n    colorstyle            Description\n    -------------------------------------------------------------------------\n    black                 \n\n    stc1                  color used by scheme stcolor\n    stc2                  color used by scheme stcolor\n    .                     \n    .                     \n    stc15                 color used by scheme stcolor\n    stblue                blue used by scheme stcolor\n    stgreen               green used by scheme stcolor\n    stred                 red used by scheme stcolor\n    styellow              yellow used by scheme stcolor\n\n    gs0                   gray scale: 0 = black\n    gs1                   gray scale: very dark gray\n    gs2                   \n    .                     \n    .                     \n    gs15                  gray scale: very light gray\n    gs16                  gray scale: 16 = white\n\n    white                 \n\n    blue                  \n    bluishgray            \n    brown                 \n    cranberry             \n    cyan                  \n    dimgray               between gs14 and gs15\n    dkgreen               dark green\n    dknavy                dark navy blue\n    dkorange              dark orange\n    eggshell              \n    emerald               \n    forest_green          \n    gold                  \n    gray                  equivalent to gs8\n    green                 \n    khaki                 \n    lavender              \n    lime                  \n    ltblue                light blue\n    ltbluishgray          light blue-gray, used by scheme s2color\n    ltkhaki               light khaki\n    magenta               \n    maroon                \n    midblue               \n    midgreen              \n    mint                  \n    navy                  \n    olive                 \n    olive_teal            \n    orange                \n    orange_red            \n    pink                  \n    purple                \n    red                   \n    sand                  \n    sandb                 bright sand\n    sienna                \n    stone                 \n    teal                  \n    yellow                \n\n                          colors used by The Economist magazine:\n    ebg                           background color\n    ebblue                        bright blue\n    edkblue                       dark blue\n    eltblue                       light blue\n    eltgreen                      light green\n    emidblue                      midblue\n    erose                         rose\n\n    none                  no color; invisible; draws nothing\n    background or bg      same color as background\n    foreground or fg      same color as foreground\n\n    \"# # #\"               RGB value; white = \"255 255 255\"\n\n    \"# # # #\"             CMYK value; yellow = \"0 0 255 0\"\n\n    \"hsv # # #\"           HSV value; white = \"hsv 0 0 1\"\n\n    colorstyle*#          color with adjusted intensity; #'s range from 0 to\n                            255\n\n    colorstyle%#          color with adjusted opacity; #s range from 0 to 100\n\n    *#                    default color with adjusted intensity\n    %#                    default color with adjusted opacity\n    -------------------------------------------------------------------------\n    When specifying RGB, CMYK, or HSV values, it is best to enclose the\n      values in quotes; type \"128 128 128\" not 128 128 128.\n\n\nDescription\n-----------\n\n    colorstyle sets the color and opacity of graph components such as lines,\n    backgrounds, and bars.  Some options allow a sequence of colorstyles with\n    colorstylelist; see [G-4] stylelists.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Remarks and examples\n\n    The above sections are not included in this help file.\n\n\nRemarks\n-------\n\n    colorstyle sets the color and opacity of graph components such as lines,\n    backgrounds, and bars.  Colors can be specified with a named color, such\n    as black, olive, and yellow, or with a color value in the RGB, CMYK, or\n    HSV format.  colorstyle can also set a component to match the background\n    color or foreground color.  Additionally, colorstyle can modify color\n    intensity, making the color lighter or darker.  Some options allow a\n    sequence of colorstyles with colorstylelist; see [G-4] stylelists.\n\n    To see a list of named colors, use graph query colorstyle.  See [G-2]\n    graph query.  For a color palette showing an individual color or\n    comparing two colors, use palette color.  See [G-2] palette.\n\n    Remarks are presented under the following headings:\n\n        Adjust opacity\n        Adjust intensity\n        Specify RGB values\n        Specify CMYK values\n        Specify HSV values\n        Export custom colors\n\n\nAdjust opacity\n--------------\n\n    Opacity is the percentage of a color that covers the background color.\n    That is, 100% means that the color fully hides the background, and 0%\n    means that the color has no coverage and is fully transparent.  If you\n    prefer to think about transparency, opacity is the inverse of\n    transparency.  Adjust opacity with the % modifier.  For example, type\n\n        green%50\n        \"0 255 0%50\"\n        %30\n\n    Omitting the color specification in the command adjusts the opacity of\n    the object while retaining the default color.  For instance, specify\n    mcolor(%30) with graph twoway scatter to use the default fill color at\n    30% opacity.\n\n    Specifying color%0 makes the object completely transparent and is\n    equivalent to color none.\n\n\nAdjust intensity\n----------------\n\n    Color intensity (brightness) can be modified by specifying a color, *,\n    and a multiplier value.  For example, type\n\n        green*.8\n        purple*1.5\n        \"0 255 255*1.2\"\n        \"hsv 240 1 1*.5\"\n\n    A value of 1 leaves the color unchanged, a value greater than 1 makes the\n    color darker, and a value less than 1 makes the color lighter.  Note that\n    there is no space between color and *, even when color is a numerical\n    value for RGB or CMYK.\n\n    Omitting the color specification in the command adjusts the intensity of\n    the object's default color.  For instance, specify bcolor(*.7) with graph\n    twoway bar to use the default fill color at reduced brightness, or\n    specify bcolor(*2) to increase the brightness of the default color.\n\n    Specifying color*0 makes the color as light as possible, but it is not\n    equivalent to color none.  color*255 makes the color as dark as possible,\n    although values much smaller than 255 usually achieve the same result.\n\n    For an example using the intensity adjustment, see Typical use in [G-2]\n    graph twoway kdensity.\n\n\nRGB values\n----------\n\n    In addition to specifying named colors such as yellow, you can specify\n    colors with RGB values.  An RGB value is a triplet of numbers ranging\n    from 0 to 255 that describes the level of red, green, and blue light that\n    must be emitted to produce a given color.  RGB is used to define colors\n    for on-screen display and in nonprofessional printing.  Examples of RGB\n    values are\n\n        red     =   255    0    0\n        green   =     0  255    0\n        blue    =     0    0  255\n        white   =   255  255  255\n        black   =     0    0    0\n        gray    =   128  128  128\n        navy    =    26   71  111\n\n\nCMYK values\n-----------\n\n    You can specify colors using CMYK values.  You will probably only use\n    CMYK values when they are provided by a journal or publisher.  You can\n    specify CMYK values either as integers from 0 to 255 or as proportions of\n    ink using real numbers from 0.0 to 1.0.  If all four values are 1 or\n    less, the numbers are taken to be proportions of ink.  For example,\n\n        red     =     0  255  255    0   or, equivalently,     0     1  1     0\n        green   =   255    0  255    0   or, equivalently,     1     0  1     0\n        blue    =   255  255    0    0   or, equivalently,     1     1  0     0\n        white   =     0    0    0    0   or, equivalently,     0     0  0     0\n        black   =     0    0    0  255   or, equivalently,     0     0  0     1\n        gray    =     0    0    0  128   or, equivalently,     0     0  0    .5\n        navy    =    85   40    0  144   or, equivalently,  .334  .157  0  .565\n\n\nHSV values\n----------\n\n    You can specify colors with HSV (hue, saturation, and value), also called\n    HSL (hue, saturation, and luminance) and HSB (hue, saturation, and\n    brightness).  HSV is often used in image editing software.  An HSV value\n    is a triplet of numbers.  So that Stata can differentiate them from RGB\n    values, HSV colors must be prefaced with hsv.  The first number specifies\n    the hue from 0 to 360, the second number specifies the saturation from 0\n    to 1, and the third number specifies the value (luminance or brightness)\n    from 0 to 1.  For example,\n\n        red     =   hsv   0     1     1\n        green   =   hsv 120     1  .502\n        blue    =   hsv 240     1     1\n        white   =   hsv   0     0     1\n        black   =   hsv   0     0     0\n        navy    =   hsv 209  .766  .435\n\n\nExport custom colors\n--------------------\n\n    graph export stores all colors as RGB+opacity values, that is, RGB values\n    0-255 and opacity values 0-1.  If you need color values from Stata in\n    CMYK format, use the graph export command with the cmyk(on) option, and\n    save the graph in one of the following formats: PostScript, Encapsulated\n    PostScript, or PDF.\n\n    You can set Stata to permanently use CMYK colors for PostScript export\n    files by typing translator set Graph2ps cmyk on and for EPS export files\n    by typing translator set Graph2eps cmyk on.\n\n    The CMYK values returned in graph export may differ from the CMYK values\n    that you entered.  This is because Stata normalizes CMYK values by\n    reducing all CMY values until one value is 0.  The difference is added to\n    the K (black) value.  For example, Stata normalizes the CMYK value 10 10\n    5 0 to 5 5 0 5.  Stata subtracts 5 from the CMY values so that Y is 0 and\n    then adds 5 to K.\n\n\nVideo example\n-------------\n\n        Transparency in Stata graphs\n\n\n\n\n代码\n%%stata\ntwoway scatter mpg weight,msymbol(D) mcolor(red) msize(medium)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "04-双变量作图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#option-by",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#option-by",
    "title": "05-双变量作图",
    "section": "1.3 Option: by",
    "text": "1.3 Option: by\n按照某个变量分组绘制散点图，可以使用by选项。by选项可以将数据按照指定变量进行分组，并为每个组绘制单独的散点图。\ntwoway scatter y x, by(groupvar)\n\n\n代码\n%%stata\ntwoway scatter mpg weight,by(foreign)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\ntwoway(scatter mpg weight if foreign ==0)(scatter mpg weight if foreign ==1),legend(label(1 \"Domestic\") label(2 \"Foreign\"))",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#sas-在-linux-的安装",
    "href": "Guide/SAS/SAS-install.html#sas-在-linux-的安装",
    "title": "00-SAS安装与vscode 扩展",
    "section": "2 SAS 在 Linux 的安装",
    "text": "2 SAS 在 Linux 的安装",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#sas-与-vscode-扩展",
    "href": "Guide/SAS/SAS-install.html#sas-与-vscode-扩展",
    "title": "00-SAS安装与vscode 扩展",
    "section": "3 SAS 与 vscode 扩展",
    "text": "3 SAS 与 vscode 扩展\nSAS VS Code 扩展轻量级，可在任何地方运行，并允许您集成 SAS 和其他语言。该工具还提供直接连接到 SAS Viya 和 SAS 9 并运行代码的功能。\n\nSAS 语法突出显示和帮助、代码完成和代码片段\n用于连接 SAS 和运行代码的配置文件配置\n支持 SAS Viya 和 SAS 9 连接\n访问 SAS 内容和库\n为 SAS、SQL、Python 和其他语言创建笔记本\n\n扩展程序可在 GitHub 上找到仓库与原代码：[vscode-sas-extension](https://github.com/sassoftware/vscode-sas-extension)\n更多关于 SAS 与 vscode 的信息可以访问：[SAS Extension for Visual Studio Code](https://developer.sas.com/programming/vs_code_extension)\n\n3.1 安装插件\n在 vscode 的扩展页面搜索 “sas” ，第一个 “official SAS ···“ 即为正确扩展：",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#配置路径",
    "href": "Guide/SAS/SAS-install.html#配置路径",
    "title": "00-SAS安装与vscode 扩展",
    "section": "4 配置路径",
    "text": "4 配置路径\nBefore you can run SAS code, you must configure the SAS extension to access your SAS 9.4 (remote or local) server or a SAS Viya server and add a connection profile.\n在运行 SAS 代码之前，您必须配置 SAS 扩展以访问 SAS 9.4（远程或本地）服务器或 SAS Viya 服务器。您必须获得 SAS 9.4 或 SAS Viya 的许可才能运行 SAS 代码。\n\n打开 SAS 程序文件。\n单击 VS Code 窗口左下方状态栏中的“无配置文件”。\n您还可以打开命令面板（F1，或Ctrl+Shift+P在 Windows 或 Linux 上，或Shift+CMD+P在 OSX 上）并找到SAS: Add New Connection Profile命令。\n按照“添加新连接配置文件”部分中的说明添加配置文件。\n创建配置文件后，状态栏项将从“无配置文件”更改为新配置文件的名称。\n\n\n更多设置可以查看[SAS Extension for Visual Studio Code Documentation](https://sassoftware.github.io/vscode-sas-extension/Configurations/Profiles/sas9local)",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#编译-sas-文件",
    "href": "Guide/SAS/SAS-install.html#编译-sas-文件",
    "title": "00-SAS安装与vscode 扩展",
    "section": "5 编译 SAS 文件",
    "text": "5 编译 SAS 文件\nSAS 文件右上角有一个 奔跑的小人 ，点击即可开始运行所选中的程序段落，并在右侧窗口输出结果。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#在-jupyter-notebook-中使用-sas",
    "href": "Guide/SAS/SAS-install.html#在-jupyter-notebook-中使用-sas",
    "title": "00-SAS安装与vscode 扩展",
    "section": "6 在 Jupyter Notebook 中使用 SAS",
    "text": "6 在 Jupyter Notebook 中使用 SAS\n\n6.1 环境准备\n安装 Anaconda 集成环境或 Python 和 SAS 软件，其中Anaconda要求Python3+；Python在Jupyter Notebook和SAS之间起一个桥梁的作用，Jupyter Notebook中的SAS代码会交给Python，Python负责将代码传递给SAS执行；然后将执行的结果返回给Jupyter Notebook显示。\nSAS版本要求9.4，也可以是 SAS Viya。\n\n\n6.2 安装SAS_KERNEL\n启动 cmd，输入命令：\n\n\n代码\npip install sas_kernel\n\n\n然后就会自动安装 sas_kernel 及其相应的依赖项。\n安装完成后可以输入命令：\n\n\n代码\njupyter kernelspec list\n\n\n来检测 sas_kernel 是否安装成功，如果成功，理论上会看到如下形式的输出：\nAvailable kernels:\n    python3    /home/sas/anaconda3/lib/python3.5/site-packages/ipykernel/resources\n    sas        /home/sas/.local/share/jupyter/kernels/sas\n\n\n6.3 修改 Python 配置文件\n安装好 sas_kernel 后找到 Anaconda 或 Python 的安装目录，会有一个相应的文件夹出现，例如我的文件路径如下：\nC:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\saspy\n在这个文件路径下找到 sascfg.py 文件，该文件中需要配置连接SAS的信息。可以配置连接本地机器的SAS；也可以配置连接远程机器的SAS Server，无论是Linux Server还是Windows Server都可以。此处就以连接本地SAS为例进行说明。\n\n打开该文件，首先是一大段注释；\n在这段注释后定义的第一个变量 SAS_config_names 用于指定连接SAS的配置方式，提供了 10 种方式：default, ssh, iomlinux, iomwin, winlocal, winiomlinux, winiomwin, httpsviya, httpviya, iomcom。默认为 default 方式。\n因为我们需要连接Windows机器本地的SAS，所以需要将 SAS_config_names 的值修改为 winlocal 。\n\n\n后续有一些安装步骤，但是大多是在2016-2020年更新的教程，无法找到复现的路径，可能相关的配置已被优化。\n包括这个 [SAS岩论 | 在Jupyter Notebook中使用SAS ](https://www.sohu.com/a/218339423_278472) 中写到的需要使用 cpW 定义 SAS 路径。\n\n\n6.4 修改系统变量\n将sas相关文件 sspiauth.dll 添加到系统环境变量，该文件很可能在如下目录：\nC:\\Program Files\\SASHome\\SASFoundation\\9.4\\core\\sasext\n（注意添加变量时不要包含 sspiauth.dll 文件本身）\nWarning: 环境变量添加完成后，要重启电脑才会生效。\n\n\n6.5 在 jupyter notebook 中使用 SAS\n新建文件，选择使用 SAS 内核，或者在 cell 中通过 magic command 指定内核。\n%%sas\n使用语法如下所示：\n\n\n代码\n%%sas\ndata iris;\n    set sashelp.iris;\nrun;\n\nproc print data=iris(obs=10);\nrun;\n\n\n在Notebook中写SAS代码了，跟Python一样，同样有代码提示、语法高亮的功能。但是你会注意到过程步的结果显示了，运行的日志去哪里了？\n如果代码运行错误或者没有输出（例如纯DATA步）的话，那么输出就是日志信息。\n能够正确运行且有输出结果的代码就不会显示日志了。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#安装sas日志组件",
    "href": "Guide/SAS/SAS-install.html#安装sas日志组件",
    "title": "00-SAS安装与vscode 扩展",
    "section": "7 安装SAS日志组件",
    "text": "7 安装SAS日志组件\n如果想要像SAS Base一样，随时查看所有程序运行的日志结果也没问题。安装一个Notebook的SAS日志扩展组件就可以了。打开Anaconda Prompt，输入以下命令安装：\njupyter nbextension install --py sas_kernel.showSASLog\n运行完毕后，输入以下命令启用SAS日志组件：\njupyter nbextension enable sas_kernel.showSASLog –py",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#连接sas-server",
    "href": "Guide/SAS/SAS-install.html#连接sas-server",
    "title": "00-SAS安装与vscode 扩展",
    "section": "8 连接SAS Server",
    "text": "8 连接SAS Server\n如果需要配置连接远程的SAS Server，如连接远程Windows机器的SAS Server，需在sascfg.py中做以下修改：\n\n将SAS_config_names的值改为“wintowin”；\n在wintowin连接方式中将参数iomhost的值修改为远程Windows机器的IP地址；将参数encoding的值修改为euc-cn；\n将cpW中5个Jar包的路径修改为远程Windows机器中SAS对应的目录。\n\n修改完毕后，启动Notebook，首次运行SAS代码时，会提示输入访问SAS Server的有效SAS用户和密码。1\n\n8.1 典型生态项目\n\nSASPy\nSASPy 是一个 Python 库，允许你通过 Python 代码与 SAS 进行交互。SAS Kernel 依赖于 SASPy，因此在使用 SAS Kernel 之前，你需要配置 SASPy。\n\n\nJupyterLab 扩展\nSAS Kernel 支持 JupyterLab 扩展，这些扩展可以提高你在 JupyterLab 中的编程效率。你可以通过以下命令安装这些扩展：\npip install sas_kernel[jlab_ext] \n\n\nNBGrader\nNBGrader 是一个用于分配和评分 Jupyter Notebook 的系统，它与 SAS Kernel 兼容。你可以使用 NBGrader 来创建和评分包含 SAS 代码的作业。\n通过这些生态项目，SAS Kernel 不仅扩展了 Jupyter Notebook 的功能，还增强了其在数据科学和分析领域的应用能力。2",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#footnotes",
    "href": "Guide/SAS/SAS-install.html#footnotes",
    "title": "00-SAS安装与vscode 扩展",
    "section": "脚注",
    "text": "脚注\n\n\nSAS岩论 | 在Jupyter Notebook中使用SAS↩︎\nSAS Kernel for Jupyter 安装与使用教程↩︎",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html",
    "href": "Guide/Stata/25-05-08-GLM.html",
    "title": "14-广义线性模型（GLM）",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)\n%%stata\nsysuse auto.dta,clear\n\n(1978 automobile data)\n大于15%会产生Bias的这个问题，只有在我们想用odds去estimaterisk的时候才会发生。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#什么是广义线性模型",
    "href": "Guide/Stata/25-05-08-GLM.html#什么是广义线性模型",
    "title": "14-广义线性模型（GLM）",
    "section": "1 什么是广义线性模型",
    "text": "1 什么是广义线性模型\n\n线性回归 Linear regression：\n\n\\(exp Y = \\beta_0 + \\beta_1*X_1+\\cdots + \\beta_n *X_n\\)\n\n广义线性回归 Generalized linear model：\n\n\\(f(exp Y)= β_0 + β_1*X_1 + \\cdots + \\beta_n* X_n\\)\nLinear regression is a special case of GLM\nSame for logistic regression, Poisson regression, Log-Binomial regression\nEven for Cox regression (Aug.31st & Sep.7th), GEE model (Sep.14th)\n\n总结：在GLM中，转化Y使得转化之后的Y和X们呈线性关系",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#simple-glm-vs.-multiple-glm",
    "href": "Guide/Stata/25-05-08-GLM.html#simple-glm-vs.-multiple-glm",
    "title": "14-广义线性模型（GLM）",
    "section": "2 Simple GLM vs. Multiple GLM",
    "text": "2 Simple GLM vs. Multiple GLM\n\n2.1 简单广义线性模型\n\\[f(exp Y)= β_0 + β_1*X_1\\]\n\n\n2.2 多元广义线性模型\n\\[f(exp Y)= β_0 + β_1*X_1 + \\cdots + \\beta_n* X_n\\]\nX变量可以是各种类型的变量（连续、分类）",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#specify-options",
    "href": "Guide/Stata/25-05-08-GLM.html#specify-options",
    "title": "14-广义线性模型（GLM）",
    "section": "3 Specify Options",
    "text": "3 Specify Options\n\n3.1 Family\nY 变量的分布类型\n\n\n3.2 Link\n把 Y 怎么转化，才和 X 成linear关系",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#linear-regression",
    "href": "Guide/Stata/25-05-08-GLM.html#linear-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "4 linear regression",
    "text": "4 linear regression\n\n4.1 Y var\n\n连续分布（continuous variable）\n如何分布? 高斯分布（Gaussian Distribution）\nModel：\n\n\\[f(exp Y)= β_0 + β_1*X_1 + \\cdots + \\beta_n* X_n\\]\nY 怎么转化才和 X 成 linear 关系？\n\n不用转化\nLink：Identity（恒等）",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#logistic-regression",
    "href": "Guide/Stata/25-05-08-GLM.html#logistic-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "5 Logistic regression",
    "text": "5 Logistic regression\n\n5.1 Y var\n\nBinary variable\n如何分布? 二项分布(Binomial)\nModel:\n\n\\[ln[P(Y=1)/P(Y=0)]= β_0 + β_1*X_1+\\cdots + β_n*X_n\\]\nExp Y = P(Y=1)\nY 怎么转化才和X成linear关系？Link：logit",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#poisson-regression",
    "href": "Guide/Stata/25-05-08-GLM.html#poisson-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "6 Poisson regression",
    "text": "6 Poisson regression\n\n6.1 Y var\n\nCount variable:整数(1,2,3,.n)\n如何分布? 泊松分布(Poisson)\nModel:\n\n\\[ln[risk of event)]= β_0 + β_1*X_1+\\cdots + β_n*X_n\\]\nY 怎么转化才和X成linear关系？Link：Log",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#log-binomial-regression",
    "href": "Guide/Stata/25-05-08-GLM.html#log-binomial-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "7 Log-binomial Regression",
    "text": "7 Log-binomial Regression\n\n7.1 Y var\n\nBinary variable\n如何分布? 二项分布(Binomial)\nModel:\n\n\\[ln[P(Y=1)]= β_0 + β_1*X_1+\\cdots + β_n*X_n\\]\nExp Y = P(Y=1)\nY 怎么转化才和X成linear关系？Link：Log\n\n\n\n%%stata\nhelp glm\n\n\n[R] glm -- Generalized linear models\n           (View complete PDF manual entry)\n\n\nSyntax\n------\n\n        glm depvar [indepvars] [if] [in] [weight] [, options]\n\n    options                     Description\n    -------------------------------------------------------------------------\n    Model\n      family(familyname)        distribution of depvar; default is\n                                  family(gaussian)\n      link(linkname)            link function; default is canonical link for\n                                  family() specified\n\n    Model 2\n      noconstant                suppress constant term\n      exposure(varname)         include ln(varname) in model with coefficient\n                                  constrained to 1\n      offset(varname)           include varname in model with coefficient\n                                  constrained to 1\n      constraints(constraints)  apply specified linear constraints\n      asis                      retain perfect predictor variables\n      mu(varname)               use varname as the initial estimate for the\n                                  mean of depvar\n      init(varname)             synonym for mu(varname)\n\n    SE/Robust\n      vce(vcetype)              vcetype may be oim, robust, cluster clustvar,\n                                  eim, opg, bootstrap, jackknife, hac kernel,\n                                  jackknife1, or unbiased\n      vfactor(#)                multiply variance matrix by scalar #\n      disp(#)                   quasilikelihood multiplier\n      scale(x2|dev|#)           set the scale parameter\n\n    Reporting\n      level(#)                  set confidence level; default is level(95)\n      eform                     report exponentiated coefficients\n      nocnsreport               do not display constraints\n      display_options           control columns and column formats, row\n                                  spacing, line width, display of omitted\n                                  variables and base and empty cells, and\n                                  factor-variable labeling\n\n    Maximization\n      ml                        use maximum likelihood optimization; the\n                                  default\n      irls                      use iterated, reweighted least-squares\n                                  optimization of the deviance\n      maximize_options          control the maximization process; seldom used\n      fisher(#)                 use the Fisher scoring Hessian or expected\n                                  information matrix (EIM)\n      search                    search for good starting values\n\n      noheader                  suppress header table from above coefficient\n                                  table\n      notable                   suppress coefficient table\n      nodisplay                 suppress the output; iteration log is still\n                                  displayed\n      collinear                 keep collinear variables\n      coeflegend                display legend instead of statistics\n    -------------------------------------------------------------------------\n\n    familyname               Description\n    -------------------------------------------------------------------------\n    gaussian                 Gaussian (normal)\n    igaussian                inverse Gaussian\n    binomial[varnameN|#N]    Bernoulli/binomial\n    poisson                  Poisson\n    nbinomial[#k|ml]         negative binomial\n    gamma                    gamma\n    -------------------------------------------------------------------------\n\n    linkname                 Description\n    -------------------------------------------------------------------------\n    identity                 identity\n    log                      log\n    logit                    logit\n    probit                   probit\n    cloglog                  cloglog\n    power #                  power\n    opower #                 odds power\n    nbinomial                negative binomial\n    loglog                   log-log\n    logc                     log-complement\n    -------------------------------------------------------------------------\n\n    indepvars may contain factor variables; see fvvarlist.\n    depvar and indepvars may contain time-series operators; see tsvarlist.\n    bayes, bootstrap, by, collect, fmm, fp, jackknife, mfp, mi estimate,\n      nestreg, rolling, statsby, stepwise, and svy are allowed; see prefix.\n      For more details, see [BAYES] bayes: glm and [FMM] fmm: glm.\n    vce(bootstrap), vce(jackknife), and vce(jackknife1) are not allowed with\n      the mi estimate prefix.\n    Weights are not allowed with the bootstrap prefix.\n    aweights are not allowed with the jackknife prefix.\n    vce(), vfactor(), disp(), scale(), irls, fisher(), noheader, notable,\n      nodisplay, and weights are not allowed with the svy prefix.\n    fweights, aweights, iweights, and pweights are allowed; see weight.\n    noheader, notable, nodisplay, collinear, and coeflegend do not appear in\n      the dialog box.\n    See [R] glm postestimation for features available after estimation.\n\n\nMenu\n----\n\n    Statistics &gt; Generalized linear models &gt; Generalized linear models (GLM)\n\n\nDescription\n-----------\n\n    glm fits generalized linear models.  It can fit models by using either\n    IRLS (maximum quasilikelihood) or Newton-Raphson (maximum likelihood)\n    optimization, which is the default.\n\n    See [U] 27 Overview of Stata estimation commands for a description of all\n    of Stata's estimation commands, several of which fit models that can also\n    be fit using glm.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Quick start\n\n        Remarks and examples\n\n        Methods and formulas\n\n    The above sections are not included in this help file.\n\n\nOptions\n-------\n\n        +-------+\n    ----+ Model +------------------------------------------------------------\n\n    family(familyname) specifies the distribution of depvar; family(gaussian)\n        is the default.\n\n    link(linkname) specifies the link function; the default is the canonical\n        link for the family() specified (except for family(nbinomial)).\n\n        +---------+\n    ----+ Model 2 +----------------------------------------------------------\n\n    noconstant, exposure(varname), offset(varname), constraints(constraints);\n        see [R] Estimation options.  constraints(constraints) is not allowed\n        with irls.\n\n    asis forces retention of perfect predictor variables and their\n        associated, perfectly predicted observations and may produce\n        instabilities in maximization; see [R] probit.  This option is\n        allowed only with option family(binomial) with a denominator of 1.\n\n    mu(varname) specifies varname as the initial estimate for the mean of \n        depvar.  This option can be useful with models that experience\n        convergence difficulties, such as family(binomial) models with power\n        or odds-power links.  init(varname) is a synonym.\n\n        +-----------+\n    ----+ SE/Robust +--------------------------------------------------------\n\n    vce(vcetype) specifies the type of standard error reported, which\n        includes types that are derived from asymptotic theory (oim, opg),\n        that are robust to some kinds of misspecification (robust), that\n        allow for intragroup correlation (cluster clustvar), and that use\n        bootstrap or jackknife methods (bootstrap, jackknife); see [R]\n        vce_option.\n\n        In addition to the standard vcetypes, glm allows the following\n        alternatives:\n\n        vce(eim) specifies that the EIM estimate of variance be used.\n\n        vce(jackknife1) specifies that the one-step jackknife estimate of\n            variance be used.\n\n        vce(hac kernel [#]) specifies that a heteroskedasticity- and\n            autocorrelation-consistent (HAC) variance estimate be used.  HAC\n            refers to the general form for combining weighted matrices to\n            form the variance estimate.  There are three kernels built into\n            glm.  kernel is a user-written program or one of\n\n                          nwest | gallant | anderson\n\n            # specifies the number of lags.  If # is not specified, N - 2 is\n            assumed.  If you wish to specify vce(hac ... ), you must tsset\n            your data before calling glm.\n\n        vce(unbiased) specifies that the unbiased sandwich estimate of\n            variance be used.\n\n    vfactor(#) specifies a scalar by which to multiply the resulting variance\n        matrix.  This option allows you to match output with other packages,\n        which may apply degrees of freedom or other small-sample corrections\n        to estimates of variance.\n\n    disp(#) multiplies the variance of depvar by # and divides the deviance\n        by #.  The resulting distributions are members of the quasilikelihood\n        family.  This option is allowed only with option irls.\n\n    scale(x2|dev|#) overrides the default scale parameter.  This option is\n        allowed only with Hessian (information matrix) variance estimates.\n\n        By default, scale(1) is assumed for the discrete distributions\n        (binomial, Poisson, and negative binomial), and scale(x2) is assumed\n        for the continuous distributions (Gaussian, gamma, and inverse\n        Gaussian).\n\n        scale(x2) specifies that the scale parameter be set to the Pearson\n        chi-squared (or generalized chi-squared) statistic divided by the\n        residual degrees of freedom, which is recommended by McCullagh and\n        Nelder (1989) as a good general choice for continuous distributions.\n\n        scale(dev) sets the scale parameter to the deviance divided by the\n        residual degrees of freedom.  This option provides an alternative to\n        scale(x2) for continuous distributions and overdispersed or\n        underdispersed discrete distributions.  This option is allowed only\n        with option irls.\n\n        scale(#) sets the scale parameter to #.  For example, using scale(1)\n        in family(gamma) models results in exponential-errors regression.\n        Additional use of link(log) rather than the default link(power -1)\n        for family(gamma) essentially reproduces Stata's streg, dist(exp)\n        nohr command (see [ST] streg) if all the observations are uncensored.\n\n        +-----------+\n    ----+ Reporting +--------------------------------------------------------\n\n    level(#); see [R] Estimation options.\n\n    eform displays the exponentiated coefficients and corresponding standard\n        errors and confidence intervals.  For family(binomial) link(logit)\n        (that is, logistic regression), exponentiation results are odds\n        ratios; for family(nbinomial) link(log) (that is, negative binomial\n        regression) and for family(poisson) link(log) (that is, Poisson\n        regression), exponentiated coefficients are incidence-rate ratios.\n\n    nocnsreport; see [R] Estimation options.\n\n    display_options:  noci, nopvalues, noomitted, vsquish, noemptycells,\n        baselevels, allbaselevels, nofvlabel, fvwrap(#), fvwrapon(style),\n        cformat(%fmt), pformat(%fmt), sformat(%fmt), and nolstretch; see [R]\n        Estimation options.\n\n        +--------------+\n    ----+ Maximization +-----------------------------------------------------\n\n    ml requests that optimization be carried out using Stata's ml commands\n        and is the default.\n\n    irls requests iterated, reweighted least-squares (IRLS) optimization of\n        the deviance instead of Newton-Raphson optimization of the log\n        likelihood.  If the irls option is not specified, the optimization is\n        carried out using Stata's ml commands, in which case all options of\n        ml maximize are also available.\n\n    maximize_options:  difficult, technique(algorithm_spec), iterate(#),\n        [no]log, trace, gradient, showstep, hessian, showtolerance,\n        tolerance(#), ltolerance(#), nrtolerance(#), nonrtolerance, and\n        from(init_specs); see [R] Maximize.  These options are seldom used.\n\n        Setting the optimization method to technique(bhhh) resets the default\n        vcetype to vce(opg).\n\n        If option irls is specified, only maximize_options iterate(), nolog,\n        trace, and ltolerance() are allowed.  With irls specified, the\n        convergence criterion is satisfied when the absolute change in\n        deviance from one iteration to the next is less than or equal to\n        ltolerance(), where ltolerance(1e-6) is the default.\n\n    fisher(#) specifies the number of Newton-Raphson steps that should use\n        the Fisher scoring Hessian or EIM before switching to the observed\n        information matrix (OIM).  This option is useful only for\n        Newton-Raphson optimization (and not when using irls).\n\n    search specifies that the command search for good starting values.  This\n        option is useful only for Newton-Raphson optimization (and not when\n        using irls).\n\n    The following options are available with glm but are not shown in the\n    dialog box:\n\n    noheader suppresses the header information from the output.  The\n        coefficient table is still displayed.\n\n    notable suppresses the table of coefficients from the output.  The header\n        information is still displayed.\n\n    nodisplay suppresses the output.  The iteration log is still displayed.\n\n    collinear, coeflegend; see [R] Estimation options.  collinear is not\n        allowed with irls.\n\n\nRemarks\n-------\n\n    Although glm can be used to fit linear regression (and, in fact, does so\n    by default), this should be viewed as an instructional feature; regress\n    produces such estimates more quickly, and many postestimation commands\n    are available to explore the adequacy of the fit; see [R] regress and [R]\n    regress postestimation.\n\n    In any case, you should specify the link function by using the link()\n    option and specify the distributional family by using family().  The\n    available link functions are\n\n                   Link function            glm option     \n                   ----------------------------------------\n                   identity                 link(identity) \n                   log                      link(log)      \n                   logit                    link(logit)    \n                   probit                   link(probit)   \n                   complementary log-log    link(cloglog)  \n                   odds power               link(opower #) \n                   power                    link(power #)  \n                   negative binomial        link(nbinomial)\n                   log-log                  link(loglog)   \n                   log-complement           link(logc)     \n\n    The available distributional families are\n\n                   Family                 glm option       \n                   ----------------------------------------\n                   Gaussian(normal)       family(gaussian) \n                   inverse Gaussian       family(igaussian)\n                   Bernoulli/binomial     family(binomial) \n                   Poisson                family(poisson)  \n                   negative binomial      family(nbinomial)\n                   gamma                  family(gamma)    \n\n    You do not have to specify both family() and link(); the default link()\n    is the canonical link for the specified family() (except for nbinomial):\n\n                    Family                  Default link  \n                    --------------------------------------\n                    family(gaussian)        link(identity)\n                    family(igaussian)       link(power -2)\n                    family(binomial)        link(logit)   \n                    family(poisson)         link(log)     \n                    family(nbinomial)       link(log)     \n                    family(gamma)           link(power -1)\n\n    If you specify both family() and link(), not all combinations make sense.\n    You may choose from the following combinations:\n\n          | id  log  logit  probit  clog  pow  opower  nbinomial  loglog  logc\n----------+-------------------------------------------------------------------\nGaussian  |  x   x                         x\ninv. Gau. |  x   x                         x\nbinomial  |  x   x     x      x       x    x     x                  x      x\nPoisson   |  x   x                         x\nneg. bin. |  x   x                         x              x\ngamma     |  x   x                         x\n\n\nExamples\n--------\n\n    ---------------------------------------------------------------------------\n    Setup\n        . webuse lbw\n\n    Generalized linear model with Bernoulli family and default logit link\n        . glm low age lwt i.race smoke ptl ht ui, family(binomial)\n\n    Replay results and report exponentiated coefficients\n        . glm, eform\n\n    ---------------------------------------------------------------------------\n    Setup\n        . webuse ldose\n\n    Generalized linear model with binomial family and default logit link\n        . glm r ldose, family(binomial n)\n\n    Generalized linear model with binomial family and cloglog link\n        . glm r ldose, family(binomial n) link(cloglog)\n\n    ---------------------------------------------------------------------------\n    Setup\n        . webuse beetle\n\n    Generalized linear model with binomial family and cloglog link\n        . glm r i.beetle ldose, family(binomial n) link(cloglog)\n\n    Replay results with 99% confidence intervals\n        . glm, level(99)\n    ---------------------------------------------------------------------------\n\n\nStored results\n--------------\n\n    glm, ml stores the following in e():\n\n    Scalars           \n      e(N)                   number of observations\n      e(k)                   number of parameters\n      e(k_eq)                number of equations in e(b)\n      e(k_eq_model)          number of equations in overall model test\n      e(k_dv)                number of dependent variables\n      e(df_m)                model degrees of freedom\n      e(df)                  residual degrees of freedom\n      e(phi)                 scale parameter\n      e(aic)                 model AIC\n      e(bic)                 model BIC\n      e(ll)                  log likelihood, if NR\n      e(N_clust)             number of clusters\n      e(chi2)                chi-squared\n      e(p)                   p-value for model test\n      e(deviance)            deviance\n      e(deviance_s)          scaled deviance\n      e(deviance_p)          Pearson deviance\n      e(deviance_ps)         scaled Pearson deviance\n      e(dispers)             dispersion\n      e(dispers_s)           scaled dispersion\n      e(dispers_p)           Pearson dispersion\n      e(dispers_ps)          scaled Pearson dispersion\n      e(nbml)                1 if negative binomial parameter estimated via\n                               ML, 0 otherwise\n      e(vf)                  factor set by vfactor(), 1 if not set\n      e(power)               power set by link(power #) or link(opower #)\n      e(rank)                rank of e(V)\n      e(ic)                  number of iterations\n      e(rc)                  return code\n      e(converged)           1 if converged, 0 otherwise\n\n    Macros            \n      e(cmd)                 glm\n      e(cmdline)             command as typed\n      e(depvar)              name of dependent variable\n      e(varfunc)             program to calculate variance function\n      e(varfunct)            variance title\n      e(varfuncf)            variance function\n      e(link)                program to calculate link function\n      e(linkt)               link title\n      e(linkf)               link function\n      e(m)                   number of binomial trials\n      e(wtype)               weight type\n      e(wexp)                weight expression\n      e(title)               title in estimation output\n      e(clustvar)            name of cluster variable\n      e(offset)              linear offset variable\n      e(chi2type)            Wald; type of model chi-squared test\n      e(cons)                noconstant, if specified\n      e(hac_kernel)          HAC kernel\n      e(hac_lag)             HAC lag\n      e(vce)                 vcetype specified in vce()\n      e(vcetype)             title used to label Std. err.\n      e(opt)                 ml or irls\n      e(opt1)                optimization title, line 1\n      e(opt2)                optimization title, line 2\n      e(which)               max or min; whether optimizer is to perform\n                               maximization or minimization\n      e(ml_method)           type of ml method\n      e(user)                name of likelihood-evaluator program\n      e(technique)           maximization technique\n      e(properties)          b V\n      e(predict)             program used to implement predict\n      e(marginsok)           predictions allowed by margins\n      e(marginsnotok)        predictions disallowed by margins\n      e(asbalanced)          factor variables fvset as asbalanced\n      e(asobserved)          factor variables fvset as asobserved\n\n    Matrices          \n      e(b)                   coefficient vector\n      e(Cns)                 constraints matrix\n      e(ilog)                iteration log (up to 20 iterations)\n      e(gradient)            gradient vector\n      e(V)                   variance-covariance matrix of the estimators\n      e(V_modelbased)        model-based variance\n\n    Functions         \n      e(sample)              marks estimation sample\n\n    In addition to the above, the following is stored in r():\n\n    Matrices          \n      r(table)               matrix containing the coefficients with their\n                               standard errors, test statistics, p-values,\n                               and confidence intervals\n\n    Note that results stored in r() are updated when the command is replayed\n    and will be replaced when any r-class command is run after the estimation\n    command.\n\n    glm, irls stores the following in e():\n\n    Scalars           \n      e(N)                   number of observations\n      e(k)                   number of parameters\n      e(k_eq_model)          number of equations in overall model test\n      e(df_m)                model degrees of freedom\n      e(df)                  residual degrees of freedom\n      e(phi)                 scale parameter\n      e(disp)                dispersion parameter\n      e(bic)                 model BIC\n      e(N_clust)             number of clusters\n      e(deviance)            deviance\n      e(deviance_s)          scaled deviance\n      e(deviance_p)          Pearson deviance\n      e(deviance_ps)         scaled Pearson deviance\n      e(dispers)             dispersion\n      e(dispers_s)           scaled dispersion\n      e(dispers_p)           Pearson dispersion\n      e(dispers_ps)          scaled Pearson dispersion\n      e(nbml)                1 if negative binomial parameter estimated via\n                               ML, 0 otherwise\n      e(vf)                  factor set by vfactor(), 1 if not set\n      e(power)               power set by link(power #) or link(opower #)\n      e(rank)                rank of e(V)\n      e(rc)                  return code\n\n    Macros            \n      e(cmd)                 glm\n      e(cmdline)             command as typed\n      e(depvar)              name of dependent variable\n      e(varfunc)             program to calculate variance function\n      e(varfunct)            variance title\n      e(varfuncf)            variance function\n      e(link)                program to calculate link function\n      e(linkt)               link title\n      e(linkf)               link function\n      e(m)                   number of binomial trials\n      e(wtype)               weight type\n      e(wexp)                weight expression\n      e(clustvar)            name of cluster variable\n      e(offset)              linear offset variable\n      e(cons)                noconstant, if specified\n      e(hac_kernel)          HAC kernel\n      e(hac_lag)             HAC lag\n      e(vce)                 vcetype specified in vce()\n      e(vcetype)             title used to label Std. err.\n      e(opt)                 ml or irls\n      e(opt1)                optimization title, line 1\n      e(opt2)                optimization title, line 2\n      e(properties)          b V\n      e(predict)             program used to implement predict\n      e(marginsok)           predictions allowed by margins\n      e(marginsnotok)        predictions disallowed by margins\n      e(asbalanced)          factor variables fvset as asbalanced\n      e(asobserved)          factor variables fvset as asobserved\n\n    Matrices          \n      e(b)                   coefficient vector\n      e(V)                   variance-covariance matrix of the estimators\n      e(V_modelbased)        model-based variance\n\n    Functions         \n      e(sample)              marks estimation sample\n\n    In addition to the above, the following is stored in r():\n\n    Matrices          \n      r(table)               matrix containing the coefficients with their\n                               standard errors, test statistics, p-values,\n                               and confidence intervals\n\n    Note that results stored in r() are updated when the command is replayed\n    and will be replaced when any r-class command is run after the estimation\n    command.\n\n\nReference\n---------\n\n    McCullagh, P., and J. A. Nelder. 1989.  Generalized Linear Models. 2nd\n        ed.  London: Chapman & Hall/CRC.\n\n\nglm command in Stata - Umbrella Command - linear regression: family(gaussian) link(identity) - logistic regression: family(binomial) link(logit) - poisson regression: family(poisson) link(log) - log-binomial regression: family(binomial) link(log)\n\n加粗部分表示，在实际代码中可以简写为加粗的字母或单词即可",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#代码的转换",
    "href": "Guide/Stata/25-05-08-GLM.html#代码的转换",
    "title": "14-广义线性模型（GLM）",
    "section": "8 代码的转换",
    "text": "8 代码的转换\n\n8.1 Linear regression:\n\nregress price weight length mpg i.rep78\nglm price weight length mpg i.rep78, family(gaussian)link(identity)\n\n\n\n8.2 Logistic regression:\n\nlogistic low age i.smoke i.race\nglm low age i.smoke i.race,family(binomial) link(logit)\nglm low age i.smoke i.race,family(binomial) link(logit) eform\n\n\neform 直接输出 OR 值，即 \\(e^{\\beta}\\)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#线性回归模型的比较",
    "href": "Guide/Stata/25-05-08-GLM.html#线性回归模型的比较",
    "title": "14-广义线性模型（GLM）",
    "section": "9 线性回归模型的比较",
    "text": "9 线性回归模型的比较\n\n%%stata\nregress price weight length mpg i.rep78\n\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(7, 61)        =      7.25\n       Model |   262008114         7  37429730.6   Prob &gt; F        =    0.0000\n    Residual |   314788844        61  5160472.86   R-squared       =    0.4542\n-------------+----------------------------------   Adj R-squared   =    0.3916\n       Total |   576796959        68  8482308.22   Root MSE        =    2271.7\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   5.186695   1.163383     4.46   0.000     2.860367    7.513022\n      length |  -124.1544   40.07637    -3.10   0.003     -204.292   -44.01671\n         mpg |  -126.8367   84.49819    -1.50   0.138    -295.8012    42.12791\n             |\n       rep78 |\n          2  |   1137.284   1803.332     0.63   0.531    -2468.701    4743.269\n          3  |   1254.642   1661.545     0.76   0.453    -2067.823    4577.108\n          4  |   2267.188   1698.018     1.34   0.187    -1128.208    5662.584\n          5  |   3850.759   1787.272     2.15   0.035     276.8886     7424.63\n             |\n       _cons |   14614.49   6155.842     2.37   0.021     2305.125    26923.86\n------------------------------------------------------------------------------\n\n\n\n%%stata\nglm price weight length mpg i.rep78, family(gaussian)link(identity)\n\n\nIteration 0:  Log likelihood = -626.90582  \n\nGeneralized linear models                         Number of obs   =         69\nOptimization     : ML                             Residual df     =         61\n                                                  Scale parameter =    5160473\nDeviance         =  314788844.4                   (1/df) Deviance =    5160473\nPearson          =  314788844.4                   (1/df) Pearson  =    5160473\n\nVariance function: V(u) = 1                       [Gaussian]\nLink function    : g(u) = u                       [Identity]\n\n                                                  AIC             =   18.40307\nLog likelihood   = -626.9058204                   BIC             =   3.15e+08\n\n------------------------------------------------------------------------------\n             |                 OIM\n       price | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   5.186695   1.163383     4.46   0.000     2.906506    7.466883\n      length |  -124.1544   40.07637    -3.10   0.002    -202.7026   -45.60612\n         mpg |  -126.8367   84.49819    -1.50   0.133    -292.4501    38.77675\n             |\n       rep78 |\n          2  |   1137.284   1803.332     0.63   0.528    -2397.182     4671.75\n          3  |   1254.642   1661.545     0.76   0.450    -2001.927    4511.211\n          4  |   2267.188   1698.018     1.34   0.182    -1060.866    5595.241\n          5  |   3850.759   1787.272     2.15   0.031     347.7711    7353.747\n             |\n       _cons |   14614.49   6155.842     2.37   0.018     2549.263    26679.72\n------------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#logistic-回归的比较",
    "href": "Guide/Stata/25-05-08-GLM.html#logistic-回归的比较",
    "title": "14-广义线性模型（GLM）",
    "section": "10 logistic 回归的比较",
    "text": "10 logistic 回归的比较\n重新导入数据：\n\n%%stata\nwebuse lbw,clear\n\n(Hosmer & Lemeshow data)\n\n\n\n%%stata\nlogistic low age i.smoke i.race\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(4)    =  15.81\n                                                        Prob &gt; chi2   = 0.0033\nLog likelihood = -109.4311                              Pseudo R2     = 0.0674\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9657186   .0322573    -1.04   0.296     .9045206    1.031057\n             |\n       smoke |\n     Smoker  |    3.00582   1.118001     2.96   0.003     1.449982    6.231081\n             |\n        race |\n      Black  |   2.749483   1.356659     2.05   0.040     1.045318    7.231924\n      Other  |   2.876948   1.167921     2.60   0.009     1.298314    6.375062\n             |\n       _cons |    .365111   .3146026    -1.17   0.242     .0674491    1.976395\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\n%%stata\nglm low age i.smoke i.race,family(binomial) link(logit) eform\n\n\nIteration 0:  Log likelihood = -109.53148  \nIteration 1:  Log likelihood = -109.43111  \nIteration 2:  Log likelihood =  -109.4311  \nIteration 3:  Log likelihood =  -109.4311  \n\nGeneralized linear models                         Number of obs   =        189\nOptimization     : ML                             Residual df     =        184\n                                                  Scale parameter =          1\nDeviance         =  218.8621974                   (1/df) Deviance =   1.189468\nPearson          =  182.9642078                   (1/df) Pearson  =   .9943707\n\nVariance function: V(u) = u*(1-u)                 [Bernoulli]\nLink function    : g(u) = ln(u/(1-u))             [Logit]\n\n                                                  AIC             =   1.210911\nLog likelihood   = -109.4310987                   BIC             =  -745.6193\n\n------------------------------------------------------------------------------\n             |                 OIM\n         low | Odds ratio   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9657186   .0322573    -1.04   0.296     .9045206    1.031057\n             |\n       smoke |\n     Smoker  |    3.00582   1.118001     2.96   0.003     1.449982    6.231081\n             |\n        race |\n      Black  |   2.749483   1.356659     2.05   0.040     1.045318    7.231924\n      Other  |   2.876948   1.167921     2.60   0.009     1.298314    6.375062\n             |\n       _cons |    .365111   .3146026    -1.17   0.242     .0674491    1.976395\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#log-binomial-model",
    "href": "Guide/Stata/25-05-08-GLM.html#log-binomial-model",
    "title": "14-广义线性模型（GLM）",
    "section": "11 Log-Binomial Model",
    "text": "11 Log-Binomial Model\nglm low age i.smoke i.race,family(binomial) link(log)\n\nLogistic regression 的一种替代\nFail to converge\nPoisson regression with robust variance estimate\n\n为了避免出现 Fail to converge 的问题，采用稳健方差估计：\nglm low age i.smoke i.race,family(poisson) link(log) robust\n\nrobust 可以缩写为 r\n\n\n%%stata\nglm low age i.smoke i.race,family(poisson) link(log) robust\n\n\nIteration 0:  Log pseudolikelihood = -124.55888  \nIteration 1:  Log pseudolikelihood = -122.39663  \nIteration 2:  Log pseudolikelihood = -122.39591  \nIteration 3:  Log pseudolikelihood = -122.39591  \n\nGeneralized linear models                         Number of obs   =        189\nOptimization     : ML                             Residual df     =        184\n                                                  Scale parameter =          1\nDeviance         =   126.791811                   (1/df) Deviance =   .6890859\nPearson          =  124.4629927                   (1/df) Pearson  =   .6764293\n\nVariance function: V(u) = u                       [Poisson]\nLink function    : g(u) = ln(u)                   [Log]\n\n                                                  AIC             =   1.348105\nLog pseudolikelihood = -122.3959055               BIC             =  -837.6896\n\n------------------------------------------------------------------------------\n             |               Robust\n         low | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |  -.0246781   .0203034    -1.22   0.224    -.0644722    .0151159\n             |\n       smoke |\n     Smoker  |   .6972155    .211848     3.29   0.001     .2820011     1.11243\n             |\n        race |\n      Black  |    .639629   .2818353     2.27   0.023     .0872419    1.192016\n      Other  |   .6813692   .2428128     2.81   0.005     .2054649    1.157273\n             |\n       _cons |  -1.286544   .5405667    -2.38   0.017    -2.346035   -.2270526\n------------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-multi-reg.html",
    "href": "Guide/Stata/25-05-07-multi-reg.html",
    "title": "12-多元线性回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-multi-reg.html#导入数据",
    "href": "Guide/Stata/25-05-07-multi-reg.html#导入数据",
    "title": "12-多元线性回归",
    "section": "1 导入数据",
    "text": "1 导入数据\n\n%%stata\nsysuse auto.dta,clear\n\n(1978 automobile data)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-multi-reg.html#区分简单线性回归和多元线性回归",
    "href": "Guide/Stata/25-05-07-multi-reg.html#区分简单线性回归和多元线性回归",
    "title": "12-多元线性回归",
    "section": "2 区分简单线性回归和多元线性回归",
    "text": "2 区分简单线性回归和多元线性回归\n简单线性回归形如：\\(y=\\beta_0+\\beta_1 x\\)，例如：\n\n%%stata\nreg price weight\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(1, 72)        =     29.42\n       Model |   184233937         1   184233937   Prob &gt; F        =    0.0000\n    Residual |   450831459        72  6261548.04   R-squared       =    0.2901\n-------------+----------------------------------   Adj R-squared   =    0.2802\n       Total |   635065396        73  8699525.97   Root MSE        =    2502.3\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   2.044063   .3768341     5.42   0.000     1.292857    2.795268\n       _cons |  -6.707353    1174.43    -0.01   0.995     -2347.89    2334.475\n------------------------------------------------------------------------------\n\n\n\\(\\beta_1\\)：汽车的重量每增加一个单位，售价增加 2.04 个单位(95%CI:1.29,2.80)\n_cons 是 \\(\\beta_0\\)，即是截距，但是要注意符号与实际的区别\n多元线性模型即不止一个自变量，形如：\\(y=\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 + \\dots\\)\n\n%%stata\nreg price weight length\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(2, 71)        =     18.91\n       Model |   220725280         2   110362640   Prob &gt; F        =    0.0000\n    Residual |   414340116        71  5835776.28   R-squared       =    0.3476\n-------------+----------------------------------   Adj R-squared   =    0.3292\n       Total |   635065396        73  8699525.97   Root MSE        =    2415.7\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   4.699065   1.122339     4.19   0.000     2.461184    6.936946\n      length |  -97.96031    39.1746    -2.50   0.015    -176.0722   -19.84838\n       _cons |   10386.54   4308.159     2.41   0.019     1796.316    18976.76\n------------------------------------------------------------------------------\n\n\n\\(\\beta_2\\)(length)：在控制 weight 变量后，汽车的 length 每增加一个单位，售价减少 97.96 个单位(95%CI:-176.07,2.-19.85)\n\n%%stata\nreg price weight length mpg\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(3, 70)        =     12.98\n       Model |   226957412         3  75652470.6   Prob &gt; F        =    0.0000\n    Residual |   408107984        70  5830114.06   R-squared       =    0.3574\n-------------+----------------------------------   Adj R-squared   =    0.3298\n       Total |   635065396        73  8699525.97   Root MSE        =    2414.6\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   4.364798   1.167455     3.74   0.000     2.036383    6.693213\n      length |  -104.8682   39.72154    -2.64   0.010    -184.0903   -25.64607\n         mpg |  -86.78928   83.94335    -1.03   0.305     -254.209    80.63046\n       _cons |   14542.43   5890.632     2.47   0.016      2793.94    26290.93\n------------------------------------------------------------------------------\n\n\n\n%%stata\nreg price weight length mpg rep78\n\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(4, 64)        =     12.68\n       Model |   255066807         4  63766701.9   Prob &gt; F        =    0.0000\n    Residual |   321730151        64  5027033.62   R-squared       =    0.4422\n-------------+----------------------------------   Adj R-squared   =    0.4074\n       Total |   576796959        68  8482308.22   Root MSE        =    2242.1\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   4.959534   1.119624     4.43   0.000     2.722827    7.196241\n      length |  -115.0177   38.56456    -2.98   0.004    -192.0592   -37.97612\n         mpg |  -106.7122   81.15836    -1.31   0.193    -268.8446    55.42027\n       rep78 |   910.9859   304.5274     2.99   0.004     302.6226    1519.349\n       _cons |   11934.51   5774.178     2.07   0.043     399.2604    23469.75\n------------------------------------------------------------------------------\n\n\n\\(\\beta_4\\) (rep78): 在控制了汽车的重量、长度、里程后，汽车的修理次数每增加一个单位，售价增加910.99个单位(95%CI:302.62,1519.35)\n这里 rep78 是一个连续变量，如果处理为多分类变量（哑变量）\n\n2.1 什么是哑变量？为什么要设置哑变量？\n当自变量X为多分类变量时，例如职业、学历、血型、疾病严重程度等等此时仅用一个回归系数来解释多分类变量之间的变化关系，及其对因变量Y的影响，就显得太不理想。\n\n\n2.2 如何设置哑变量\n\n在多分类变量前加上 “i.” 告诉Stata这不是连续变量\n形如：\n\n\n%%stata\nreg price weight length mpg i.rep78\n\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(7, 61)        =      7.25\n       Model |   262008114         7  37429730.6   Prob &gt; F        =    0.0000\n    Residual |   314788844        61  5160472.86   R-squared       =    0.4542\n-------------+----------------------------------   Adj R-squared   =    0.3916\n       Total |   576796959        68  8482308.22   Root MSE        =    2271.7\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   5.186695   1.163383     4.46   0.000     2.860367    7.513022\n      length |  -124.1544   40.07637    -3.10   0.003     -204.292   -44.01671\n         mpg |  -126.8367   84.49819    -1.50   0.138    -295.8012    42.12791\n             |\n       rep78 |\n          2  |   1137.284   1803.332     0.63   0.531    -2468.701    4743.269\n          3  |   1254.642   1661.545     0.76   0.453    -2067.823    4577.108\n          4  |   2267.188   1698.018     1.34   0.187    -1128.208    5662.584\n          5  |   3850.759   1787.272     2.15   0.035     276.8886     7424.63\n             |\n       _cons |   14614.49   6155.842     2.37   0.021     2305.125    26923.86\n------------------------------------------------------------------------------\n\n\nrep78 2:在控制了汽车的重量、长度、里程后，汽 车的修理次数两次和一次相比，售价增加1137.28个单 位(95%CI:-2468.70,4743.27)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-multi-reg.html#回归分析需要注意的事情",
    "href": "Guide/Stata/25-05-07-multi-reg.html#回归分析需要注意的事情",
    "title": "12-多元线性回归",
    "section": "3 回归分析需要注意的事情",
    "text": "3 回归分析需要注意的事情\n\n时刻注意纳入模型的观测值数量\n\nCase-wise Deletion：只有纳入模型的变量都没有缺失值，这个观测值才会被纳入回归模型分析（有缺失会被系统默认剔除）\n\n如何让\\(\\beta_0\\)有意义？\n\n使用\\(y=\\beta_0+\\beta_1(x-\\bar x)\\)\n让\\(\\beta_0\\geq 0\\)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html",
    "href": "Guide/Stata/25-05-07-ANOVA.html",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html#方差分析的假设",
    "href": "Guide/Stata/25-05-07-ANOVA.html#方差分析的假设",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.1 方差分析的假设",
    "text": "1.1 方差分析的假设\n\n假设1:y变量为连续变量\n假设2:有一个包含2个及以上分类、且组别间相互独立的x变量\n假设3:每组间和组内的观测值相互独立\n假设4:每组内没有明显异常值\n假设5:每组内y变量符合正态分布\n假设6:进行方差齐性检验，观察每组的方差是否相等\n\n总结为：独立、正态、方差齐",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html#数据导入",
    "href": "Guide/Stata/25-05-07-ANOVA.html#数据导入",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.2 数据导入",
    "text": "1.2 数据导入\n\n%%stata\nwebuse systolic,clear\n\n(Systolic blood pressure data)\n\n\n\n1.2.1 查看一下数据形式与结构\n\n%%stata\nlist\n\n\n     +---------------------------+\n     | drug   disease   systolic |\n     |---------------------------|\n  1. |    1         1         42 |\n  2. |    1         1         44 |\n  3. |    1         1         36 |\n  4. |    1         1         13 |\n  5. |    1         1         19 |\n     |---------------------------|\n  6. |    1         1         22 |\n  7. |    1         2         33 |\n  8. |    1         2         26 |\n  9. |    1         2         33 |\n 10. |    1         2         21 |\n     |---------------------------|\n 11. |    1         3         31 |\n 12. |    1         3         -3 |\n 13. |    1         3         25 |\n 14. |    1         3         25 |\n 15. |    1         3         24 |\n     |---------------------------|\n 16. |    2         1         28 |\n 17. |    2         1         23 |\n 18. |    2         1         34 |\n 19. |    2         1         42 |\n 20. |    2         1         13 |\n     |---------------------------|\n 21. |    2         2         34 |\n 22. |    2         2         33 |\n 23. |    2         2         31 |\n 24. |    2         2         36 |\n 25. |    2         3          3 |\n     |---------------------------|\n 26. |    2         3         26 |\n 27. |    2         3         28 |\n 28. |    2         3         32 |\n 29. |    2         3          4 |\n 30. |    2         3         16 |\n     |---------------------------|\n 31. |    3         1          1 |\n 32. |    3         1         29 |\n 33. |    3         1         19 |\n 34. |    3         2         11 |\n 35. |    3         2          9 |\n     |---------------------------|\n 36. |    3         2          7 |\n 37. |    3         2          1 |\n 38. |    3         2         -6 |\n 39. |    3         3         21 |\n 40. |    3         3          1 |\n     |---------------------------|\n 41. |    3         3          9 |\n 42. |    3         3          3 |\n 43. |    4         1         24 |\n 44. |    4         1          9 |\n 45. |    4         1         22 |\n     |---------------------------|\n 46. |    4         1         -2 |\n 47. |    4         1         15 |\n 48. |    4         2         27 |\n 49. |    4         2         12 |\n 50. |    4         2         12 |\n     |---------------------------|\n 51. |    4         2         -5 |\n 52. |    4         2         16 |\n 53. |    4         2         15 |\n 54. |    4         3         22 |\n 55. |    4         3          7 |\n     |---------------------------|\n 56. |    4         3         25 |\n 57. |    4         3          5 |\n 58. |    4         3         12 |\n     +---------------------------+\n\n\n\n%%stata\ncodebook drug\n\n\n-------------------------------------------------------------------------------\ndrug                                                                  Drug used\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [1,4]                         Units: 1\n         Unique values: 4                         Missing .: 0/58\n\n            Tabulation: Freq.  Value\n                           15  1\n                           15  2\n                           12  3\n                           16  4\n\n\n\n%%stata\nsum systolic,detail\n\n\n                 Increment in systolic b.p.\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%           -6             -6\n 5%           -3             -5\n10%            1             -3       Obs                  58\n25%            9             -2       Sum of wgt.          58\n\n50%           21                      Mean           18.87931\n                        Largest       Std. dev.      12.80087\n75%           28             36\n90%           34             42       Variance       163.8624\n95%           42             42       Skewness       -.094992\n99%           44             44       Kurtosis        2.13251",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html#假设4每组内没有明显的异常值",
    "href": "Guide/Stata/25-05-07-ANOVA.html#假设4每组内没有明显的异常值",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.3 假设4：每组内没有明显的异常值",
    "text": "1.3 假设4：每组内没有明显的异常值\n最直观的方式就是用箱型图进行展示数据分布\n\n%%stata\ngraph box systolic,over(drug)\n\n\n\n\n\n\n\n\n\n%%stata\ngraph box systolic",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html#假设5每组内y变量符合正态分布",
    "href": "Guide/Stata/25-05-07-ANOVA.html#假设5每组内y变量符合正态分布",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.4 假设5：每组内y变量符合正态分布",
    "text": "1.4 假设5：每组内y变量符合正态分布\n一般使用下面三种方法中的一种检验正态性就可以，一般使用 Shapiro-Wilk检验 较为普遍。\n\n1.4.1 偏度与峰度\n使用 sktest 检验偏度与峰度，P值大于0.05，就不能说不符合正态分布\n\n%%stata\nsktest systolic\n\n\nSkewness and kurtosis tests for normality\n                                                         ----- Joint test -----\n    Variable |       Obs   Pr(skewness)   Pr(kurtosis)   Adj chi2(2)  Prob&gt;chi2\n-------------+-----------------------------------------------------------------\n    systolic |        58         0.7452         0.0529          4.03     0.1331\n\n\n\n\n1.4.2 Shapiro-Wilk W test\nShapiro-Wilk检验，是一种基于相关性的算法。计算可得到一个相关系数，它越接近1就越表明数据和正态分布拟合得越好。\n它基于Shapiro和Wilk于1965年提出的检验统计量。以下是其基本原理和用途：\n基本原理：\n\n零假设（Null Hypothesis）：Shapiro-Wilk检验的零假设是数据集来自于正态分布。这意味着，如果数据确实服从正态分布，则零假设成立。\n计算Shapiro-Wilk统计量：检验首先计算Shapiro-Wilk统计量，这是一个衡量数据与正态分布拟合的度量。该统计量基于数据的观察值和正态分布的期望值之间的差异。//Shapiro-Wilk检验的统计量（W统计量）是通过与理论正态分布的期望值进行比较来判断样本数据是否符合正态分布。Shapiro-Wilk检验的原假设是样本数据符合正态分布。统计量的计算基于样本数据的排序顺序和回归分析的概念。W统计量越接近1，表示样本数据越接近正态分布。\n与临界值比较：接下来，Shapiro-Wilk统计量与临界值进行比较。临界值是根据所选的显著性水平（通常为5%）和数据集的大小计算得出的。如果Shapiro-Wilk统计量小于临界值，就意味着数据不太可能来自于正态分布。\n做出决策：根据统计量与临界值的比较，可以决定是否拒绝零假设。如果统计量足够小，小于临界值，通常会拒绝零假设，这意味着数据不服从正态分布。否则，不能拒绝零假设，这表示数据可能服从正态分布。\n\nNotices： Shapiro-Wilk正态性检验对检验样本大小有一定的要求。具体来说，Shapiro-Wilk检验在样本大小较小（通常小于大约50-200，具体取决于不同文献和实践）时可能不太适用，并且在这种情况下其效力可能会降低。\n\n%%stata\nswilk systolic\n\n\n                   Shapiro–Wilk W test for normal data\n\n    Variable |        Obs       W           V         z       Prob&gt;z\n-------------+------------------------------------------------------\n    systolic |         58    0.97803      1.162     0.323    0.37331\n\n\nShapiro-Francia test\nShapiro-Francia检验是一种用于检验数据是否来自正态分布的统计方法。它是Shapiro-Wilk检验的一个变种，通常适用于小到中等样本大小的数据集。Shapiro-Francia检验的核心思想是通过计算统计量来评估数据的正态性。\nShapiro-Francia检验的零假设是数据来自正态分布，而备择假设是数据不来自正态分布。检验的结果会生成一个p-value，如果p-value较小（通常小于0.05），则通常会拒绝零假设，表明数据不符合正态分布。如果p-value较大，则无法拒绝零假设，表明数据可能来自正态分布。\n这种检验方法对于小到中等样本大小的数据集通常效果良好，并且可以用于确定数据是否符合正态分布的假设。\n虽然Shapiro-Francia检验在小样本中通常效果较好，但它也依赖于权重参数的准确性，因此在某些情况下可能不如其他正态性检验方法稳健。因此，在使用Shapiro-Francia检验时，应谨慎选择合适的样本大小和检验方法，以确保可靠的结果。\n\n%%stata\nsfrancia systolic\n\n\n                  Shapiro–Francia W' test for normal data\n\n    Variable |       Obs       W'          V'        z       Prob&gt;z\n-------------+-----------------------------------------------------\n    systolic |        58    0.98522      0.866    -0.275    0.60824",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html#单因素方差分析",
    "href": "Guide/Stata/25-05-07-ANOVA.html#单因素方差分析",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.5 单因素方差分析",
    "text": "1.5 单因素方差分析\n语法：\noneway yvar xvar,[,option]\n如果得到的 Prob&gt;F 结果是 &gt;0.05 则说明至少有两个组的平均值是有显著差别的。\nProb&gt;chi2 = 得到的结果如果 &gt;0.05 则说明各组满足方差齐性。\n\n%%stata\noneway systolic drug\n\n\n                        Analysis of variance\n    Source              SS         df      MS            F     Prob &gt; F\n------------------------------------------------------------------------\nBetween groups      3133.23851      3   1044.41284      9.09     0.0001\n Within groups      6206.91667     54   114.942901\n------------------------------------------------------------------------\n    Total           9340.15517     57   163.862371\n\nBartlett's equal-variances test: chi2(3) =   1.0063    Prob&gt;chi2 = 0.800\n\n\n\n1.5.1 使用两两比较\n一般使用 bonferroni 比较\n\n%%stata\noneway systolic drug, bonferroni \n\n\n                        Analysis of variance\n    Source              SS         df      MS            F     Prob &gt; F\n------------------------------------------------------------------------\nBetween groups      3133.23851      3   1044.41284      9.09     0.0001\n Within groups      6206.91667     54   114.942901\n------------------------------------------------------------------------\n    Total           9340.15517     57   163.862371\n\nBartlett's equal-variances test: chi2(3) =   1.0063    Prob&gt;chi2 = 0.800\n\n            Comparison of Increment in systolic b.p. by Drug used\n                                (Bonferroni)\nRow Mean-|\nCol Mean |          1          2          3\n---------+---------------------------------\n       2 |   -.533333\n         |      1.000\n         |\n       3 |   -17.3167   -16.7833\n         |      0.001      0.001\n         |\n       4 |   -12.5667   -12.0333       4.75\n         |      0.012      0.017      1.000\n\n\n\n%%stata\noneway systolic drug, bonferroni tab\n\n\n            |  Summary of Increment in systolic\n            |                b.p.\n  Drug used |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n          1 |   26.066667   11.677002          15\n          2 |   25.533333    11.61813          15\n          3 |        8.75     10.0193          12\n          4 |        13.5   9.3238047          16\n------------+------------------------------------\n      Total |    18.87931   12.800874          58\n\n                        Analysis of variance\n    Source              SS         df      MS            F     Prob &gt; F\n------------------------------------------------------------------------\nBetween groups      3133.23851      3   1044.41284      9.09     0.0001\n Within groups      6206.91667     54   114.942901\n------------------------------------------------------------------------\n    Total           9340.15517     57   163.862371\n\nBartlett's equal-variances test: chi2(3) =   1.0063    Prob&gt;chi2 = 0.800\n\n            Comparison of Increment in systolic b.p. by Drug used\n                                (Bonferroni)\nRow Mean-|\nCol Mean |          1          2          3\n---------+---------------------------------\n       2 |   -.533333\n         |      1.000\n         |\n       3 |   -17.3167   -16.7833\n         |      0.001      0.001\n         |\n       4 |   -12.5667   -12.0333       4.75\n         |      0.012      0.017      1.000",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-chi2-fisher-test.html",
    "href": "Guide/Stata/25-05-06-chi2-fisher-test.html",
    "title": "08-卡方检验与Fisher精确检验",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "08-卡方检验与Fisher精确检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-chi2-fisher-test.html#数据导入",
    "href": "Guide/Stata/25-05-06-chi2-fisher-test.html#数据导入",
    "title": "08-卡方检验与Fisher精确检验",
    "section": "1 数据导入",
    "text": "1 数据导入\n此章节使用 Stata 自带的 1988 年 美国的 National Longitudinal Study of Young Women Data 进行分析和示例。\n\n%%stata\nsysuse nlsw88.dta, clear\n\n(NLSW, 1988 extract)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "08-卡方检验与Fisher精确检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-chi2-fisher-test.html#chi2-test",
    "href": "Guide/Stata/25-05-06-chi2-fisher-test.html#chi2-test",
    "title": "08-卡方检验与Fisher精确检验",
    "section": "2 \\(\\chi^2\\) test",
    "text": "2 \\(\\chi^2\\) test\n卡方检验(\\(\\chi^2\\))，主要用于检验两个或多个分类变量之间的关联性。它通过比较实际观察频数与期望频数之间的差异来判断变量是否独立，从而揭示不同类别之间是否存在统计学上的显著关系。\n基本语法：\ntabulate var1 var2,chi2\ntabulate 可以简写为 tab，后续都用 tab 替代 tabulate\n\n%%stata\ntab race married, chi2\n\n\n           |        Married\n      Race |    Single    Married |     Total\n-----------+----------------------+----------\n     White |       487      1,150 |     1,637 \n     Black |       309        274 |       583 \n     Other |         8         18 |        26 \n-----------+----------------------+----------\n     Total |       804      1,442 |     2,246 \n\n          Pearson chi2(2) = 101.4215   Pr = 0.000\n\n\n\n2.1 查看每一个单元格对于 \\(\\chi^2\\) 检验的贡献:\n语法：\ntab var1 var2, cchi2 chi2\n\n%%stata\ntab race married, cchi2 chi2\n\n\n+-------------------+\n| Key               |\n|-------------------|\n|     frequency     |\n| chi2 contribution |\n+-------------------+\n\n           |        Married\n      Race |    Single    Married |     Total\n-----------+----------------------+----------\n     White |       487      1,150 |     1,637 \n           |      16.7        9.3 |      26.0 \n-----------+----------------------+----------\n     Black |       309        274 |       583 \n           |      48.2       26.9 |      75.1 \n-----------+----------------------+----------\n     Other |         8         18 |        26 \n           |       0.2        0.1 |       0.3 \n-----------+----------------------+----------\n     Total |       804      1,442 |     2,246 \n           |      65.1       36.3 |     101.4 \n\n          Pearson chi2(2) = 101.4215   Pr = 0.000\n\n\n\n\n2.2 查看期望频数（理论频数）\n语法：\ntab var1 var2, chi2 expected\n\n%%stata\ntab race married, chi2 expected\n\n\n+--------------------+\n| Key                |\n|--------------------|\n|     frequency      |\n| expected frequency |\n+--------------------+\n\n           |        Married\n      Race |    Single    Married |     Total\n-----------+----------------------+----------\n     White |       487      1,150 |     1,637 \n           |     586.0    1,051.0 |   1,637.0 \n-----------+----------------------+----------\n     Black |       309        274 |       583 \n           |     208.7      374.3 |     583.0 \n-----------+----------------------+----------\n     Other |         8         18 |        26 \n           |       9.3       16.7 |      26.0 \n-----------+----------------------+----------\n     Total |       804      1,442 |     2,246 \n           |     804.0    1,442.0 |   2,246.0 \n\n          Pearson chi2(2) = 101.4215   Pr = 0.000",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "08-卡方检验与Fisher精确检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-chi2-fisher-test.html#fishers-exact-test",
    "href": "Guide/Stata/25-05-06-chi2-fisher-test.html#fishers-exact-test",
    "title": "08-卡方检验与Fisher精确检验",
    "section": "3 Fisher’s exact test",
    "text": "3 Fisher’s exact test\n当列联表中理论频数(期望频数)&lt;5时，我们可以增加样本量、删去理论频数太少的行或列、或者合并某些行或列。\n当然也可以使用Fisher精确检验来检验，并且任何样本量都可以使用Fisher精确检验。\n语法：\ntab var1 var2, exact\n\n%%stata\ntab race married, exact\n\n\nEnumerating sample-space combinations:\nstage 3:  enumerations = 1\nstage 2:  enumerations = 27\nstage 1:  enumerations = 0\n\n           |        Married\n      Race |    Single    Married |     Total\n-----------+----------------------+----------\n     White |       487      1,150 |     1,637 \n     Black |       309        274 |       583 \n     Other |         8         18 |        26 \n-----------+----------------------+----------\n     Total |       804      1,442 |     2,246 \n\n           Fisher's exact =                 0.000",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "08-卡方检验与Fisher精确检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-chi2-fisher-test.html#notices",
    "href": "Guide/Stata/25-05-06-chi2-fisher-test.html#notices",
    "title": "08-卡方检验与Fisher精确检验",
    "section": "4 Notices",
    "text": "4 Notices\n在一般的统计学教程或书籍中，有如下的说明：\n\n总例数≥40，所有理论频数≥5，看Pearson Chi-Square结果;\n总例数≥40，出现1个理论频数≥1且&lt;5，\\(\\chi^2\\)检验需进行连续性校正，这时以Continuity Correction结果为准\n总例数≥40，至少2个理论频数≥1且&lt;5，看Fisher’s Exact Test结果;\n总例数&lt;40或者出现理论频数&lt;1，看Fisher’s Exact Test结果\n\n针对上述 第2点 中对\\(\\chi^2\\)检验需进行连续性校正，在 Stata 中一般不是必须的：\n\nStata不自带卡方检验的连续性矫正\nStata有用户自写的package可以实现连续性矫正，但是并不推荐:卡方检验的连续性矫正并不是必须的、也不是最推荐的方法\n在样本量足够大的时候，使用卡方检验时，是否使用卡方检验的连续性校正区别很小;使用Fisher精确检验也是没有问题的\n在样本量小的时候(通常是某个格子期望频&lt;5)，可直接使用Fisher精确检验，亦不需要使用“卡方检验+连续性矫正”",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "08-卡方检验与Fisher精确检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#lowess命令的语法",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#lowess命令的语法",
    "title": "05-双变量作图",
    "section": "2.1 lowess命令的语法：",
    "text": "2.1 lowess命令的语法：\nlowess yvar xvar [if] [in] [, options]\n其中，yvar 是因变量，xvar 是自变量。options 是一些可选参数，用来进一步调整光滑曲线的形状和拟合效果。例如，可以通过指定 span 参数来控制局部回归的窗口大小，较小的 span 值会导致更平滑的曲线，而较大的 span 值会导致更接近原始数据的曲线。\noptions 有如下选项：\n\n\n\nLowess-syntax\n\n\nlowess 命令生成的光滑曲线可以通过绘图命令来展示，例如使用 twoway scatter 命令可以在散点图上叠加绘制光滑曲线。lowess — Lowess smoothing",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#lowess-命令的示例",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#lowess-命令的示例",
    "title": "05-双变量作图",
    "section": "2.2 lowess 命令的示例：",
    "text": "2.2 lowess 命令的示例：\n\n\n代码\n%%stata\nsysuse auto\nlowess mpg weight, gen(lowess_mpg)\ntwoway scatter mpg weight || line lowess_mpg weight\n\n\n\n. sysuse auto\n(1978 automobile data)\n\n. lowess mpg weight, gen(lowess_mpg)\n\n. twoway scatter mpg weight || line lowess_mpg weight\n\n. \n\n\n\n\n\n\n\n\n\n上述代码首先载入Stata自带的汽车数据集auto，然后使用lowess命令生成一个光滑曲线，将结果保存在变量lowess_mpg中。最后利用twoway scatter命令绘制散点图，并使用line命令在散点图上叠加绘制光滑曲线。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#在-jupyter-notebook-中使用-sas",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#在-jupyter-notebook-中使用-sas",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "1 在 Jupyter Notebook 中使用 SAS",
    "text": "1 在 Jupyter Notebook 中使用 SAS\n\n1.1 环境准备\n安装 Anaconda 集成环境或 Python 和 SAS 软件，其中Anaconda要求Python3+；Python在Jupyter Notebook和SAS之间起一个桥梁的作用，Jupyter Notebook中的SAS代码会交给Python，Python负责将代码传递给SAS执行；然后将执行的结果返回给Jupyter Notebook显示。\nSAS版本要求9.4，也可以是 SAS Viya。\n\n\n1.2 安装SAS_KERNEL\n启动 cmd，输入命令：\npip install sas_kernel\n然后就会自动安装 sas_kernel 及其相应的依赖项。\n安装完成后可以输入命令：\njupyter kernelspec list\n来检测 sas_kernel 是否安装成功，如果成功，理论上会看到如下形式的输出：\nAvailable kernels:\n    python3    /home/sas/anaconda3/lib/python3.5/site-packages/ipykernel/resources\n    sas        /home/sas/.local/share/jupyter/kernels/sas\n\n\n1.3 修改 Python 配置文件\n安装好 sas_kernel 后找到 Anaconda 或 Python 的安装目录，会有一个相应的文件夹出现，例如我的文件路径如下：\nC:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\saspy\n在这个文件路径下找到 sascfg.py 文件，该文件中需要配置连接SAS的信息。可以配置连接本地机器的SAS；也可以配置连接远程机器的SAS Server，无论是Linux Server还是Windows Server都可以。此处就以连接本地SAS为例进行说明。\n\n打开该文件，首先是一大段注释；\n在这段注释后定义的第一个变量 SAS_config_names 用于指定连接SAS的配置方式，提供了 10 种方式：default, ssh, iomlinux, iomwin, winlocal, winiomlinux, winiomwin, httpsviya, httpviya, iomcom。默认为 default 方式。\n因为我们需要连接Windows机器本地的SAS，所以需要将 SAS_config_names 的值修改为 winlocal 。\n\n\n后续有一些安装步骤，但是大多是在2016-2020年更新的教程，无法找到复现的路径，可能相关的配置已被优化。\n包括这个 SAS岩论 | 在Jupyter Notebook中使用SAS 中写到的需要使用 cpW 定义 SAS 路径。\n目前（2025年），我摸索出来的办法是：\n修改 saspath 路径，指定 sas.exe 文件,路径形如：C:/Program Files/SASHome/SASFoundation/9.4/sas.exe\n\n\n\nsaspath\n\n\n修改 encoding 为 euc-cn: 将winlocal连接方式中的参数 encoding 的值修改为 euc-cn 。因为这是SAS在Windows下的默认编码方式。如果编码方式不对，中文会出现乱码。或者 utf8，如果和 SAS 软件有冲突，可能会造成 SAS 软件乱码。\n\n\n1.4 修改系统变量\n将sas相关文件 sspiauth.dll 添加到系统环境变量，该文件很可能在如下目录：\nC:\\Program Files\\SASHome\\SASFoundation\\9.4\\core\\sasext\n（注意添加变量时不要包含 sspiauth.dll 文件本身）\nWarning: 环境变量添加完成后，要重启电脑才会生效。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#在-jupyter-notebook-中使用-sas-1",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#在-jupyter-notebook-中使用-sas-1",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "2 在 jupyter notebook 中使用 SAS",
    "text": "2 在 jupyter notebook 中使用 SAS\n新建文件，选择使用 SAS 内核，或者在 cell 中通过 magic command 指定内核。\n%%SAS\nwarning: 在单元格中直接调用 SAS 之前，需要在文档中加载一次 magic 扩展 以定义 %%SAS\n%load_ext saspy.sas_magic\n使用语法如下所示：\n\n\n代码\n%%SAS\ndata prg1_1;\n    input x @@;\ndatalines;\n60 142 195 80 242 220 190 25 212 38 236 95\n;\nrun;\nproc means data=prg1_1;\n    var x;\n    quit;\n\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 8788\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: x\n\n\n数目\n均值\n标准差\n最小值\n最大值\n\n\n\n\n12\n144.5833333\n80.9797487\n25.0000000\n242.0000000\n\n\n\n\n\n\n\n\n\n\n在Notebook中写SAS代码了，跟Python一样，同样有代码提示、语法高亮的功能。但是你会注意到过程步的结果显示了，运行的日志去哪里了？\n如果代码运行错误或者没有输出（例如纯DATA步）的话，那么输出就是日志信息。\n能够正确运行且有输出结果的代码就不会显示日志了。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#安装sas日志组件",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#安装sas日志组件",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "3 安装SAS日志组件",
    "text": "3 安装SAS日志组件\n如果想要像SAS Base一样，随时查看所有程序运行的日志结果也没问题。安装一个 Notebook 的 SAS 日志扩展组件就可以了。打开 Anaconda Prompt，输入以下命令安装：\njupyter nbextension install --py sas_kernel.showSASLog\n运行完毕后，输入以下命令启用 SAS 日志组件：\njupyter nbextension enable sas_kernel.showSASLog –py",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#连接sas-server",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#连接sas-server",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "4 连接SAS Server",
    "text": "4 连接SAS Server\n如果需要配置连接远程的SAS Server，如连接远程Windows机器的SAS Server，需在sascfg.py中做以下修改：\n\n将SAS_config_names的值改为“wintowin”；\n在wintowin连接方式中将参数iomhost的值修改为远程Windows机器的IP地址；将参数encoding的值修改为euc-cn；\n将cpW中5个Jar包的路径修改为远程Windows机器中SAS对应的目录。\n\n修改完毕后，启动Notebook，首次运行SAS代码时，会提示输入访问SAS Server的有效SAS用户和密码。1",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#典型生态项目",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#典型生态项目",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "5 典型生态项目",
    "text": "5 典型生态项目\n\n5.1 SASPy\nSASPy 是一个 Python 库，允许你通过 Python 代码与 SAS 进行交互。SAS Kernel 依赖于 SASPy，因此在使用 SAS Kernel 之前，你需要配置 SASPy。\n\n\n5.2 JupyterLab 扩展\nSAS Kernel 支持 JupyterLab 扩展，这些扩展可以提高你在 JupyterLab 中的编程效率。你可以通过以下命令安装这些扩展：\npip install sas_kernel[jlab_ext] \n\n\n5.3 NBGrader\nNBGrader 是一个用于分配和评分 Jupyter Notebook 的系统，它与 SAS Kernel 兼容。你可以使用 NBGrader 来创建和评分包含 SAS 代码的作业。\n通过这些生态项目，SAS Kernel 不仅扩展了 Jupyter Notebook 的功能，还增强了其在数据科学和分析领域的应用能力。2",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#footnotes",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#footnotes",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "脚注",
    "text": "脚注\n\n\nSAS岩论 | 在Jupyter Notebook中使用SAS↩︎\nSAS Kernel for Jupyter 安装与使用教程↩︎",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#sas-在-linux-的安装",
    "href": "Guide/SAS/25-03-11-SAS-install.html#sas-在-linux-的安装",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "0.2 SAS 在 Linux 的安装",
    "text": "0.2 SAS 在 Linux 的安装",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#sas-与-vscode-扩展",
    "href": "Guide/SAS/25-03-11-SAS-install.html#sas-与-vscode-扩展",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "",
    "text": "SAS VS Code 扩展轻量级，可在任何地方运行，并允许您集成 SAS 和其他语言。该工具还提供直接连接到 SAS Viya 和 SAS 9 并运行代码的功能。\n\nSAS 语法突出显示和帮助、代码完成和代码片段\n用于连接 SAS 和运行代码的配置文件配置\n支持 SAS Viya 和 SAS 9 连接\n访问 SAS 内容和库\n为 SAS、SQL、Python 和其他语言创建笔记本\n\n扩展程序可在 GitHub 上找到仓库与原代码：[vscode-sas-extension](https://github.com/sassoftware/vscode-sas-extension)\n更多关于 SAS 与 vscode 的信息可以访问：[SAS Extension for Visual Studio Code](https://developer.sas.com/programming/vs_code_extension)\n\n\n在 vscode 的扩展页面搜索 “sas” ，第一个 “official SAS ···“ 即为正确扩展：",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#配置路径",
    "href": "Guide/SAS/25-03-11-SAS-install.html#配置路径",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "",
    "text": "Before you can run SAS code, you must configure the SAS extension to access your SAS 9.4 (remote or local) server or a SAS Viya server and add a connection profile.\n在运行 SAS 代码之前，您必须配置 SAS 扩展以访问 SAS 9.4（远程或本地）服务器或 SAS Viya 服务器。您必须获得 SAS 9.4 或 SAS Viya 的许可才能运行 SAS 代码。\n\n打开 SAS 程序文件。\n单击 VS Code 窗口左下方状态栏中的“无配置文件”。 您还可以打开命令面板（F1，或Ctrl+Shift+P在 Windows 或 Linux 上，或Shift+CMD+P在 OSX 上）并找到SAS: Add New Connection Profile命令。\n按照“添加新连接配置文件”部分中的说明添加配置文件。\n创建配置文件后，状态栏项将从“无配置文件”更改为新配置文件的名称。\n\n\n更多设置可以查看[SAS Extension for Visual Studio Code Documentation](https://sassoftware.github.io/vscode-sas-extension/Configurations/Profiles/sas9local)",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#编译-sas-文件",
    "href": "Guide/SAS/25-03-11-SAS-install.html#编译-sas-文件",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "",
    "text": "SAS 文件右上角有一个 奔跑的小人 ，点击即可开始运行所选中的程序段落，并在右侧窗口输出结果。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#在-jupyter-notebook-中使用-sas",
    "href": "Guide/SAS/25-03-11-SAS-install.html#在-jupyter-notebook-中使用-sas",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "",
    "text": "安装 Anaconda 集成环境或 Python 和 SAS 软件，其中要求Python3.4+；\nSAS 需要 SAS 9.4+ 或 SAS Viya 3.1+；\nPython在Jupyter Notebook和SAS之间起一个桥梁的作用，Jupyter Notebook中的SAS代码会交给Python，Python负责将代码传递给SAS执行；\n然后将执行的结果返回给Jupyter Notebook显示。\n\nSAS版本要求9.4，也可以是 SAS Viya。\n\n\n\n启动 cmd，输入命令：\npip install saspy\nspecific release：\npip install http://github.com/sassoftware/saspy/releases/saspy-X.X.X.tar.gz\n然后就会自动安装 saspy 及其相应的依赖项。\n最好的更新或重装方式：\npip uninstall -y saspy\npip install saspy\n\n\n\nuv init name-of-project\ncd name-of-project\nuv add saspy # adds saspy to your project from PyPI\n\n\n\nconda create --name name-of-my-environment\nconda install --channel conda-forge saspy # Installs latest version of saspy from conda-forge channel.\nconda install --channel conda-forge saspy==X.X.X # Where X.X.X is the version you'd like to install.\n安装完成后可以输入命令：\njupyter kernelspec list\n来检测 saspy 是否安装成功，如果成功，理论上会看到如下形式的输出：\nAvailable kernels:\n    python3    /home/sas/anaconda3/lib/python3.5/site-packages/ipykernel/resources\n    sas        /home/sas/.local/share/jupyter/kernels/sas\n\n\n\n安装好 saspy 后找到 Anaconda 或 Python 的安装目录，会有一个相应的文件夹出现，例如我的文件路径如下：\nC:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\saspy\n在这个文件路径下找到 sascfg.py 文件，该文件中需要配置连接SAS的信息。可以配置连接本地机器的SAS；也可以配置连接远程机器的SAS Server，无论是Linux Server还是Windows Server都可以。此处就以连接本地SAS为例进行说明。\n\n打开该文件，首先是一大段注释；\n在这段注释后定义的第一个变量 SAS_config_names 用于指定连接SAS的配置方式，提供了 10 种方式：default, ssh, iomlinux, iomwin, winlocal, winiomlinux, winiomwin, httpsviya, httpviya, iomcom。默认为 default 方式。\n因为我们需要连接Windows机器本地的SAS，所以需要将 SAS_config_names 的值修改为 winlocal 。\n\n\nsascfg.py 的内容（2025年版配置文件）\n#\n# Copyright SAS Institute\n#\n#  Licensed under the Apache License, Version 2.0 (the License);\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\n# THIS IS AN EXAMPLE CONFIG FILE. PLEASE CREATE YOUR OWN sascfg_personal.py FILE USING THE APPROPRIATE TEMPLATES FROM BELOW\n# SEE THE CONFIGURATION DOC AT https://sassoftware.github.io/saspy/install.html#configuration\n\n\n# Configuration Names for SAS - python List\n# This is the list of allowed configuration definitions that can be used. The definition are defined below.\n# if there is more than one name in the list, and cfgname= is not specified in SASsession(), then the user\n# will be prompted to choose which configuration to use.\n#\n# The various options for the different access methods can be specified on the SASsession() i.e.:\n# sas = SASsession(cfgname='default', options='-fullstimer', user='me')\n#\n# Based upon the lock_down configuration option below, you may or may not be able to override option\n# that are defined already. Any necessary option (like user, pw for IOM or HTTP) that are not defined will be\n# prompted for at run time. To dissallow overrides of as OPTION, when you don't have a value, simply\n# specify options=''. This way it's specified so it can't be overridden, even though you don't have any\n# specific value you want applied.\n#\n#SAS_config_names = ['default', 'ssh', 'iomlinux', 'iomwin', 'winlocal', 'winiomlinux', 'winiomwin', 'httpsviya', 'httpviya', 'iomcom']\n#\n\nSAS_config_names=['winlocal']\n\n\n\n# Configuration options for saspy - python Dict   # not required unless changing any of the defaults\n# valid key are:\n#\n# 'lock_down' - True | False. True = Prevent runtime overrides of SAS_Config values below\n#\n# 'verbose'   - True | False. True = Allow print statements for debug type messages\n#\n# 'prompt'    - True | False. True = Allow prompting as necessary\n#\nSAS_config_options = {'lock_down': False,\n                      'verbose'  : True,\n                      'prompt'   : True\n                     }\n\n\n\n# Configuration options for SAS output. By default output is HTML 5.0 (using \"ods html5\" statement) but certain templates might not work\n# properly with HTML 5.0 so it can also be set to HTML 4.0 instead (using \"ods html\" statement). This option will only work when using IOM\n# in local mode. Note that HTML 4.0 will generate images separately which clutters the workspace and if you download the notebook as HTML,\n# the HTML file will need to be put in the same folder as the images for them to appear.\n# valid keys are:\n#\n# 'output' = ['html5', 'html']\n# 'style'  = any valid style   # this will be the default for SASsession.HTML_Style, which you can also change dynamically in your code\n#\n#\nSAS_output_options = {'output' : 'html5',       # not required unless changing any of the default\n                      'style'  : 'HTMLBlue'}\n\n\n# Configuration Definitions\n#\n# For STDIO and STDIO over SSH access methods\n# These need path to SASHome and optional startup options - python Dict\n# The default path to the sas start up script is: /opt/sasinside/SASHome/SASFoundation/9.4/sas\n# A usual install path is: /opt/sasinside/SASHome\n#\n# The encoding is figured out by saspy. You don't need to specify it, unless you just want to get rid of the message about which encoding was determined.\n#\n# valid keys are:\n# 'saspath'  - [REQUIRED] path to SAS startup script i.e.: /opt/sasinside/SASHome/SASFoundation/9.4/sas\n# 'options'  - SAS options to include in the start up command line - Python List\n# 'encoding' - This is the python encoding value that matches the SAS session encoding your SAS session is using\n#\n# For passwordless ssh connection, the following are also reuqired:\n# 'ssh'     - [REQUIRED] the ssh command to run\n# 'host'    - [REQUIRED] the host to connect to\n#\n# Additional valid keys for ssh:\n# 'port'    - [integer] the remote ssh port\n# 'tunnel'  - [integer] local port to open via reverse tunnel, if remote host cannot otherwise reach this client\n#\ndefault  = {'saspath'  : 'C:/Program Files/SASHome/SASFoundation/9.4/sas.exe'\n            }\n\nwinlocal = {\n    'saspath': 'C:\\\\Program Files\\\\SASHome\\\\SASFoundation\\\\9.4\\\\sas.exe'\n}\n\n# If you installed SAS by default path,the above path maybe effect for your Windows.\n\nssh      = {'saspath' : '/opt/sasinside/SASHome/SASFoundation/9.4/bin/sas_en',\n            'ssh'     : '/usr/bin/ssh',\n            'host'    : 'remote.linux.host',\n            'encoding': 'latin1',\n            'options' : [\"-fullstimer\"]\n            }\n\n\n# For IOM (Grid Manager or any IOM) and Local Windows via IOM access method\n# These configuration definitions are for connecting over IOM. This is designed to be used to connect to any Workspace server, including SAS Grid, via Grid Manager\n# and also to connect to a local Windows SAS session. The client side (python and java) for this access method can be either Linux or Windows.\n# The STDIO access method above is only for Linux. PC SAS requires this IOM interface.\n#\n# The absence of the iomhost option triggers local Windows SAS mode. In this case none of 'iomhost', 'iomport', 'omruser', 'omrpw' are needed.\n# a local SAS session is started up and connected to.\n#\n# The encoding is figured out by saspy. You don't need to specify it, unless you just want to get rid of the message about which encoding was determined.\n\n# NONE OF THE PATHS IN THESE EAMPLES ARE RIGHT FOR YOUT INSTALL. YOU HAVE TO CHANGE THE PATHS TO BE CORRECT FOR YOUR INSTALLATION\n#\n# valid keys are:\n# 'java'      - [REQUIRED] the path to the java executable to use\n# 'iomhost'   - [REQUIRED for remote IOM case, Don't specify to use a local Windows Session] the resolvable host name, or ip to the IOM server to connect to\n# 'iomport'   - [REQUIRED for remote IOM case, Don't specify to use a local Windows Session] the port IOM is listening on\n# 'authkey'   - identifier for user/password credentials to read from .authinfo file. Eliminates prompting for credentials.\n# 'omruser'   - not suggested        [REQUIRED for remote IOM case but PROMPTED for at runtime] Don't specify to use a local Windows Session\n# 'omrpw'     - really not suggested [REQUIRED for remote IOM case but PROMPTED for at runtime] Don't specify to use a local Windows Session\n# 'encoding'  - This is the python encoding value that matches the SAS session encoding of the IOM server you are connecting to\n# 'appserver' - name of physical workspace server (when more than one app server defined in OMR) i.e.: 'SASApp - Workspace Server'\n# 'sspi'      - boolean. use IWA instead of user/pw to connect to the IOM workspace server\n\n\niomlinux = {'java'      : '/usr/bin/java',\n            'iomhost'   : 'linux.iom.host',\n            'iomport'   : 8591,\n            }\n\niomwin   = {'java'      : '/usr/bin/java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            }\n\nwinlocal = {'java'      : 'java',\n            'encoding'  : 'EUC-CN',\n            }\n\nwiniomlinux = {'java'   : 'java',\n            'iomhost'   : 'linux.iom.host',\n            'iomport'   : 8591,\n            }\n\nwiniomwin  = {'java'    : 'java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            }\n\nwiniomIWA  = {'java'    : 'java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            'sspi'      : True\n            }\n\n\n# For Remote and Local IOM access methods using COM interface\n# These configuration definitions are for connecting over IOM using COM. This\n# access method is for Windows clients connecting to remote hosts. Local\n# SAS instances may also be supported.\n#\n# This access method does not require a Java dependency.\n#\n# Valid Keys:\n#   iomhost     - Required for remote connections only. The Resolvable SAS\n#                 server dns name.\n#   iomport     - Required for remote connections only. The SAS workspace\n#                 server port. Generally 8591 on standard remote\n#                 installations. For local connections, 0 is the default.\n#   class_id    - Required for remote connections only. The IOM workspace\n#                 server class identifier. Use `PROC IOMOPERATE` to identify\n#                 the correct value. This option is ignored on local connections.\n#   provider    - [REQUIRED] IOM provider. \"sas.iomprovider\" is recommended.\n#   encoding    - This is the python encoding value that matches the SAS\n#                 session encoding of the IOM server.\n#   omruser     - SAS user. This option is ignored on local connections.\n#   omrpw       - SAS password. This option is ignored on local connections.\n#   authkey     - Identifier for credentials to read from .authinfo file.\n\niomcom = {\n    'iomhost' : 'mynode.mycompany.org',\n    'iomport' : 8591,\n    'provider': 'sas.iomprovider',\n    'encoding': 'windows-1252'}\n\n\n# HTTP access method to connect to the Compute Service\n# These need ip addr, other values will be prompted for - python Dict\n# valid keys are:\n# 'url'     - (Required if ip not specified) The URL to Viya, of the form \"http[s]://host.idenifier[:port]\".\n#             When this is specified, ip= will not be used, as the host's ip is retrieved from the url. Also, ssl= is\n#             set based upon http or https and port= is also parsed from the url, if provided, else defaulted based\n#             upon the derived ssl= value. So neither ip, port nor ssl are needed when url= is used.\n# 'ip'      - (Required if url not specified) The resolvable host name, or IP address to the Viya Compute Service\n# 'port'    - port; the code Defaults this to based upon the 'ssl' key; 443 default else 80\n# 'ssl'     - whether to use HTTPS or just HTTP protocal. Default is True, using ssl and poort 443\n# 'context' - context name defined on the compute service  [PROMTED for at runtime if more than one defined]\n# 'authkey' - identifier for user/password credentials to read from .authinfo file. Eliminates prompting for credentials.\n# 'options' - SAS options to include (no '-' (dashes), just option names and values)\n# 'user'    - not suggested [REQUIRED but PROMTED for at runtime]\n# 'pw'      - really not suggested [REQUIRED but PROMTED for at runtime]\n#\n#\n\nhttpsviya = {'url'     : 'https://viya.deployment.com',\n             'context' : 'SAS Studio compute context',\n             'authkey' : 'viya_user-pw',\n             'options' : [\"fullstimer\", \"memsize=1G\"]\n             }\n\nhttpviya = {'url'     : 'https://sastpw.rndk8s.openstack.sas.com:23456',\n           #'port'    :  23456,   # can put different port here or ^ if it's not using the default port\n            'context' : 'SAS Studio compute context',\n            'authkey' : 'viya_user-pw',\n            'options' : [\"fullstimer\", \"memsize=1G\"]\n            }\n\n\n\n\n这是一个示例配置文件。请使用下面的相应模板创建您自己的 sascfg_personal.py 文件。有关配置的详细信息，请参阅以下文档：https://sassoftware.github.io/saspy/install.html#configuration\n\n\n\n\n这是允许使用的配置定义列表。定义如下所示。如果列表中有多个名称，并且在 SASsession() 中未指定 cfgname=，则系统将提示用户选择要使用的配置。\nSASsession() 中可以指定不同访问方法的各种选项，例如： sas = SASsession(cfgname='default', options='-fullstimer', user='me')\n根据下面的 lock_down 配置选项，您可能能够或不能够覆盖已定义的选项。任何必要但未定义的选项（如 IOM 或 HTTP 的 user、pw）将在运行时提示输入。要禁止覆盖作为 OPTION 的选项（当您没有值时），只需指定 options=''。这样它就被指定了，即使您没有要应用的特定值，也无法被覆盖。\n#SAS_config_names = ['default', 'ssh', 'iomlinux', 'iomwin', 'winlocal', 'winiomlinux', 'winiomwin', 'httpsviya', 'httpviya', 'iomcom']\n\nSAS_config_names=['winlocal']\n\n\n\n\n这些选项不是必需的，除非您需要更改任何默认值。有效的键包括：\n\n‘lock_down’: True | False。设置为 True 可防止在运行时覆盖下面的 SAS_Config 值。\n‘verbose’: True | False。设置为 True 可允许打印调试类型的消息。\n‘prompt’: True | False。设置为 True 可在必要时允许提示。\n\n\nSAS_config_options = {'lock_down': False,\n                      'verbose'  : True,\n                      'prompt'   : True\n                     }\n\n\n\n\nSAS 输出默认为 HTML 5.0（使用 “ods html5” 语句），但某些模板可能无法与 HTML 5.0 正常工作，因此也可以将其设置为 HTML 4.0（使用 “ods html” 语句）。此选项仅在使用 IOM 本地模式时有效。请注意，HTML 4.0 会单独生成图像，这会使工作区变得杂乱；如果您将笔记本下载为 HTML，HTML 文件需要与图像放在同一文件夹中才能显示图像。\n有效的键包括：\n\n‘output’: ['html5', 'html']\n‘style’: 任何有效的样式。这将是 SASsession.HTML_Style 的默认值，您也可以在代码中动态更改它。\n\n\nSAS_output_options = {'output' : 'html5',       # not required unless changing any of the default\n                      'style'  : 'HTMLBlue'}\n\n\n\n\n\n\n\n这些需要 SASHome 的路径和可选的启动选项——Python 字典。 SAS 启动脚本的默认路径是：/opt/sasinside/SASHome/SASFoundation/9.4/sas 常见的安装路径是：/opt/sasinside/SASHome\n编码由 saspy 自动识别。您不需要指定它，除非您想消除有关确定编码的消息。\n有效的键包括：\n\n‘saspath’: [必需] SAS 启动脚本的路径，例如：/opt/sasinside/SASHome/SASFoundation/9.4/sas\n‘options’: 要包含在启动命令行中的 SAS 选项 - Python 列表\n‘encoding’: 与您的 SAS 会话使用的 SAS 会话编码匹配的 Python 编码值\n\n对于无密码 SSH 连接，还需要以下项：\n\n‘ssh’: [必需] 要运行的 ssh 命令\n‘host’: [必需] 要连接的主机\n\nSSH 的其他有效键包括：\n\n‘port’: [整数] 远程 SSH 端口\n‘tunnel’: [整数] 如果远程主机无法连接此客户端，则通过反向隧道打开的本地端口\n\n\ndefault  = {'saspath'  : 'C:/Program Files/SASHome/SASFoundation/9.4/sas.exe'\n            }\n\nwinlocal = {\n    'saspath': 'C:\\\\Program Files\\\\SASHome\\\\SASFoundation\\\\9.4\\\\sas.exe'\n}\n\n# If you installed SAS by default path,the above path maybe effect for your Windows.\n\nssh      = {'saspath' : '/opt/sasinside/SASHome/SASFoundation/9.4/bin/sas_en',\n            'ssh'     : '/usr/bin/ssh',\n            'host'    : 'remote.linux.host',\n            'encoding': 'latin1',\n            'options' : [\"-fullstimer\"]\n            }\n\n\n\n\n这些配置定义用于通过 IOM 进行连接。这旨在用于连接到任何工作区服务器，包括 SAS Grid（通过 Grid Manager），以及连接到本地 Windows SAS 会话。此访问方法的客户端（Python 和 Java）可以是 Linux 或 Windows。上面提到的 STDIO 访问方法仅适用于 Linux。PC SAS 需要此 IOM 接口。\n缺少 iomhost 选项会触发本地 Windows SAS 模式。在这种情况下，不需要 iomhost、iomport、omruser、omrpw 中的任何一个。系统会启动并连接到本地 SAS 会话。\n编码由 saspy 自动识别。您不需要指定它，除非您想消除有关确定编码的消息。\n这些示例中的所有路径都不适合您的安装。您必须更改路径以使其与您的安装相符。\n有效的键包括：\n\n‘java’: [必需] 要使用的 Java 可执行文件的路径\n‘iomhost’: [远程 IOM 情况必需，不指定则使用本地 Windows 会话] 要连接的 IOM 服务器的可解析主机名或 IP\n‘iomport’: [远程 IOM 情况必需，不指定则使用本地 Windows 会话] IOM 正在监听的端口\n‘authkey’: 用于从 .authinfo 文件读取用户/密码凭据的标识符。消除了凭据提示。\n‘omruser’: 不建议使用 [远程 IOM 情况必需，但在运行时提示输入] 不指定则使用本地 Windows 会话\n‘omrpw’: 强烈不建议使用 [远程 IOM 情况必需，但在运行时提示输入] 不指定则使用本地 Windows 会话\n‘encoding’: 与您要连接的 IOM 服务器的 SAS 会话编码匹配的 Python 编码值\n‘appserver’: 物理工作区服务器的名称（当 OMR 中定义了多个应用程序服务器时），例如：‘SASApp - Workspace Server’\n‘sspi’: 布尔值。使用 IWA 而不是用户/密码连接到 IOM 工作区服务器\n\n\niomlinux = {'java'      : '/usr/bin/java',\n            'iomhost'   : 'linux.iom.host',\n            'iomport'   : 8591,\n            }\n\niomwin   = {'java'      : '/usr/bin/java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            }\n\nwinlocal = {'java'      : 'java',\n            'encoding'  : 'EUC-CN',\n            }\n\nwiniomlinux = {'java'   : 'java',\n            'iomhost'   : 'linux.iom.host',\n            'iomport'   : 8591,\n            }\n\nwiniomwin  = {'java'    : 'java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            }\n\nwiniomIWA  = {'java'    : 'java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            'sspi'      : True\n            }\n\n\n\n\n这些配置定义用于通过 COM 连接到 IOM。此访问方法适用于连接到远程主机的 Windows 客户端。本地 SAS 实例也可能受支持。\n此访问方法不需要 Java 依赖项。\n有效键：\n\niomhost: 仅适用于远程连接。可解析的 SAS 服务器 DNS 名称。\niomport: 仅适用于远程连接。SAS 工作区服务器端口。在标准远程安装中通常为 8591。对于本地连接，默认值为 0。\nclass_id: 仅适用于远程连接。IOM 工作区服务器类标识符。使用 PROC IOMOPERATE 来识别正确的值。此选项在本地连接上被忽略。\nprovider: [必需] IOM 提供程序。建议使用 “sas.iomprovider”。\nencoding: 与 IOM 服务器的 SAS 会话编码匹配的 Python 编码值。\nomruser: SAS 用户。此选项在本地连接上被忽略。\nomrpw: SAS 密码。此选项在本地连接上被忽略。\nauthkey: 用于从 .authinfo 文件读取凭据的标识符。\n\n\niomcom = {\n    'iomhost' : 'mynode.mycompany.org',\n    'iomport' : 8591,\n    'provider': 'sas.iomprovider',\n    'encoding': 'windows-1252'}\n\n\n\n\n这些需要 IP 地址，其他值将在运行时提示输入 - Python Dict。\n有效键包括：\n\n‘url’: （如果未指定 IP 则必需）Viya 的 URL，形式为 “http[s]://host.identifier[:port]”。当指定此项时，ip= 将不被使用，因为主机的 IP 是从 URL 中检索的。此外，ssl= 会根据 http 或 https 设置，并且 port= 也会从 URL 中解析（如果提供），否则会根据派生的 ssl= 值采用默认值。因此，当使用 url= 时，不需要 ip、port 或 ssl。\n‘ip’: （如果未指定 URL 则必需）Viya 计算服务的可解析主机名或 IP 地址\n‘port’: 端口；代码会根据 ‘ssl’ 键将其默认为 443（如果使用 SSL）或 80（否则）\n‘ssl’: 是否使用 HTTPS 或仅使用 HTTP 协议。默认为 True，使用 SSL 和端口 443。\n‘context’: 在计算服务上定义的上下文名称 [如果定义了多个，则在运行时提示选择]\n‘authkey’: 用于从 .authinfo 文件读取用户/密码凭据的标识符。消除了凭据提示。\n‘options’: 要包含的 SAS 选项（不带 ‘-’（破折号），只有选项名称和值）\n‘user’: 不建议使用 [必需，但在运行时提示输入]\n‘pw’: 强烈不建议使用 [必需，但在运行时提示输入]\n\n\nhttpsviya = {'url'     : 'https://viya.deployment.com',\n             'context' : 'SAS Studio compute context',\n             'authkey' : 'viya_user-pw',\n             'options' : [\"fullstimer\", \"memsize=1G\"]\n             }\n\nhttpviya = {'url'     : 'https://sastpw.rndk8s.openstack.sas.com:23456',\n           #'port'    :  23456,   # can put different port here or ^ if it's not using the default port\n            'context' : 'SAS Studio compute context',\n            'authkey' : 'viya_user-pw',\n            'options' : [\"fullstimer\", \"memsize=1G\"]\n            }\n后续有一些安装步骤，但是大多是在2016-2020年更新的教程，无法找到复现的路径，可能相关的配置已被优化。\n包括这个 [SAS岩论 | 在Jupyter Notebook中使用SAS ](https://www.sohu.com/a/218339423_278472) 中写到的需要使用 cpW 定义 SAS 路径。\n\n\n\n\n将sas相关文件 sspiauth.dll 添加到系统环境变量，该文件很可能在如下目录：\nC:\\Program Files\\SASHome\\SASFoundation\\9.4\\core\\sasext\n（注意添加变量时不要包含 sspiauth.dll 文件本身）\nWarning: 环境变量添加完成后，要重启电脑才会生效。\n\n\n\n新建文件，选择使用 SAS 内核，或者在 cell 中通过 magic command 指定内核。\n%%sas\n使用语法如下所示：\n%%sas\ndata iris;\n    set sashelp.iris;\nrun;\n\nproc print data=iris(obs=10);\nrun;\n在Notebook中写SAS代码了，跟Python一样，同样有代码提示、语法高亮的功能。但是你会注意到过程步的结果显示了，运行的日志去哪里了？\n如果代码运行错误或者没有输出（例如纯DATA步）的话，那么输出就是日志信息。\n能够正确运行且有输出结果的代码就不会显示日志了。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#安装sas日志组件",
    "href": "Guide/SAS/25-03-11-SAS-install.html#安装sas日志组件",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "",
    "text": "如果想要像SAS Base一样，随时查看所有程序运行的日志结果也没问题。安装一个Notebook的SAS日志扩展组件就可以了。打开Anaconda Prompt，输入以下命令安装：\njupyter nbextension install --py sas_kernel.showSASLog\n运行完毕后，输入以下命令启用SAS日志组件：\njupyter nbextension enable sas_kernel.showSASLog –py",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#连接sas-server",
    "href": "Guide/SAS/25-03-11-SAS-install.html#连接sas-server",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "",
    "text": "如果需要配置连接远程的SAS Server，如连接远程Windows机器的SAS Server，需在sascfg.py中做以下修改：\n\n将SAS_config_names的值改为“wintowin”；\n在wintowin连接方式中将参数iomhost的值修改为远程Windows机器的IP地址；将参数encoding的值修改为euc-cn；\n将cpW中5个Jar包的路径修改为远程Windows机器中SAS对应的目录。\n\n修改完毕后，启动Notebook，首次运行SAS代码时，会提示输入访问SAS Server的有效SAS用户和密码。1",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#footnotes",
    "href": "Guide/SAS/25-03-11-SAS-install.html#footnotes",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "脚注",
    "text": "脚注\n\n\nSAS岩论 | 在Jupyter Notebook中使用SAS↩︎\nSAS Kernel for Jupyter 安装与使用教程↩︎",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-09-Stata-intro.html",
    "href": "Guide/Stata/25-03-09-Stata-intro.html",
    "title": "00-Stata Intro and Record",
    "section": "",
    "text": "关于 Stata 大二上统计学课的时候，老师在课上提了一嘴，说：“等你们以后读研了，就不用 SPSS 这种工具了，就会开始用 Stata、R 这些工具了“。也确实，本科期间确实就是一个 SPSS 管了四年，因为实在脱离他的应用场景，加之，老师就只会 SPSS ，那就没办法咯。\n想起那时候，专业两个班的 SPSS 软件基本上都是我去装的，老师弄不会，同学们更不会，我比较喜欢摸索，所以摸索出了这些，找到了安装包和密钥，然后拷在 U盘 里，课前课后课中就是给他们装软件，有时候，有些系统还装不上，某为就是，同学的某为一直装不上，当时看是因为缺 Java 环境，但是装了 Java JDK 还是不行，遂放弃。\n等到毕业的时候，想着看能不能用 Stata 做一下毕业论文的数据分析，最后太忙，没时间也没精力，用了 SPSS 结束。\n老师也是到了我大四的时候在哪里自学 Stata ，不过要说的是，在 AI 成熟以前，没有 AI 的辅助情况下，从0开始去学一门技能或程序，没有捷径，耗时耗力。现在逐渐理解，因为自己当时抱着 Python 的几本书，看了两三年也没有啥进展，等到 AI 出来了，不懂的就问 AI，节省了很多时间和精力；也和理解力的提升有关，进展迅速。\n回归正题，Stata 是一款用于数据科学的统计软件，其功能强大，但是对比 Python、R、MATLAB等程序或软件，还是略显不足，但是对于一般情况的数据分析，Stata 是够用的，其主要的优点是语法简洁和有诸多可以拿来即用的包，同时作为一款商业软件，其价格相较于 SAS 是很低的（但是换算RMB仍然很高），其支持相较于 R 等也可以说是较为丰富的（庞大的社区），还有跨平台使用等优势，这里不一一列举。\nStata 的安装很简单，互联网上有很多教程，但是关键的一点是获得 Key (许可证和激活秘钥)，当然互联网亦有诸多的资源可供选择。\n然后根据研究目的，选择合适的模型，找到前辈们写的代码，拿来增删改查，再结合 AI 的辅助，跑通，分析，写作。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "00-Stata Intro and Record"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#stem-plot茎叶图",
    "href": "Guide/Stata/25-05-03-describe-index.html#stem-plot茎叶图",
    "title": "03-统计描述指标",
    "section": "1.6 stem plot(茎叶图)",
    "text": "1.6 stem plot(茎叶图)\n语法：\nstem var",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-RR-CAL.html",
    "href": "Guide/Stata/25-05-06-RR-CAL.html",
    "title": "09-RR值与OR值的计算",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "09-RR值与OR值的计算"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-RR-CAL.html#rr值relative-risk",
    "href": "Guide/Stata/25-05-06-RR-CAL.html#rr值relative-risk",
    "title": "09-RR值与OR值的计算",
    "section": "1.1 RR值（Relative Risk）",
    "text": "1.1 RR值（Relative Risk）",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "09-RR值与OR值的计算"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-RR-CAL.html#数据导入",
    "href": "Guide/Stata/25-05-06-RR-CAL.html#数据导入",
    "title": "09-RR值与OR值的计算",
    "section": "1.2 数据导入",
    "text": "1.2 数据导入\n此章节使用网络数据，csxmpl\n\n%%stata\nwebuse csxmpl, clear\nlist\n\n\n. webuse csxmpl, clear\n\n. list\n\n     +------------------+\n     | case   exp   pop |\n     |------------------|\n  1. |    1     1     7 |\n  2. |    1     0    12 |\n  3. |    0     1     9 |\n  4. |    0     0     2 |\n     +------------------+\n\n. \n\n\n\n%%stata\ncs case exp [fweight = pop]\n\n\n                 |        Exposed         |\n                 |   Exposed   Unexposed  |      Total\n-----------------+------------------------+-----------\n           Cases |         7          12  |         19\n        Noncases |         9           2  |         11\n-----------------+------------------------+-----------\n           Total |        16          14  |         30\n                 |                        |\n            Risk |     .4375    .8571429  |   .6333333\n                 |                        |\n                 |      Point estimate    |    [95% conf. interval]\n                 |------------------------+------------------------\n Risk difference |        -.4196429       |   -.7240828   -.1152029 \n      Risk ratio |         .5104167       |    .2814332    .9257086 \n Prev. frac. ex. |         .4895833       |    .0742914    .7185668 \n Prev. frac. pop |         .2611111       |\n                 +-------------------------------------------------\n                               chi2(1) =     5.66  Pr&gt;chi2 = 0.0173\n\n\n\n1.2.1 RR 值计算\ncs var_case var_exp [if] [in] [weight] [,cs_options]\ncsi #a #b #c #d [,csi_options]\n计算 RR 值使用 cs 命令，它是 cohort study 的缩写\n上面 [fweight = pop] 中使用 pop 进行频数加权 fweight\n[,cs_options] 可以修改置信区间等",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "09-RR值与OR值的计算"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-RR-CAL.html#or值odds-ratio",
    "href": "Guide/Stata/25-05-06-RR-CAL.html#or值odds-ratio",
    "title": "09-RR值与OR值的计算",
    "section": "1.3 OR值（Odds Ratio）",
    "text": "1.3 OR值（Odds Ratio）\n\n1.3.1 RR值与OR值的区别\nRR值：Cohort study或者RCT中，研究者前瞻性地观察“暴露组”和“非暴露组”的发病情况，之后通过RR来评价“暴露组”研究对象的发病风险是“非暴露组”研究对象的多少倍?这个“多少倍”就是RR值\nOR值：在回顾性研究(如case-control)中，研究对象是已经患病的“病例组”和未患病的“对照组”，研究者回顾性地调查病例组和对照组的暴露情况，因此无法计算发病率等指标。\n想知道相对风险仍是我们最终的目的。\n因此我们可以使用 OR值 来近似估计 RR值。\n\n当终点事件发生率较低时，OR值可以近似为RR值（\\(&lt;15%\\)）\n当终点事件发生率较高时，OR会“夸大”RR值\n\nOR值相对于RR值“更远离1”\n当RR值大于1时，OR大于RR(\\(1&lt;RR&lt;OR\\))\n当RR值小于1时，OR小于RR(\\(OR&lt;RR&lt;1\\))\n终点事件发生率越高时，OR越会overestimate\n\n\n对于多列研究/RCT，可以报告OR值吗？\n\n可以，但是不够准确/精准\nRR值对于效应值的估计更加准确\nRR值对于临床意义的解释更加明确\nRegression model中：对于结局是二分类变量的研究，logistic回归只能提供OR值，不能提供RR值(当结局发生率高时，应该使用log-binomial回归或者使用带有稳健方差估计的泊松回归，直接提供RR值)\n\n\n\n1.3.2 OR值的计算\n语法：\ncc var_case var_exp [if] [in] [weight] [,cc_options]\ncci #a #b #c #d [,cci_options]\n\n\n1.3.3 数据载入\n\n%%stata\nwebuse ccxmpl,clear\nlist\n\n\n. webuse ccxmpl,clear\n\n. list\n\n     +-----------------------+\n     | case   exposed    pop |\n     |-----------------------|\n  1. |    1         1      4 |\n  2. |    1         0    386 |\n  3. |    0         1      4 |\n  4. |    0         0   1250 |\n     +-----------------------+\n\n. \n\n\n\n%%stata\ncc case exp [fweight = pop]\n\n                                                         Proportion\n                 |   Exposed   Unexposed  |      Total      exposed\n-----------------+------------------------+------------------------\n           Cases |         4         386  |        390       0.0103\n        Controls |         4        1250  |       1254       0.0032\n-----------------+------------------------+------------------------\n           Total |         8        1636  |       1644       0.0049\n                 |                        |\n                 |      Point estimate    |    [95% conf. interval]\n                 |------------------------+------------------------\n      Odds ratio |         3.238342       |    .5997233    17.45614 (exact)\n Attr. frac. ex. |            .6912       |   -.6674356    .9427136 (exact)\n Attr. frac. pop |         .0070892       |\n                 +-------------------------------------------------\n                               chi2(1) =     3.07  Pr&gt;chi2 = 0.0799\n\n\n\n%%stata\ncci 4 386 4 1250\n\n                                                         Proportion\n                 |   Exposed   Unexposed  |      Total      exposed\n-----------------+------------------------+------------------------\n           Cases |         4         386  |        390       0.0103\n        Controls |         4        1250  |       1254       0.0032\n-----------------+------------------------+------------------------\n           Total |         8        1636  |       1644       0.0049\n                 |                        |\n                 |      Point estimate    |    [95% conf. interval]\n                 |------------------------+------------------------\n      Odds ratio |         3.238342       |    .5997233    17.45614 (exact)\n Attr. frac. ex. |            .6912       |   -.6674356    .9427136 (exact)\n Attr. frac. pop |         .0070892       |\n                 +-------------------------------------------------\n                               chi2(1) =     3.07  Pr&gt;chi2 = 0.0799",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "09-RR值与OR值的计算"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html",
    "href": "Guide/Stata/25-05-07-log-binary.html",
    "title": "13-二分类Logistic回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html#什么时候该用-logistic-回归",
    "href": "Guide/Stata/25-05-07-log-binary.html#什么时候该用-logistic-回归",
    "title": "13-二分类Logistic回归",
    "section": "1 什么时候该用 Logistic 回归",
    "text": "1 什么时候该用 Logistic 回归\n当outcome发生率 &gt;15% 时logistic regression得出的OR值会overestimate实际的RR值\n\n传统的 Logistic Regression（得出OR值）\nMantel-Haenszel（得出RR值）\nPoisson Regression with robust variance estimate\n\n\n1.1 新方法\n\n1998 Zhang and Yu What’s the Relative Risk?\n\n\\[RR=\\frac{OR}{(1-P_0)+(P_0\\times OR)}\\]\n\n2003 McNutt Outcomes Estimating the Relative Risk in Cohort Studies and Clinical Trials of Common\n金标准：Log Binomial",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html#二分类-logistic-模型的假设",
    "href": "Guide/Stata/25-05-07-log-binary.html#二分类-logistic-模型的假设",
    "title": "13-二分类Logistic回归",
    "section": "2 二分类 Logistic 模型的假设",
    "text": "2 二分类 Logistic 模型的假设\n\n假设1:因变量(结局)是二分类变量。\n假设2:有至少1个自变量，自变量可以是连续变量，也可以是分类变量，\n假设3:每条观测间相互独立。分类变量(包括因变量和自变量)的分类必须全面且每一个分类间互斥\n假设4:最小样本量要求为自变量数目的15倍，但一些研究者认为样本量应达到自变量数目的50倍\n假设5:连续的自变量与因变量的logit转换值之间存在线性关系。\n假设6:自变量之间无多重共线性，\n假设7:没有明显的离群点、杠杆点和强影响点。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html#做-logistic-回归的要求",
    "href": "Guide/Stata/25-05-07-log-binary.html#做-logistic-回归的要求",
    "title": "13-二分类Logistic回归",
    "section": "3 做 Logistic 回归的要求",
    "text": "3 做 Logistic 回归的要求\n\nY是二分类变量\nY的发生率 &lt;15%",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html#导入数据",
    "href": "Guide/Stata/25-05-07-log-binary.html#导入数据",
    "title": "13-二分类Logistic回归",
    "section": "4 导入数据",
    "text": "4 导入数据\n变量 low 是我们的结局事件，我们想看什么因素和孩子的 low birthweight 相关\n\n%%stata\nwebuse lbw,clear\n\n(Hosmer & Lemeshow data)\n\n\n\n%%stata\ntab low\n\n\nBirthweight |\n     &lt;2500g |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          0 |        130       68.78       68.78\n          1 |         59       31.22      100.00\n------------+-----------------------------------\n      Total |        189      100.00\n\n\nDisclaimer: 本节的数据集中，结局事件发生率远远大于15%，应使用Log binomial模型进行分析。这里使用Logistic regression进行分析仅仅为了讲 解如何使用Stata进行操作、以及为下节的Log-binomial进行铺垫。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html#logistic-regression",
    "href": "Guide/Stata/25-05-07-log-binary.html#logistic-regression",
    "title": "13-二分类Logistic回归",
    "section": "5 Logistic regression",
    "text": "5 Logistic regression\n语法：\nlogistic y x1 x2 x3 ...\n\n5.1 Model 1: \\(low=\\beta_0+\\beta_1 age\\)\n\n%%stata\nlogistic low age\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(1)    =   2.76\n                                                        Prob &gt; chi2   = 0.0966\nLog likelihood = -115.95598                             Pseudo R2     = 0.0118\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9501333   .0299423    -1.62   0.105     .8932232    1.010669\n       _cons |      1.469   1.075492     0.53   0.599     .3498129    6.168901\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\\(\\beta_1\\)：母亲的年龄每增加1岁，孩子低体重的风险是之前的0.95倍(95%CI:0.89,1.01)\n\n\n5.2 Model 2: \\(low=\\beta_0+\\beta_1 age+\\beta_2 smoke\\)\n\n%%stata\nlogistic low age i.smoke\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(2)    =   7.40\n                                                        Prob &gt; chi2   = 0.0248\nLog likelihood = -113.63815                             Pseudo R2     = 0.0315\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9514394   .0304194    -1.56   0.119     .8936482    1.012968\n             |\n       smoke |\n     Smoker  |   1.997405    .642777     2.15   0.032     1.063027    3.753081\n       _cons |   1.062798   .8048781     0.08   0.936     .2408901    4.689025\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\\(\\beta_1\\)：控制了母亲的吸烟状况以后，母亲的年龄每增加1岁孩子低体重的风险是之前的0.95倍(95% CI:0.89,1.01)\n\n\n5.3 Model 2: \\(low=\\beta_0+\\beta_1 age+\\beta_2 smoke+\\beta_3 race\\)\n\n%%stata\nlogistic low age i.smoke i.race\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(4)    =  15.81\n                                                        Prob &gt; chi2   = 0.0033\nLog likelihood = -109.4311                              Pseudo R2     = 0.0674\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9657186   .0322573    -1.04   0.296     .9045206    1.031057\n             |\n       smoke |\n     Smoker  |    3.00582   1.118001     2.96   0.003     1.449982    6.231081\n             |\n        race |\n      Black  |   2.749483   1.356659     2.05   0.040     1.045318    7.231924\n      Other  |   2.876948   1.167921     2.60   0.009     1.298314    6.375062\n             |\n       _cons |    .365111   .3146026    -1.17   0.242     .0674491    1.976395\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\\(\\beta_1\\)：控制了母亲的吸烟状况和种族以后，母亲的年龄每增加1岁孩子低体重的风险是之前的0.97倍(95%CI:0.90,1.03)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-simple-line-reg.html",
    "href": "Guide/Stata/25-05-07-simple-line-reg.html",
    "title": "11-简单线性回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-simple-line-reg.html#导入数据",
    "href": "Guide/Stata/25-05-07-simple-line-reg.html#导入数据",
    "title": "11-简单线性回归",
    "section": "1 导入数据",
    "text": "1 导入数据\n\n%%stata\nsysuse auto.dta,clear\n\n(1978 automobile data)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-simple-line-reg.html#线性回归的假设",
    "href": "Guide/Stata/25-05-07-simple-line-reg.html#线性回归的假设",
    "title": "11-简单线性回归",
    "section": "2 线性回归的假设",
    "text": "2 线性回归的假设\n\n假设1:y是连续变量\n假设2:x可以被定义为连续变量（也可以是哑变量）\n假设3:y和x之间存在线性关系\n假设4:具有相互独立的观测值\n假设5:不存在显著的outlier\n假设6:等方差性\n假设7:residual近似正态分布\n\n总结而言：线性、独立、正态、方差齐\n\n2.1 假设3:y和x之间存在线性关系\n通过 scatter plot 或 Lowess plot 进行查看\n如果不符合线性关系怎么办？\n\nspline：一次方项，分段fit直线\nQuadratic：二次方项\nCubic：三次方项\nRestricted cubic：三次方项（头尾近乎直线）\n\n\n%%stata\ntwoway scatter mpg weight\n\n\n\n\n\n\n\n\n\n%%stata\ntwoway lowess mpg weight\n\n\n\n\n\n\n\n\nscatter plot和lowess plot输出在一起\n\n%%stata\nlowess mpg weight\n\n\n\n\n\n\n\n\n\n%%stata\ntwoway (scatter mpg weight)(lowess mpg weight)\n\n\n\n\n\n\n\n\n\n\n2.2 假设4:具有相互独立的观测值\n\n可以使用杜宾-瓦特森(Durbin-Watson)统计量\nStata对于非time series数据不设有这个统计量的检测\n更多情况下，不需要测量这个假设是否成立\n如果是相互关联的观测值：GEE模型、Multi-level模型\n\n\n\n2.3 假设5:不存在显著的outlier\n\nBoxplot 或 Violin Plot\n\n\n%%stata\ngraph box mpg\n\n\n\n\n\n\n\n\n\n%%stata\nvioplot mpg\n\n\n\n\n\n\n\n\n\n\n2.4 假设6:等方差性\n\n使用 Residual-versus-fitted plot\n代码:rvfplot\n\n做等方差之前需要做回归的分析，分析及结果如下：\n\n%%stata\nreg mpg weight\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(1, 72)        =    134.62\n       Model |   1591.9902         1   1591.9902   Prob &gt; F        =    0.0000\n    Residual |  851.469256        72  11.8259619   R-squared       =    0.6515\n-------------+----------------------------------   Adj R-squared   =    0.6467\n       Total |  2443.45946        73  33.4720474   Root MSE        =    3.4389\n\n------------------------------------------------------------------------------\n         mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |  -.0060087   .0005179   -11.60   0.000    -.0070411   -.0049763\n       _cons |   39.44028   1.614003    24.44   0.000     36.22283    42.65774\n------------------------------------------------------------------------------\n\n\n\n%%stata\nrvfplot\n\n\n\n\n\n\n\n\n从上图可以看出，并不是完全等方差，特别是在 Fitted value 在 25-30 区间内时\n\n\n2.5 假设7:residual近似正态分布\n\nStep 1:得到残差\nStep 2:使用正态分布的检验方法\n\n直方图\n使用 qq plot 观测\n偏度峰度、Shapiro-Wilk检验、Shapiro-Francia检验\n\n\n先建立一个 resid 变量：\n\n%%stata\npredict resid,residual\n\n再对 resid 变量进行查看和检验\n下面分别是 直方图 和 Q-Q图\n\n%%stata\nhist resid\n\n(bin=8, start=-6.9593482, width=2.5970982)\n\n\n\n\n\n\n\n\n\n\n%%stata\n//qq plot\nqnorm resid\n\n\n. //qq plot\n. qnorm resid\n\n. \n\n\n\n\n\n\n\n\n\n\n\n2.6 对残差使用偏度、峰度进行检验\n\n%%stata\nsktest resid\n\n\nSkewness and kurtosis tests for normality\n                                                         ----- Joint test -----\n    Variable |       Obs   Pr(skewness)   Pr(kurtosis)   Adj chi2(2)  Prob&gt;chi2\n-------------+-----------------------------------------------------------------\n       resid |        74         0.0000         0.0010         20.82     0.0000\n\n\n\n\n2.7 使用 Shapiro-Wilk 检验\n\n%%stata\nswilk resid\n\n\n                   Shapiro–Wilk W test for normal data\n\n    Variable |        Obs       W           V         z       Prob&gt;z\n-------------+------------------------------------------------------\n       resid |         74    0.89593      6.702     4.150    0.00002",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-simple-line-reg.html#简单线性回归",
    "href": "Guide/Stata/25-05-07-simple-line-reg.html#简单线性回归",
    "title": "11-简单线性回归",
    "section": "3 简单线性回归",
    "text": "3 简单线性回归\n\n3.1 语法\nregress depvar [indepvars] [if] [in] [weight] [,option]\n具体参见：regress — Linear regression",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html",
    "href": "Guide/Stata/25-05-08-survival-ana.html",
    "title": "15-生存分析",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#生存分析",
    "href": "Guide/Stata/25-05-08-survival-ana.html#生存分析",
    "title": "15-生存分析",
    "section": "1 生存分析",
    "text": "1 生存分析\n\n描述一个组内个体的生存时间\n\n寿命表法(Life tables methods)\n非参数Kaplan-Meier曲线\n\n比较两个或多个组的生存时间\n\nLog-rank test\n\n研究生存时间和变量之间的关系\n\n半参数Cox比例风险模型\n参数生存分析模型",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#k-m曲线的历史",
    "href": "Guide/Stata/25-05-08-survival-ana.html#k-m曲线的历史",
    "title": "15-生存分析",
    "section": "2 K-M曲线的历史",
    "text": "2 K-M曲线的历史\n\n1958年，Dr. Kaplan和DrMeier 介绍了一种全新的、解决随访期间RightCensoring问题的生存分析方法\n特点:精确地记录并利用每个个体发生终点事件的具体时间，在任何一个终点事件发生的时间点计算出一个新的、基于之前所有信息的Cumulative survival\n优点:\n\n相比于Life-table method，更加充分地利用了信息，给出更准确的统计量\n非参数估计方法:不要求总体的分布形式，因此非常适合生存分析时使用\nK-M曲线可以很直观地表现出两组或多组的生存率或死亡率，适合在文章中展示",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#导入数据",
    "href": "Guide/Stata/25-05-08-survival-ana.html#导入数据",
    "title": "15-生存分析",
    "section": "3 导入数据",
    "text": "3 导入数据\n使用 Patient Survival in Drug Trial 数据集\n\n%%stata\nwebuse drugtr,clear\n\n(Patient survival in drug trial)\n\n\n将数据恢复成普通的数据格式：stset,clear\n\nStata已经将这个数据集设置成了生存数据的格式，导入数据集后，将数 据集恢复成普通的数据格式，这样才是我们在临床研究中见到的数据结构。\n\n\n%%stata\nstset,clear",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#数据集的初步观察",
    "href": "Guide/Stata/25-05-08-survival-ana.html#数据集的初步观察",
    "title": "15-生存分析",
    "section": "4 数据集的初步观察",
    "text": "4 数据集的初步观察\n\n%%stata\nlist in 5/10\n\n\n     +------------------------------+\n     | studyt~e   died   drug   age |\n     |------------------------------|\n  5. |        4      1      0    56 |\n  6. |        4      1      0    67 |\n  7. |        5      1      0    63 |\n  8. |        5      1      0    58 |\n  9. |        8      1      0    56 |\n     |------------------------------|\n 10. |        8      0      0    58 |\n     +------------------------------+\n\n\n\n%%stata\ncodebook drug\n\n\n-------------------------------------------------------------------------------\ndrug                                                      Drug type (0=placebo)\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [0,1]                         Units: 1\n         Unique values: 2                         Missing .: 0/48\n\n            Tabulation: Freq.  Value\n                           20  0\n                           28  1\n\n\n\n%%stata\ncodebook studytime\n\n\n-------------------------------------------------------------------------------\nstudytime                                        Months to death or end of exp.\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [1,39]                        Units: 1\n         Unique values: 28                        Missing .: 0/48\n\n                  Mean:    15.5\n             Std. dev.: 10.2563\n\n           Percentiles:     10%       25%       50%       75%       90%\n                              4       7.5      12.5        23        32\n\n\n\n%%stata\ncodebook died\n\n\n-------------------------------------------------------------------------------\ndied                                                          1 if patient died\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [0,1]                         Units: 1\n         Unique values: 2                         Missing .: 0/48\n\n            Tabulation: Freq.  Value\n                           17  0\n                           31  1\n\n\n\n%%stata\ncodebook age\n\n\n-------------------------------------------------------------------------------\nage                                              Patient's age at start of exp.\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [47,67]                       Units: 1\n         Unique values: 18                        Missing .: 0/48\n\n                  Mean: 55.875\n             Std. dev.: 5.6592\n\n           Percentiles:    10%       25%       50%       75%       90%\n                            49      50.5        56        60        65",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#数据申明代码操作",
    "href": "Guide/Stata/25-05-08-survival-ana.html#数据申明代码操作",
    "title": "15-生存分析",
    "section": "5 数据申明——代码操作",
    "text": "5 数据申明——代码操作\n\n告诉Stata: 终点事件(Failure variable),随访时间(Time variable)\nstset timevar, failure(failvar[==numlist])\n\ntimevar: 随访时间变量\nfailvar: 终点事件变量\nnumlist: 终点时间变量中，哪个(哪些)值代表发生了终点事件?\n\n\n\n%%stata\nstset studytime,failure(died==1)\n\n\nSurvival-time data settings\n\n         Failure event: died==1\nObserved time interval: (0, studytime]\n     Exit on or before: failure\n\n--------------------------------------------------------------------------\n         48  total observations\n          0  exclusions\n--------------------------------------------------------------------------\n         48  observations remaining, representing\n         31  failures in single-record/single-failure data\n        744  total analysis time at risk and under observation\n                                                At risk from t =         0\n                                     Earliest observed entry t =         0\n                                          Last observed exit t =        39",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#生存数据再观测",
    "href": "Guide/Stata/25-05-08-survival-ana.html#生存数据再观测",
    "title": "15-生存分析",
    "section": "6 生存数据再观测",
    "text": "6 生存数据再观测\nNotice：必须要在指定数据集为生存分析数据集之后(stset 之后)才能使用任何其他的 st 开始的命令。\n\n%%stata\nstsum\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n         |               Incidence     Number of   |------ Survival time -----|\n         | Time at risk       rate      subjects        25%       50%       75%\n---------+---------------------------------------------------------------------\n   Total |          744   .0416667            48          8        17        33\n\n\n\n%%stata\nstdescribe\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n                                   |-------------- Per subject --------------|\nCategory                   Total        Mean         Min     Median        Max\n------------------------------------------------------------------------------\nNumber of subjects            48   \nNumber of records             48           1           1          1          1\n\nEntry time (first)                         0           0          0          0\nExit time (final)                       15.5           1       12.5         39\n\nSubjects with gap              0   \nTime on gap                    0   \nTime at risk                 744        15.5           1       12.5         39\n\nFailures                      31    .6458333           0          1          1\n------------------------------------------------------------------------------\n\n\n\n%%stata\nsts list\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\nKaplan–Meier survivor function\n\n             At                  Survivor      Std.\n  Time     risk   Fail   Lost    function     error     [95% conf. int.]\n------------------------------------------------------------------------\n     1       48      2      0      0.9583    0.0288     0.8435    0.9894\n     2       46      1      0      0.9375    0.0349     0.8186    0.9794\n     3       45      1      0      0.9167    0.0399     0.7930    0.9679\n     4       44      2      0      0.8750    0.0477     0.7427    0.9418\n     5       42      2      0      0.8333    0.0538     0.6943    0.9129\n     6       40      2      1      0.7917    0.0586     0.6474    0.8820\n     7       37      1      0      0.7703    0.0608     0.6236    0.8656\n     8       36      3      1      0.7061    0.0661     0.5546    0.8143\n     9       32      0      1      0.7061    0.0661     0.5546    0.8143\n    10       31      1      1      0.6833    0.0678     0.5302    0.7957\n    11       29      2      1      0.6362    0.0708     0.4807    0.7564\n    12       26      2      0      0.5872    0.0733     0.4304    0.7145\n    13       24      1      0      0.5628    0.0742     0.4060    0.6931\n    15       23      1      1      0.5383    0.0749     0.3821    0.6712\n    16       21      1      0      0.5127    0.0756     0.3570    0.6483\n    17       20      1      1      0.4870    0.0761     0.3326    0.6249\n    19       18      0      2      0.4870    0.0761     0.3326    0.6249\n    20       16      0      1      0.4870    0.0761     0.3326    0.6249\n    22       15      2      0      0.4221    0.0786     0.2680    0.5684\n    23       13      2      0      0.3572    0.0788     0.2087    0.5083\n    24       11      1      0      0.3247    0.0780     0.1809    0.4771\n    25       10      1      1      0.2922    0.0767     0.1543    0.4449\n    28        8      1      1      0.2557    0.0753     0.1247    0.4093\n    32        6      0      2      0.2557    0.0753     0.1247    0.4093\n    33        4      1      0      0.1918    0.0791     0.0676    0.3634\n    34        3      0      1      0.1918    0.0791     0.0676    0.3634\n    35        2      0      1      0.1918    0.0791     0.0676    0.3634\n    39        1      0      1      0.1918    0.0791     0.0676    0.3634\n------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#k-m曲线的绘制",
    "href": "Guide/Stata/25-05-08-survival-ana.html#k-m曲线的绘制",
    "title": "15-生存分析",
    "section": "7 K-M曲线的绘制",
    "text": "7 K-M曲线的绘制\n语法：\nsts graph [if] [in] [,options]\n[,options] 不是必须，可以形如：,by(var)，这样就会按照 var 的分类绘制不同的线\n\n%%stata\nsts graph,by(drug)\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n7.1 图像展现更多的信息\n\n%%stata\nsts graph if age&lt;50,by(drug)\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n%%stata\nsts graph,by(drug) risktable\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n\n7.2 复杂绘图示例\n\n%%stata\nlabel drop drug_label\nlabel define drug_label 0 \"安慰剂\" 1 \"试验药\"\nlabel values drug drug_label\n\n\n. label drop drug_label\n\n. label define drug_label 0 \"安慰剂\" 1 \"试验药\"\n\n. label values drug drug_label\n\n. \n\n\n\n%%stata\nsts graph, ///\n    by(drug) ci atrisk ///\n    xlabel(0(5)40) ylabel(0(0.2)1) ///\n    legend(position(6) ring(1) cols(2) rowgap(1) colgap(1)) ///\n    xtitle(\"x轴的标签\") ytitle(\"y轴的标签\") ///\n    title(\"这里是标题\", size(medsmall)) ///\n    subtitle(\"这里是副标题\", size(small)) ///\n    caption(\"注释\", size(vsmall)) ///\n    note(\"数据来源：xxx\", size(vsmall)) ///\n    graphregion(margin(10 10 10 10)) ///\n    plotregion(margin(5 5 5 5))\n\n\n. sts graph, ///\n&gt;     by(drug) ci atrisk ///\n&gt;     xlabel(0(5)40) ylabel(0(0.2)1) ///\n&gt;     legend(position(6) ring(1) cols(2) rowgap(1) colgap(1)) ///\n&gt;     xtitle(\"x轴的标签\") ytitle(\"y轴的标签\") ///\n&gt;     title(\"这里是标题\", size(medsmall)) ///\n&gt;     subtitle(\"这里是副标题\", size(small)) ///\n&gt;     caption(\"注释\", size(vsmall)) ///\n&gt;     note(\"数据来源：xxx\", size(vsmall)) ///\n&gt;     graphregion(margin(10 10 10 10)) ///\n&gt;     plotregion(margin(5 5 5 5))\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#检验组间差别log-rank-test",
    "href": "Guide/Stata/25-05-08-survival-ana.html#检验组间差别log-rank-test",
    "title": "15-生存分析",
    "section": "8 检验组间差别（Log-Rank Test）",
    "text": "8 检验组间差别（Log-Rank Test）\n\n%%stata\nsts test drug\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\nEquality of survivor functions\nLog-rank test\n\n      |  Observed       Expected\ndrug  |    events         events\n------+-------------------------\n    0 |        19           7.25\n    1 |        12          23.75\n------+-------------------------\nTotal |        31          31.00\n\n                chi2(1) =  28.27\n                Pr&gt;chi2 = 0.0000",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-02-16-SAS-intro.html",
    "href": "Guide/SAS/25-02-16-SAS-intro.html",
    "title": "00-SAS-Intro and Record",
    "section": "",
    "text": "SAS的历史很长，很强大，但是现代化做的一般（交互界面）。\n使用场景也是有限的，至少一般情况下用不上这么高级的工具。\n但是在某些领域又是极其重要的，像银行和药企，他们要追求足够的稳定和严谨，那么多年不曾有重大改变且一向以稳定著称的 SAS 自然可以很好的满足这一需求。\n\nSAS 对学术研究的支持是比较不友好的。正版太贵，除非学校有提供，个人基本不可能使用正版，这里下载破解版，搞SID(SAS的授权证书)需要时间成本，还容易有安装问题，没错你可以选择使用SAS的教育版，不过谁用谁知道。\n安装比较麻烦，尤其是在Linux上，我曾用两周的时间折腾在 Linux 上安装一个 SAS ，最后以失败告终，且在互联网上找不到解决的方案，AI也束手无策。\nSAS不开源，意味着你看到某些论文，里面使用一些比较新的统计分析方法，SAS不大可能有现成代码可以使用，而 Python 和 R 则大几率有现成的包可以调用，这里也会节省不少时间。\nSAS的强大一方面是性能稳定，可以处理几十上百GB的数据而不容易崩溃，但是医学数据一般容量比较小，并不是非得SAS才能跑的动。\nSAS相比其他编程语言来说是独树一帜(奇葩)的存在（proc和data步独步天下），从语法上面来说并没有什么和它相接近的语言，相反 R 和 Python 则会和一般的编程语言例如 Java, C 等有一些类似的地方，对以后万一还需要学习其他语言或者学习以后新诞生的编程软件诞有一定帮助。\nSAS 的支持有限，互联网上关于 SAS 的使用信息较少，一般都在出版的书中有可复现的内容，也没有像 Python 和 R 等活跃的社区可以提供较多的互动和支持，编程遇上问题不容易找到答案。\n\n用肯定能用，但是在使用中占多大的比重，就需要权衡一下，在 AI 时代，不一定要全部掌握，看得懂，知道怎么做，应该也可以了，当然如果要深入，那就另说。\n每个人的资源和时间都是有限的，用最少的资源和时间做最多的事才是最重要的。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "00-SAS-Intro and Record"
    ]
  },
  {
    "objectID": "Guide/SAS/25-02-16-SAS-intro.html#footnotes",
    "href": "Guide/SAS/25-02-16-SAS-intro.html#footnotes",
    "title": "00-SAS-Intro and Record",
    "section": "脚注",
    "text": "脚注\n\n\nSAS 程式(SAS 9.4) 繁體中文出現亂碼怎麼辦?↩︎",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "00-SAS-Intro and Record"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html",
    "href": "Guide/Stata/25-05-08-GEE.html",
    "title": "17-广义估计方程(GEE)",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#广义估计方程",
    "href": "Guide/Stata/25-05-08-GEE.html#广义估计方程",
    "title": "17-广义估计方程(GEE)",
    "section": "1 广义估计方程",
    "text": "1 广义估计方程\n广义估计方程（Generalized Estimating Equations, GEE）是一种用于分析具有相关性数据的统计方法，特别适用于纵向数据和重复测量数据的分析。\n\n1.1 基本概念\n广义估计方程是一种扩展的回归模型，旨在处理因变量之间可能存在的相关性。它于1986年由Liang和Zeger首次提出，主要用于估计广义线性模型的参数，尤其是在数据存在重复测量或相关性时。GEE通过准似然估计方法来处理这些数据，适用于多种类型的因变量，包括连续型、二分类和计数数据等。\n\n\n1.2 应用场景\n\n纵向数据分析：如临床试验中对同一组受试者在不同时间点的测量数据进行分析。\n重复测量数据：例如在心理学和社会科学研究中，研究同一对象在不同条件下的表现。\n组间比较：用于比较不同组别在某一时间点或多个时间点的差异。\n\n\n\n1.3 优势与特点\n\n处理相关性：GEE能够有效处理因变量之间的相关性，避免了传统方法（如方差分析）在数据不独立时可能导致的偏差。\n灵活性：适用于多种类型的因变量，且对自变量的数据类型没有严格要求，可以处理定类和定量数据。\n模型选择：根据因变量的分布特征选择合适的模型，如泊松回归、负二项回归等。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#什么是纵向数据分析",
    "href": "Guide/Stata/25-05-08-GEE.html#什么是纵向数据分析",
    "title": "17-广义估计方程(GEE)",
    "section": "2 什么是纵向数据分析",
    "text": "2 什么是纵向数据分析\nExamples oflongitudinal dataanalysis(LDA,纵向数据分析)\n\nChild BP measured at each annual visit from 3 to 9 years old\nInfant weight measured at 3, 6, 9, 12 months\nIn a 3-arm cross-over trial, short chain fatty acid measured at the end ofeach\n\n\n2.1 纵向数据 vs. 横向数据\n纵向数据是指对同一组受试个体或者受试单元在不同时间点上的重复观测若干次，得到由截面和时间序列融合在一起的数据。\n因而纵向数据具有自相关性、生态单位聚集性、测量次数与测量时间间隔的非均衡性等特点。\n\nEstablish temporaltrends(时序:A在B之前发生)\nSeparate cohort effects from aging effects\nChildren have different baseline values in reading abilities\nThe trajectories of reading abilities differs by people\n\n\n\n2.2 方差公式\n\\[Var(X-Y)\nVar((1)X+(-1)Y)=(1^2)Var(X)+(-1^2)Var(Y)+2(1)(-1)Cov(X,Y)\\]\n在非配对 t 检验中，\\(Cov(X,Y)\\)为0，此时 \\(Var(X-Y)\\)\n打破线性相关的假设",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#gee-模型",
    "href": "Guide/Stata/25-05-08-GEE.html#gee-模型",
    "title": "17-广义估计方程(GEE)",
    "section": "3 GEE 模型",
    "text": "3 GEE 模型\n\n\n\ncorrelation\ndescription\n\n\n\n\nexchangeable\nexchangeable\n\n\nindependent\nindependent\n\n\nunstructured\nunstructured\n\n\nfixed matname\nuser-specified\n\n\nar #\nautoregressive of order #\n\n\nstationary #\nstationary of order #\n\n\nnonstationary #\nnonstationary of order #\n\n\n\n\n3.1 GEE 常用步骤\n\n使用 independent 回归结构\n稳健标准误估计（robust variance estimate）；如果用错模型，稳健后的结果也会相差不远\n使用 QIC（AIC for GEE）评估GEE模型",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#数据导入",
    "href": "Guide/Stata/25-05-08-GEE.html#数据导入",
    "title": "17-广义估计方程(GEE)",
    "section": "4 数据导入",
    "text": "4 数据导入\nData:NLS Women 14-24 in 1968\n\n%%stata\nwebuse union,clear\n\n(NLS Women 14-24 in 1968)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#数据观测",
    "href": "Guide/Stata/25-05-08-GEE.html#数据观测",
    "title": "17-广义估计方程(GEE)",
    "section": "5 数据观测",
    "text": "5 数据观测\n\n%%stata\nlist in 1/10\n\n\n     +----------------------------------------------------------------+\n     | idcode   year   age   grade   not_smsa   south   union   black |\n     |----------------------------------------------------------------|\n  1. |      1     72    20      12          0       0       1       1 |\n  2. |      1     77    25      12          0       0       0       1 |\n  3. |      1     80    28      12          0       0       1       1 |\n  4. |      1     83    31      12          0       0       1       1 |\n  5. |      1     85    33      12          0       0       1       1 |\n     |----------------------------------------------------------------|\n  6. |      1     87    35      12          0       0       1       1 |\n  7. |      1     88    37      12          0       0       1       1 |\n  8. |      2     71    19      12          0       0       0       1 |\n  9. |      2     77    25      12          0       0       1       1 |\n 10. |      2     78    26      12          0       0       1       1 |\n     +----------------------------------------------------------------+",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#用法",
    "href": "Guide/Stata/25-05-08-GEE.html#用法",
    "title": "17-广义估计方程(GEE)",
    "section": "6 用法",
    "text": "6 用法\n\n6.1 GEE 准备\nxtset studyid timevar\n\nstudyid: unique ID for each participant\ntimevar: timevar will usually be a variable that counts 1,2,…, and is to be interpreted as firstyear ofsurvey, second year,…, or first month oftreatment, second month.\n\n\n%%stata\nxtset id year\n\n\nPanel variable: idcode (unbalanced)\n Time variable: year, 70 to 88, but with gaps\n         Delta: 1 unit\n\n\n\n\n6.2 GEE 命令\nxtgee y x_1,x_2,x_3,…[if] [in] [weight] ,family(family) link(link) corr(correlation structure) robust\n可以使用 help gee 查看更多信息\n\n%%stata\nxtgee union age grade not_smsa south, family(binomial) link(logit) corr(ind) robust\n\n\nIteration 1:  Tolerance = 1.940e-12\n\nGEE population-averaged model                      Number of obs    =   26,200\nGroup variable: idcode                             Number of groups =    4,434\nFamily: Binomial                                   Obs per group:  \nLink:   Logit                                                   min =        1\nCorrelation: independent                                        avg =      5.9\n                                                                max =       12\n                                                   Wald chi2(4)     =   160.30\nScale parameter = 1                                Prob &gt; chi2      =   0.0000\n\nPearson chi2(26200)  = 26242.04                    Deviance         = 27093.19\nDispersion (Pearson) = 1.001604                    Dispersion       = 1.034091\n\n                                 (Std. err. adjusted for clustering on idcode)\n------------------------------------------------------------------------------\n             |               Robust\n       union | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |    .011683   .0033035     3.54   0.000     .0052082    .0181577\n       grade |    .048511   .0139346     3.48   0.000     .0211997    .0758223\n    not_smsa |  -.2214007   .0713343    -3.10   0.002    -.3612134    -.081588\n       south |  -.6470985   .0629803   -10.27   0.000    -.7705376   -.5236594\n       _cons |  -1.941974   .1973105    -9.84   0.000    -2.328695   -1.555253\n------------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-Cox-reg.html",
    "href": "Guide/Stata/25-05-08-Cox-reg.html",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-Cox-reg.html#数据导入",
    "href": "Guide/Stata/25-05-08-Cox-reg.html#数据导入",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "1 数据导入",
    "text": "1 数据导入\n在一个抗癌药物的Clinical Trial中，48名患者被随机分配到新药组(28人)和安慰剂组(20人)，研究人员想知道新药是否影响患者的生存情况。\nData：Patient Survival in Drug Trial\n\n%%stata\nwebuse drugtr,clear\n\n(Patient survival in drug trial)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-Cox-reg.html#cox回归",
    "href": "Guide/Stata/25-05-08-Cox-reg.html#cox回归",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "2 Cox回归",
    "text": "2 Cox回归\nCox回归分析，也称为比例风险回归模型（Proportional Hazards Model，简称Cox模型），是由英国统计学家D.R.Cox于1972年提出的一种半参数回归模型。\n该模型主要用于生存分析，能够同时分析多个因素对生存期的影响，并且可以处理带有截尾生存时间的数据。\n\n2.1 基本概念\n在介绍Cox回归模型之前，需要了解几个相关的概念：\n\n生存函数：表示观察对象的生存时间大于某时刻的概率。\n死亡函数：表示观察对象的生存时间不大于某时刻的概率。\n死亡密度函数：表示观察对象在某时刻的瞬时死亡率。\n危险率函数：表示生存时间已达到某时刻的观察对象在该时刻的瞬时死亡率1。\n\n\n\n2.2 基本原理\nCox回归模型的基本形式为： \\[h(t|X) = h_0(t) \\exp(\\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p)\\]\n其中， \\(h(t|X)\\) 是在给定协变量 \\(X\\) 时的危险率，\\(h_0(t)\\) 是基准危险率，\\(\\beta_i\\) 是需要估计的回归系数。\n\n\n2.3 Cox回归模型的假设包括：\n\n比例风险假设：各危险因素的作用不随时间变化。\n对数线性假设：协变量与对数风险比呈线性关系。\n\n\n\n2.4 偏回归系数的意义\n偏回归系数 \\(\\beta_i\\) 的流行病学含义是在其他协变量不变的情况下，协变量每增加一个测定单位时所引起的相对危险度的自然对数的改变量。\n\n\n2.5 假设检验\nCox回归模型中的偏回归系数可以通过建立偏似然函数，利用Newton-Raphson迭代法求得。常用的假设检验方法包括似然比检验、Wald检验和记分检验。\nCox回归模型由于其灵活性和广泛应用，成为生存分析中最常用的多因素分析方法之一。\n\n%%stata\nstcox drug\n\n\n        Failure _d: died\n  Analysis time _t: studytime\n\nIteration 0:  Log likelihood = -99.911448\nIteration 1:  Log likelihood = -88.254734\nIteration 2:  Log likelihood = -88.001551\nIteration 3:  Log likelihood =  -88.00019\nRefining estimates:\nIteration 0:  Log likelihood =  -88.00019\n\nCox regression with Breslow method for ties\n\nNo. of subjects =  48                                   Number of obs =     48\nNo. of failures =  31\nTime at risk    = 744\n                                                        LR chi2(1)    =  23.82\nLog likelihood = -88.00019                              Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n          _t | Haz. ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        drug |   .1327581   .0584002    -4.59   0.000     .0560555    .3144157\n------------------------------------------------------------------------------\n\n\n这里 Drug 的 coefficient(\\(\\beta\\)) : 新药组终点事件发生风险是安慰剂组的 13.3%(95% CI: 5.6%,31.4%)\n控制年龄：\n\n%%stata\nstcox drug age\n\n\n        Failure _d: died\n  Analysis time _t: studytime\n\nIteration 0:  Log likelihood = -99.911448\nIteration 1:  Log likelihood = -83.551879\nIteration 2:  Log likelihood = -83.324009\nIteration 3:  Log likelihood = -83.323546\nRefining estimates:\nIteration 0:  Log likelihood = -83.323546\n\nCox regression with Breslow method for ties\n\nNo. of subjects =  48                                   Number of obs =     48\nNo. of failures =  31\nTime at risk    = 744\n                                                        LR chi2(2)    =  33.18\nLog likelihood = -83.323546                             Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n          _t | Haz. ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        drug |   .1048772   .0477017    -4.96   0.000     .0430057    .2557622\n         age |   1.120325   .0417711     3.05   0.002     1.041375     1.20526\n------------------------------------------------------------------------------\n\n\n这里 Drug 的 coefficient(\\(\\beta_1\\)) : 在控制了患者的年龄后，新药组终点事件发生风险是安慰剂组的 10.5%(95% CI: 4.3%,25.6%)\n反过来，这里 Age 的 coefficient(\\(\\beta_2\\)) : 在控制了治疗方法后，患者年龄每增加 1 岁，发生终点事件风险增加 12%(95% CI: 4.1%,20.5%)\n\n\n2.6 Cox回归的命令语法\nstcox var1 var2 var3 ... [if] [in] [, options]\nNotice\n\n必须要在指定Data为Suvivaldata之后(stset之后)才能使用任何st开头的命令\n由于我们已经在一开始将数据转化为Survivaldata的时候指定过终点事件(Failure variable)、时间变量(Time variable)，我们在这里只需要设置需要在回归方程中independent variable即可\nExample\n\nstcox drug age\nstcox drug age if age &lt; 50",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-Cox-reg.html#ph-假定检验",
    "href": "Guide/Stata/25-05-08-Cox-reg.html#ph-假定检验",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "3 PH 假定检验",
    "text": "3 PH 假定检验\n\n3.1 使用统计检验法\nCox回归模型在应用时，有一个非常重要的前提条件，即比例风险（Proportional hazards）假定，简称PH假定。\nNotice:PH假定的检验基于上一步进行的Cox回归。\n\\(H_0\\)：纳入Cox回归模型的变量满足PH假定\n\\(P&gt;0.05\\)，不能拒绝\\(H_0\\)\n\n%%stata\nestat phtest\n\n\nTest of proportional-hazards assumption\n\nTime function: Analysis time\n------------------------------------------------\n             |     chi2       df       Prob&gt;chi2\n-------------+----------------------------------\n Global test |     0.43        2          0.8064\n------------------------------------------------\n\n\n\n\n3.2 PH假定 使用图像法\n使用- ln(-ln(生存))图法，判断标准是如果待评价的变量分成的亚组曲线平行或近似平行，则满足PH假定。\nstphplot,by(bar1) adjust(var2 var3)\nvar1 是自变量名，var2 等是希望控制的变量。\nNotice:这个命令不一定要跟在cox回归之后\n\n%%stata\nstphplot,by(drug) adjust(age)\n\n\n        Failure _d: died\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n\n3.3 如果不满足PH假定\n\n一般只要两组生存曲线趋势一致、不明显交叉即可判定PH假定成立\n如果PH假定不成立，可以加上时间(time)和暴露 (exposure,比如本例之中的drug)的交互项(interaction term), time*exposure\n也可以对于不同的时间段分别分析(e.g.0-10，10-20,&gt;20)\n参数生存分析模型:streg进行参数生存分析",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-Ord-Logistic-Reg.html",
    "href": "Guide/Stata/25-05-08-Ord-Logistic-Reg.html",
    "title": "18-有序多分类Logistic回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "18-有序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-2-data-manag.html",
    "href": "Guide/Stata/25-05-09-2-data-manag.html",
    "title": "20-数据整理",
    "section": "",
    "text": "使用 Do-file 完成后续的数据整理操作",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "20-数据整理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-2-data-manag.html#stata-须知",
    "href": "Guide/Stata/25-05-09-2-data-manag.html#stata-须知",
    "title": "20-数据整理",
    "section": "1 Stata 须知",
    "text": "1 Stata 须知\n\nStata区分大小写! Stata is case-sensitive!\n常见符号:\n\n=是 赋值(e.g· gen x = 1)\n==是 恒等(e.g· gen x = 1 if y == 2)\n|是 或者(e.g· gen x = 1 if y == 2 | y == 3)\n&是 且(e.g· gen x = 1 if y == 2 & z == 3) ## 数据整理的常见步骤\n\n查看工作路径(pwd);改变工作路径(cd)\n导入Excel文件(import excel),CSV文件(import delimited),dta文件(use)\n添加变量或变量数值标签(label)\n生成新变量(gen)或统计量变量(egen)\n将观测值按照变量数值大小排序(sort;gsort)\n改变变量前后顺序(order)\n将数据集进行长宽转换(reshape)\n合并数据集(merge)\n删除(drop)或保留(keep)观测值或变量\n导出Excel,CSV文件(export)或dta文件(save) %%skip\n\n\nStata添加注释的方法\nStata中绿色的文字是不运行的\n\n// 跟在command之后的注释 \n/// 代表行内换行 \n* 星号作为一行的开头时，这一行都是注释 \n/* 两个星号之间加多行注释 */ \n/*  Purpose: \n    Author: \n    Date created: \n*/\n\n* 清除Stata内存里的所有数据和宏等 \nclear all\n\n//set maxvar 8000 \n* 如果你的变量数&gt;5000，需要设置。例如:set maxvar 8000 \n* 请注意:Stata IC最多只有2,048个变量;SE:32,767;MP:120，000\n\n* 得到现在的工作路径，每个人肯定都不一样 \npwd\n\n* 改变到自己的工作路径\n* 改变工作路径到\"/Users/username/Desktop/Stata'\ncd \"/Users/username/Desktop/Stata\"\n\n* 把child.csv,child.xlsx,child.dta,anthro.dta这4个文件放到工作路径底下\n\n* 查看工作路径底下都有什么文件 \ndir\n\n* 如果现在有打开的Log文件，那么关闭它 \ncapture log close  \n* 打开新的Log文件 \nlog using Lecture.log,replace\n\n***********导入dta数据*************** \n* 在这里，没有指定\"child.dta\"这个文件的位置，那么Stata就会到工作路径下面去找这个文件 \n* 逗号后面的clear指的是清除现有的数据，这样才能腾出地方导入新的数据\n\nuse \"child.dta\",clear \n* 或者(文件路径中如果有空格，一定需要用\"\"包裹路径)\nuse \"/Users/username/Desktop/Stata/child.dta\", clear\n\n**************导入csv文件*******************\nimport delimited\"child.csv\", clear\n\n* 当然，你也可以指定这个文件的位置，是一样的\nimport delimited \"/Users/username/Desktop/Stata/child.csv\", clear\n\n* 在导入CSV文件时，Stata自己判断是否把CSV文件的第一行作为变量名导入\n* 我们可以让Stata不把CSV文件的第一行作为变量名导入\nimport delimited\"child.csv\",varnames(nonames) clear\n\n* 我们也可以让Stata必须把CSV文件的第一行作为变量名导入\nimport delimited\"child.csv\",varnames(1) clear\n\n*我们可以让Stata导入的时候把变量名全变成大写\nimport delimited\"child.csv\",varnames(1) case(upper) clear\n\n* 或者全变成小写\nimport delimited \"child.csv\",varnames(1) case(lower) clear\n* 或者保留CSV里的形式\nimport delimited \"child.csv\",varnames(1) case(preserve) clear\n\n*******************导入Excel文件**************\nimport excel \"child.xlsx\",clear\n* 或者\nimport excel\"/Users/username/Desktop/Stata/child.xlsx\",clear\n\n* 在导入Excel文件时，Stata默认第一行不是变量名\n* 我们可以让Stata把第一行当做变量名导入\nimport excel \"child.xlsx\",firstrow clear\n\n* 我们也可以指定导入Excel文件里的哪个Sheet\nimport excel \"child.xlsx\",firstrow sheet(\"Sheet1\")clear\n\n* 我们可以让stata导入的时候把变量名全变成大写\nimport excel \"child.xlsx\",firstrow case(upper) clear\n\n* 或者全变成小写\nimport excel \"child.xlsx\",firstrow case(lower)clear\n\n* 或者保留Excel里的形式\nimport excel \"child.xlsx\",firstrow case(preserve) clear\nimport excel \"/Users/username/Desktop/Stata/child.xlsx\"， ///\nsheet(\"Sheet1\")firstrow clear\n\n*************数据清理*****************\n/*\n我们以CHILD_SEX变量为例，进行数据的清洗和整理CHILD SEX现在是字符型变量:\"1\"是Boy\",\"2\"是Girl,\"M\"是missing\n*/\ncodebook CHILD SEX",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "20-数据整理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-2-data-manag.html#数据清理和排序",
    "href": "Guide/Stata/25-05-09-2-data-manag.html#数据清理和排序",
    "title": "20-数据整理",
    "section": "2 数据清理和排序",
    "text": "2 数据清理和排序\n继续上述的分析步骤，进入数据清理\n* 改变工作路径到\"/Users/username/Desktop/Stata'\ncd \"/Users/username/Desktop/Stata\"\n\n* 或者保留Excel里的形式\nimport excel \"child.xlsx\",firstrow case(preserve) clear\nimport excel \"/Users/username/Desktop/Stata/child.xlsx\"， ///\nsheet(\"Sheet1\")firstrow clear\n\n*************数据清理*****************\n/*\n我们以CHILD_SEX变量为例，进行数据的清洗和整理CHILD SEX现在是字符型变量:\"1\"是Boy\",\"2\"是Girl,\"M\"是missing\n*/\ncodebook CHILD_SEX\n\n/*\n我们将CHILD_SEX替换成数值型变量。我们使用destring命令在destring的时候，我们可以通过option \"force\"把不是数字的字符(如\"M\")替换为缺失值\n(destring CHILD SEX,force replace)\n但是，需要更加小心:万一有人输入的是\"B\"，我们其实应该替换为1。\n为了保险起见，我们手动把\"M“改成\".\"\"\n*/\nreplace CHILD_SEX = \".\" if CHILD_SEX == \"M\"\n\n/*\n注意:\n字符型变量的观测值只能替换成字符型，因此我们只能把\"M\"替换成\".\"\"，不能替换成 . \n比如，这个就是错的:\nreplace CHILD_SEX = . if CHILD_SEX = \"M\"\nStata会告诉你: type mismatch r(109);\n*/\n\ndestring CHILD_SEX, replace\n\n/*\ndestring的时候:\nreplace option替代现在的CHILD_SEX变量\ndestring CHILD_SEX，replace\ngenerate option生成一个新的变量\ndestring CHILD_SEX, generate(CHILD_SEX_2)\n*/\n\n* Destring以后，我们再来看一下CHILD_SEX的信息\ncodebook CHILD_SEX\n/*\n我们当然可以对于每一个变量进行这样的destring操作，不过比较麻烦\n我写了一个小的loop，可以对每一个变量进行destring操作\n*/\nforeach v of varlist _all { //对于所有变量进行循环\n    capture confirm string var 'v' //先确定这个变量是不是string\n    if _rc==0{ //如果是的话\n    replace 'v' = \".\" if 'v' == \"M\"//将\"'M\"替换成\".\"\n    destring 'v',replace\n    }\n    }",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "20-数据整理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html",
    "href": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html",
    "title": "18-有序多分类Logistic回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "18-有序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html#多分类变量",
    "href": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html#多分类变量",
    "title": "18-有序多分类Logistic回归",
    "section": "1 多分类变量",
    "text": "1 多分类变量\n多分类变量主要分为有序多分类和无序多分类变量\n\n1.1 有许多分类变量\n疾病分期；严重程度；发展阶段等\n\n\n1.2 无序多分类变量\n方位（东、南、西、北）；品牌等",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "18-有序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html#有序多分类",
    "href": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html#有序多分类",
    "title": "18-有序多分类Logistic回归",
    "section": "2 有序多分类",
    "text": "2 有序多分类\n\n2.1 有序多分类的原理\n\n将y变量的n个分类拆分成n-1个二分类Logistic回归\n例子中的Excellent; Good; Average; Fair; Poor拆分成:\n\nPoor vs. Excellent + Good + Average+Fair\nFair+Poor vs. Excellent + Good+ Average\nAverage + Fair + Poor vs. Excellent + Good\nGood + Average + Fair + Poor vs. Excellent\n\n\n\n\n2.2 Proportional odds 假定\n\n多个二元Logistic回归中，除了\\(\\beta_0\\)以外的系数相等 \\[Odds(Poor)/Odds(Excellent+Good+Average+Fair)\\\\\n= Odds(Fair+Poor)/Odds(Excellent+Good+Average)\\\\\n=Odds(Average+Fair+Poor)/Odds(Excellent+Good)\\\\\n= Odds(Good+Average+Fair+Poor)/Odds(Excellent)\\]\nProportionalodds假定是否成立更多是由研究问题的自身性质决定，可以用数据进行检测，但数据本身可能有Bias\n如果该假定不成立:当做无序多分类Logistic回归",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "18-有序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html#导入数据",
    "href": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html#导入数据",
    "title": "18-有序多分类Logistic回归",
    "section": "3 导入数据",
    "text": "3 导入数据\n1977年汽车修理记录数据\n\n%%stata\nwebuse fullauto.dta,clear\n\n(Automobile models)\n\n\n\n3.1 结局变量\noutcome：车辆维修状况\n\n%%stata\ncodebook rep77\n\n\n-------------------------------------------------------------------------------\nrep77                                                        Repair record 1977\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n                 Label: repair\n\n                 Range: [1,5]                         Units: 1\n         Unique values: 5                         Missing .: 8/74\n\n            Tabulation: Freq.   Numeric  Label\n                            3         1  Poor\n                           11         2  Fair\n                           27         3  Average\n                           20         4  Good\n                            5         5  Excellent\n                            8         .  \n\n\n\n\n3.2 暴露变量\nexposure：是否为进口车\n\n%%stata\ncodebook foreign\n\n\n-------------------------------------------------------------------------------\nforeign                                                                 Foreign\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n                 Label: foreign\n\n                 Range: [0,1]                         Units: 1\n         Unique values: 2                         Missing .: 0/74\n\n            Tabulation: Freq.   Numeric  Label\n                           52         0  Domestic\n                           22         1  Foreign",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "18-有序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html#卡方检验",
    "href": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html#卡方检验",
    "title": "18-有序多分类Logistic回归",
    "section": "4 卡方检验",
    "text": "4 卡方检验\n\\(H_0\\)：车辆是否为进口车和车辆维修状况没有关系\n\n%%stata\ntab foreign rep77,chi2\n\n\n           |                   Repair record 1977\n   Foreign |      Poor       Fair    Average       Good  Excellent |     Total\n-----------+-------------------------------------------------------+----------\n  Domestic |         2         10         20         13          0 |        45 \n   Foreign |         1          1          7          7          5 |        21 \n-----------+-------------------------------------------------------+----------\n     Total |         3         11         27         20          5 |        66 \n\n          Pearson chi2(4) =  13.8619   Pr = 0.008\n\n\n\\(P=0.008&lt;0.05\\)，在\\(\\alpha=0.05\\) 的检验水准下，拒绝零假设，得出结论:车辆是否为进口车和车辆维修状况有关系",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "18-有序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html#有序logistic回归",
    "href": "Guide/Stata/25-05-08-5-Ord-Logistic-Reg.html#有序logistic回归",
    "title": "18-有序多分类Logistic回归",
    "section": "5 有序Logistic回归",
    "text": "5 有序Logistic回归\n\n5.1 语法\nologit y x1 x2 x3 ...xn [if] [in] [weight] [,options]\n\n最常用的 [,options] 是 or,他可以直接给出OR值\nExamples：\n\nologit rep77 foreign\nologit rep77 foreign, or\nologit rep77 foreign length mpg, or\n\n\n\n%%stata\nologit rep77 foreign\n\n\nIteration 0:  Log likelihood = -89.895098  \nIteration 1:  Log likelihood = -85.951765  \nIteration 2:  Log likelihood = -85.908227  \nIteration 3:  Log likelihood = -85.908161  \nIteration 4:  Log likelihood = -85.908161  \n\nOrdered logistic regression                             Number of obs =     66\n                                                        LR chi2(1)    =   7.97\n                                                        Prob &gt; chi2   = 0.0047\nLog likelihood = -85.908161                             Pseudo R2     = 0.0444\n\n------------------------------------------------------------------------------\n       rep77 | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     foreign |   1.455878   .5308951     2.74   0.006     .4153425    2.496413\n-------------+----------------------------------------------------------------\n       /cut1 |  -2.765562   .5988208                     -3.939229   -1.591895\n       /cut2 |  -.9963603   .3217706                     -1.627019   -.3657016\n       /cut3 |   .9426153   .3136398                      .3278925    1.557338\n       /cut4 |   3.123351   .5423257                      2.060412     4.18629\n------------------------------------------------------------------------------\n\n\n进口车（foreign=1）和国产车（foreign=0）比：\n\\[Odds=e^{-1.46}=0.23\\]\n\n更高维修状况等级为reference，在更低维修状况的odds\n\n也可以是：\n\\[Odds=e^{\\beta}=e^{1.46}=4.29\\]\n\n更低维修状况等级为reference，在更高维修状况的odds\n\n进口(Foreign=1)车和国产车相比(Foreign=0)，在“更低的车辆维修状况等级”的odds是在“更高维修状况等级”的0.23倍\n一般使用如下假释：进口(Foreign=1)车和国产车相比(Foreign=0)，在“更高的车辆维修状况等级”的odds是在“更低维修状况等级”的4.29倍\n\n%%stata\nologit rep77 foreign,or\n\n\nIteration 0:  Log likelihood = -89.895098  \nIteration 1:  Log likelihood = -85.951765  \nIteration 2:  Log likelihood = -85.908227  \nIteration 3:  Log likelihood = -85.908161  \nIteration 4:  Log likelihood = -85.908161  \n\nOrdered logistic regression                             Number of obs =     66\n                                                        LR chi2(1)    =   7.97\n                                                        Prob &gt; chi2   = 0.0047\nLog likelihood = -85.908161                             Pseudo R2     = 0.0444\n\n------------------------------------------------------------------------------\n       rep77 | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     foreign |   4.288246   2.276609     2.74   0.006      1.51489    12.13888\n-------------+----------------------------------------------------------------\n       /cut1 |  -2.765562   .5988208                     -3.939229   -1.591895\n       /cut2 |  -.9963603   .3217706                     -1.627019   -.3657016\n       /cut3 |   .9426153   .3136398                      .3278925    1.557338\n       /cut4 |   3.123351   .5423257                      2.060412     4.18629\n------------------------------------------------------------------------------\nNote: Estimates are transformed only in the first equation to odds ratios.\n\n\n\n%%stata\nologit rep77 foreign length mpg,or\n\n\nIteration 0:  Log likelihood = -89.895098  \nIteration 1:  Log likelihood = -78.775147  \nIteration 2:  Log likelihood = -78.254294  \nIteration 3:  Log likelihood = -78.250719  \nIteration 4:  Log likelihood = -78.250719  \n\nOrdered logistic regression                             Number of obs =     66\n                                                        LR chi2(3)    =  23.29\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -78.250719                             Pseudo R2     = 0.1295\n\n------------------------------------------------------------------------------\n       rep77 | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     foreign |    18.1162   14.32342     3.66   0.000     3.846558    85.32223\n      length |   1.086354    .024682     3.65   0.000      1.03904    1.135823\n         mpg |   1.259567   .0887425     3.28   0.001     1.097109     1.44608\n-------------+----------------------------------------------------------------\n       /cut1 |   17.92748   5.551191                      7.047344    28.80761\n       /cut2 |   19.86506    5.59648                      8.896161    30.83396\n       /cut3 |   22.10331   5.708936                        10.914    33.29262\n       /cut4 |   24.69213   5.890754                      13.14647     36.2378\n------------------------------------------------------------------------------\nNote: Estimates are transformed only in the first equation to odds ratios.\n\n\n\n在控制了汽车的长度、里程之后，进口车有着更高车辆维修状况等级的odds是国产车的18.12倍(95%CI:3.85,85.32)\n在控制了汽车的产地、里程之后，车辆每增加1 inch，有更高车辆维修状况的odds增加8.64%(95% CI: 3.90,13.58)\nmpg",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "18-有序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-3-Cox-reg.html",
    "href": "Guide/Stata/25-05-08-3-Cox-reg.html",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-3-Cox-reg.html#数据导入",
    "href": "Guide/Stata/25-05-08-3-Cox-reg.html#数据导入",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "1 数据导入",
    "text": "1 数据导入\n在一个抗癌药物的Clinical Trial中，48名患者被随机分配到新药组(28人)和安慰剂组(20人)，研究人员想知道新药是否影响患者的生存情况。\nData：Patient Survival in Drug Trial\n\n%%stata\nwebuse drugtr,clear\n\n(Patient survival in drug trial)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-3-Cox-reg.html#cox回归",
    "href": "Guide/Stata/25-05-08-3-Cox-reg.html#cox回归",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "2 Cox回归",
    "text": "2 Cox回归\nCox回归分析，也称为比例风险回归模型（Proportional Hazards Model，简称Cox模型），是由英国统计学家D.R.Cox于1972年提出的一种半参数回归模型。\n该模型主要用于生存分析，能够同时分析多个因素对生存期的影响，并且可以处理带有截尾生存时间的数据。\n\n2.1 基本概念\n在介绍Cox回归模型之前，需要了解几个相关的概念：\n\n生存函数：表示观察对象的生存时间大于某时刻的概率。\n死亡函数：表示观察对象的生存时间不大于某时刻的概率。\n死亡密度函数：表示观察对象在某时刻的瞬时死亡率。\n危险率函数：表示生存时间已达到某时刻的观察对象在该时刻的瞬时死亡率1。\n\n\n\n2.2 基本原理\nCox回归模型的基本形式为： \\[h(t|X) = h_0(t) \\exp(\\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p)\\]\n其中， \\(h(t|X)\\) 是在给定协变量 \\(X\\) 时的危险率，\\(h_0(t)\\) 是基准危险率，\\(\\beta_i\\) 是需要估计的回归系数。\n\n\n2.3 Cox回归模型的假设包括：\n\n比例风险假设：各危险因素的作用不随时间变化。\n对数线性假设：协变量与对数风险比呈线性关系。\n\n\n\n2.4 偏回归系数的意义\n偏回归系数 \\(\\beta_i\\) 的流行病学含义是在其他协变量不变的情况下，协变量每增加一个测定单位时所引起的相对危险度的自然对数的改变量。\n\n\n2.5 假设检验\nCox回归模型中的偏回归系数可以通过建立偏似然函数，利用Newton-Raphson迭代法求得。常用的假设检验方法包括似然比检验、Wald检验和记分检验。\nCox回归模型由于其灵活性和广泛应用，成为生存分析中最常用的多因素分析方法之一。\n\n%%stata\nstcox drug\n\n\n        Failure _d: died\n  Analysis time _t: studytime\n\nIteration 0:  Log likelihood = -99.911448\nIteration 1:  Log likelihood = -88.254734\nIteration 2:  Log likelihood = -88.001551\nIteration 3:  Log likelihood =  -88.00019\nRefining estimates:\nIteration 0:  Log likelihood =  -88.00019\n\nCox regression with Breslow method for ties\n\nNo. of subjects =  48                                   Number of obs =     48\nNo. of failures =  31\nTime at risk    = 744\n                                                        LR chi2(1)    =  23.82\nLog likelihood = -88.00019                              Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n          _t | Haz. ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        drug |   .1327581   .0584002    -4.59   0.000     .0560555    .3144157\n------------------------------------------------------------------------------\n\n\n这里 Drug 的 coefficient(\\(\\beta\\)) : 新药组终点事件发生风险是安慰剂组的 13.3%(95% CI: 5.6%,31.4%)\n控制年龄：\n\n%%stata\nstcox drug age\n\n\n        Failure _d: died\n  Analysis time _t: studytime\n\nIteration 0:  Log likelihood = -99.911448\nIteration 1:  Log likelihood = -83.551879\nIteration 2:  Log likelihood = -83.324009\nIteration 3:  Log likelihood = -83.323546\nRefining estimates:\nIteration 0:  Log likelihood = -83.323546\n\nCox regression with Breslow method for ties\n\nNo. of subjects =  48                                   Number of obs =     48\nNo. of failures =  31\nTime at risk    = 744\n                                                        LR chi2(2)    =  33.18\nLog likelihood = -83.323546                             Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n          _t | Haz. ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        drug |   .1048772   .0477017    -4.96   0.000     .0430057    .2557622\n         age |   1.120325   .0417711     3.05   0.002     1.041375     1.20526\n------------------------------------------------------------------------------\n\n\n这里 Drug 的 coefficient(\\(\\beta_1\\)) : 在控制了患者的年龄后，新药组终点事件发生风险是安慰剂组的 10.5%(95% CI: 4.3%,25.6%)\n反过来，这里 Age 的 coefficient(\\(\\beta_2\\)) : 在控制了治疗方法后，患者年龄每增加 1 岁，发生终点事件风险增加 12%(95% CI: 4.1%,20.5%)\n\n\n2.6 Cox回归的命令语法\nstcox var1 var2 var3 ... [if] [in] [, options]\nNotice\n\n必须要在指定Data为Suvivaldata之后(stset之后)才能使用任何st开头的命令\n由于我们已经在一开始将数据转化为Survivaldata的时候指定过终点事件(Failure variable)、时间变量(Time variable)，我们在这里只需要设置需要在回归方程中independent variable即可\nExample\n\nstcox drug age\nstcox drug age if age &lt; 50",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-3-Cox-reg.html#ph-假定检验",
    "href": "Guide/Stata/25-05-08-3-Cox-reg.html#ph-假定检验",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "3 PH 假定检验",
    "text": "3 PH 假定检验\n\n3.1 使用统计检验法\nCox回归模型在应用时，有一个非常重要的前提条件，即比例风险（Proportional hazards）假定，简称PH假定。\nNotice:PH假定的检验基于上一步进行的Cox回归。\n\\(H_0\\)：纳入Cox回归模型的变量满足PH假定\n\\(P&gt;0.05\\)，不能拒绝\\(H_0\\)\n\n%%stata\nestat phtest\n\n\nTest of proportional-hazards assumption\n\nTime function: Analysis time\n------------------------------------------------\n             |     chi2       df       Prob&gt;chi2\n-------------+----------------------------------\n Global test |     0.43        2          0.8064\n------------------------------------------------\n\n\n\n\n3.2 PH假定 使用图像法\n使用- ln(-ln(生存))图法，判断标准是如果待评价的变量分成的亚组曲线平行或近似平行，则满足PH假定。\nstphplot,by(bar1) adjust(var2 var3)\nvar1 是自变量名，var2 等是希望控制的变量。\nNotice:这个命令不一定要跟在cox回归之后\n\n%%stata\nstphplot,by(drug) adjust(age)\n\n\n        Failure _d: died\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n\n3.3 如果不满足PH假定\n\n一般只要两组生存曲线趋势一致、不明显交叉即可判定PH假定成立\n如果PH假定不成立，可以加上时间(time)和暴露 (exposure,比如本例之中的drug)的交互项(interaction term), time*exposure\n也可以对于不同的时间段分别分析(e.g.0-10，10-20,&gt;20)\n参数生存分析模型:streg进行参数生存分析",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html",
    "href": "Guide/Stata/25-05-08-1-GLM.html",
    "title": "14-广义线性模型（GLM）",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)\n%%stata\nsysuse auto.dta,clear\n\n(1978 automobile data)\n大于15%会产生Bias的这个问题，只有在我们想用odds去estimaterisk的时候才会发生。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html#什么是广义线性模型",
    "href": "Guide/Stata/25-05-08-1-GLM.html#什么是广义线性模型",
    "title": "14-广义线性模型（GLM）",
    "section": "1 什么是广义线性模型",
    "text": "1 什么是广义线性模型\n\n线性回归 Linear regression：\n\n\\(exp Y = \\beta_0 + \\beta_1*X_1+\\cdots + \\beta_n *X_n\\)\n\n广义线性回归 Generalized linear model：\n\n\\(f(exp Y)= β_0 + β_1*X_1 + \\cdots + \\beta_n* X_n\\)\nLinear regression is a special case of GLM\nSame for logistic regression, Poisson regression, Log-Binomial regression\nEven for Cox regression (Aug.31st & Sep.7th), GEE model (Sep.14th)\n\n总结：在GLM中，转化Y使得转化之后的Y和X们呈线性关系",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html#simple-glm-vs.-multiple-glm",
    "href": "Guide/Stata/25-05-08-1-GLM.html#simple-glm-vs.-multiple-glm",
    "title": "14-广义线性模型（GLM）",
    "section": "2 Simple GLM vs. Multiple GLM",
    "text": "2 Simple GLM vs. Multiple GLM\n\n2.1 简单广义线性模型\n\\[f(exp Y)= β_0 + β_1*X_1\\]\n\n\n2.2 多元广义线性模型\n\\[f(exp Y)= β_0 + β_1*X_1 + \\cdots + \\beta_n* X_n\\]\nX变量可以是各种类型的变量（连续、分类）",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html#specify-options",
    "href": "Guide/Stata/25-05-08-1-GLM.html#specify-options",
    "title": "14-广义线性模型（GLM）",
    "section": "3 Specify Options",
    "text": "3 Specify Options\n\n3.1 Family\nY 变量的分布类型\n\n\n3.2 Link\n把 Y 怎么转化，才和 X 成linear关系",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html#linear-regression",
    "href": "Guide/Stata/25-05-08-1-GLM.html#linear-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "4 linear regression",
    "text": "4 linear regression\n\n4.1 Y var\n\n连续分布（continuous variable）\n如何分布? 高斯分布（Gaussian Distribution）\nModel：\n\n\\[f(exp Y)= β_0 + β_1*X_1 + \\cdots + \\beta_n* X_n\\]\nY 怎么转化才和 X 成 linear 关系？\n\n不用转化\nLink：Identity（恒等）",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html#logistic-regression",
    "href": "Guide/Stata/25-05-08-1-GLM.html#logistic-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "5 Logistic regression",
    "text": "5 Logistic regression\n\n5.1 Y var\n\nBinary variable\n如何分布? 二项分布(Binomial)\nModel:\n\n\\[ln[P(Y=1)/P(Y=0)]= β_0 + β_1*X_1+\\cdots + β_n*X_n\\]\nExp Y = P(Y=1)\nY 怎么转化才和X成linear关系？Link：logit",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html#poisson-regression",
    "href": "Guide/Stata/25-05-08-1-GLM.html#poisson-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "6 Poisson regression",
    "text": "6 Poisson regression\n\n6.1 Y var\n\nCount variable:整数(1,2,3,.n)\n如何分布? 泊松分布(Poisson)\nModel:\n\n\\[ln[risk of event)]= β_0 + β_1*X_1+\\cdots + β_n*X_n\\]\nY 怎么转化才和X成linear关系？Link：Log",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html#log-binomial-regression",
    "href": "Guide/Stata/25-05-08-1-GLM.html#log-binomial-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "7 Log-binomial Regression",
    "text": "7 Log-binomial Regression\n\n7.1 Y var\n\nBinary variable\n如何分布? 二项分布(Binomial)\nModel:\n\n\\[ln[P(Y=1)]= β_0 + β_1*X_1+\\cdots + β_n*X_n\\]\nExp Y = P(Y=1)\nY 怎么转化才和X成linear关系？Link：Log\n\n\n\n%%stata\nhelp glm\n\n\n[R] glm -- Generalized linear models\n           (View complete PDF manual entry)\n\n\nSyntax\n------\n\n        glm depvar [indepvars] [if] [in] [weight] [, options]\n\n    options                     Description\n    -------------------------------------------------------------------------\n    Model\n      family(familyname)        distribution of depvar; default is\n                                  family(gaussian)\n      link(linkname)            link function; default is canonical link for\n                                  family() specified\n\n    Model 2\n      noconstant                suppress constant term\n      exposure(varname)         include ln(varname) in model with coefficient\n                                  constrained to 1\n      offset(varname)           include varname in model with coefficient\n                                  constrained to 1\n      constraints(constraints)  apply specified linear constraints\n      asis                      retain perfect predictor variables\n      mu(varname)               use varname as the initial estimate for the\n                                  mean of depvar\n      init(varname)             synonym for mu(varname)\n\n    SE/Robust\n      vce(vcetype)              vcetype may be oim, robust, cluster clustvar,\n                                  eim, opg, bootstrap, jackknife, hac kernel,\n                                  jackknife1, or unbiased\n      vfactor(#)                multiply variance matrix by scalar #\n      disp(#)                   quasilikelihood multiplier\n      scale(x2|dev|#)           set the scale parameter\n\n    Reporting\n      level(#)                  set confidence level; default is level(95)\n      eform                     report exponentiated coefficients\n      nocnsreport               do not display constraints\n      display_options           control columns and column formats, row\n                                  spacing, line width, display of omitted\n                                  variables and base and empty cells, and\n                                  factor-variable labeling\n\n    Maximization\n      ml                        use maximum likelihood optimization; the\n                                  default\n      irls                      use iterated, reweighted least-squares\n                                  optimization of the deviance\n      maximize_options          control the maximization process; seldom used\n      fisher(#)                 use the Fisher scoring Hessian or expected\n                                  information matrix (EIM)\n      search                    search for good starting values\n\n      noheader                  suppress header table from above coefficient\n                                  table\n      notable                   suppress coefficient table\n      nodisplay                 suppress the output; iteration log is still\n                                  displayed\n      collinear                 keep collinear variables\n      coeflegend                display legend instead of statistics\n    -------------------------------------------------------------------------\n\n    familyname               Description\n    -------------------------------------------------------------------------\n    gaussian                 Gaussian (normal)\n    igaussian                inverse Gaussian\n    binomial[varnameN|#N]    Bernoulli/binomial\n    poisson                  Poisson\n    nbinomial[#k|ml]         negative binomial\n    gamma                    gamma\n    -------------------------------------------------------------------------\n\n    linkname                 Description\n    -------------------------------------------------------------------------\n    identity                 identity\n    log                      log\n    logit                    logit\n    probit                   probit\n    cloglog                  cloglog\n    power #                  power\n    opower #                 odds power\n    nbinomial                negative binomial\n    loglog                   log-log\n    logc                     log-complement\n    -------------------------------------------------------------------------\n\n    indepvars may contain factor variables; see fvvarlist.\n    depvar and indepvars may contain time-series operators; see tsvarlist.\n    bayes, bootstrap, by, collect, fmm, fp, jackknife, mfp, mi estimate,\n      nestreg, rolling, statsby, stepwise, and svy are allowed; see prefix.\n      For more details, see [BAYES] bayes: glm and [FMM] fmm: glm.\n    vce(bootstrap), vce(jackknife), and vce(jackknife1) are not allowed with\n      the mi estimate prefix.\n    Weights are not allowed with the bootstrap prefix.\n    aweights are not allowed with the jackknife prefix.\n    vce(), vfactor(), disp(), scale(), irls, fisher(), noheader, notable,\n      nodisplay, and weights are not allowed with the svy prefix.\n    fweights, aweights, iweights, and pweights are allowed; see weight.\n    noheader, notable, nodisplay, collinear, and coeflegend do not appear in\n      the dialog box.\n    See [R] glm postestimation for features available after estimation.\n\n\nMenu\n----\n\n    Statistics &gt; Generalized linear models &gt; Generalized linear models (GLM)\n\n\nDescription\n-----------\n\n    glm fits generalized linear models.  It can fit models by using either\n    IRLS (maximum quasilikelihood) or Newton-Raphson (maximum likelihood)\n    optimization, which is the default.\n\n    See [U] 27 Overview of Stata estimation commands for a description of all\n    of Stata's estimation commands, several of which fit models that can also\n    be fit using glm.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Quick start\n\n        Remarks and examples\n\n        Methods and formulas\n\n    The above sections are not included in this help file.\n\n\nOptions\n-------\n\n        +-------+\n    ----+ Model +------------------------------------------------------------\n\n    family(familyname) specifies the distribution of depvar; family(gaussian)\n        is the default.\n\n    link(linkname) specifies the link function; the default is the canonical\n        link for the family() specified (except for family(nbinomial)).\n\n        +---------+\n    ----+ Model 2 +----------------------------------------------------------\n\n    noconstant, exposure(varname), offset(varname), constraints(constraints);\n        see [R] Estimation options.  constraints(constraints) is not allowed\n        with irls.\n\n    asis forces retention of perfect predictor variables and their\n        associated, perfectly predicted observations and may produce\n        instabilities in maximization; see [R] probit.  This option is\n        allowed only with option family(binomial) with a denominator of 1.\n\n    mu(varname) specifies varname as the initial estimate for the mean of \n        depvar.  This option can be useful with models that experience\n        convergence difficulties, such as family(binomial) models with power\n        or odds-power links.  init(varname) is a synonym.\n\n        +-----------+\n    ----+ SE/Robust +--------------------------------------------------------\n\n    vce(vcetype) specifies the type of standard error reported, which\n        includes types that are derived from asymptotic theory (oim, opg),\n        that are robust to some kinds of misspecification (robust), that\n        allow for intragroup correlation (cluster clustvar), and that use\n        bootstrap or jackknife methods (bootstrap, jackknife); see [R]\n        vce_option.\n\n        In addition to the standard vcetypes, glm allows the following\n        alternatives:\n\n        vce(eim) specifies that the EIM estimate of variance be used.\n\n        vce(jackknife1) specifies that the one-step jackknife estimate of\n            variance be used.\n\n        vce(hac kernel [#]) specifies that a heteroskedasticity- and\n            autocorrelation-consistent (HAC) variance estimate be used.  HAC\n            refers to the general form for combining weighted matrices to\n            form the variance estimate.  There are three kernels built into\n            glm.  kernel is a user-written program or one of\n\n                          nwest | gallant | anderson\n\n            # specifies the number of lags.  If # is not specified, N - 2 is\n            assumed.  If you wish to specify vce(hac ... ), you must tsset\n            your data before calling glm.\n\n        vce(unbiased) specifies that the unbiased sandwich estimate of\n            variance be used.\n\n    vfactor(#) specifies a scalar by which to multiply the resulting variance\n        matrix.  This option allows you to match output with other packages,\n        which may apply degrees of freedom or other small-sample corrections\n        to estimates of variance.\n\n    disp(#) multiplies the variance of depvar by # and divides the deviance\n        by #.  The resulting distributions are members of the quasilikelihood\n        family.  This option is allowed only with option irls.\n\n    scale(x2|dev|#) overrides the default scale parameter.  This option is\n        allowed only with Hessian (information matrix) variance estimates.\n\n        By default, scale(1) is assumed for the discrete distributions\n        (binomial, Poisson, and negative binomial), and scale(x2) is assumed\n        for the continuous distributions (Gaussian, gamma, and inverse\n        Gaussian).\n\n        scale(x2) specifies that the scale parameter be set to the Pearson\n        chi-squared (or generalized chi-squared) statistic divided by the\n        residual degrees of freedom, which is recommended by McCullagh and\n        Nelder (1989) as a good general choice for continuous distributions.\n\n        scale(dev) sets the scale parameter to the deviance divided by the\n        residual degrees of freedom.  This option provides an alternative to\n        scale(x2) for continuous distributions and overdispersed or\n        underdispersed discrete distributions.  This option is allowed only\n        with option irls.\n\n        scale(#) sets the scale parameter to #.  For example, using scale(1)\n        in family(gamma) models results in exponential-errors regression.\n        Additional use of link(log) rather than the default link(power -1)\n        for family(gamma) essentially reproduces Stata's streg, dist(exp)\n        nohr command (see [ST] streg) if all the observations are uncensored.\n\n        +-----------+\n    ----+ Reporting +--------------------------------------------------------\n\n    level(#); see [R] Estimation options.\n\n    eform displays the exponentiated coefficients and corresponding standard\n        errors and confidence intervals.  For family(binomial) link(logit)\n        (that is, logistic regression), exponentiation results are odds\n        ratios; for family(nbinomial) link(log) (that is, negative binomial\n        regression) and for family(poisson) link(log) (that is, Poisson\n        regression), exponentiated coefficients are incidence-rate ratios.\n\n    nocnsreport; see [R] Estimation options.\n\n    display_options:  noci, nopvalues, noomitted, vsquish, noemptycells,\n        baselevels, allbaselevels, nofvlabel, fvwrap(#), fvwrapon(style),\n        cformat(%fmt), pformat(%fmt), sformat(%fmt), and nolstretch; see [R]\n        Estimation options.\n\n        +--------------+\n    ----+ Maximization +-----------------------------------------------------\n\n    ml requests that optimization be carried out using Stata's ml commands\n        and is the default.\n\n    irls requests iterated, reweighted least-squares (IRLS) optimization of\n        the deviance instead of Newton-Raphson optimization of the log\n        likelihood.  If the irls option is not specified, the optimization is\n        carried out using Stata's ml commands, in which case all options of\n        ml maximize are also available.\n\n    maximize_options:  difficult, technique(algorithm_spec), iterate(#),\n        [no]log, trace, gradient, showstep, hessian, showtolerance,\n        tolerance(#), ltolerance(#), nrtolerance(#), nonrtolerance, and\n        from(init_specs); see [R] Maximize.  These options are seldom used.\n\n        Setting the optimization method to technique(bhhh) resets the default\n        vcetype to vce(opg).\n\n        If option irls is specified, only maximize_options iterate(), nolog,\n        trace, and ltolerance() are allowed.  With irls specified, the\n        convergence criterion is satisfied when the absolute change in\n        deviance from one iteration to the next is less than or equal to\n        ltolerance(), where ltolerance(1e-6) is the default.\n\n    fisher(#) specifies the number of Newton-Raphson steps that should use\n        the Fisher scoring Hessian or EIM before switching to the observed\n        information matrix (OIM).  This option is useful only for\n        Newton-Raphson optimization (and not when using irls).\n\n    search specifies that the command search for good starting values.  This\n        option is useful only for Newton-Raphson optimization (and not when\n        using irls).\n\n    The following options are available with glm but are not shown in the\n    dialog box:\n\n    noheader suppresses the header information from the output.  The\n        coefficient table is still displayed.\n\n    notable suppresses the table of coefficients from the output.  The header\n        information is still displayed.\n\n    nodisplay suppresses the output.  The iteration log is still displayed.\n\n    collinear, coeflegend; see [R] Estimation options.  collinear is not\n        allowed with irls.\n\n\nRemarks\n-------\n\n    Although glm can be used to fit linear regression (and, in fact, does so\n    by default), this should be viewed as an instructional feature; regress\n    produces such estimates more quickly, and many postestimation commands\n    are available to explore the adequacy of the fit; see [R] regress and [R]\n    regress postestimation.\n\n    In any case, you should specify the link function by using the link()\n    option and specify the distributional family by using family().  The\n    available link functions are\n\n                   Link function            glm option     \n                   ----------------------------------------\n                   identity                 link(identity) \n                   log                      link(log)      \n                   logit                    link(logit)    \n                   probit                   link(probit)   \n                   complementary log-log    link(cloglog)  \n                   odds power               link(opower #) \n                   power                    link(power #)  \n                   negative binomial        link(nbinomial)\n                   log-log                  link(loglog)   \n                   log-complement           link(logc)     \n\n    The available distributional families are\n\n                   Family                 glm option       \n                   ----------------------------------------\n                   Gaussian(normal)       family(gaussian) \n                   inverse Gaussian       family(igaussian)\n                   Bernoulli/binomial     family(binomial) \n                   Poisson                family(poisson)  \n                   negative binomial      family(nbinomial)\n                   gamma                  family(gamma)    \n\n    You do not have to specify both family() and link(); the default link()\n    is the canonical link for the specified family() (except for nbinomial):\n\n                    Family                  Default link  \n                    --------------------------------------\n                    family(gaussian)        link(identity)\n                    family(igaussian)       link(power -2)\n                    family(binomial)        link(logit)   \n                    family(poisson)         link(log)     \n                    family(nbinomial)       link(log)     \n                    family(gamma)           link(power -1)\n\n    If you specify both family() and link(), not all combinations make sense.\n    You may choose from the following combinations:\n\n          | id  log  logit  probit  clog  pow  opower  nbinomial  loglog  logc\n----------+-------------------------------------------------------------------\nGaussian  |  x   x                         x\ninv. Gau. |  x   x                         x\nbinomial  |  x   x     x      x       x    x     x                  x      x\nPoisson   |  x   x                         x\nneg. bin. |  x   x                         x              x\ngamma     |  x   x                         x\n\n\nExamples\n--------\n\n    ---------------------------------------------------------------------------\n    Setup\n        . webuse lbw\n\n    Generalized linear model with Bernoulli family and default logit link\n        . glm low age lwt i.race smoke ptl ht ui, family(binomial)\n\n    Replay results and report exponentiated coefficients\n        . glm, eform\n\n    ---------------------------------------------------------------------------\n    Setup\n        . webuse ldose\n\n    Generalized linear model with binomial family and default logit link\n        . glm r ldose, family(binomial n)\n\n    Generalized linear model with binomial family and cloglog link\n        . glm r ldose, family(binomial n) link(cloglog)\n\n    ---------------------------------------------------------------------------\n    Setup\n        . webuse beetle\n\n    Generalized linear model with binomial family and cloglog link\n        . glm r i.beetle ldose, family(binomial n) link(cloglog)\n\n    Replay results with 99% confidence intervals\n        . glm, level(99)\n    ---------------------------------------------------------------------------\n\n\nStored results\n--------------\n\n    glm, ml stores the following in e():\n\n    Scalars           \n      e(N)                   number of observations\n      e(k)                   number of parameters\n      e(k_eq)                number of equations in e(b)\n      e(k_eq_model)          number of equations in overall model test\n      e(k_dv)                number of dependent variables\n      e(df_m)                model degrees of freedom\n      e(df)                  residual degrees of freedom\n      e(phi)                 scale parameter\n      e(aic)                 model AIC\n      e(bic)                 model BIC\n      e(ll)                  log likelihood, if NR\n      e(N_clust)             number of clusters\n      e(chi2)                chi-squared\n      e(p)                   p-value for model test\n      e(deviance)            deviance\n      e(deviance_s)          scaled deviance\n      e(deviance_p)          Pearson deviance\n      e(deviance_ps)         scaled Pearson deviance\n      e(dispers)             dispersion\n      e(dispers_s)           scaled dispersion\n      e(dispers_p)           Pearson dispersion\n      e(dispers_ps)          scaled Pearson dispersion\n      e(nbml)                1 if negative binomial parameter estimated via\n                               ML, 0 otherwise\n      e(vf)                  factor set by vfactor(), 1 if not set\n      e(power)               power set by link(power #) or link(opower #)\n      e(rank)                rank of e(V)\n      e(ic)                  number of iterations\n      e(rc)                  return code\n      e(converged)           1 if converged, 0 otherwise\n\n    Macros            \n      e(cmd)                 glm\n      e(cmdline)             command as typed\n      e(depvar)              name of dependent variable\n      e(varfunc)             program to calculate variance function\n      e(varfunct)            variance title\n      e(varfuncf)            variance function\n      e(link)                program to calculate link function\n      e(linkt)               link title\n      e(linkf)               link function\n      e(m)                   number of binomial trials\n      e(wtype)               weight type\n      e(wexp)                weight expression\n      e(title)               title in estimation output\n      e(clustvar)            name of cluster variable\n      e(offset)              linear offset variable\n      e(chi2type)            Wald; type of model chi-squared test\n      e(cons)                noconstant, if specified\n      e(hac_kernel)          HAC kernel\n      e(hac_lag)             HAC lag\n      e(vce)                 vcetype specified in vce()\n      e(vcetype)             title used to label Std. err.\n      e(opt)                 ml or irls\n      e(opt1)                optimization title, line 1\n      e(opt2)                optimization title, line 2\n      e(which)               max or min; whether optimizer is to perform\n                               maximization or minimization\n      e(ml_method)           type of ml method\n      e(user)                name of likelihood-evaluator program\n      e(technique)           maximization technique\n      e(properties)          b V\n      e(predict)             program used to implement predict\n      e(marginsok)           predictions allowed by margins\n      e(marginsnotok)        predictions disallowed by margins\n      e(asbalanced)          factor variables fvset as asbalanced\n      e(asobserved)          factor variables fvset as asobserved\n\n    Matrices          \n      e(b)                   coefficient vector\n      e(Cns)                 constraints matrix\n      e(ilog)                iteration log (up to 20 iterations)\n      e(gradient)            gradient vector\n      e(V)                   variance-covariance matrix of the estimators\n      e(V_modelbased)        model-based variance\n\n    Functions         \n      e(sample)              marks estimation sample\n\n    In addition to the above, the following is stored in r():\n\n    Matrices          \n      r(table)               matrix containing the coefficients with their\n                               standard errors, test statistics, p-values,\n                               and confidence intervals\n\n    Note that results stored in r() are updated when the command is replayed\n    and will be replaced when any r-class command is run after the estimation\n    command.\n\n    glm, irls stores the following in e():\n\n    Scalars           \n      e(N)                   number of observations\n      e(k)                   number of parameters\n      e(k_eq_model)          number of equations in overall model test\n      e(df_m)                model degrees of freedom\n      e(df)                  residual degrees of freedom\n      e(phi)                 scale parameter\n      e(disp)                dispersion parameter\n      e(bic)                 model BIC\n      e(N_clust)             number of clusters\n      e(deviance)            deviance\n      e(deviance_s)          scaled deviance\n      e(deviance_p)          Pearson deviance\n      e(deviance_ps)         scaled Pearson deviance\n      e(dispers)             dispersion\n      e(dispers_s)           scaled dispersion\n      e(dispers_p)           Pearson dispersion\n      e(dispers_ps)          scaled Pearson dispersion\n      e(nbml)                1 if negative binomial parameter estimated via\n                               ML, 0 otherwise\n      e(vf)                  factor set by vfactor(), 1 if not set\n      e(power)               power set by link(power #) or link(opower #)\n      e(rank)                rank of e(V)\n      e(rc)                  return code\n\n    Macros            \n      e(cmd)                 glm\n      e(cmdline)             command as typed\n      e(depvar)              name of dependent variable\n      e(varfunc)             program to calculate variance function\n      e(varfunct)            variance title\n      e(varfuncf)            variance function\n      e(link)                program to calculate link function\n      e(linkt)               link title\n      e(linkf)               link function\n      e(m)                   number of binomial trials\n      e(wtype)               weight type\n      e(wexp)                weight expression\n      e(clustvar)            name of cluster variable\n      e(offset)              linear offset variable\n      e(cons)                noconstant, if specified\n      e(hac_kernel)          HAC kernel\n      e(hac_lag)             HAC lag\n      e(vce)                 vcetype specified in vce()\n      e(vcetype)             title used to label Std. err.\n      e(opt)                 ml or irls\n      e(opt1)                optimization title, line 1\n      e(opt2)                optimization title, line 2\n      e(properties)          b V\n      e(predict)             program used to implement predict\n      e(marginsok)           predictions allowed by margins\n      e(marginsnotok)        predictions disallowed by margins\n      e(asbalanced)          factor variables fvset as asbalanced\n      e(asobserved)          factor variables fvset as asobserved\n\n    Matrices          \n      e(b)                   coefficient vector\n      e(V)                   variance-covariance matrix of the estimators\n      e(V_modelbased)        model-based variance\n\n    Functions         \n      e(sample)              marks estimation sample\n\n    In addition to the above, the following is stored in r():\n\n    Matrices          \n      r(table)               matrix containing the coefficients with their\n                               standard errors, test statistics, p-values,\n                               and confidence intervals\n\n    Note that results stored in r() are updated when the command is replayed\n    and will be replaced when any r-class command is run after the estimation\n    command.\n\n\nReference\n---------\n\n    McCullagh, P., and J. A. Nelder. 1989.  Generalized Linear Models. 2nd\n        ed.  London: Chapman & Hall/CRC.\n\n\nglm command in Stata - Umbrella Command - linear regression: family(gaussian) link(identity) - logistic regression: family(binomial) link(logit) - poisson regression: family(poisson) link(log) - log-binomial regression: family(binomial) link(log)\n\n加粗部分表示，在实际代码中可以简写为加粗的字母或单词即可",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html#代码的转换",
    "href": "Guide/Stata/25-05-08-1-GLM.html#代码的转换",
    "title": "14-广义线性模型（GLM）",
    "section": "8 代码的转换",
    "text": "8 代码的转换\n\n8.1 Linear regression:\n\nregress price weight length mpg i.rep78\nglm price weight length mpg i.rep78, family(gaussian)link(identity)\n\n\n\n8.2 Logistic regression:\n\nlogistic low age i.smoke i.race\nglm low age i.smoke i.race,family(binomial) link(logit)\nglm low age i.smoke i.race,family(binomial) link(logit) eform\n\n\neform 直接输出 OR 值，即 \\(e^{\\beta}\\)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html#线性回归模型的比较",
    "href": "Guide/Stata/25-05-08-1-GLM.html#线性回归模型的比较",
    "title": "14-广义线性模型（GLM）",
    "section": "9 线性回归模型的比较",
    "text": "9 线性回归模型的比较\n\n%%stata\nregress price weight length mpg i.rep78\n\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(7, 61)        =      7.25\n       Model |   262008114         7  37429730.6   Prob &gt; F        =    0.0000\n    Residual |   314788844        61  5160472.86   R-squared       =    0.4542\n-------------+----------------------------------   Adj R-squared   =    0.3916\n       Total |   576796959        68  8482308.22   Root MSE        =    2271.7\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   5.186695   1.163383     4.46   0.000     2.860367    7.513022\n      length |  -124.1544   40.07637    -3.10   0.003     -204.292   -44.01671\n         mpg |  -126.8367   84.49819    -1.50   0.138    -295.8012    42.12791\n             |\n       rep78 |\n          2  |   1137.284   1803.332     0.63   0.531    -2468.701    4743.269\n          3  |   1254.642   1661.545     0.76   0.453    -2067.823    4577.108\n          4  |   2267.188   1698.018     1.34   0.187    -1128.208    5662.584\n          5  |   3850.759   1787.272     2.15   0.035     276.8886     7424.63\n             |\n       _cons |   14614.49   6155.842     2.37   0.021     2305.125    26923.86\n------------------------------------------------------------------------------\n\n\n\n%%stata\nglm price weight length mpg i.rep78, family(gaussian)link(identity)\n\n\nIteration 0:  Log likelihood = -626.90582  \n\nGeneralized linear models                         Number of obs   =         69\nOptimization     : ML                             Residual df     =         61\n                                                  Scale parameter =    5160473\nDeviance         =  314788844.4                   (1/df) Deviance =    5160473\nPearson          =  314788844.4                   (1/df) Pearson  =    5160473\n\nVariance function: V(u) = 1                       [Gaussian]\nLink function    : g(u) = u                       [Identity]\n\n                                                  AIC             =   18.40307\nLog likelihood   = -626.9058204                   BIC             =   3.15e+08\n\n------------------------------------------------------------------------------\n             |                 OIM\n       price | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   5.186695   1.163383     4.46   0.000     2.906506    7.466883\n      length |  -124.1544   40.07637    -3.10   0.002    -202.7026   -45.60612\n         mpg |  -126.8367   84.49819    -1.50   0.133    -292.4501    38.77675\n             |\n       rep78 |\n          2  |   1137.284   1803.332     0.63   0.528    -2397.182     4671.75\n          3  |   1254.642   1661.545     0.76   0.450    -2001.927    4511.211\n          4  |   2267.188   1698.018     1.34   0.182    -1060.866    5595.241\n          5  |   3850.759   1787.272     2.15   0.031     347.7711    7353.747\n             |\n       _cons |   14614.49   6155.842     2.37   0.018     2549.263    26679.72\n------------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html#logistic-回归的比较",
    "href": "Guide/Stata/25-05-08-1-GLM.html#logistic-回归的比较",
    "title": "14-广义线性模型（GLM）",
    "section": "10 logistic 回归的比较",
    "text": "10 logistic 回归的比较\n重新导入数据：\n\n%%stata\nwebuse lbw,clear\n\n(Hosmer & Lemeshow data)\n\n\n\n%%stata\nlogistic low age i.smoke i.race\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(4)    =  15.81\n                                                        Prob &gt; chi2   = 0.0033\nLog likelihood = -109.4311                              Pseudo R2     = 0.0674\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9657186   .0322573    -1.04   0.296     .9045206    1.031057\n             |\n       smoke |\n     Smoker  |    3.00582   1.118001     2.96   0.003     1.449982    6.231081\n             |\n        race |\n      Black  |   2.749483   1.356659     2.05   0.040     1.045318    7.231924\n      Other  |   2.876948   1.167921     2.60   0.009     1.298314    6.375062\n             |\n       _cons |    .365111   .3146026    -1.17   0.242     .0674491    1.976395\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\n%%stata\nglm low age i.smoke i.race,family(binomial) link(logit) eform\n\n\nIteration 0:  Log likelihood = -109.53148  \nIteration 1:  Log likelihood = -109.43111  \nIteration 2:  Log likelihood =  -109.4311  \nIteration 3:  Log likelihood =  -109.4311  \n\nGeneralized linear models                         Number of obs   =        189\nOptimization     : ML                             Residual df     =        184\n                                                  Scale parameter =          1\nDeviance         =  218.8621974                   (1/df) Deviance =   1.189468\nPearson          =  182.9642078                   (1/df) Pearson  =   .9943707\n\nVariance function: V(u) = u*(1-u)                 [Bernoulli]\nLink function    : g(u) = ln(u/(1-u))             [Logit]\n\n                                                  AIC             =   1.210911\nLog likelihood   = -109.4310987                   BIC             =  -745.6193\n\n------------------------------------------------------------------------------\n             |                 OIM\n         low | Odds ratio   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9657186   .0322573    -1.04   0.296     .9045206    1.031057\n             |\n       smoke |\n     Smoker  |    3.00582   1.118001     2.96   0.003     1.449982    6.231081\n             |\n        race |\n      Black  |   2.749483   1.356659     2.05   0.040     1.045318    7.231924\n      Other  |   2.876948   1.167921     2.60   0.009     1.298314    6.375062\n             |\n       _cons |    .365111   .3146026    -1.17   0.242     .0674491    1.976395\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-1-GLM.html#log-binomial-model",
    "href": "Guide/Stata/25-05-08-1-GLM.html#log-binomial-model",
    "title": "14-广义线性模型（GLM）",
    "section": "11 Log-Binomial Model",
    "text": "11 Log-Binomial Model\nglm low age i.smoke i.race,family(binomial) link(log)\n\nLogistic regression 的一种替代\nFail to converge\nPoisson regression with robust variance estimate\n\n为了避免出现 Fail to converge 的问题，采用稳健方差估计：\nglm low age i.smoke i.race,family(poisson) link(log) robust\n\nrobust 可以缩写为 r\n\n\n%%stata\nglm low age i.smoke i.race,family(poisson) link(log) robust\n\n\nIteration 0:  Log pseudolikelihood = -124.55888  \nIteration 1:  Log pseudolikelihood = -122.39663  \nIteration 2:  Log pseudolikelihood = -122.39591  \nIteration 3:  Log pseudolikelihood = -122.39591  \n\nGeneralized linear models                         Number of obs   =        189\nOptimization     : ML                             Residual df     =        184\n                                                  Scale parameter =          1\nDeviance         =   126.791811                   (1/df) Deviance =   .6890859\nPearson          =  124.4629927                   (1/df) Pearson  =   .6764293\n\nVariance function: V(u) = u                       [Poisson]\nLink function    : g(u) = ln(u)                   [Log]\n\n                                                  AIC             =   1.348105\nLog pseudolikelihood = -122.3959055               BIC             =  -837.6896\n\n------------------------------------------------------------------------------\n             |               Robust\n         low | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |  -.0246781   .0203034    -1.22   0.224    -.0644722    .0151159\n             |\n       smoke |\n     Smoker  |   .6972155    .211848     3.29   0.001     .2820011     1.11243\n             |\n        race |\n      Black  |    .639629   .2818353     2.27   0.023     .0872419    1.192016\n      Other  |   .6813692   .2428128     2.81   0.005     .2054649    1.157273\n             |\n       _cons |  -1.286544   .5405667    -2.38   0.017    -2.346035   -.2270526\n------------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-3-multi-reg.html",
    "href": "Guide/Stata/25-05-07-3-multi-reg.html",
    "title": "12-多元线性回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-3-multi-reg.html#导入数据",
    "href": "Guide/Stata/25-05-07-3-multi-reg.html#导入数据",
    "title": "12-多元线性回归",
    "section": "1 导入数据",
    "text": "1 导入数据\n\n%%stata\nsysuse auto.dta,clear\n\n(1978 automobile data)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-3-multi-reg.html#区分简单线性回归和多元线性回归",
    "href": "Guide/Stata/25-05-07-3-multi-reg.html#区分简单线性回归和多元线性回归",
    "title": "12-多元线性回归",
    "section": "2 区分简单线性回归和多元线性回归",
    "text": "2 区分简单线性回归和多元线性回归\n简单线性回归形如：\\(y=\\beta_0+\\beta_1 x\\)，例如：\n\n%%stata\nreg price weight\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(1, 72)        =     29.42\n       Model |   184233937         1   184233937   Prob &gt; F        =    0.0000\n    Residual |   450831459        72  6261548.04   R-squared       =    0.2901\n-------------+----------------------------------   Adj R-squared   =    0.2802\n       Total |   635065396        73  8699525.97   Root MSE        =    2502.3\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   2.044063   .3768341     5.42   0.000     1.292857    2.795268\n       _cons |  -6.707353    1174.43    -0.01   0.995     -2347.89    2334.475\n------------------------------------------------------------------------------\n\n\n\\(\\beta_1\\)：汽车的重量每增加一个单位，售价增加 2.04 个单位(95%CI:1.29,2.80)\n_cons 是 \\(\\beta_0\\)，即是截距，但是要注意符号与实际的区别\n多元线性模型即不止一个自变量，形如：\\(y=\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 + \\dots\\)\n\n%%stata\nreg price weight length\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(2, 71)        =     18.91\n       Model |   220725280         2   110362640   Prob &gt; F        =    0.0000\n    Residual |   414340116        71  5835776.28   R-squared       =    0.3476\n-------------+----------------------------------   Adj R-squared   =    0.3292\n       Total |   635065396        73  8699525.97   Root MSE        =    2415.7\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   4.699065   1.122339     4.19   0.000     2.461184    6.936946\n      length |  -97.96031    39.1746    -2.50   0.015    -176.0722   -19.84838\n       _cons |   10386.54   4308.159     2.41   0.019     1796.316    18976.76\n------------------------------------------------------------------------------\n\n\n\\(\\beta_2\\)(length)：在控制 weight 变量后，汽车的 length 每增加一个单位，售价减少 97.96 个单位(95%CI:-176.07,2.-19.85)\n\n%%stata\nreg price weight length mpg\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(3, 70)        =     12.98\n       Model |   226957412         3  75652470.6   Prob &gt; F        =    0.0000\n    Residual |   408107984        70  5830114.06   R-squared       =    0.3574\n-------------+----------------------------------   Adj R-squared   =    0.3298\n       Total |   635065396        73  8699525.97   Root MSE        =    2414.6\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   4.364798   1.167455     3.74   0.000     2.036383    6.693213\n      length |  -104.8682   39.72154    -2.64   0.010    -184.0903   -25.64607\n         mpg |  -86.78928   83.94335    -1.03   0.305     -254.209    80.63046\n       _cons |   14542.43   5890.632     2.47   0.016      2793.94    26290.93\n------------------------------------------------------------------------------\n\n\n\n%%stata\nreg price weight length mpg rep78\n\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(4, 64)        =     12.68\n       Model |   255066807         4  63766701.9   Prob &gt; F        =    0.0000\n    Residual |   321730151        64  5027033.62   R-squared       =    0.4422\n-------------+----------------------------------   Adj R-squared   =    0.4074\n       Total |   576796959        68  8482308.22   Root MSE        =    2242.1\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   4.959534   1.119624     4.43   0.000     2.722827    7.196241\n      length |  -115.0177   38.56456    -2.98   0.004    -192.0592   -37.97612\n         mpg |  -106.7122   81.15836    -1.31   0.193    -268.8446    55.42027\n       rep78 |   910.9859   304.5274     2.99   0.004     302.6226    1519.349\n       _cons |   11934.51   5774.178     2.07   0.043     399.2604    23469.75\n------------------------------------------------------------------------------\n\n\n\\(\\beta_4\\) (rep78): 在控制了汽车的重量、长度、里程后，汽车的修理次数每增加一个单位，售价增加910.99个单位(95%CI:302.62,1519.35)\n这里 rep78 是一个连续变量，如果处理为多分类变量（哑变量）\n\n2.1 什么是哑变量？为什么要设置哑变量？\n当自变量X为多分类变量时，例如职业、学历、血型、疾病严重程度等等此时仅用一个回归系数来解释多分类变量之间的变化关系，及其对因变量Y的影响，就显得太不理想。\n\n\n2.2 如何设置哑变量\n\n在多分类变量前加上 “i.” 告诉Stata这不是连续变量\n形如：\n\n\n%%stata\nreg price weight length mpg i.rep78\n\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(7, 61)        =      7.25\n       Model |   262008114         7  37429730.6   Prob &gt; F        =    0.0000\n    Residual |   314788844        61  5160472.86   R-squared       =    0.4542\n-------------+----------------------------------   Adj R-squared   =    0.3916\n       Total |   576796959        68  8482308.22   Root MSE        =    2271.7\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   5.186695   1.163383     4.46   0.000     2.860367    7.513022\n      length |  -124.1544   40.07637    -3.10   0.003     -204.292   -44.01671\n         mpg |  -126.8367   84.49819    -1.50   0.138    -295.8012    42.12791\n             |\n       rep78 |\n          2  |   1137.284   1803.332     0.63   0.531    -2468.701    4743.269\n          3  |   1254.642   1661.545     0.76   0.453    -2067.823    4577.108\n          4  |   2267.188   1698.018     1.34   0.187    -1128.208    5662.584\n          5  |   3850.759   1787.272     2.15   0.035     276.8886     7424.63\n             |\n       _cons |   14614.49   6155.842     2.37   0.021     2305.125    26923.86\n------------------------------------------------------------------------------\n\n\nrep78 2:在控制了汽车的重量、长度、里程后，汽 车的修理次数两次和一次相比，售价增加1137.28个单 位(95%CI:-2468.70,4743.27)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-3-multi-reg.html#回归分析需要注意的事情",
    "href": "Guide/Stata/25-05-07-3-multi-reg.html#回归分析需要注意的事情",
    "title": "12-多元线性回归",
    "section": "3 回归分析需要注意的事情",
    "text": "3 回归分析需要注意的事情\n\n时刻注意纳入模型的观测值数量\n\nCase-wise Deletion：只有纳入模型的变量都没有缺失值，这个观测值才会被纳入回归模型分析（有缺失会被系统默认剔除）\n\n如何让\\(\\beta_0\\)有意义？\n\n使用\\(y=\\beta_0+\\beta_1(x-\\bar x)\\)\n让\\(\\beta_0\\geq 0\\)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-1-ANOVA.html",
    "href": "Guide/Stata/25-05-07-1-ANOVA.html",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-1-ANOVA.html#方差分析的假设",
    "href": "Guide/Stata/25-05-07-1-ANOVA.html#方差分析的假设",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.1 方差分析的假设",
    "text": "1.1 方差分析的假设\n\n假设1:y变量为连续变量\n假设2:有一个包含2个及以上分类、且组别间相互独立的x变量\n假设3:每组间和组内的观测值相互独立\n假设4:每组内没有明显异常值\n假设5:每组内y变量符合正态分布\n假设6:进行方差齐性检验，观察每组的方差是否相等\n\n总结为：独立、正态、方差齐",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-1-ANOVA.html#数据导入",
    "href": "Guide/Stata/25-05-07-1-ANOVA.html#数据导入",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.2 数据导入",
    "text": "1.2 数据导入\n\n%%stata\nwebuse systolic,clear\n\n(Systolic blood pressure data)\n\n\n\n1.2.1 查看一下数据形式与结构\n\n%%stata\nlist\n\n\n     +---------------------------+\n     | drug   disease   systolic |\n     |---------------------------|\n  1. |    1         1         42 |\n  2. |    1         1         44 |\n  3. |    1         1         36 |\n  4. |    1         1         13 |\n  5. |    1         1         19 |\n     |---------------------------|\n  6. |    1         1         22 |\n  7. |    1         2         33 |\n  8. |    1         2         26 |\n  9. |    1         2         33 |\n 10. |    1         2         21 |\n     |---------------------------|\n 11. |    1         3         31 |\n 12. |    1         3         -3 |\n 13. |    1         3         25 |\n 14. |    1         3         25 |\n 15. |    1         3         24 |\n     |---------------------------|\n 16. |    2         1         28 |\n 17. |    2         1         23 |\n 18. |    2         1         34 |\n 19. |    2         1         42 |\n 20. |    2         1         13 |\n     |---------------------------|\n 21. |    2         2         34 |\n 22. |    2         2         33 |\n 23. |    2         2         31 |\n 24. |    2         2         36 |\n 25. |    2         3          3 |\n     |---------------------------|\n 26. |    2         3         26 |\n 27. |    2         3         28 |\n 28. |    2         3         32 |\n 29. |    2         3          4 |\n 30. |    2         3         16 |\n     |---------------------------|\n 31. |    3         1          1 |\n 32. |    3         1         29 |\n 33. |    3         1         19 |\n 34. |    3         2         11 |\n 35. |    3         2          9 |\n     |---------------------------|\n 36. |    3         2          7 |\n 37. |    3         2          1 |\n 38. |    3         2         -6 |\n 39. |    3         3         21 |\n 40. |    3         3          1 |\n     |---------------------------|\n 41. |    3         3          9 |\n 42. |    3         3          3 |\n 43. |    4         1         24 |\n 44. |    4         1          9 |\n 45. |    4         1         22 |\n     |---------------------------|\n 46. |    4         1         -2 |\n 47. |    4         1         15 |\n 48. |    4         2         27 |\n 49. |    4         2         12 |\n 50. |    4         2         12 |\n     |---------------------------|\n 51. |    4         2         -5 |\n 52. |    4         2         16 |\n 53. |    4         2         15 |\n 54. |    4         3         22 |\n 55. |    4         3          7 |\n     |---------------------------|\n 56. |    4         3         25 |\n 57. |    4         3          5 |\n 58. |    4         3         12 |\n     +---------------------------+\n\n\n\n%%stata\ncodebook drug\n\n\n-------------------------------------------------------------------------------\ndrug                                                                  Drug used\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [1,4]                         Units: 1\n         Unique values: 4                         Missing .: 0/58\n\n            Tabulation: Freq.  Value\n                           15  1\n                           15  2\n                           12  3\n                           16  4\n\n\n\n%%stata\nsum systolic,detail\n\n\n                 Increment in systolic b.p.\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%           -6             -6\n 5%           -3             -5\n10%            1             -3       Obs                  58\n25%            9             -2       Sum of wgt.          58\n\n50%           21                      Mean           18.87931\n                        Largest       Std. dev.      12.80087\n75%           28             36\n90%           34             42       Variance       163.8624\n95%           42             42       Skewness       -.094992\n99%           44             44       Kurtosis        2.13251",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-1-ANOVA.html#假设4每组内没有明显的异常值",
    "href": "Guide/Stata/25-05-07-1-ANOVA.html#假设4每组内没有明显的异常值",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.3 假设4：每组内没有明显的异常值",
    "text": "1.3 假设4：每组内没有明显的异常值\n最直观的方式就是用箱型图进行展示数据分布\n\n%%stata\ngraph box systolic,over(drug)\n\n\n\n\n\n\n\n\n\n%%stata\ngraph box systolic",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-1-ANOVA.html#假设5每组内y变量符合正态分布",
    "href": "Guide/Stata/25-05-07-1-ANOVA.html#假设5每组内y变量符合正态分布",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.4 假设5：每组内y变量符合正态分布",
    "text": "1.4 假设5：每组内y变量符合正态分布\n一般使用下面三种方法中的一种检验正态性就可以，一般使用 Shapiro-Wilk检验 较为普遍。\n\n1.4.1 偏度与峰度\n使用 sktest 检验偏度与峰度，P值大于0.05，就不能说不符合正态分布\n\n%%stata\nsktest systolic\n\n\nSkewness and kurtosis tests for normality\n                                                         ----- Joint test -----\n    Variable |       Obs   Pr(skewness)   Pr(kurtosis)   Adj chi2(2)  Prob&gt;chi2\n-------------+-----------------------------------------------------------------\n    systolic |        58         0.7452         0.0529          4.03     0.1331\n\n\n\n\n1.4.2 Shapiro-Wilk W test\nShapiro-Wilk检验，是一种基于相关性的算法。计算可得到一个相关系数，它越接近1就越表明数据和正态分布拟合得越好。\n它基于Shapiro和Wilk于1965年提出的检验统计量。以下是其基本原理和用途：\n基本原理：\n\n零假设（Null Hypothesis）：Shapiro-Wilk检验的零假设是数据集来自于正态分布。这意味着，如果数据确实服从正态分布，则零假设成立。\n计算Shapiro-Wilk统计量：检验首先计算Shapiro-Wilk统计量，这是一个衡量数据与正态分布拟合的度量。该统计量基于数据的观察值和正态分布的期望值之间的差异。//Shapiro-Wilk检验的统计量（W统计量）是通过与理论正态分布的期望值进行比较来判断样本数据是否符合正态分布。Shapiro-Wilk检验的原假设是样本数据符合正态分布。统计量的计算基于样本数据的排序顺序和回归分析的概念。W统计量越接近1，表示样本数据越接近正态分布。\n与临界值比较：接下来，Shapiro-Wilk统计量与临界值进行比较。临界值是根据所选的显著性水平（通常为5%）和数据集的大小计算得出的。如果Shapiro-Wilk统计量小于临界值，就意味着数据不太可能来自于正态分布。\n做出决策：根据统计量与临界值的比较，可以决定是否拒绝零假设。如果统计量足够小，小于临界值，通常会拒绝零假设，这意味着数据不服从正态分布。否则，不能拒绝零假设，这表示数据可能服从正态分布。\n\nNotices： Shapiro-Wilk正态性检验对检验样本大小有一定的要求。具体来说，Shapiro-Wilk检验在样本大小较小（通常小于大约50-200，具体取决于不同文献和实践）时可能不太适用，并且在这种情况下其效力可能会降低。\n\n%%stata\nswilk systolic\n\n\n                   Shapiro–Wilk W test for normal data\n\n    Variable |        Obs       W           V         z       Prob&gt;z\n-------------+------------------------------------------------------\n    systolic |         58    0.97803      1.162     0.323    0.37331\n\n\nShapiro-Francia test\nShapiro-Francia检验是一种用于检验数据是否来自正态分布的统计方法。它是Shapiro-Wilk检验的一个变种，通常适用于小到中等样本大小的数据集。Shapiro-Francia检验的核心思想是通过计算统计量来评估数据的正态性。\nShapiro-Francia检验的零假设是数据来自正态分布，而备择假设是数据不来自正态分布。检验的结果会生成一个p-value，如果p-value较小（通常小于0.05），则通常会拒绝零假设，表明数据不符合正态分布。如果p-value较大，则无法拒绝零假设，表明数据可能来自正态分布。\n这种检验方法对于小到中等样本大小的数据集通常效果良好，并且可以用于确定数据是否符合正态分布的假设。\n虽然Shapiro-Francia检验在小样本中通常效果较好，但它也依赖于权重参数的准确性，因此在某些情况下可能不如其他正态性检验方法稳健。因此，在使用Shapiro-Francia检验时，应谨慎选择合适的样本大小和检验方法，以确保可靠的结果。\n\n%%stata\nsfrancia systolic\n\n\n                  Shapiro–Francia W' test for normal data\n\n    Variable |       Obs       W'          V'        z       Prob&gt;z\n-------------+-----------------------------------------------------\n    systolic |        58    0.98522      0.866    -0.275    0.60824",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-1-ANOVA.html#单因素方差分析",
    "href": "Guide/Stata/25-05-07-1-ANOVA.html#单因素方差分析",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.5 单因素方差分析",
    "text": "1.5 单因素方差分析\n语法：\noneway yvar xvar,[,option]\n如果得到的 Prob&gt;F 结果是 &gt;0.05 则说明至少有两个组的平均值是有显著差别的。\nProb&gt;chi2 = 得到的结果如果 &gt;0.05 则说明各组满足方差齐性。\n\n%%stata\noneway systolic drug\n\n\n                        Analysis of variance\n    Source              SS         df      MS            F     Prob &gt; F\n------------------------------------------------------------------------\nBetween groups      3133.23851      3   1044.41284      9.09     0.0001\n Within groups      6206.91667     54   114.942901\n------------------------------------------------------------------------\n    Total           9340.15517     57   163.862371\n\nBartlett's equal-variances test: chi2(3) =   1.0063    Prob&gt;chi2 = 0.800\n\n\n\n1.5.1 使用两两比较\n一般使用 bonferroni 比较\n\n%%stata\noneway systolic drug, bonferroni \n\n\n                        Analysis of variance\n    Source              SS         df      MS            F     Prob &gt; F\n------------------------------------------------------------------------\nBetween groups      3133.23851      3   1044.41284      9.09     0.0001\n Within groups      6206.91667     54   114.942901\n------------------------------------------------------------------------\n    Total           9340.15517     57   163.862371\n\nBartlett's equal-variances test: chi2(3) =   1.0063    Prob&gt;chi2 = 0.800\n\n            Comparison of Increment in systolic b.p. by Drug used\n                                (Bonferroni)\nRow Mean-|\nCol Mean |          1          2          3\n---------+---------------------------------\n       2 |   -.533333\n         |      1.000\n         |\n       3 |   -17.3167   -16.7833\n         |      0.001      0.001\n         |\n       4 |   -12.5667   -12.0333       4.75\n         |      0.012      0.017      1.000\n\n\n\n%%stata\noneway systolic drug, bonferroni tab\n\n\n            |  Summary of Increment in systolic\n            |                b.p.\n  Drug used |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n          1 |   26.066667   11.677002          15\n          2 |   25.533333    11.61813          15\n          3 |        8.75     10.0193          12\n          4 |        13.5   9.3238047          16\n------------+------------------------------------\n      Total |    18.87931   12.800874          58\n\n                        Analysis of variance\n    Source              SS         df      MS            F     Prob &gt; F\n------------------------------------------------------------------------\nBetween groups      3133.23851      3   1044.41284      9.09     0.0001\n Within groups      6206.91667     54   114.942901\n------------------------------------------------------------------------\n    Total           9340.15517     57   163.862371\n\nBartlett's equal-variances test: chi2(3) =   1.0063    Prob&gt;chi2 = 0.800\n\n            Comparison of Increment in systolic b.p. by Drug used\n                                (Bonferroni)\nRow Mean-|\nCol Mean |          1          2          3\n---------+---------------------------------\n       2 |   -.533333\n         |      1.000\n         |\n       3 |   -17.3167   -16.7833\n         |      0.001      0.001\n         |\n       4 |   -12.5667   -12.0333       4.75\n         |      0.012      0.017      1.000",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-2-simple-line-reg.html",
    "href": "Guide/Stata/25-05-07-2-simple-line-reg.html",
    "title": "11-简单线性回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-2-simple-line-reg.html#导入数据",
    "href": "Guide/Stata/25-05-07-2-simple-line-reg.html#导入数据",
    "title": "11-简单线性回归",
    "section": "1 导入数据",
    "text": "1 导入数据\n\n%%stata\nsysuse auto.dta,clear\n\n(1978 automobile data)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-2-simple-line-reg.html#线性回归的假设",
    "href": "Guide/Stata/25-05-07-2-simple-line-reg.html#线性回归的假设",
    "title": "11-简单线性回归",
    "section": "2 线性回归的假设",
    "text": "2 线性回归的假设\n\n假设1:y是连续变量\n假设2:x可以被定义为连续变量（也可以是哑变量）\n假设3:y和x之间存在线性关系\n假设4:具有相互独立的观测值\n假设5:不存在显著的outlier\n假设6:等方差性\n假设7:residual近似正态分布\n\n总结而言：线性、独立、正态、方差齐\n\n2.1 假设3:y和x之间存在线性关系\n通过 scatter plot 或 Lowess plot 进行查看\n如果不符合线性关系怎么办？\n\nspline：一次方项，分段fit直线\nQuadratic：二次方项\nCubic：三次方项\nRestricted cubic：三次方项（头尾近乎直线）\n\n\n%%stata\ntwoway scatter mpg weight\n\n\n\n\n\n\n\n\n\n%%stata\ntwoway lowess mpg weight\n\n\n\n\n\n\n\n\nscatter plot和lowess plot输出在一起\n\n%%stata\nlowess mpg weight\n\n\n\n\n\n\n\n\n\n%%stata\ntwoway (scatter mpg weight)(lowess mpg weight)\n\n\n\n\n\n\n\n\n\n\n2.2 假设4:具有相互独立的观测值\n\n可以使用杜宾-瓦特森(Durbin-Watson)统计量\nStata对于非time series数据不设有这个统计量的检测\n更多情况下，不需要测量这个假设是否成立\n如果是相互关联的观测值：GEE模型、Multi-level模型\n\n\n\n2.3 假设5:不存在显著的outlier\n\nBoxplot 或 Violin Plot\n\n\n%%stata\ngraph box mpg\n\n\n\n\n\n\n\n\n\n%%stata\nvioplot mpg\n\n\n\n\n\n\n\n\n\n\n2.4 假设6:等方差性\n\n使用 Residual-versus-fitted plot\n代码:rvfplot\n\n做等方差之前需要做回归的分析，分析及结果如下：\n\n%%stata\nreg mpg weight\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(1, 72)        =    134.62\n       Model |   1591.9902         1   1591.9902   Prob &gt; F        =    0.0000\n    Residual |  851.469256        72  11.8259619   R-squared       =    0.6515\n-------------+----------------------------------   Adj R-squared   =    0.6467\n       Total |  2443.45946        73  33.4720474   Root MSE        =    3.4389\n\n------------------------------------------------------------------------------\n         mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |  -.0060087   .0005179   -11.60   0.000    -.0070411   -.0049763\n       _cons |   39.44028   1.614003    24.44   0.000     36.22283    42.65774\n------------------------------------------------------------------------------\n\n\n\n%%stata\nrvfplot\n\n\n\n\n\n\n\n\n从上图可以看出，并不是完全等方差，特别是在 Fitted value 在 25-30 区间内时\n\n\n2.5 假设7:residual近似正态分布\n\nStep 1:得到残差\nStep 2:使用正态分布的检验方法\n\n直方图\n使用 qq plot 观测\n偏度峰度、Shapiro-Wilk检验、Shapiro-Francia检验\n\n\n先建立一个 resid 变量：\n\n%%stata\npredict resid,residual\n\n再对 resid 变量进行查看和检验\n下面分别是 直方图 和 Q-Q图\n\n%%stata\nhist resid\n\n(bin=8, start=-6.9593482, width=2.5970982)\n\n\n\n\n\n\n\n\n\n\n%%stata\n//qq plot\nqnorm resid\n\n\n. //qq plot\n. qnorm resid\n\n. \n\n\n\n\n\n\n\n\n\n\n\n2.6 对残差使用偏度、峰度进行检验\n\n%%stata\nsktest resid\n\n\nSkewness and kurtosis tests for normality\n                                                         ----- Joint test -----\n    Variable |       Obs   Pr(skewness)   Pr(kurtosis)   Adj chi2(2)  Prob&gt;chi2\n-------------+-----------------------------------------------------------------\n       resid |        74         0.0000         0.0010         20.82     0.0000\n\n\n\n\n2.7 使用 Shapiro-Wilk 检验\n\n%%stata\nswilk resid\n\n\n                   Shapiro–Wilk W test for normal data\n\n    Variable |        Obs       W           V         z       Prob&gt;z\n-------------+------------------------------------------------------\n       resid |         74    0.89593      6.702     4.150    0.00002",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-2-simple-line-reg.html#简单线性回归",
    "href": "Guide/Stata/25-05-07-2-simple-line-reg.html#简单线性回归",
    "title": "11-简单线性回归",
    "section": "3 简单线性回归",
    "text": "3 简单线性回归\n\n3.1 语法\nregress depvar [indepvars] [if] [in] [weight] [,option]\n具体参见：regress — Linear regression",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-4-log-binary.html",
    "href": "Guide/Stata/25-05-07-4-log-binary.html",
    "title": "13-二分类Logistic回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-4-log-binary.html#什么时候该用-logistic-回归",
    "href": "Guide/Stata/25-05-07-4-log-binary.html#什么时候该用-logistic-回归",
    "title": "13-二分类Logistic回归",
    "section": "1 什么时候该用 Logistic 回归",
    "text": "1 什么时候该用 Logistic 回归\n当outcome发生率 &gt;15% 时logistic regression得出的OR值会overestimate实际的RR值\n\n传统的 Logistic Regression（得出OR值）\nMantel-Haenszel（得出RR值）\nPoisson Regression with robust variance estimate\n\n\n1.1 新方法\n\n1998 Zhang and Yu What’s the Relative Risk?\n\n\\[RR=\\frac{OR}{(1-P_0)+(P_0\\times OR)}\\]\n\n2003 McNutt Outcomes Estimating the Relative Risk in Cohort Studies and Clinical Trials of Common\n金标准：Log Binomial",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-4-log-binary.html#二分类-logistic-模型的假设",
    "href": "Guide/Stata/25-05-07-4-log-binary.html#二分类-logistic-模型的假设",
    "title": "13-二分类Logistic回归",
    "section": "2 二分类 Logistic 模型的假设",
    "text": "2 二分类 Logistic 模型的假设\n\n假设1:因变量(结局)是二分类变量。\n假设2:有至少1个自变量，自变量可以是连续变量，也可以是分类变量，\n假设3:每条观测间相互独立。分类变量(包括因变量和自变量)的分类必须全面且每一个分类间互斥\n假设4:最小样本量要求为自变量数目的15倍，但一些研究者认为样本量应达到自变量数目的50倍\n假设5:连续的自变量与因变量的logit转换值之间存在线性关系。\n假设6:自变量之间无多重共线性，\n假设7:没有明显的离群点、杠杆点和强影响点。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-4-log-binary.html#做-logistic-回归的要求",
    "href": "Guide/Stata/25-05-07-4-log-binary.html#做-logistic-回归的要求",
    "title": "13-二分类Logistic回归",
    "section": "3 做 Logistic 回归的要求",
    "text": "3 做 Logistic 回归的要求\n\nY是二分类变量\nY的发生率 &lt;15%",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-4-log-binary.html#导入数据",
    "href": "Guide/Stata/25-05-07-4-log-binary.html#导入数据",
    "title": "13-二分类Logistic回归",
    "section": "4 导入数据",
    "text": "4 导入数据\n变量 low 是我们的结局事件，我们想看什么因素和孩子的 low birthweight 相关\n\n%%stata\nwebuse lbw,clear\n\n(Hosmer & Lemeshow data)\n\n\n\n%%stata\ntab low\n\n\nBirthweight |\n     &lt;2500g |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          0 |        130       68.78       68.78\n          1 |         59       31.22      100.00\n------------+-----------------------------------\n      Total |        189      100.00\n\n\nDisclaimer: 本节的数据集中，结局事件发生率远远大于15%，应使用Log binomial模型进行分析。这里使用Logistic regression进行分析仅仅为了讲 解如何使用Stata进行操作、以及为下节的Log-binomial进行铺垫。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-4-log-binary.html#logistic-regression",
    "href": "Guide/Stata/25-05-07-4-log-binary.html#logistic-regression",
    "title": "13-二分类Logistic回归",
    "section": "5 Logistic regression",
    "text": "5 Logistic regression\n语法：\nlogistic y x1 x2 x3 ...\n\n5.1 Model 1: \\(low=\\beta_0+\\beta_1 age\\)\n\n%%stata\nlogistic low age\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(1)    =   2.76\n                                                        Prob &gt; chi2   = 0.0966\nLog likelihood = -115.95598                             Pseudo R2     = 0.0118\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9501333   .0299423    -1.62   0.105     .8932232    1.010669\n       _cons |      1.469   1.075492     0.53   0.599     .3498129    6.168901\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\\(\\beta_1\\)：母亲的年龄每增加1岁，孩子低体重的风险是之前的0.95倍(95%CI:0.89,1.01)\n\n\n5.2 Model 2: \\(low=\\beta_0+\\beta_1 age+\\beta_2 smoke\\)\n\n%%stata\nlogistic low age i.smoke\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(2)    =   7.40\n                                                        Prob &gt; chi2   = 0.0248\nLog likelihood = -113.63815                             Pseudo R2     = 0.0315\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9514394   .0304194    -1.56   0.119     .8936482    1.012968\n             |\n       smoke |\n     Smoker  |   1.997405    .642777     2.15   0.032     1.063027    3.753081\n       _cons |   1.062798   .8048781     0.08   0.936     .2408901    4.689025\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\\(\\beta_1\\)：控制了母亲的吸烟状况以后，母亲的年龄每增加1岁孩子低体重的风险是之前的0.95倍(95% CI:0.89,1.01)\n\n\n5.3 Model 2: \\(low=\\beta_0+\\beta_1 age+\\beta_2 smoke+\\beta_3 race\\)\n\n%%stata\nlogistic low age i.smoke i.race\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(4)    =  15.81\n                                                        Prob &gt; chi2   = 0.0033\nLog likelihood = -109.4311                              Pseudo R2     = 0.0674\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9657186   .0322573    -1.04   0.296     .9045206    1.031057\n             |\n       smoke |\n     Smoker  |    3.00582   1.118001     2.96   0.003     1.449982    6.231081\n             |\n        race |\n      Black  |   2.749483   1.356659     2.05   0.040     1.045318    7.231924\n      Other  |   2.876948   1.167921     2.60   0.009     1.298314    6.375062\n             |\n       _cons |    .365111   .3146026    -1.17   0.242     .0674491    1.976395\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\\(\\beta_1\\)：控制了母亲的吸烟状况和种族以后，母亲的年龄每增加1岁孩子低体重的风险是之前的0.97倍(95%CI:0.90,1.03)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-2-survival-ana.html",
    "href": "Guide/Stata/25-05-08-2-survival-ana.html",
    "title": "15-生存分析",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-2-survival-ana.html#生存分析",
    "href": "Guide/Stata/25-05-08-2-survival-ana.html#生存分析",
    "title": "15-生存分析",
    "section": "1 生存分析",
    "text": "1 生存分析\n\n描述一个组内个体的生存时间\n\n寿命表法(Life tables methods)\n非参数Kaplan-Meier曲线\n\n比较两个或多个组的生存时间\n\nLog-rank test\n\n研究生存时间和变量之间的关系\n\n半参数Cox比例风险模型\n参数生存分析模型",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-2-survival-ana.html#k-m曲线的历史",
    "href": "Guide/Stata/25-05-08-2-survival-ana.html#k-m曲线的历史",
    "title": "15-生存分析",
    "section": "2 K-M曲线的历史",
    "text": "2 K-M曲线的历史\n\n1958年，Dr. Kaplan和DrMeier 介绍了一种全新的、解决随访期间RightCensoring问题的生存分析方法\n特点:精确地记录并利用每个个体发生终点事件的具体时间，在任何一个终点事件发生的时间点计算出一个新的、基于之前所有信息的Cumulative survival\n优点:\n\n相比于Life-table method，更加充分地利用了信息，给出更准确的统计量\n非参数估计方法:不要求总体的分布形式，因此非常适合生存分析时使用\nK-M曲线可以很直观地表现出两组或多组的生存率或死亡率，适合在文章中展示",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-2-survival-ana.html#导入数据",
    "href": "Guide/Stata/25-05-08-2-survival-ana.html#导入数据",
    "title": "15-生存分析",
    "section": "3 导入数据",
    "text": "3 导入数据\n使用 Patient Survival in Drug Trial 数据集\n\n%%stata\nwebuse drugtr,clear\n\n(Patient survival in drug trial)\n\n\n将数据恢复成普通的数据格式：stset,clear\n\nStata已经将这个数据集设置成了生存数据的格式，导入数据集后，将数 据集恢复成普通的数据格式，这样才是我们在临床研究中见到的数据结构。\n\n\n%%stata\nstset,clear",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-2-survival-ana.html#数据集的初步观察",
    "href": "Guide/Stata/25-05-08-2-survival-ana.html#数据集的初步观察",
    "title": "15-生存分析",
    "section": "4 数据集的初步观察",
    "text": "4 数据集的初步观察\n\n%%stata\nlist in 5/10\n\n\n     +------------------------------+\n     | studyt~e   died   drug   age |\n     |------------------------------|\n  5. |        4      1      0    56 |\n  6. |        4      1      0    67 |\n  7. |        5      1      0    63 |\n  8. |        5      1      0    58 |\n  9. |        8      1      0    56 |\n     |------------------------------|\n 10. |        8      0      0    58 |\n     +------------------------------+\n\n\n\n%%stata\ncodebook drug\n\n\n-------------------------------------------------------------------------------\ndrug                                                      Drug type (0=placebo)\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [0,1]                         Units: 1\n         Unique values: 2                         Missing .: 0/48\n\n            Tabulation: Freq.  Value\n                           20  0\n                           28  1\n\n\n\n%%stata\ncodebook studytime\n\n\n-------------------------------------------------------------------------------\nstudytime                                        Months to death or end of exp.\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [1,39]                        Units: 1\n         Unique values: 28                        Missing .: 0/48\n\n                  Mean:    15.5\n             Std. dev.: 10.2563\n\n           Percentiles:     10%       25%       50%       75%       90%\n                              4       7.5      12.5        23        32\n\n\n\n%%stata\ncodebook died\n\n\n-------------------------------------------------------------------------------\ndied                                                          1 if patient died\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [0,1]                         Units: 1\n         Unique values: 2                         Missing .: 0/48\n\n            Tabulation: Freq.  Value\n                           17  0\n                           31  1\n\n\n\n%%stata\ncodebook age\n\n\n-------------------------------------------------------------------------------\nage                                              Patient's age at start of exp.\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [47,67]                       Units: 1\n         Unique values: 18                        Missing .: 0/48\n\n                  Mean: 55.875\n             Std. dev.: 5.6592\n\n           Percentiles:    10%       25%       50%       75%       90%\n                            49      50.5        56        60        65",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-2-survival-ana.html#数据申明代码操作",
    "href": "Guide/Stata/25-05-08-2-survival-ana.html#数据申明代码操作",
    "title": "15-生存分析",
    "section": "5 数据申明——代码操作",
    "text": "5 数据申明——代码操作\n\n告诉Stata: 终点事件(Failure variable),随访时间(Time variable)\nstset timevar, failure(failvar[==numlist])\n\ntimevar: 随访时间变量\nfailvar: 终点事件变量\nnumlist: 终点时间变量中，哪个(哪些)值代表发生了终点事件?\n\n\n\n%%stata\nstset studytime,failure(died==1)\n\n\nSurvival-time data settings\n\n         Failure event: died==1\nObserved time interval: (0, studytime]\n     Exit on or before: failure\n\n--------------------------------------------------------------------------\n         48  total observations\n          0  exclusions\n--------------------------------------------------------------------------\n         48  observations remaining, representing\n         31  failures in single-record/single-failure data\n        744  total analysis time at risk and under observation\n                                                At risk from t =         0\n                                     Earliest observed entry t =         0\n                                          Last observed exit t =        39",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-2-survival-ana.html#生存数据再观测",
    "href": "Guide/Stata/25-05-08-2-survival-ana.html#生存数据再观测",
    "title": "15-生存分析",
    "section": "6 生存数据再观测",
    "text": "6 生存数据再观测\nNotice：必须要在指定数据集为生存分析数据集之后(stset 之后)才能使用任何其他的 st 开始的命令。\n\n%%stata\nstsum\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n         |               Incidence     Number of   |------ Survival time -----|\n         | Time at risk       rate      subjects        25%       50%       75%\n---------+---------------------------------------------------------------------\n   Total |          744   .0416667            48          8        17        33\n\n\n\n%%stata\nstdescribe\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n                                   |-------------- Per subject --------------|\nCategory                   Total        Mean         Min     Median        Max\n------------------------------------------------------------------------------\nNumber of subjects            48   \nNumber of records             48           1           1          1          1\n\nEntry time (first)                         0           0          0          0\nExit time (final)                       15.5           1       12.5         39\n\nSubjects with gap              0   \nTime on gap                    0   \nTime at risk                 744        15.5           1       12.5         39\n\nFailures                      31    .6458333           0          1          1\n------------------------------------------------------------------------------\n\n\n\n%%stata\nsts list\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\nKaplan–Meier survivor function\n\n             At                  Survivor      Std.\n  Time     risk   Fail   Lost    function     error     [95% conf. int.]\n------------------------------------------------------------------------\n     1       48      2      0      0.9583    0.0288     0.8435    0.9894\n     2       46      1      0      0.9375    0.0349     0.8186    0.9794\n     3       45      1      0      0.9167    0.0399     0.7930    0.9679\n     4       44      2      0      0.8750    0.0477     0.7427    0.9418\n     5       42      2      0      0.8333    0.0538     0.6943    0.9129\n     6       40      2      1      0.7917    0.0586     0.6474    0.8820\n     7       37      1      0      0.7703    0.0608     0.6236    0.8656\n     8       36      3      1      0.7061    0.0661     0.5546    0.8143\n     9       32      0      1      0.7061    0.0661     0.5546    0.8143\n    10       31      1      1      0.6833    0.0678     0.5302    0.7957\n    11       29      2      1      0.6362    0.0708     0.4807    0.7564\n    12       26      2      0      0.5872    0.0733     0.4304    0.7145\n    13       24      1      0      0.5628    0.0742     0.4060    0.6931\n    15       23      1      1      0.5383    0.0749     0.3821    0.6712\n    16       21      1      0      0.5127    0.0756     0.3570    0.6483\n    17       20      1      1      0.4870    0.0761     0.3326    0.6249\n    19       18      0      2      0.4870    0.0761     0.3326    0.6249\n    20       16      0      1      0.4870    0.0761     0.3326    0.6249\n    22       15      2      0      0.4221    0.0786     0.2680    0.5684\n    23       13      2      0      0.3572    0.0788     0.2087    0.5083\n    24       11      1      0      0.3247    0.0780     0.1809    0.4771\n    25       10      1      1      0.2922    0.0767     0.1543    0.4449\n    28        8      1      1      0.2557    0.0753     0.1247    0.4093\n    32        6      0      2      0.2557    0.0753     0.1247    0.4093\n    33        4      1      0      0.1918    0.0791     0.0676    0.3634\n    34        3      0      1      0.1918    0.0791     0.0676    0.3634\n    35        2      0      1      0.1918    0.0791     0.0676    0.3634\n    39        1      0      1      0.1918    0.0791     0.0676    0.3634\n------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-2-survival-ana.html#k-m曲线的绘制",
    "href": "Guide/Stata/25-05-08-2-survival-ana.html#k-m曲线的绘制",
    "title": "15-生存分析",
    "section": "7 K-M曲线的绘制",
    "text": "7 K-M曲线的绘制\n语法：\nsts graph [if] [in] [,options]\n[,options] 不是必须，可以形如：,by(var)，这样就会按照 var 的分类绘制不同的线\n\n%%stata\nsts graph,by(drug)\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n7.1 图像展现更多的信息\n\n%%stata\nsts graph if age&lt;50,by(drug)\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n%%stata\nsts graph,by(drug) risktable\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n\n7.2 复杂绘图示例\n\n%%stata\nlabel drop drug_label\nlabel define drug_label 0 \"安慰剂\" 1 \"试验药\"\nlabel values drug drug_label\n\n\n. label drop drug_label\n\n. label define drug_label 0 \"安慰剂\" 1 \"试验药\"\n\n. label values drug drug_label\n\n. \n\n\n\n%%stata\nsts graph, ///\n    by(drug) ci atrisk ///\n    xlabel(0(5)40) ylabel(0(0.2)1) ///\n    legend(position(6) ring(1) cols(2) rowgap(1) colgap(1)) ///\n    xtitle(\"x轴的标签\") ytitle(\"y轴的标签\") ///\n    title(\"这里是标题\", size(medsmall)) ///\n    subtitle(\"这里是副标题\", size(small)) ///\n    caption(\"注释\", size(vsmall)) ///\n    note(\"数据来源：xxx\", size(vsmall)) ///\n    graphregion(margin(10 10 10 10)) ///\n    plotregion(margin(5 5 5 5))\n\n\n. sts graph, ///\n&gt;     by(drug) ci atrisk ///\n&gt;     xlabel(0(5)40) ylabel(0(0.2)1) ///\n&gt;     legend(position(6) ring(1) cols(2) rowgap(1) colgap(1)) ///\n&gt;     xtitle(\"x轴的标签\") ytitle(\"y轴的标签\") ///\n&gt;     title(\"这里是标题\", size(medsmall)) ///\n&gt;     subtitle(\"这里是副标题\", size(small)) ///\n&gt;     caption(\"注释\", size(vsmall)) ///\n&gt;     note(\"数据来源：xxx\", size(vsmall)) ///\n&gt;     graphregion(margin(10 10 10 10)) ///\n&gt;     plotregion(margin(5 5 5 5))\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-2-survival-ana.html#检验组间差别log-rank-test",
    "href": "Guide/Stata/25-05-08-2-survival-ana.html#检验组间差别log-rank-test",
    "title": "15-生存分析",
    "section": "8 检验组间差别（Log-Rank Test）",
    "text": "8 检验组间差别（Log-Rank Test）\n\n%%stata\nsts test drug\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\nEquality of survivor functions\nLog-rank test\n\n      |  Observed       Expected\ndrug  |    events         events\n------+-------------------------\n    0 |        19           7.25\n    1 |        12          23.75\n------+-------------------------\nTotal |        31          31.00\n\n                chi2(1) =  28.27\n                Pr&gt;chi2 = 0.0000",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-4-GEE.html",
    "href": "Guide/Stata/25-05-08-4-GEE.html",
    "title": "17-广义估计方程(GEE)",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-4-GEE.html#广义估计方程",
    "href": "Guide/Stata/25-05-08-4-GEE.html#广义估计方程",
    "title": "17-广义估计方程(GEE)",
    "section": "1 广义估计方程",
    "text": "1 广义估计方程\n广义估计方程（Generalized Estimating Equations, GEE）是一种用于分析具有相关性数据的统计方法，特别适用于纵向数据和重复测量数据的分析。\n\n1.1 基本概念\n广义估计方程是一种扩展的回归模型，旨在处理因变量之间可能存在的相关性。它于1986年由Liang和Zeger首次提出，主要用于估计广义线性模型的参数，尤其是在数据存在重复测量或相关性时。GEE通过准似然估计方法来处理这些数据，适用于多种类型的因变量，包括连续型、二分类和计数数据等。\n\n\n1.2 应用场景\n\n纵向数据分析：如临床试验中对同一组受试者在不同时间点的测量数据进行分析。\n重复测量数据：例如在心理学和社会科学研究中，研究同一对象在不同条件下的表现。\n组间比较：用于比较不同组别在某一时间点或多个时间点的差异。\n\n\n\n1.3 优势与特点\n\n处理相关性：GEE能够有效处理因变量之间的相关性，避免了传统方法（如方差分析）在数据不独立时可能导致的偏差。\n灵活性：适用于多种类型的因变量，且对自变量的数据类型没有严格要求，可以处理定类和定量数据。\n模型选择：根据因变量的分布特征选择合适的模型，如泊松回归、负二项回归等。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-4-GEE.html#什么是纵向数据分析",
    "href": "Guide/Stata/25-05-08-4-GEE.html#什么是纵向数据分析",
    "title": "17-广义估计方程(GEE)",
    "section": "2 什么是纵向数据分析",
    "text": "2 什么是纵向数据分析\nExamples oflongitudinal dataanalysis(LDA,纵向数据分析)\n\nChild BP measured at each annual visit from 3 to 9 years old\nInfant weight measured at 3, 6, 9, 12 months\nIn a 3-arm cross-over trial, short chain fatty acid measured at the end ofeach\n\n\n2.1 纵向数据 vs. 横向数据\n纵向数据是指对同一组受试个体或者受试单元在不同时间点上的重复观测若干次，得到由截面和时间序列融合在一起的数据。\n因而纵向数据具有自相关性、生态单位聚集性、测量次数与测量时间间隔的非均衡性等特点。\n\nEstablish temporaltrends(时序:A在B之前发生)\nSeparate cohort effects from aging effects\nChildren have different baseline values in reading abilities\nThe trajectories of reading abilities differs by people\n\n\n\n2.2 方差公式\n\\[Var(X-Y)\nVar((1)X+(-1)Y)=(1^2)Var(X)+(-1^2)Var(Y)+2(1)(-1)Cov(X,Y)\\]\n在非配对 t 检验中，\\(Cov(X,Y)\\)为0，此时 \\(Var(X-Y)\\)\n打破线性相关的假设",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-4-GEE.html#gee-模型",
    "href": "Guide/Stata/25-05-08-4-GEE.html#gee-模型",
    "title": "17-广义估计方程(GEE)",
    "section": "3 GEE 模型",
    "text": "3 GEE 模型\n\n\n\ncorrelation\ndescription\n\n\n\n\nexchangeable\nexchangeable\n\n\nindependent\nindependent\n\n\nunstructured\nunstructured\n\n\nfixed matname\nuser-specified\n\n\nar #\nautoregressive of order #\n\n\nstationary #\nstationary of order #\n\n\nnonstationary #\nnonstationary of order #\n\n\n\n\n3.1 GEE 常用步骤\n\n使用 independent 回归结构\n稳健标准误估计（robust variance estimate）；如果用错模型，稳健后的结果也会相差不远\n使用 QIC（AIC for GEE）评估GEE模型",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-4-GEE.html#数据导入",
    "href": "Guide/Stata/25-05-08-4-GEE.html#数据导入",
    "title": "17-广义估计方程(GEE)",
    "section": "4 数据导入",
    "text": "4 数据导入\nData:NLS Women 14-24 in 1968\n\n%%stata\nwebuse union,clear\n\n(NLS Women 14-24 in 1968)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-4-GEE.html#数据观测",
    "href": "Guide/Stata/25-05-08-4-GEE.html#数据观测",
    "title": "17-广义估计方程(GEE)",
    "section": "5 数据观测",
    "text": "5 数据观测\n\n%%stata\nlist in 1/10\n\n\n     +----------------------------------------------------------------+\n     | idcode   year   age   grade   not_smsa   south   union   black |\n     |----------------------------------------------------------------|\n  1. |      1     72    20      12          0       0       1       1 |\n  2. |      1     77    25      12          0       0       0       1 |\n  3. |      1     80    28      12          0       0       1       1 |\n  4. |      1     83    31      12          0       0       1       1 |\n  5. |      1     85    33      12          0       0       1       1 |\n     |----------------------------------------------------------------|\n  6. |      1     87    35      12          0       0       1       1 |\n  7. |      1     88    37      12          0       0       1       1 |\n  8. |      2     71    19      12          0       0       0       1 |\n  9. |      2     77    25      12          0       0       1       1 |\n 10. |      2     78    26      12          0       0       1       1 |\n     +----------------------------------------------------------------+",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-4-GEE.html#用法",
    "href": "Guide/Stata/25-05-08-4-GEE.html#用法",
    "title": "17-广义估计方程(GEE)",
    "section": "6 用法",
    "text": "6 用法\n\n6.1 GEE 准备\nxtset studyid timevar\n\nstudyid: unique ID for each participant\ntimevar: timevar will usually be a variable that counts 1,2,…, and is to be interpreted as firstyear ofsurvey, second year,…, or first month oftreatment, second month.\n\n\n%%stata\nxtset id year\n\n\nPanel variable: idcode (unbalanced)\n Time variable: year, 70 to 88, but with gaps\n         Delta: 1 unit\n\n\n\n\n6.2 GEE 命令\nxtgee y x_1,x_2,x_3,…[if] [in] [weight] ,family(family) link(link) corr(correlation structure) robust\n可以使用 help gee 查看更多信息\n\n%%stata\nxtgee union age grade not_smsa south, family(binomial) link(logit) corr(ind) robust\n\n\nIteration 1:  Tolerance = 1.940e-12\n\nGEE population-averaged model                      Number of obs    =   26,200\nGroup variable: idcode                             Number of groups =    4,434\nFamily: Binomial                                   Obs per group:  \nLink:   Logit                                                   min =        1\nCorrelation: independent                                        avg =      5.9\n                                                                max =       12\n                                                   Wald chi2(4)     =   160.30\nScale parameter = 1                                Prob &gt; chi2      =   0.0000\n\nPearson chi2(26200)  = 26242.04                    Deviance         = 27093.19\nDispersion (Pearson) = 1.001604                    Dispersion       = 1.034091\n\n                                 (Std. err. adjusted for clustering on idcode)\n------------------------------------------------------------------------------\n             |               Robust\n       union | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |    .011683   .0033035     3.54   0.000     .0052082    .0181577\n       grade |    .048511   .0139346     3.48   0.000     .0211997    .0758223\n    not_smsa |  -.2214007   .0713343    -3.10   0.002    -.3612134    -.081588\n       south |  -.6470985   .0629803   -10.27   0.000    -.7705376   -.5236594\n       _cons |  -1.941974   .1973105    -9.84   0.000    -2.328695   -1.555253\n------------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-1-multinomail-logistic-reg.html",
    "href": "Guide/Stata/25-05-09-1-multinomail-logistic-reg.html",
    "title": "19-无序多分类Logistic回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "19-无序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-1-multinomail-logistic-reg.html#无序多分类logistic回归",
    "href": "Guide/Stata/25-05-09-1-multinomail-logistic-reg.html#无序多分类logistic回归",
    "title": "19-无序多分类Logistic回归",
    "section": "1 无序多分类Logistic回归",
    "text": "1 无序多分类Logistic回归\nProportional odds 假定满足\n\n%%stata\nwebuse fullauto.dta,clear\n\n(Automobile models)\n\n\n\n%%stata\nologit rep77 foreign,or\n\n\nIteration 0:  Log likelihood = -89.895098  \nIteration 1:  Log likelihood = -85.951765  \nIteration 2:  Log likelihood = -85.908227  \nIteration 3:  Log likelihood = -85.908161  \nIteration 4:  Log likelihood = -85.908161  \n\nOrdered logistic regression                             Number of obs =     66\n                                                        LR chi2(1)    =   7.97\n                                                        Prob &gt; chi2   = 0.0047\nLog likelihood = -85.908161                             Pseudo R2     = 0.0444\n\n------------------------------------------------------------------------------\n       rep77 | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     foreign |   4.288246   2.276609     2.74   0.006      1.51489    12.13888\n-------------+----------------------------------------------------------------\n       /cut1 |  -2.765562   .5988208                     -3.939229   -1.591895\n       /cut2 |  -.9963603   .3217706                     -1.627019   -.3657016\n       /cut3 |   .9426153   .3136398                      .3278925    1.557338\n       /cut4 |   3.123351   .5423257                      2.060412     4.18629\n------------------------------------------------------------------------------\nNote: Estimates are transformed only in the first equation to odds ratios.\n\n\n进口车(Foreign=1)有着更高车辆维修状况等级的odds是国产车(Foreign=0)的4.29倍(95% CI: 1.51,12.13)\nProportional odds 假定不满足\n使用 Generalized Ordinal Logistic Regression\n需要安装 gologit2 命令\n\n%%stata\nssc install gologit2\n\nchecking gologit2 consistency and verifying not already installed...\ninstalling into C:\\Users\\asus\\ado\\plus\\...\ninstallation complete.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "19-无序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-1-multinomail-logistic-reg.html#gologit2-命令",
    "href": "Guide/Stata/25-05-09-1-multinomail-logistic-reg.html#gologit2-命令",
    "title": "19-无序多分类Logistic回归",
    "section": "2 gologit2 命令",
    "text": "2 gologit2 命令\n\n2.1 满足Proportional Odds假定\ngologit2 y x x₂ x.., pl or\n这个command和 ologit command 给出的结果相同\n\n\n2.2 不满足Proportional0dds假定\ngologit2 y x x₂ x. ., npl or\n\npl & npl 分别表示满足 parallel\n\n\n\n2.3 检验是否满足Proportional Odds假定\nLikelihood-ratio test:lrtest\n\n%%stata\ngologit2 rep77 foreign,pl or\n\n\nGeneralized Ordered Logit Estimates                     Number of obs =     66\n                                                        LR chi2(1)    =   7.97\n                                                        Prob &gt; chi2   = 0.0047\nLog likelihood = -85.908161                             Pseudo R2     = 0.0444\n\n ( 1)  [Poor]foreign - [Fair]foreign = 0\n ( 2)  [Fair]foreign - [Average]foreign = 0\n ( 3)  [Average]foreign - [Good]foreign = 0\n------------------------------------------------------------------------------\n       rep77 | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nPoor         |\n     foreign |   4.288247   2.276609     2.74   0.006      1.51489    12.13888\n       _cons |   15.88797   9.514049     4.62   0.000     4.913051    51.37901\n-------------+----------------------------------------------------------------\nFair         |\n     foreign |   4.288247   2.276609     2.74   0.006      1.51489    12.13888\n       _cons |   2.708406   .8714855     3.10   0.002     1.441525    5.088683\n-------------+----------------------------------------------------------------\nAverage      |\n     foreign |   4.288247   2.276609     2.74   0.006      1.51489    12.13888\n       _cons |   .3896075   .1221964    -3.01   0.003     .2106962    .7204404\n-------------+----------------------------------------------------------------\nGood         |\n     foreign |   4.288247   2.276609     2.74   0.006      1.51489    12.13888\n       _cons |   .0440095   .0238675    -5.76   0.000     .0152026    .1274015\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\n%%stata\ngologit2 rep77 foreign,npl or\n\n\nGeneralized Ordered Logit Estimates                     Number of obs =     66\n                                                        LR chi2(4)    =  15.24\n                                                        Prob &gt; chi2   = 0.0042\nLog likelihood = -82.27372                              Pseudo R2     = 0.0848\n\n------------------------------------------------------------------------------\n       rep77 | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nPoor         |\n     foreign |   .9300305   1.166495    -0.06   0.954     .0795928    10.86727\n       _cons |   21.50014   15.55202     4.24   0.000     5.208693    88.74704\n-------------+----------------------------------------------------------------\nFair         |\n     foreign |   3.453614   2.818944     1.52   0.129     .6974251    17.10213\n       _cons |   2.750213   .9271033     3.00   0.003     1.420445    5.324862\n-------------+----------------------------------------------------------------\nAverage      |\n     foreign |   3.281111   1.804947     2.16   0.031     1.116279    9.644262\n       _cons |   .4062893   .1336252    -2.74   0.006     .2132467    .7740847\n-------------+----------------------------------------------------------------\nGood         |\n     foreign |   3.94e+07   6.55e+10     0.01   0.992            0           .\n       _cons |   7.93e-09   .0000132    -0.01   0.991            0           .\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n当Proportional Odds假定不成立时\n进口车(Foreign=1)和国产车(Foreign=0)比:\n\nOdds(Excellent+Good+Average+Fair)/Odds(Poor)= 0.93\nOdds(Excellent+Good+Average)/Odds(Fair+Poor)= 3.45\nOdds(Excellent+Good)/Odds(Average+Fair+Poor)= 3.28\nOdds(Excellent)/Odds(Good+Average+Fair+Poor)= 3.94*10^7\n\n\n\n2.4 检查Proportional Odds假定是否成立\n\\(H_0\\):Non-Proportional Odds 模型可以更好解释结局变量各个等级之间关系\n\n%%stata\ngologit2 rep77 foreign,pl or \nest store A\ngologit2 rep77 foreign,npl or \nest store B\nlrtest A B //Likelihood-ratio test\n\n\n. gologit2 rep77 foreign,pl or \n\nGeneralized Ordered Logit Estimates                     Number of obs =     66\n                                                        LR chi2(1)    =   7.97\n                                                        Prob &gt; chi2   = 0.0047\nLog likelihood = -85.908161                             Pseudo R2     = 0.0444\n\n ( 1)  [Poor]foreign - [Fair]foreign = 0\n ( 2)  [Fair]foreign - [Average]foreign = 0\n ( 3)  [Average]foreign - [Good]foreign = 0\n------------------------------------------------------------------------------\n       rep77 | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nPoor         |\n     foreign |   4.288247   2.276609     2.74   0.006      1.51489    12.13888\n       _cons |   15.88797   9.514049     4.62   0.000     4.913051    51.37901\n-------------+----------------------------------------------------------------\nFair         |\n     foreign |   4.288247   2.276609     2.74   0.006      1.51489    12.13888\n       _cons |   2.708406   .8714855     3.10   0.002     1.441525    5.088683\n-------------+----------------------------------------------------------------\nAverage      |\n     foreign |   4.288247   2.276609     2.74   0.006      1.51489    12.13888\n       _cons |   .3896075   .1221964    -3.01   0.003     .2106962    .7204404\n-------------+----------------------------------------------------------------\nGood         |\n     foreign |   4.288247   2.276609     2.74   0.006      1.51489    12.13888\n       _cons |   .0440095   .0238675    -5.76   0.000     .0152026    .1274015\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n. est store A\n\n. gologit2 rep77 foreign,npl or \n\nGeneralized Ordered Logit Estimates                     Number of obs =     66\n                                                        LR chi2(4)    =  15.24\n                                                        Prob &gt; chi2   = 0.0042\nLog likelihood = -82.27372                              Pseudo R2     = 0.0848\n\n------------------------------------------------------------------------------\n       rep77 | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nPoor         |\n     foreign |   .9300305   1.166495    -0.06   0.954     .0795928    10.86727\n       _cons |   21.50014   15.55202     4.24   0.000     5.208693    88.74704\n-------------+----------------------------------------------------------------\nFair         |\n     foreign |   3.453614   2.818944     1.52   0.129     .6974251    17.10213\n       _cons |   2.750213   .9271033     3.00   0.003     1.420445    5.324862\n-------------+----------------------------------------------------------------\nAverage      |\n     foreign |   3.281111   1.804947     2.16   0.031     1.116279    9.644262\n       _cons |   .4062893   .1336252    -2.74   0.006     .2132467    .7740847\n-------------+----------------------------------------------------------------\nGood         |\n     foreign |   3.94e+07   6.55e+10     0.01   0.992            0           .\n       _cons |   7.93e-09   .0000132    -0.01   0.991            0           .\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n. est store B\n\n. lrtest A B //Likelihood-ratio test\n\nLikelihood-ratio test\nAssumption: A nested within B\n\n LR chi2(3) =   7.27\nProb &gt; chi2 = 0.0638\n\n. \n\n\n根据 Likelihood-ratio test 得出的结果，\\(P=0.0638&gt;0.05\\)，拒绝\\(H_0\\): Non-Proportional 0dds并没有更好解释结局变量各个等级之间关系。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "19-无序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-1-multinomail-logistic-reg.html#无序多分类-logistic-回归",
    "href": "Guide/Stata/25-05-09-1-multinomail-logistic-reg.html#无序多分类-logistic-回归",
    "title": "19-无序多分类Logistic回归",
    "section": "3 无序多分类 Logistic 回归",
    "text": "3 无序多分类 Logistic 回归\n\n把结局变量的某个分类作为reference，然后比较结局变量其他分类相对于reference的相对风险(Relative Risk)\n\n\\[RR_j=Pr(cat=j)/Pr(reference\\ cat)\\] \\[log(RR_j)=\\beta_{0j}+\\beta_{1j}X_1+\\cdots +\\beta_{pj}X_p\\]\nnotice：cat 是 category 的缩写",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "19-无序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-1-multinomail-logistic-reg.html#有序和无序多分类比较",
    "href": "Guide/Stata/25-05-09-1-multinomail-logistic-reg.html#有序和无序多分类比较",
    "title": "19-无序多分类Logistic回归",
    "section": "4 有序和无序多分类比较",
    "text": "4 有序和无序多分类比较\n\n有序多分类 Logistic 回归：\n\n\\(RR_j=Pr(cat&gt;j)/Pr(cat\\leq j)\\)\nologit y x_1 x_2 x_3 ...,or\n\n无序多分类Logistic回归:\n\n\\(RR_j=Pr(cat=j)/Pr(reference\\ cat)\\)\nmlogit y x_1 x_2x x_3...,rrr baseoutcome(j)\n\n\n\nmlogit 是 multi logit 的缩写\n\n\nbaseoutcome(j) 如果不指定， Stata 会自动选择\n\n\n%%stata\nmlogit rep77 foreign,rrr baseoutcome(1)\n\n\nIteration 0:  Log likelihood = -89.895098  \nIteration 1:  Log likelihood = -85.605381  \nIteration 2:  Log likelihood = -82.670821  \nIteration 3:  Log likelihood = -82.335383  \nIteration 4:  Log likelihood =  -82.28077  \nIteration 5:  Log likelihood = -82.274431  \nIteration 6:  Log likelihood = -82.273851  \nIteration 7:  Log likelihood = -82.273742  \nIteration 8:  Log likelihood = -82.273725  \nIteration 9:  Log likelihood =  -82.27372  \n\nMultinomial logistic regression                         Number of obs =     66\n                                                        LR chi2(4)    =  15.24\n                                                        Prob &gt; chi2   = 0.0042\nLog likelihood = -82.27372                              Pseudo R2     = 0.0848\n\n------------------------------------------------------------------------------\n       rep77 |        RRR   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nPoor         |  (base outcome)\n-------------+----------------------------------------------------------------\nFair         |\n     foreign |   .2000452   .3225721    -1.00   0.318     .0084834    4.717229\n       _cons |   5.000509   3.873398     2.08   0.038     1.095653    22.82209\n-------------+----------------------------------------------------------------\nAverage      |\n     foreign |   .7001516   .9110327    -0.27   0.784      .054653    8.969536\n       _cons |   10.00009   7.416364     3.10   0.002     2.337371    42.78389\n-------------+----------------------------------------------------------------\nGood         |\n     foreign |   1.076972   1.412458     0.06   0.955     .0823847    14.07869\n       _cons |   6.500016   4.937183     2.46   0.014     1.466803    28.80429\n-------------+----------------------------------------------------------------\nExcellent    |\n     foreign |   1.32e+07   1.52e+10     0.01   0.989            0           .\n       _cons |   3.79e-07   .0004353    -0.01   0.990            0           .\n------------------------------------------------------------------------------\nNote: _cons estimates baseline relative risk for each outcome.\n\n\n进口车(Foreign=1)和国产车(Foreign=0)比:\n\nRisk(Fair)/Risk(Poor)=0.20\nRisk(Average)/Risk(Poor)=0.70\nRisk(Good)/Risk(Poor)= 1.08\nRisk(Excellent)/Risk(Poor)= 1.32*10^7\n\n\nRisk(Excellent)/Risk(Poor)= 1.32*10^7，这个结果之所以如此大，是因为有一个 Excellent 样本是0，所以估计有偏",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "19-无序多分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-2-data-manage.html",
    "href": "Guide/Stata/25-05-09-2-data-manage.html",
    "title": "20-数据清洗与整理",
    "section": "",
    "text": "使用 Do-file 完成后续的数据整理操作",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "20-数据清洗与整理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-2-data-manage.html#sortorder变量名排序",
    "href": "Guide/Stata/25-05-09-2-data-manage.html#sortorder变量名排序",
    "title": "20-数据清洗与整理",
    "section": "4.1 sort/order：变量名排序",
    "text": "4.1 sort/order：变量名排序\n********************观测值排序********************\n* 排序之前，看前10个观测值\nList CHILD_PIDX CHILD_DOB_YEAR in 1/10\n\n* 排序(从小到大)\nsort CHILD_DOB_YEAR\nlist CHILD_PIDX CHILD_DOB_YEAR in 1/10\n\ngsort +CHILD_DOB_YEAR\nlist CHILD_PIDX CHILD_DOB_YEAR in 1/10\n\n* 排序(从大到小)\ngsort -CHILD_DOB_YEAR\nlist CHILD_PIDX CHILD_DOB_YEAR in 1/10\n\n* 也可以sort字符型变量\n* 从A-Z\ngsort +CENSUS_REGION\nlist CHILD_PIDX CENSUS_REGION in 1/10\n\n*从Z-A\ngsort -CENSUS_REGION\nlist CHILD_PIDX CENSUS_REGION in 1/10\n\n*****************变量名排序******************** \n* 看数据集中都有什么变量（并不是排列方式）\nds\n\n* 打开data browser\nbrowse\n\n* 看变量如何排列\nlist in 1/5\n\n* 改变变量排列顺序\norder CHILD_DOB_YEAR,after(CHILD_PIDX)\nlist in 1/5\n* 或者进入data browser进行查看\nbrowse\n\norder CHILDD0B_QTR,before(MOM_PIDX)\nlist in 1/5\n\n4.1.1 合并数据集\n/*\n语法形式是:\n打开一个数据集以后(master dataset):\nOne-to-one merge on specified key variables\n    merge 1:1 varlist using filename [, options]\nMany-to-one merge on specified key variables\n    merge m:1 varlist using filename [,options]\nOne-to-many merge on specified key variables\n    merge 1:m varlist using filename [, options]\nMany-to-many merge on specified key variables\n    merge m:m varlist using filename , options]\n因为现在我们的\"child\"数据集每个孩子只有1个观测值，\n而我们要合并的\"'anthro\"数据集(using dataset)每个孩子有多个观测值,\n所以在这里建议使用 1:m\n*/\n\nmerge 1:m CHILD_PIDX using \"anthro.dta\"\n* 大家也可以试试下面这个(删去前面的\"//\")\n* 但是请注意:这两个merge command只能留一个\n//merge m:m CHILD_PIDX using \"anthro.dta\"\n\n* 想保留既有maternal information又有infant anthropology measurement的孩子\n* 所以保留 _merge==3 的记录\nkeep ifmerge == 3\ndrop _merge\n\nsort CHILD_PIDX\n\n\n4.1.2 reshape\n在数据分析中，尤其是面板数据分析和多层次回归分析，经常需要将数据从宽形（wide form）转换为长形（long form），或反之。Stata提供了一个强大的命令reshape来实现这种转换，它可以根据数据的结构和需要转换的方向，灵活地处理各种情况。\nreshape命令的基本语法\nreshape命令的基本语法结构如下：\nreshape long stubnames, i(varlist) [options]\nreshape wide stubnames, i(varlist) [options]\n其中，i(varlist)是必须的，用于指定ID变量。j(varname [values])在从宽形转换为长形时，varname是现有变量；从长形转换为宽形时，varname是新变量。stubnames是变量名的公共部分。\nreshape wide CHILD_ADJ_AGE WEIGHT-HEAD_CIRC, i(CHILD_PIDX) j(VISIT)\n* 后悔了?\nreshape long",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "20-数据清洗与整理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-2-data-manage.html#egen生成变量统计量",
    "href": "Guide/Stata/25-05-09-2-data-manage.html#egen生成变量统计量",
    "title": "20-数据清洗与整理",
    "section": "4.2 egen：生成变量统计量",
    "text": "4.2 egen：生成变量统计量\nby CHILD_PIDX: egen max_visit = max(VISIT)\nby CHILD_PIDX: egen min_visit = min(VISIT)\nby CHILD_PIDX: egen mean_visit = mean(VISIT)\nby CHILD_PIDX: egen median_visit = median(VISIT)\n\nreshape Wide CHILD_ADJ_AGE WEIGHT-HEAD_CIRC,i(CHILD_PIDX) j(VISIT)\negen height_mean = rowean(HEIGHT6 HEIGHT12 HEIGHT24 HEIGHT36)\negen height_miss = rowmiss(HEIGHT6 HEIGHT12 HEIGHT24 HEIGHT36)\negen height_nonmiss = rownonmiss(HEIGHT6 HEIGHT12 HEIGHT24 HEIGHT36)\nreshape long\n\n* explore egen\nhelp egen\n\n4.2.1 keep/drop\n/*\nkeep/drop 后面接变量名:保留/除变量\nkeep varl var2 var3 :保留变量var1 var2 var3\ndrop varl var2 var3 :去掉变量var1 var2 var3\n\nkeep/drop 后面接 if +条件判断:保留/删除观测值\nkeep if ... :保留满足...条件的观测值\ndrop if ... :去掉满足...条件的观测值\n*/\n\n//比如:\ndrop MOM_EDUCATION_LOW_WRONG MOM_EDUCATION_LOW_RIGHT1 MOM_EDUCATION_LOW_RIGHT2\nkeep CHILD_PIDX MOM_EDUCATION CHILD_SEX\n//keep if CHILD DOB_YEAR &gt;= 2011\n//keep if CHILD DOB_YEAR ==2011 & CITIZENSHIP_PERC == 1",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "20-数据清洗与整理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-09-2-data-manage.html#导出数据",
    "href": "Guide/Stata/25-05-09-2-data-manage.html#导出数据",
    "title": "20-数据清洗与整理",
    "section": "4.3 导出数据",
    "text": "4.3 导出数据\n* 存储为DTA格式的数据\nsave \"Sample\",replace\n//不指定文件类型的话，自动存为DTA数据\n//不指定位置的话，存到工作路径\n//当然，也可以指定位置\nsave \"/Users/username/Desktop/Stata/Sample\", replace\n\n* 导出为Excel格式的数据\nexport excel \"Sample\", replace //有问题!\nexport excel \"Sample.xlsx\", replace //有问题!\n\n* 是不会自动导出变量名的，必须指定\nexport excel \"Sample.xlsx\",firstrow(variables) replace //\"变量\"作为第一行\n\nexport excel \"Sample.xlsx\",firstrow(varlabels) replace //“变量标签\"作为第一行\n\nexport excel \"Sample.xlsx\",firstrow(variables) nolabel replace //不导出数值标签\n\n* 导出为CSV格式的数据\nexport delimited \"Sample.csv\"，replace //变量名自动作为第一行导出\nexport delimited \"Sample.csv\"，nolabel replace //不导出数值标签\n\nlog close\nend.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "20-数据清洗与整理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-10-advanced-plot.html",
    "href": "Guide/Stata/25-05-10-advanced-plot.html",
    "title": "21-复杂与精准绘图",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "21-复杂与精准绘图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-10-advanced-plot.html#指定路径",
    "href": "Guide/Stata/25-05-10-advanced-plot.html#指定路径",
    "title": "21-复杂与精准绘图",
    "section": "1 指定路径",
    "text": "1 指定路径\n\n%%stata\nclear all\npwd\ncd \"C:\\Users\\asus\\Desktop\\test\\Stata-test\"\ndir\n\n\n. clear all\n\n. pwd\nc:\\Users\\asus\\Desktop\\R\\quarto\\Med-Stat-Notes\\Guide\\Stata\n\n. cd \"C:\\Users\\asus\\Desktop\\test\\Stata-test\"\nC:\\Users\\asus\\Desktop\\test\\Stata-test\n\n. dir\n  &lt;dir&gt;   5/10/25 18:51  .                 \n  &lt;dir&gt;   5/10/25 18:51  ..                \n  84.9k   5/10/25 18:48  Graph_test.pdf    \n 206.4k   5/10/25 18:51  Graph_test.png    \n   5.7k   5/05/25 12:23  Statatest.smcl    \n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "21-复杂与精准绘图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-10-advanced-plot.html#导入数据",
    "href": "Guide/Stata/25-05-10-advanced-plot.html#导入数据",
    "title": "21-复杂与精准绘图",
    "section": "2 导入数据",
    "text": "2 导入数据\n依旧使用 auto.dta 数据集\n\n%%stata\nsysuse auto.dta,clear\n\n(1978 automobile data)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "21-复杂与精准绘图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-10-advanced-plot.html#数据初步观测",
    "href": "Guide/Stata/25-05-10-advanced-plot.html#数据初步观测",
    "title": "21-复杂与精准绘图",
    "section": "3 数据初步观测",
    "text": "3 数据初步观测\n\n%%stata\ndescribe\n\n\nContains data from C:\\Program Files\\Stata18/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2022 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "21-复杂与精准绘图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-10-advanced-plot.html#选择y和x",
    "href": "Guide/Stata/25-05-10-advanced-plot.html#选择y和x",
    "title": "21-复杂与精准绘图",
    "section": "4 选择y和x",
    "text": "4 选择y和x\n使用 auto 数据集中的 weight 和 length 作为 y 变量和 x 变量，使用 foreign 作为分类变量\n首先画一个简单的散点图\n\n%%stata\ntwoway scatter weight length,by(foreign)\n\n\n\n\n\n\n\n\n\n4.1 子图合一\n\n%%stata\ntwoway(scatter weight length if foreign ==0)(scatter weight length if foreign ==1)\n\n\n\n\n\n\n\n\n\n\n4.2 数据点抖动\njitter(#) 中的数字控制抖动的幅度，值越大，抖动越明显\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink))\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink))\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.3 绘制线性拟合线\n使用 lfit 命令\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfit weight length if foreign ==0,lcolor(blue)) ///\n(lfit weight length if foreign ==1,lcolor(pink))\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfit weight length if foreign ==0,lcolor(blue)) ///\n&gt; (lfit weight length if foreign ==1,lcolor(pink))\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.4 给拟合线添加置信区间\n使用 lfitci 命令\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,lcolor(blue)) ///\n(lfitci weight length if foreign ==1,lcolor(pink))\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,lcolor(blue)) ///\n&gt; (lfitci weight length if foreign ==1,lcolor(pink))\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.5 修改 CI style\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick))\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick))\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.6 添加 x 和 y 的标签\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\nxtitle(\"weight\") ytitle(\"length\")\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\n&gt; xtitle(\"weight\") ytitle(\"length\")\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.7 改变 x 和 y 的范围（range）\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\nxtitle(\"weight\") ytitle(\"length\") ///\nxlabel(140(10)240) ylabel(1000(500)5000)\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\n&gt; xtitle(\"weight\") ytitle(\"length\") ///\n&gt; xlabel(140(10)240) ylabel(1000(500)5000)\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.8 添加细分标记（tick）\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\nxtitle(\"weight\") ytitle(\"length\") ///\nxlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)5000)\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\n&gt; xtitle(\"weight\") ytitle(\"length\") ///\n&gt; xlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)500\n&gt; 0)\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.9 添加标题\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\nxtitle(\"weight\") ytitle(\"length\") ///\nxlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)5000) ///\ntitle(\"Association Between Weight and Length\") ///\nsubtitle(\"by Region\") ///\ncaption(\"Data Source:1978 auto data\") ///\nnote(\"With 95% CI\")\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\n&gt; xtitle(\"weight\") ytitle(\"length\") ///\n&gt; xlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)500\n&gt; 0) ///\n&gt; title(\"Association Between Weight and Length\") ///\n&gt; subtitle(\"by Region\") ///\n&gt; caption(\"Data Source:1978 auto data\") ///\n&gt; note(\"With 95% CI\")\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.10 修改标签名\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\nxtitle(\"weight\") ytitle(\"length\") ///\nxlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)5000) ///\ntitle(\"Association Between Weight and Length\") ///\nsubtitle(\"by Region\") ///\ncaption(\"Data Source:1978 auto data\") ///\nnote(\"With 95% CI\") ///\nlegend(label(1 \"Domestic\") label(2 \"Foreign\"))\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\n&gt; xtitle(\"weight\") ytitle(\"length\") ///\n&gt; xlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)500\n&gt; 0) ///\n&gt; title(\"Association Between Weight and Length\") ///\n&gt; subtitle(\"by Region\") ///\n&gt; caption(\"Data Source:1978 auto data\") ///\n&gt; note(\"With 95% CI\") ///\n&gt; legend(label(1 \"Domestic\") label(2 \"Foreign\"))\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.11 移除其他标签\n只保留前两个标签\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\nxtitle(\"weight\") ytitle(\"length\") ///\nxlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)5000) ///\ntitle(\"Association Between Weight and Length\") ///\nsubtitle(\"by Region\") ///\ncaption(\"Data Source:1978 auto data\") ///\nnote(\"With 95% CI\") ///\nlegend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2)) \n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\n&gt; xtitle(\"weight\") ytitle(\"length\") ///\n&gt; xlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)500\n&gt; 0) ///\n&gt; title(\"Association Between Weight and Length\") ///\n&gt; subtitle(\"by Region\") ///\n&gt; caption(\"Data Source:1978 auto data\") ///\n&gt; note(\"With 95% CI\") ///\n&gt; legend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2)) \n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.12 改变标签的位置\n移入图中\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\nxtitle(\"weight\") ytitle(\"length\") ///\nxlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)5000) ///\ntitle(\"Association Between Weight and Length\") ///\nsubtitle(\"by Region\") ///\ncaption(\"Data Source:1978 auto data\") ///\nnote(\"With 95% CI\") ///\nlegend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2) ring(0))\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\n&gt; xtitle(\"weight\") ytitle(\"length\") ///\n&gt; xlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)500\n&gt; 0) ///\n&gt; title(\"Association Between Weight and Length\") ///\n&gt; subtitle(\"by Region\") ///\n&gt; caption(\"Data Source:1978 auto data\") ///\n&gt; note(\"With 95% CI\") ///\n&gt; legend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2) ring(0))\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.13 指定标签位置\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\nxtitle(\"weight\") ytitle(\"length\") ///\nxlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)5000) ///\ntitle(\"Association Between Weight and Length\") ///\nsubtitle(\"by Region\") ///\ncaption(\"Data Source:1978 auto data\") ///\nnote(\"With 95% CI\") ///\nlegend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2) ring(0) pos(10))\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\n&gt; xtitle(\"weight\") ytitle(\"length\") ///\n&gt; xlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)500\n&gt; 0) ///\n&gt; title(\"Association Between Weight and Length\") ///\n&gt; subtitle(\"by Region\") ///\n&gt; caption(\"Data Source:1978 auto data\") ///\n&gt; note(\"With 95% CI\") ///\n&gt; legend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2) ring(0) pos(10))\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.14 修改图的边距\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\nxtitle(\"weight\") ytitle(\"length\") ///\nxlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)5000) ///\ntitle(\"Association Between Weight and Length\") ///\nsubtitle(\"by Region\") ///\ncaption(\"Data Source:1978 auto data\") ///\nnote(\"With 95% CI\") ///\nlegend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2) ring(0) pos(10) ///\nregion(lwidth(none))) graphregion(margin(tiny))\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\n&gt; xtitle(\"weight\") ytitle(\"length\") ///\n&gt; xlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)500\n&gt; 0) ///\n&gt; title(\"Association Between Weight and Length\") ///\n&gt; subtitle(\"by Region\") ///\n&gt; caption(\"Data Source:1978 auto data\") ///\n&gt; note(\"With 95% CI\") ///\n&gt; legend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2) ring(0) pos(10) ///\n&gt; region(lwidth(none))) graphregion(margin(tiny))\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.15 修改标题尺寸\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\nxtitle(\"weight\") ytitle(\"length\") ///\nxlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)5000) ///\ntitle(\"Association Between Weight and Length\", size(medianlarge)) ///\nsubtitle(\"by Region\", size(small)) ///\ncaption(\"Data Source:1978 auto data\", size(small)) ///\nnote(\"With 95% CI\", size(small)) ///\nlegend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2) ring(0) pos(10) ///\nregion(lwidth(none))) graphregion(margin(tiny))\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\n&gt; xtitle(\"weight\") ytitle(\"length\") ///\n&gt; xlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)500\n&gt; 0) ///\n&gt; title(\"Association Between Weight and Length\", size(medianlarge)) ///\n&gt; subtitle(\"by Region\", size(small)) ///\n&gt; caption(\"Data Source:1978 auto data\", size(small)) ///\n&gt; note(\"With 95% CI\", size(small)) ///\n&gt; legend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2) ring(0) pos(10) ///\n&gt; region(lwidth(none))) graphregion(margin(tiny))\n(note:  named style medianlarge not found in class gsize, default attributes\n    used)\n\n. \n\n\n\n\n\n\n\n\n\n\n\n4.16 修改字体\n一般杂志要求使用 Times New Roman 字体\n\n%%stata\ngraph set window fontface \"Times New Roman\"\n\nnot available in non-GUI version\n\n\nNotice：上面输出结果为： not available in non-GUI version\n这是因为：当前使用的是 Stata 的非图形界面（non-GUI）版本，比如在 Stata MP batch mode、Stata in terminal (Unix/Linux)、或通过 Jupyter Notebook、Python pystata接口 调用的 Stata 环境。这些环境中不支持 graph set window 命令，因为这个命令是为了设置 GUI 图形窗口的字体，而非图形文件的输出。\n因此需要更改方式指定字体形式，GPT 给出如下修改建议\n\n导出图像文件时使用 Times New Roman 字体：graph export mygraph.tif, fontface(\"Times New Roman\") width(800) replace\n在绘图时直接指定字体：twoway (scatter y x), title(\"My Title\", fontface(\"Times New Roman\"))\n\n但是上述解决方案均无法运行，无法正确生成对应的图像，因此此处跳过设置字体",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "21-复杂与精准绘图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-10-advanced-plot.html#导出图像",
    "href": "Guide/Stata/25-05-10-advanced-plot.html#导出图像",
    "title": "21-复杂与精准绘图",
    "section": "5 导出图像",
    "text": "5 导出图像\n可选择的导出格式很多，请根据实际需要进行选择，并且可以调整像素大小，一般来说 2400 足够满足大多数期刊需求\n导出命令必须紧接在绘图命令之后，不然会报错\n\n%%stata\ntwoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue)) ///\n(scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n(lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n(lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\nxtitle(\"weight\") ytitle(\"length\") ///\nxlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)5000) ///\ntitle(\"Association Between Weight and Length\", size(medianlarge)) ///\nsubtitle(\"by Region\", size(small)) ///\ncaption(\"Data Source:1978 auto data\", size(small)) ///\nnote(\"With 95% CI\", size(small)) ///\nlegend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2) ring(0) pos(10) ///\nregion(lwidth(none))) graphregion(margin(tiny))\n\n* change the pixel and specify the format\ngraph export \"Graph_test.svg\", width(2400) replace\n\n\n. twoway(scatter weight length if foreign ==0,jitter(2) msize(tiny) mcolor(blue\n&gt; )) ///\n&gt; (scatter weight length if foreign ==1,jitter(2) msize(tiny) mcolor(pink)) ///\n&gt; (lfitci weight length if foreign ==0,clcolor(blue) clwidth(thick)) ///\n&gt; (lfitci weight length if foreign ==1,clcolor(pink) clwidth(thick)), ///\n&gt; xtitle(\"weight\") ytitle(\"length\") ///\n&gt; xlabel(140(10)240) ylabel(1000(1000)5000) xtick(140(5)240) ytick(1000(500)500\n&gt; 0) ///\n&gt; title(\"Association Between Weight and Length\", size(medianlarge)) ///\n&gt; subtitle(\"by Region\", size(small)) ///\n&gt; caption(\"Data Source:1978 auto data\", size(small)) ///\n&gt; note(\"With 95% CI\", size(small)) ///\n&gt; legend(label(1 \"Domestic\") label(2 \"Foreign\") order(1 2) ring(0) pos(10) ///\n&gt; region(lwidth(none))) graphregion(margin(tiny))\n(note:  named style medianlarge not found in class gsize, default attributes\n    used)\n\n. \n. * change the pixel and specify the format\n. graph export \"Graph_test.svg\", width(2400) replace\n(file Graph_test.svg not found)\nfile Graph_test.svg saved as SVG format\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "21-复杂与精准绘图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-10-coefplot.html",
    "href": "Guide/Stata/25-05-10-coefplot.html",
    "title": "22-coefplot",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "22-coefplot"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-10-coefplot.html#coefplot",
    "href": "Guide/Stata/25-05-10-coefplot.html#coefplot",
    "title": "22-coefplot",
    "section": "1 coefplot",
    "text": "1 coefplot\nStata module for plotting regression coefficients and other results\ncoefplot plots results from estimation commands or Stata matrices. Results from multiple models or matrices can be combined in a single graph. The default behavior of coefplot is to draw markers for coefficients and horizontal spikes for confidence intervals. However, coefplot can also produce various other types of graphs.\n具体可以参见：\ncoefplot github.com/benjann/coefplot\n\n1.1 安装 coefplot\n\n%%stata\nssc install coefplot, replace\n\nchecking coefplot consistency and verifying not already installed...\nall files already exist and are up to date.\n\n\n\n\n1.2 查看使用文档\n在控制台输入：help coefplot\n\n\n1.3 建议\n如果在分析中使用该包辅助分析，请 cite related information，like：\n\nJann, Ben (2014). Plotting regression coefficients and other estimates. The Stata Journal 14(4): 708-737.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "22-coefplot"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-10-coefplot.html#导入数据",
    "href": "Guide/Stata/25-05-10-coefplot.html#导入数据",
    "title": "22-coefplot",
    "section": "2 导入数据",
    "text": "2 导入数据\n\n%%stata\nsysuse auto.dta,clear\n\n(1978 automobile data)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "22-coefplot"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-10-coefplot.html#线性回归",
    "href": "Guide/Stata/25-05-10-coefplot.html#线性回归",
    "title": "22-coefplot",
    "section": "3 线性回归",
    "text": "3 线性回归\n\n%%stata\nreg price mpg trunk length turn\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(4, 69)        =      5.79\n       Model |   159570047         4  39892511.8   Prob &gt; F        =    0.0004\n    Residual |   475495349        69  6891236.94   R-squared       =    0.2513\n-------------+----------------------------------   Adj R-squared   =    0.2079\n       Total |   635065396        73  8699525.97   Root MSE        =    2625.1\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -186.8417   88.17601    -2.12   0.038     -362.748   -10.93533\n       trunk |  -12.72642   104.8785    -0.12   0.904    -221.9534    196.5005\n      length |   54.55294   35.56248     1.53   0.130    -16.39227    125.4981\n        turn |  -200.3248   140.0166    -1.43   0.157    -479.6502    79.00066\n       _cons |   8009.893   6205.538     1.29   0.201    -4369.817     20389.6\n------------------------------------------------------------------------------\n\n\n\n%%stata\ncoefplot\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot,drop(_cons)\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot,drop(_cons mpg)\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot,keep(trunk turn)\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot,drop(_cons) xline(0)\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot,drop(_cons) xline(0) ciopts(recast(rcap))\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot,drop(_cons) ciopts(recast(rcap)) vertical\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot,drop(_cons) yline(0) ciopts(recast(rcap)) vertical",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "22-coefplot"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-10-coefplot.html#logit",
    "href": "Guide/Stata/25-05-10-coefplot.html#logit",
    "title": "22-coefplot",
    "section": "4 Logit",
    "text": "4 Logit\n这里得到的 coef 都是 log(Odds)\n\n%%stata\nlogit foreign mpg trunk length turn\n\n\nIteration 0:  Log likelihood =  -45.03321  \nIteration 1:  Log likelihood = -27.669278  \nIteration 2:  Log likelihood = -25.257138  \nIteration 3:  Log likelihood = -25.074055  \nIteration 4:  Log likelihood = -25.073487  \nIteration 5:  Log likelihood = -25.073487  \n\nLogistic regression                                     Number of obs =     74\n                                                        LR chi2(4)    =  39.92\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -25.073487                             Pseudo R2     = 0.4432\n\n------------------------------------------------------------------------------\n     foreign | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -.1229744   .0824787    -1.49   0.136    -.2846296    .0386808\n       trunk |  -.0029202   .1167932    -0.03   0.980    -.2318307    .2259904\n      length |  -.0438113   .0396085    -1.11   0.269    -.1214426      .03382\n        turn |  -.4878082   .1809598    -2.70   0.007    -.8424828   -.1331335\n       _cons |   28.35416   7.987812     3.55   0.000     12.69833    44.00998\n------------------------------------------------------------------------------\n\n\n\n%%stata\ncoefplot,drop(_cons) xline(0)\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot,drop(_cons) xline(0) xtitle(\"Log Odds\")\n\n\n\n\n\n\n\n\n\n4.1 subgroup\n\n%%stata\nreg price mpg trunk length turn if foreign == 0\nest store D\nreg price mpg trunk length turn if foreign == 1\nest store F\n\ncoefplot D F,drop(_cons) xline(0)\n\n\n. reg price mpg trunk length turn if foreign == 0\n\n      Source |       SS           df       MS      Number of obs   =        52\n-------------+----------------------------------   F(4, 47)        =      4.46\n       Model |   134498637         4  33624659.2   Prob &gt; F        =    0.0039\n    Residual |   354696164        47  7546726.89   R-squared       =    0.2749\n-------------+----------------------------------   Adj R-squared   =    0.2132\n       Total |   489194801        51  9592054.92   Root MSE        =    2747.1\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -186.1083    161.561    -1.15   0.255    -511.1271    138.9104\n       trunk |  -60.83919   153.2709    -0.40   0.693    -369.1805    247.5021\n      length |   52.68785   53.74091     0.98   0.332    -55.42491    160.8006\n        turn |  -15.89922   183.7335    -0.09   0.931    -385.5233    353.7249\n       _cons |   984.7459   10995.08     0.09   0.929     -21134.5    23103.99\n------------------------------------------------------------------------------\n\n. est store D\n\n. reg price mpg trunk length turn if foreign == 1\n\n      Source |       SS           df       MS      Number of obs   =        22\n-------------+----------------------------------   F(4, 17)        =     11.08\n       Model |   104341804         4    26085451   Prob &gt; F        =    0.0001\n    Residual |  40021408.7        17  2354200.51   R-squared       =    0.7228\n-------------+----------------------------------   Adj R-squared   =    0.6575\n       Total |   144363213        21   6874438.7   Root MSE        =    1534.3\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -55.72135   70.93618    -0.79   0.443    -205.3836    93.94092\n       trunk |   160.2078   115.4879     1.39   0.183    -83.45041     403.866\n      length |   137.5743   35.85164     3.84   0.001     61.93393    213.2146\n        turn |   -54.3835   339.5595    -0.16   0.875    -770.7914    662.0244\n       _cons |  -15324.62   11573.58    -1.32   0.203    -39742.73    9093.486\n------------------------------------------------------------------------------\n\n. est store F\n\n. \n. coefplot D F,drop(_cons) xline(0)\n\n. \n\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot (D, label(\"Domestic\")) (F, label(\"Foreign\")), ///\ndrop(_cons) xline(0)\n\n\n. coefplot (D, label(\"Domestic\")) (F, label(\"Foreign\")), ///\n&gt; drop(_cons) xline(0)\n\n. \n\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot (D, label(\"Domestic\")) (F, label(\"Foreign\")), ///\ndrop(_cons) xline(0) ciopts(recast(rcap)) msymbol(S)\n\n\n. coefplot (D, label(\"Domestic\")) (F, label(\"Foreign\")), ///\n&gt; drop(_cons) xline(0) ciopts(recast(rcap)) msymbol(S)\n\n. \n\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot (D, label(\"Domestic\") msymbol(S)) ///\n(F, label(\"Foreign\") msymbol(C)), ///\ndrop(_cons) xline(0) ciopts(recast(rcap)) \n\n\n. coefplot (D, label(\"Domestic\") msymbol(S)) ///\n&gt; (F, label(\"Foreign\") msymbol(C)), ///\n&gt; drop(_cons) xline(0) ciopts(recast(rcap)) \n(note:  named style C not found in class symbol, default attributes used)\n\n. \n\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot (D, label(\"Domestic\") msymbol(S)) ///\n(F, label(\"Foreign\") msymbol(C)), ///\ndrop(_cons) xline(0) ciopts(recast(rcap)) nooffsets\n\n\n. coefplot (D, label(\"Domestic\") msymbol(S)) ///\n&gt; (F, label(\"Foreign\") msymbol(C)), ///\n&gt; drop(_cons) xline(0) ciopts(recast(rcap)) nooffsets\n(note:  named style C not found in class symbol, default attributes used)\n\n. \n\n\n\n\n\n\n\n\n\n\n%%stata\ncoefplot (D, label(\"Domestic\") msymbol(S) offset(0.1)) ///\n(F, label(\"Foreign\") msymbol(C) offset(-0.1)), ///\ndrop(_cons) xline(0) ciopts(recast(rcap)) offset\n\n\n. coefplot (D, label(\"Domestic\") msymbol(S) offset(0.1)) ///\n&gt; (F, label(\"Foreign\") msymbol(C) offset(-0.1)), ///\n&gt; drop(_cons) xline(0) ciopts(recast(rcap)) offset\n(note:  named style C not found in class symbol, default attributes used)\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "22-coefplot"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html",
    "href": "Guide/Stata/25-05-11-2-DID.html",
    "title": "24-双重差分(DID)",
    "section": "",
    "text": "双重差分回归 (DID) 用于评估一个事件的因果效应，其方法是比较事件发生的单元集合（处理组）与事件未发生的单元集合（控制组）。\nDID 背后的逻辑是，如果事件从未发生，处理组和控制组之间的差异应该随着时间的推移保持不变。\nDID 通过比较处理组和控制组在事件发生前后的差异来估计事件的因果效应。\nDID 法是一种无法随机分配样本情况下的替代方法，主要应用于区域行的策略评估问题。\n目标：获取相对同质的策略组和控制组，这个“相对”是指除策略影响外，策略组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n对于相对同质的策略组和控制组，DID法通过第一次的差分消除这个基本固定的差异，通过第二次的差分消除时间趋势的影响，评估策略带来的实际效应。\n从DID 法的目标中可知，该方法面对的实验数据是面板数据（多个时间点的截面数据组成面板数据），即在策略干预时间点前，至少有两个时间点的数据。\n\\[\ny = \\alpha_0 +\\alpha_1g +\\alpha_2T + \\alpha_3gT + \\epsilon\n\\] \\(\\alpha_0\\)为常数项，\\(\\alpha_1\\)为处理组和控制组的差异，\\(\\alpha_2\\)为时间效应，\\(\\epsilon\\)为误差项。 \\(\\alpha_3\\)为交互项的系数，表示处理组和控制组在事件发生前后的差异。\n其中，\\(y\\)为结果变量，\\(g\\)为处理组和控制组的虚拟变量，\\(T\\)为时间虚拟变量，\\(gT\\)为交互项。 \\(\\alpha_3\\)为DID估计量，表示处理组和控制组在事件发生前后的差异。\nDID 模型的有效性检验\n为了保证该模型的有效性，在试验设计时需要满足平行趋势假设：在事件发生前，处理组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n平行趋势，即策略组和控制组在干预前保持相同的变化趋势。\n3种常见的平行趋势的检验方法：\n\n画图法：画出处理组和控制组在事件发生前后的结果变量的变化趋势图，观察两组的变化趋势是否平行。\n统计检验法：使用t检验或F检验等统计方法，检验处理组和控制组在事件发生前的结果变量的差异是否显著。\n伪DID法：在事件发生前，随机选择一个时间点，将处理组和控制组的结果变量进行差分，检验差分后的结果变量是否显著。\n\n\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#什么是-did",
    "href": "Guide/Stata/25-05-11-2-DID.html#什么是-did",
    "title": "24-双重差分(DID)",
    "section": "",
    "text": "双重差分回归 (DID) 用于评估一个事件的因果效应，其方法是比较事件发生的单元集合（处理组）与事件未发生的单元集合（控制组）。\nDID 背后的逻辑是，如果事件从未发生，处理组和控制组之间的差异应该随着时间的推移保持不变。\nDID 通过比较处理组和控制组在事件发生前后的差异来估计事件的因果效应。\nDID 法是一种无法随机分配样本情况下的替代方法，主要应用于区域行的策略评估问题。\n目标：获取相对同质的策略组和控制组，这个“相对”是指除策略影响外，策略组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n对于相对同质的策略组和控制组，DID法通过第一次的差分消除这个基本固定的差异，通过第二次的差分消除时间趋势的影响，评估策略带来的实际效应。\n从DID 法的目标中可知，该方法面对的实验数据是面板数据（多个时间点的截面数据组成面板数据），即在策略干预时间点前，至少有两个时间点的数据。\n\\[\ny = \\alpha_0 +\\alpha_1g +\\alpha_2T + \\alpha_3gT + \\epsilon\n\\] \\(\\alpha_0\\)为常数项，\\(\\alpha_1\\)为处理组和控制组的差异，\\(\\alpha_2\\)为时间效应，\\(\\epsilon\\)为误差项。 \\(\\alpha_3\\)为交互项的系数，表示处理组和控制组在事件发生前后的差异。\n其中，\\(y\\)为结果变量，\\(g\\)为处理组和控制组的虚拟变量，\\(T\\)为时间虚拟变量，\\(gT\\)为交互项。 \\(\\alpha_3\\)为DID估计量，表示处理组和控制组在事件发生前后的差异。\nDID 模型的有效性检验\n为了保证该模型的有效性，在试验设计时需要满足平行趋势假设：在事件发生前，处理组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n平行趋势，即策略组和控制组在干预前保持相同的变化趋势。\n3种常见的平行趋势的检验方法：\n\n画图法：画出处理组和控制组在事件发生前后的结果变量的变化趋势图，观察两组的变化趋势是否平行。\n统计检验法：使用t检验或F检验等统计方法，检验处理组和控制组在事件发生前的结果变量的差异是否显著。\n伪DID法：在事件发生前，随机选择一个时间点，将处理组和控制组的结果变量进行差分，检验差分后的结果变量是否显著。\n\n\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#导入数据",
    "href": "Guide/Stata/25-05-11-2-DID.html#导入数据",
    "title": "24-双重差分(DID)",
    "section": "2 导入数据",
    "text": "2 导入数据\n使用 Princeton University 提供的示例程序与数据集，具体参见：Differences‐in‐Differences (using Stata)\n\n%%stata\nuse \"http://dss.princeton.edu/training/Panel101.dta\", clear",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#创建变量",
    "href": "Guide/Stata/25-05-11-2-DID.html#创建变量",
    "title": "24-双重差分(DID)",
    "section": "3 创建变量",
    "text": "3 创建变量\n\n3.1 创建时间虚拟变量\n创建一个虚拟变量来指示治疗开始的时间。假设治疗始于 1994 年。在这种情况下，1994 年之前的值为 0，1994 年之后的值为 1。如果您已经创建了虚拟变量，请跳过此步骤。\n\n%%stata\ngen time = (year&gt;=1994) & !missing(year)\n\n\n\n3.2 创建治疗虚拟变量\n创建一个虚拟变量来标识接受治疗的组。在本例中，假设代码为 5、6 和 7 的国家/地区接受了治疗 (=1)。代码为 1-4 的国家/地区未接受治疗 (=0)。如果您已经创建了虚拟变量，请跳过此步骤。\n\n%%stata\ngen treated = (country&gt;4) & !missing(country)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#创建交互项",
    "href": "Guide/Stata/25-05-11-2-DID.html#创建交互项",
    "title": "24-双重差分(DID)",
    "section": "4 创建交互项",
    "text": "4 创建交互项\n在时间和治疗之间创建交互。我们将此交互称为 “did”\n\n%%stata\ngen did = time*treated",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#估计-did-估计量",
    "href": "Guide/Stata/25-05-11-2-DID.html#估计-did-估计量",
    "title": "24-双重差分(DID)",
    "section": "5 估计 DID 估计量",
    "text": "5 估计 DID 估计量\n\n%%stata\nreg y time treated did, r\n\n\nLinear regression                               Number of obs     =         70\n                                                F(3, 66)          =       2.17\n                                                Prob &gt; F          =     0.0998\n                                                R-squared         =     0.0827\n                                                Root MSE          =     3.0e+09\n\n------------------------------------------------------------------------------\n             |               Robust\n           y | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        time |   2.29e+09   9.00e+08     2.54   0.013     4.92e+08    4.09e+09\n     treated |   1.78e+09   1.05e+09     1.70   0.094    -3.11e+08    3.86e+09\n         did |  -2.52e+09   1.45e+09    -1.73   0.088    -5.42e+09    3.81e+08\n       _cons |   3.58e+08   7.61e+08     0.47   0.640    -1.16e+09    1.88e+09\n------------------------------------------------------------------------------\n\n\ndid 的系数是 双重差分 的估计量。效果在置信水准为 10% 时显著，且治疗措施产生了负面影响。\n估计 DID 中的差分估计量（使用 # 方法，无需生成交互项）\n\n%%stata\nreg y time##treated, r\n\n\nLinear regression                               Number of obs     =         70\n                                                F(3, 66)          =       2.17\n                                                Prob &gt; F          =     0.0998\n                                                R-squared         =     0.0827\n                                                Root MSE          =     3.0e+09\n\n------------------------------------------------------------------------------\n             |               Robust\n           y | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      1.time |   2.29e+09   9.00e+08     2.54   0.013     4.92e+08    4.09e+09\n   1.treated |   1.78e+09   1.05e+09     1.70   0.094    -3.11e+08    3.86e+09\n             |\ntime#treated |\n        1 1  |  -2.52e+09   1.45e+09    -1.73   0.088    -5.42e+09    3.81e+08\n             |\n       _cons |   3.58e+08   7.61e+08     0.47   0.640    -1.16e+09    1.88e+09\n------------------------------------------------------------------------------\n\n\n变量 time#treated 的系数即为 DID 估计量（对应前述示例中的 did ）。该估计在10%的显著性水平下显著，显示处理措施产生了负向影响",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#使用-diff-命令",
    "href": "Guide/Stata/25-05-11-2-DID.html#使用-diff-命令",
    "title": "24-双重差分(DID)",
    "section": "6 使用 diff 命令",
    "text": "6 使用 diff 命令\ndiff 由外部宏包提供，需要用户进行安装才可以使用：\n\n%%stata\nssc install diff\n\nchecking diff consistency and verifying not already installed...\ninstalling into C:\\Users\\asus\\ado\\plus\\...\ninstallation complete.\n\n\n\n%%stata\ndiff y, t(treated) p(time)\n\n\nDIFFERENCE-IN-DIFFERENCES ESTIMATION RESULTS\n--------------------------------------------\nNumber of observations in the DIFF-IN-DIFF: 70\n            Before         After    \n   Control: 16             24          40\n   Treated: 12             18          30\n            28             42\n--------------------------------------------------------\n Outcome var.   | y       | S. Err. |   |t|   |  P&gt;|t|\n----------------+---------+---------+---------+---------\nBefore          |         |         |         | \n   Control      |  3.6e+08|         |         | \n   Treated      |  2.1e+09|         |         | \n   Diff (T-C)   |  1.8e+09|  1.1e+09| 1.58    | 0.120\nAfter           |         |         |         | \n   Control      |  2.6e+09|         |         | \n   Treated      |  1.9e+09|         |         | \n   Diff (T-C)   | -7.4e+08|  9.2e+08| 0.81    | 0.422\n                |         |         |         | \nDiff-in-Diff    | -2.5e+09|  1.5e+09| 1.73    | 0.088*\n--------------------------------------------------------\nR-square:    0.08\n* Means and Standard Errors are estimated by linear regression\n**Inference: *** p&lt;0.01; ** p&lt;0.05; * p&lt;0.1\n\n\n使用 help diff 查看更多的细节和选项。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#双重差分",
    "href": "Guide/Stata/25-05-11-2-DID.html#双重差分",
    "title": "24-双重差分(DID)",
    "section": "7 双重差分",
    "text": "7 双重差分",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#前后虚拟变量",
    "href": "Guide/Stata/25-05-11-2-DID.html#前后虚拟变量",
    "title": "24-双重差分(DID)",
    "section": "8 前后虚拟变量",
    "text": "8 前后虚拟变量\n\n8.1 创建一个指示变量\n\n0 ：表示事件发生之前的时间\n1 ：表示事件发生之时及其之后的时间\n\n\n\n8.2 导入数据\n\n%%stata\nuse \"C:\\Users\\asus\\Desktop\\R\\quarto\\Med-Stat-Notes\\Data\\WDI.dta\", clear\n\n\n虚拟事件 X 在 2009 年发生，影响所有国家\n构建一个事件前后虚拟变量：0 表示事件发生前，1 表示事件发生后\n\n\n%%stata\ngen after = (year &gt;= 2009) if !missing(year)\n\n\n\n8.3 检查数据类型\n\n%%stata\ntab year after\n\n\n           |         after\n      Time |         0          1 |     Total\n-----------+----------------------+----------\n      2000 |       126          0 |       126 \n      2001 |       126          0 |       126 \n      2002 |       126          0 |       126 \n      2003 |       126          0 |       126 \n      2004 |       126          0 |       126 \n      2005 |       126          0 |       126 \n      2006 |       126          0 |       126 \n      2007 |       126          0 |       126 \n      2008 |       126          0 |       126 \n      2009 |         0        126 |       126 \n      2010 |         0        126 |       126 \n      2011 |         0        126 |       126 \n      2012 |         0        126 |       126 \n      2013 |         0        126 |       126 \n      2014 |         0        126 |       126 \n      2015 |         0        126 |       126 \n      2016 |         0        126 |       126 \n      2017 |         0        126 |       126 \n      2018 |         0        126 |       126 \n      2019 |         0        126 |       126 \n      2020 |         0        126 |       126 \n      2021 |         0        126 |       126 \n-----------+----------------------+----------\n     Total |     1,134      1,638 |     2,772",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#干预变量",
    "href": "Guide/Stata/25-05-11-2-DID.html#干预变量",
    "title": "24-双重差分(DID)",
    "section": "9 干预变量",
    "text": "9 干预变量\n创建一个指示变量，用于识别接受处理的观测单位，其中：\n\n0 表示从未接受处理的单位，例如，从未实施相关政策的州；\n1 表示接受过处理的单位，例如，实施过相关政策的州。\n\n例如，若 abc、xyz 和 cgi 三个州属于处理组，且州名称为字符串格式，则可按如下方式创建处理变量：\ngen treated = (state == \"abc\" | /// state == \"xyz\" | ///\nstate == \"cgi\")  if !missing(state)\n在本示例中，接受处理的国家已另存为一个虚拟的 Stata 数据集，其中包含一个名为 treated、取值为 1 的变量。\n接下来我们将该文件合并，以便在主数据集中获得处理变量。\n\n%%stata\nmerge m:1 country using \"C:\\Users\\asus\\Desktop\\R\\quarto\\Med-Stat-Notes\\Data\\Treated.dta\", gen(merge1)\n\n\n    Result                      Number of obs\n    -----------------------------------------\n    Not matched                         1,276\n        from master                     1,276  (merge1==1)\n        from using                          0  (merge1==2)\n\n    Matched                             1,496  (merge1==3)\n    -----------------------------------------\n\n\n未处理的单位将显示为缺失值（.）\n\n%%stata\nreplace treated = 0 if treated == .\n\n(1,276 real changes made)\n\n\n查看数据\n\n%%stata\ntab country treated\n\n\n                      |        treated\n         Country Name |         0          1 |     Total\n----------------------+----------------------+----------\n              Albania |         0         22 |        22 \n              Algeria |        22          0 |        22 \n            Argentina |        22          0 |        22 \n              Armenia |         0         22 |        22 \n            Australia |        22          0 |        22 \n              Austria |        22          0 |        22 \n         Bahamas, The |        22          0 |        22 \n           Bangladesh |         0         22 |        22 \n              Belarus |         0         22 |        22 \n              Belgium |        22          0 |        22 \n               Belize |        22          0 |        22 \n                Benin |        22          0 |        22 \n               Bhutan |         0         22 |        22 \n              Bolivia |         0         22 |        22 \nBosnia and Herzegov.. |         0         22 |        22 \n             Botswana |        22          0 |        22 \n               Brazil |        22          0 |        22 \n    Brunei Darussalam |         0         22 |        22 \n             Bulgaria |         0         22 |        22 \n              Burundi |         0         22 |        22 \n             Cambodia |         0         22 |        22 \n             Cameroon |         0         22 |        22 \n               Canada |        22          0 |        22 \n                Chile |         0         22 |        22 \n             Colombia |         0         22 |        22 \n              Comoros |        22          0 |        22 \n     Congo, Dem. Rep. |        22          0 |        22 \n          Congo, Rep. |        22          0 |        22 \n           Costa Rica |         0         22 |        22 \n        Cote d'Ivoire |        22          0 |        22 \n              Croatia |         0         22 |        22 \n                 Cuba |        22          0 |        22 \n               Cyprus |         0         22 |        22 \n              Czechia |         0         22 |        22 \n              Denmark |        22          0 |        22 \n   Dominican Republic |         0         22 |        22 \n              Ecuador |        22          0 |        22 \n     Egypt, Arab Rep. |         0         22 |        22 \n          El Salvador |         0         22 |        22 \n              Estonia |         0         22 |        22 \n             Eswatini |         0         22 |        22 \n              Finland |        22          0 |        22 \n               France |        22          0 |        22 \n                Gabon |        22          0 |        22 \n              Germany |        22          0 |        22 \n               Greece |         0         22 |        22 \n            Guatemala |         0         22 |        22 \n                Haiti |        22          0 |        22 \n             Honduras |        22          0 |        22 \n Hong Kong SAR, China |         0         22 |        22 \n              Hungary |        22          0 |        22 \n              Iceland |         0         22 |        22 \n                India |        22          0 |        22 \n            Indonesia |         0         22 |        22 \n   Iran, Islamic Rep. |         0         22 |        22 \n              Ireland |         0         22 |        22 \n               Israel |        22          0 |        22 \n                Italy |        22          0 |        22 \n                Japan |         0         22 |        22 \n               Jordan |        22          0 |        22 \n           Kazakhstan |         0         22 |        22 \n                Kenya |        22          0 |        22 \n          Korea, Rep. |        22          0 |        22 \n      Kyrgyz Republic |        22          0 |        22 \n               Latvia |        22          0 |        22 \n              Lebanon |        22          0 |        22 \n            Lithuania |         0         22 |        22 \n           Luxembourg |         0         22 |        22 \n     Macao SAR, China |         0         22 |        22 \n           Madagascar |         0         22 |        22 \n             Malaysia |         0         22 |        22 \n                 Mali |         0         22 |        22 \n                Malta |        22          0 |        22 \n           Mauritania |         0         22 |        22 \n            Mauritius |        22          0 |        22 \n               Mexico |         0         22 |        22 \n              Moldova |         0         22 |        22 \n              Morocco |         0         22 |        22 \n           Mozambique |         0         22 |        22 \n              Namibia |        22          0 |        22 \n          Netherlands |         0         22 |        22 \n          New Zealand |         0         22 |        22 \n            Nicaragua |         0         22 |        22 \n                Niger |        22          0 |        22 \n              Nigeria |        22          0 |        22 \n      North Macedonia |         0         22 |        22 \n               Norway |        22          0 |        22 \n                 Oman |         0         22 |        22 \n             Pakistan |        22          0 |        22 \n               Panama |         0         22 |        22 \n             Paraguay |         0         22 |        22 \n                 Peru |        22          0 |        22 \n          Philippines |        22          0 |        22 \n               Poland |         0         22 |        22 \n             Portugal |        22          0 |        22 \n              Romania |         0         22 |        22 \n   Russian Federation |        22          0 |        22 \n               Rwanda |         0         22 |        22 \n         Saudi Arabia |         0         22 |        22 \n              Senegal |        22          0 |        22 \n               Serbia |        22          0 |        22 \n         Sierra Leone |        22          0 |        22 \n            Singapore |         0         22 |        22 \n      Slovak Republic |        22          0 |        22 \n             Slovenia |         0         22 |        22 \n      Solomon Islands |        22          0 |        22 \n         South Africa |        22          0 |        22 \n                Spain |         0         22 |        22 \n            Sri Lanka |         0         22 |        22 \n                Sudan |         0         22 |        22 \n               Sweden |         0         22 |        22 \n          Switzerland |         0         22 |        22 \n             Tanzania |        22          0 |        22 \n             Thailand |        22          0 |        22 \n          Timor-Leste |        22          0 |        22 \n              Tunisia |         0         22 |        22 \n              Turkiye |         0         22 |        22 \n               Uganda |         0         22 |        22 \n              Ukraine |         0         22 |        22 \n       United Kingdom |        22          0 |        22 \n        United States |         0         22 |        22 \n              Uruguay |         0         22 |        22 \n           Uzbekistan |        22          0 |        22 \n              Vietnam |         0         22 |        22 \n   West Bank and Gaza |        22          0 |        22 \n             Zimbabwe |        22          0 |        22 \n----------------------+----------------------+----------\n                Total |     1,276      1,496 |     2,772",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#did-指示变量",
    "href": "Guide/Stata/25-05-11-2-DID.html#did-指示变量",
    "title": "24-双重差分(DID)",
    "section": "10 DID 指示变量",
    "text": "10 DID 指示变量\nDID（双重差分）指示变量是”处理变量”和”事件前后变量”的交互项。\n在本示例中，我们将处理变量命名为 treated，事件前后变量命名为 after（可根据自己的数据替换变量名）。\n下面创建 DID 指示变量:\n\n%%stata\ngen did = after * treated\n\n创建一个带标签的数值型变量，用作分组或面板变量。这是为了让 Stata 的相关命令能够识别数据中的面板结构\n\n%%stata\nencode country, gen(country1)\n\n将数据设置为面板数据格式（仅适用于以 xt 开头的命令）\n\n%%stata\nxtset country1 year\n\n\nPanel variable: country1 (strongly balanced)\n Time variable: year, 2000 to 2021\n         Delta: 1 unit\n\n\n事件在所有处理组中同时发生\n使用 Stata 的 xtdidregress 或 didregress 命令进行差分中的差分分析\n\n如果是面板数据，使用 xtdidregress；\n如果是重复截面数据（即不同时点的抽样调查），使用 didregress\n面板数据（panel data）：同一组单位在多个时间点被观察（如同一批国家或个人被多次追踪）\n重复截面数据（repeated cross-sectional data）：每个时间点抽取不同的单位样本（如每年全国抽样调查）",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#使用-stata-的-xtdidregress-命令",
    "href": "Guide/Stata/25-05-11-2-DID.html#使用-stata-的-xtdidregress-命令",
    "title": "24-双重差分(DID)",
    "section": "11 使用 Stata 的 xtdidregress 命令",
    "text": "11 使用 Stata 的 xtdidregress 命令\n仅适用于 Stata 17 及以上版本（手动估计方法见后续）。\n如需查看该命令的详细说明与示例，请输入：help xtdidregress\n\n%%stata\nxtdidregress (gdppc) (did), group(country1) time(year)\n\n\nTreatment and time information\n\nTime variable: year\nControl:       did = 0\nTreatment:     did = 1\n-----------------------------------\n             |   Control  Treatment\n-------------+---------------------\nGroup        |\n    country1 |        58         68\n-------------+---------------------\nTime         |\n     Minimum |      2000       2009\n     Maximum |      2000       2009\n-----------------------------------\n\nDifference-in-differences regression                     Number of obs = 2,772\nData type: Longitudinal\n\n                             (Std. err. adjusted for 126 clusters in country1)\n------------------------------------------------------------------------------\n             |               Robust\n       gdppc | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATET         |\n         did |\n   (1 vs 0)  |   1164.492   610.0838     1.91   0.059    -42.93971    2371.923\n------------------------------------------------------------------------------\nNote: ATET estimate adjusted for panel effects and time effects.\n\n\n\n11.1 平行趋势\n如需了解 didregress 的后估计命令的详细信息和示例，请输入：help xtdidregress_postestimation\n运行 xtdidregress\n\n%%stata\nxtdidregress (gdppc) (did), group(country1) time(year)\n\n\nTreatment and time information\n\nTime variable: year\nControl:       did = 0\nTreatment:     did = 1\n-----------------------------------\n             |   Control  Treatment\n-------------+---------------------\nGroup        |\n    country1 |        58         68\n-------------+---------------------\nTime         |\n     Minimum |      2000       2009\n     Maximum |      2000       2009\n-----------------------------------\n\nDifference-in-differences regression                     Number of obs = 2,772\nData type: Longitudinal\n\n                             (Std. err. adjusted for 126 clusters in country1)\n------------------------------------------------------------------------------\n             |               Robust\n       gdppc | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATET         |\n         did |\n   (1 vs 0)  |   1164.492   610.0838     1.91   0.059    -42.93971    2371.923\n------------------------------------------------------------------------------\nNote: ATET estimate adjusted for panel effects and time effects.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#xtdidregress-的可视化",
    "href": "Guide/Stata/25-05-11-2-DID.html#xtdidregress-的可视化",
    "title": "24-双重差分(DID)",
    "section": "12 xtdidregress 的可视化",
    "text": "12 xtdidregress 的可视化\n可视化的示例和更多信息，请输入：help xtdidregress_postestimation\n\n%%stata\nxtdidregress (gdppc) (did), group(country1) time(year) \nestat trendplots, ytitle(GDP pc)\n\n\n. xtdidregress (gdppc) (did), group(country1) time(year) \n\nTreatment and time information\n\nTime variable: year\nControl:       did = 0\nTreatment:     did = 1\n-----------------------------------\n             |   Control  Treatment\n-------------+---------------------\nGroup        |\n    country1 |        58         68\n-------------+---------------------\nTime         |\n     Minimum |      2000       2009\n     Maximum |      2000       2009\n-----------------------------------\n\nDifference-in-differences regression                     Number of obs = 2,772\nData type: Longitudinal\n\n                             (Std. err. adjusted for 126 clusters in country1)\n------------------------------------------------------------------------------\n             |               Robust\n       gdppc | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATET         |\n         did |\n   (1 vs 0)  |   1164.492   610.0838     1.91   0.059    -42.93971    2371.923\n------------------------------------------------------------------------------\nNote: ATET estimate adjusted for panel effects and time effects.\n\n. estat trendplots, ytitle(GDP pc)\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-2-DID.html#使用-ols-固定效应回归手动估计",
    "href": "Guide/Stata/25-05-11-2-DID.html#使用-ols-固定效应回归手动估计",
    "title": "24-双重差分(DID)",
    "section": "13 使用 OLS 固定效应回归（手动估计）",
    "text": "13 使用 OLS 固定效应回归（手动估计）\n\n13.1 双重差分的基础回归：所有单位在同一时间经历事件\n\n创建一个带标签的数值型变量，用作分组或面板变量\n\nencode country, gen(country1)\n\nDID 回归中不需要单独包括 after 和 treated 变量，因为已包含面板和时间固定效应\n\n在使用固定效应模型（如 xtreg, fe 或 areg 带 absorb()）进行差分中的差分回归时，只需要包含处理组与时间的交互项（即 treated#after），因为：\n\n个体固定效应 已控制了处理组与对照组之间的时间不变差异；\n时间固定效应 已控制了所有单位随时间的共同趋势。\n\nxtreg gdppc did i.year, fe vce(cluster country1)\n变量 did 的回归系数即为双重差分（DID）的估计量。该效应在 95% 显著性水平下不显著（P&gt;|t| &gt; 0.05），因此我们可以认为该事件对因变量没有显著影响。\n\n\n13.2 可视化平行趋势\nbysort year treated: egen mean_gdppc = mean(gdppc) \ntwoway line mean_gdppc year if treated == 0, sort || /// \nline mean_gdppc year if treated == 1, sort lpattern(dash) /// \nlegend(label(1 \"Control\") label(2 \"Treated\")) /// \nxline(2009)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "24-双重差分(DID)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-1-empirical-ana.html",
    "href": "Guide/Stata/25-05-11-1-empirical-ana.html",
    "title": "23-Empericial Analysis Intro",
    "section": "",
    "text": "社会科学研究方法之一，着眼于当前社会或学科现实，通过事例和经验等从理论上推理说明，那就属于实证分析。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "23-Empericial Analysis Intro"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-1-empirical-ana.html#实证分析的定义",
    "href": "Guide/Stata/25-05-11-1-empirical-ana.html#实证分析的定义",
    "title": "23-Empericial Analysis Intro",
    "section": "",
    "text": "社会科学研究方法之一，着眼于当前社会或学科现实，通过事例和经验等从理论上推理说明，那就属于实证分析。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "23-Empericial Analysis Intro"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-1-empirical-ana.html#基本含义",
    "href": "Guide/Stata/25-05-11-1-empirical-ana.html#基本含义",
    "title": "23-Empericial Analysis Intro",
    "section": "2 基本含义",
    "text": "2 基本含义\n实证研究方法概述实证研究（Empirical Research）方法是一种与规范研究（Normative Research）方法相对应的方法，它是基于观察和试验取得的大量事实、数据，利用统计推断的理论和技术，并经过严格的经验检验，而且引进数量模型，对社会现象进行数量分析的一种方法，其目的在于揭示各种社会现象的本质联系。相比规范研究方法，实证研究方法主要进行定量分析，依据数据说话，使其对社会问题的研究更精确、更科学。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "23-Empericial Analysis Intro"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-1-empirical-ana.html#应用场景",
    "href": "Guide/Stata/25-05-11-1-empirical-ana.html#应用场景",
    "title": "23-Empericial Analysis Intro",
    "section": "3 应用场景",
    "text": "3 应用场景\n实证分析广泛应用于经济学、社会学、心理学等领域，常用于市场分析、社会政策和健康医疗等实际问题的研究。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "23-Empericial Analysis Intro"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-1-empirical-ana.html#特点",
    "href": "Guide/Stata/25-05-11-1-empirical-ana.html#特点",
    "title": "23-Empericial Analysis Intro",
    "section": "4 特点",
    "text": "4 特点\n\n强调客观性和可验证性：实证分析通过搜集和分析数据来推断某个变量与另一个变量之间的关系，强调客观性和可验证性，而不是主观臆断或理论推演。\n使用统计学和计量经济学等方法：实证分析使用统计学和计量经济学等方法对数据进行处理和分析，以获得准确、可靠的结论。\n重视样本的大小和质量：实证分析需要足够大的样本来支持结论的可靠性，同时也需要注意样本的质量和代表性。\n推崇可重复性：实证分析的研究结果需要可重复，以验证结论的可靠性和稳定性。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "23-Empericial Analysis Intro"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-1-empirical-ana.html#实证研究的优势与局限",
    "href": "Guide/Stata/25-05-11-1-empirical-ana.html#实证研究的优势与局限",
    "title": "23-Empericial Analysis Intro",
    "section": "5 实证研究的优势与局限",
    "text": "5 实证研究的优势与局限\n实证研究方法的优势体现在以下三个方面：一是坚持因果规律是基本前提。有利于提升实证研究的逻辑可靠性。二是坚持归纳主义的原则。归纳主义注重从广泛的经验中总结出规律性的东西，这能提升实证研究的普遍适用性。三是坚持价值中立的原则。这从研究立场上提升了实证研究的客观性和科学性，中立的价值判断也让实证研究的结果更令人信服。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "23-Empericial Analysis Intro"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-1-empirical-ana.html#实证分析-stata-实现",
    "href": "Guide/Stata/25-05-11-1-empirical-ana.html#实证分析-stata-实现",
    "title": "23-Empericial Analysis Intro",
    "section": "6 实证分析 Stata 实现",
    "text": "6 实证分析 Stata 实现\n\n预备知识: 基本统计信息回顾\n线性模型: 线性回归，模型假设及预测量特性\n线性模型: 约束模型，哑变量交叉项\n线性模型: 异方差与序列相关，\n面板模型: 面板数据、内生性、固定效应\n计量方法: 因果识别、工具变量、DID与RDD\n非线性模型: 最大似然原理，Logit/Probit等\n时间序列模型: 稳定性及检验，AR,MA,ARMA等\n\n实证分析的学习笔记来着【实证计量及STATA操作】应用数据分析（全13集）",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "23-Empericial Analysis Intro"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-11-1-empirical-ana.html#统计学基本知识",
    "href": "Guide/Stata/25-05-11-1-empirical-ana.html#统计学基本知识",
    "title": "23-Empericial Analysis Intro",
    "section": "7 统计学基本知识",
    "text": "7 统计学基本知识\n\n随机变量\n分布函数 和 密度函数\n联合分布，边缘分布，条件分布\n期望、方差、协方差、相关系数、偏度、峰度\n条件期望、性质、迭代期望法则\n条件方差\n典型分布:正态分布、卡方分布、t分布、F分布",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "23-Empericial Analysis Intro"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html",
    "href": "Guide/SAS/25-05-22-SAS-Base.html",
    "title": "03-SAS Base",
    "section": "",
    "text": "逻辑概念：行、列结构的表格，每行称为一个观测，每列称为一个变量。类似于矩阵，但可以整列都取字符型值，并且变量名也是数据集 的一部分。\n等同于数据库术语“表(table)”。\n物理存储：操作系统中的文件。包括描述部分（文件创建信息、变量 名、变量的类型等属性）和数据部分。\n名字（变量名、数据集名等）：大小写不区分，其它规定与一般的高级 语言相同(由字母、下划线、数字组成，第一个字符必须是字母或下划 线，不允许使用汉字以及小数点、空格、减号等特殊字符)，不超过32 个字符长。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#sas-数据集",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#sas-数据集",
    "title": "03-SAS Base",
    "section": "",
    "text": "逻辑概念：行、列结构的表格，每行称为一个观测，每列称为一个变量。类似于矩阵，但可以整列都取字符型值，并且变量名也是数据集 的一部分。\n等同于数据库术语“表(table)”。\n物理存储：操作系统中的文件。包括描述部分（文件创建信息、变量 名、变量的类型等属性）和数据部分。\n名字（变量名、数据集名等）：大小写不区分，其它规定与一般的高级 语言相同(由字母、下划线、数字组成，第一个字符必须是字母或下划 线，不允许使用汉字以及小数点、空格、减号等特殊字符)，不超过32 个字符长。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#sas-逻辑库",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#sas-逻辑库",
    "title": "03-SAS Base",
    "section": "2 SAS 逻辑库",
    "text": "2 SAS 逻辑库\n\nSAS 逻辑库(library，也称数据库) 与SAS 数据集的关系，基本等同 于操作系统中子目录与子目录中文件的关系。\nSAS 逻辑库有一个“库名(libref)”，可以认做是操作系统中子目录的 一个SAS 中使用的别名。库名长度不超过8 个字符。\n建立SAS 逻辑库，实际是指把一个操作系统子目录与一个库名联系起 来。用 libname 语句或快捷图标中的 “New library” 图标。\n三个预定义的库: WORK、SASUSER、SASHELP。\n数据集的访问格式为“库名. 数据集名”格式，比如SASHELP 库中 的CLASS 数据集写成 SASHELP.CLASS 。\nWORK 库是“临时库”：其中的SAS 文件在退出SAS 系统时自动删 除。\n另外，WORK 库中的数据集可以用“一水平名”访问，即可以省略 “WORK.”。\nSashelp :永久逻辑库，包含样本数据及控制 SAS 在您的环境下如何工作的其他文件， 它是只读逻辑库。\nSasuser :永久逻辑库，包含的 SAS 文件位于存储个人设置的 Profile 目录下， 这也是便于您存储个人文件的逻辑库。\nSAS 资源管理器(Explorer) 管理SAS 文件。\n除数据集外，SAS 数据库中还可以包含叫做“SAS 目录簿”(SAS catalog) 的SAS 文件，主要用来保存设置、程序代码、图形这样的非 规整数据。\n在SAS 资源管理器中双击某一数据集进入 VIEWTABLE 数据管理界面。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#sasinsight",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#sasinsight",
    "title": "03-SAS Base",
    "section": "3 SAS/INSIGHT",
    "text": "3 SAS/INSIGHT\n注意，在SAS 9.4版本，不再支持 INSIGHT 。\nNote that SAS/INSIGHT® software was discontinued beginning in SAS 9.4 TS1M0.",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#sas-语句",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#sas-语句",
    "title": "03-SAS Base",
    "section": "4 SAS 语句",
    "text": "4 SAS 语句\n\nSAS 系统强大的数据管理能力、计算能力、分析能力依赖于作为其基 础的SAS 语言。\nSAS 语言是一个专用的数据管理与分析语言，它的数据管理功能类似 于数据库语言(如FoxPro1)，但又添加了一般高级程序设计语言的许多 成分(如分支、循环、数组)，以及专用于数据管理、统计计算的函数。\nSAS 系统的数据管理、报表、图形、统计分析等功能都可以用SAS 语 言程序来调用，只要指定要完成的任务就可以由SAS 系统按照预先设 计好的程序去执行。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#sas-语句的基本构成",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#sas-语句的基本构成",
    "title": "03-SAS Base",
    "section": "5 SAS 语句的基本构成",
    "text": "5 SAS 语句的基本构成\n\nSAS 语言程序由数据步和过程步组成。\n数据步用来生成数据集、计算、整理数据;\n过程步用来对数据进行分析、报告。\nSAS 语言的基本单位是语句, 每个SAS 语句一般由一个关键字(如 DATA, PROC, INPUT, CARDS, BY) 开头, 包含SAS 名字、特殊字 符、运算符等, 以分号结束。\nSAS 关键字是用于SAS 语句开头的特殊单词, SAS 语句除了赋值、累 加、注释、空语句以外都以关键字开头。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#sas-表达式",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#sas-表达式",
    "title": "03-SAS Base",
    "section": "6 SAS 表达式",
    "text": "6 SAS 表达式\n\nSAS 数据步程序中的计算用表达式完成。\n表达式把常量、变量、函数调用用运算符、括号连接起来得到一个计算结果。\n\n\n6.1 SAS 常量\n\nSAS 常量主要有数值型、字符型两种, 并且还提供了用于表达日期、时 间的数据类型。例如\n\n数值型：12, -7.5, 2:5E-10\n字符型：’Beijing’, ”Li Ming”, ” 李明”\n\n数值型常数可以用整数、定点实数、科学计数法实数表示。\n字符型常数为两边用单撇号或两边用双撇号包围的若干字符。当字符 串内容有单撇号时，可以用双撇号界定，当字符串内容有双撇号时，可 以用单撇号界定。另外，在字符串中，重复的单撇号可以作为一个单 撇号内容，如 ’Tom” cat’ 内容是 “Tom’s cat” 。\n\n\n\n6.2 SAS 日期和时间\n\nSAS 日期型常量如：’13JUL1998’d;\n日期型常数是在表示日期的字符串后加一个字母d(大小写均可), 中间 没有空格。日期值保存为从1960 年1 月1 日后经过的天数。\n时间型常量如：’14:20:15.32’t, ’14:20’t(没有秒);\n时间型常数是在表示时间的字符串后加一个字母t。保存为秒数。\n日期时间型常量如: ’13JUL1998:14:20:15.32’dt;\n日期时间型常数在表示日期时间的字符串后加字母 dt。保存为从1960 年1 月1 日0 时后经过的秒数。\n\n\n\n6.3 缺失值\n\n数据处理时经常遇到缺失值，如：\n\n数据遗失；\n被访者拒绝回答；\n节假日无数据。\n\nSAS 程序中用一个单独的小数点来表示数值型缺失值常量。\n对于字符型数据，用只有一个空格的字符串’ ’表示缺失值。\n变量X 是否数值型缺失值可以用X=.来判断，是否字符型缺失值可以用X=’ ’ 来判断。\n数值型缺失值在排序和比较时排在最小端。\n数值型缺失值参与计算的结果也是缺失的。\n\n\n\n6.4 SAS 变量及类型\n\nSAS 变量的基本类型有两种：数值型和字符型。\n日期、时间等变量存为数值型。\n存储长度：SAS 的数值型变量可以存储任意整数、定点实数、浮点实 数, 一般不关心其区别。缺省用8 个字符存储, 精度有16 到17 位有 效数字。\nSAS 在读入字符型变量时缺省的长度是8 个字符, 但是如果在INPUT 语句中输入字符型变量时指定了长度则不受此限制，最多可存32767 个字符。\n直接用赋值语句定义的字符型变量的缺省长度是第一次赋值的字符串 长度。\n可以用LENGTH 语句直接指定变量长度, LENGTH 语句一般应出现 在变量定义之前, 格式为:\n\nLENGTH 字符型变量名 $ 长度;",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#sas-运算符",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#sas-运算符",
    "title": "03-SAS Base",
    "section": "7 SAS 运算符",
    "text": "7 SAS 运算符\n\n7.1 SAS 算数运算符\n\nSAS 运算符包括算术、比较、逻辑等运算符。\n算术运算符为+, -, *, /, **, 运算优先级按通常的优先规则。其中两个星号**表示乘方运算。\n\n\n\n7.2 SAS 比较运算符\n\n比较运算符用于比较值大小, 包括\n\n\n\n=\nˆ=\n&gt;\n&lt;\n&gt;=\n&lt;=\nIN\n\n\n\n\nEQ\nNE\nGT\nLT\nGE\nLE\n\n\n\n\n比较运算符得到“真”或“假”的结果，真用1 表示，假用0 表示。\n两个字符型的比较是把短的一个右边补空格到两个长度相同后比较。\n在=, &gt;, &lt;, &gt;=, &lt;= 后面加一个冒号变成如 =: 这样则可以只比较与短的一个等长的部分。\n运算符 IN 是一个SAS 特有的比较运算符, 用来检查某个变量的取值 是否在一个给定列表中, 比如 prov in (' 北京', ' 天津', ' 上海', ' 重庆') 可以判断变量prov 的取值是否为四个直辖市之一。\n列表为圆括号界定的用逗号或空格分开的若干常量。\n也可以使用 NOT IN 表示 “不属于”。\n\n\n\n7.3 SAS 逻辑运算符\n\n逻辑运算符用来连接比较得到的结果以构成复杂的条件, 有三种逻辑 运算符: &(AND)  |(OR) ˆ(NOT)\n复杂的逻辑表达式最好用括号表示其运算优先级以免误记优先规则，这样也有利于阅读程序。\n\n\n\n7.4 其它运算符\n\n||(两个连续的 | 号)：连接两个字符串；\n&lt;&gt;：用于取两个运算值中较大一个(比如3 &lt;&gt; 5 结果为5)；\n&gt;&lt;：用于取两个运算值中较小一个(比如 3 &gt;&lt; 5 结果为3)。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#sas-程序规则",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#sas-程序规则",
    "title": "03-SAS Base",
    "section": "8 SAS 程序规则",
    "text": "8 SAS 程序规则\n\n8.1 基本规则\n\nSAS 程序由语句构成。\n每个语句以分号结尾(最常见的 SAS 编程错误就是丢失分号)。\n一个语句可以写到多行(不需任何续行标志), 也可以在一行连续写几个语句。\nSAS 语言中只要允许用一个空格的地方就可以加入任意多个空白(空格、制表符、回车)。\n允许用空格的地方是名字周围、运算符周围。\n\n\n\n8.2 SAS 程序规则示例\nproc print\n        data=c9501;\n    by avg;\nrun;\n等价于：\nproc print data=c9501;by avg;run;",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#sas-注释",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#sas-注释",
    "title": "03-SAS Base",
    "section": "9 SAS 注释",
    "text": "9 SAS 注释\n\n在SAS 程序中可以加入注释, 注释使用C 语言语法, 用 /* 和 */ 在两端界定注释, 这种注释可以出现在任何允许加入空格的位置, 可以占多行。\n另一种注释是把以星号 * 开头的行作为注释。\n一般只把注释单独占一行或若干行, 不把注释与程序代码放在同 一行。\n注释的另一个作用是把某些代码暂时屏蔽使其不能运行。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#footnotes",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#footnotes",
    "title": "03-SAS Base",
    "section": "脚注",
    "text": "脚注\n\n\nFoxPro是一种面向过程的编程语言和数据库管理系统，也是一种面向对象的编程语言，适用于MS-DOS 、Windows、Macintosh和UNIX。最初由Fox Software发布，FoxPro在1992年被微软整体收购后，由Microsoft负责发布和支持工作。FoxPro的最终发布版本是2.6。https://zh.wikipedia.org/zh-cn/FoxPro↩︎",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-24-SAS-program.html",
    "href": "Guide/SAS/25-05-24-SAS-program.html",
    "title": "04-SAS 程序基础",
    "section": "",
    "text": "import saspy\nsas = saspy.SASsession()\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 34172\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "04-SAS 程序基础"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-24-SAS-program.html#什么是-sas-程序",
    "href": "Guide/SAS/25-05-24-SAS-program.html#什么是-sas-程序",
    "title": "04-SAS 程序基础",
    "section": "1 什么是 SAS 程序",
    "text": "1 什么是 SAS 程序\nSAS 程序是在编辑器窗口中编辑的一段SAS语句,提交后可以在日志窗口中显示有关信息和提示,在输出窗口中显示运行过程的结果。下面通过一个简单的例子,来说明程序的结构。\n\n1.1 12份肝炎患者血清谷丙转氨酶——计算均数\n\n%%SAS\ndata prg1_1;\n    input x @@;\ndatalines;\n60 142 195 80 242 220 190 25 212 38 236 95\n;\nrun;\nproc means data=prg1_1;\n    var x;\n    quit;\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 25752\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: x\n\n\n数目\n均值\n标准差\n最小值\n最大值\n\n\n\n\n12\n144.5833333\n80.9797487\n25.0000000\n242.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 程序结构\n一个完整的 SAS程序一般由数据步(DATA STEP)和过程步(PROC STEP)两部分组成。\n数据步以关键词 DATA 开头,过程步以 PROC 开头,以 RUN 结束。PROC 为英文单词 PROCEDURE 的缩写,读作 prok。\n数据步的作用为指定数据集的名称,定义数据集的变量(如变量名称、变量类型等)和读入原始数据。\n本例数据步从 data prgl_1; 开始到数据下面出现的 run; 结束,建立了名为 prg1-1 的数据集。如果后面还有 SAS语句,该 run; 可省略不写。\n过程步的作用是调用现有的 SAS 过程对指定的数据集进行统计分析。\n本例过程步执行的是 means 过程,计算数据集( 默认数据集为 prg1-1 ,可用 data= 数据集 指定要分析的数据集,本例为 prg1-1)中数据的例数、均数、标准差、最小值和最大值等统计量。\n过程步从 proc means; 开始,到 quit; 结束。有时也可用 run; 结束。\n\n\n1.3 程序语法规范\nSAS程序由语句组成,每个语句以分号 ; 作为结束符号。同一行中可以有多个语句,一个语句也可以分几行编写。为方便检查和修改,每 行可输人一个语句,每个语句中各个元素以一个或几个空格分隔。输入程序语句时,可在光标闪烁处逐个字母输入,一行语句结束后,按 ENTER 换行,继续输人。值得注意的是, datalines 语句后面的数据必须另起一行输人,数据输入完毕后,必须另起一行,输入分号 ; 表示数据输入结束。\n\n\n1.4 程序运行\n当程序语句被确认正确无误后,可以将程序提交系统运行。提交程序的方法有以下几种:\n\n在执行指令的文本框中键入 SUBMIT 或 SUB ,然后点击“√”或按 ENTER 键。\n点击主菜单中的 运行(run) ,再点击 提交(submit)。\n使用功能键 F8 或自己定义的功能键。\n点击工具栏上的 run(submit) 一个奔跑的小人进行运行。\n\n\n\n1.5 程序修改\n通常情况下,在程序运行完毕后,要先检查日志窗口中的日志,看程序语句有无错误。 如果程序语句编写有误,而且该错误能被 SAS 系统纠正,则在日志窗口中会出现红色下划线和 错误标记,并用绿色字体提示错误,而程序照常运行。\n修改程序语句,首先将窗口切换到编辑器窗口,在原来有错误的地方修改程序语句,然后再提交运行。 有时需反复几次,直到日志窗口不再出现错误提示为止。\n\n\n1.6 程序保存\n程序语句编辑无误后,可以将编辑好的程序以文件的形式保存下来,以备以后检查或修改。\n保存程序可在命令框中键入指令 file '路径+文件名',该处路径为绝对路径,文件名的后缀必须是 .sas ,而且路径 + 文件名必须用单引号 括起来，如 file 'c:\\prgl.sas' 也可以通过 文件 菜单中的 保存 选项来保存程序文件。在出现的 保存 对话框中选择好路径,再键入文件名,此时不必加后缀,因为系统将默认后缀名为 sas 。也可以通过工具栏上的快捷方式保存程序文件,只需点击工具栏上的回就会出现 保存 对话框,然后按上述方法保存文件。\n\n\n1.7 程序调用\n有时,程序已经以文件的形式保存下来了，再作同样的处理时可不必再编辑程序,可以调用已有的程序完成统计运算。调用程序可用指令 INCLUDE 指令后面需加上用单引号括起来的文件所在的绝对路径和带后缀名为 .sas 的文件名,也可以用 文件 菜单中的 打开程序 选项来完成。\n也可以用快捷方式调用程序文件,点击工具栏上的 file 图标,以后的操作与上述 打开程序 操作相同同。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "04-SAS 程序基础"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#常用指令",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#常用指令",
    "title": "03-SAS Base",
    "section": "10 常用指令",
    "text": "10 常用指令\n命令框在 SAS视窗管理系统中主菜单的下方,是一个执行 SAS 指令的工具，在该工具的文本输人框中可以输人指令,用于实现对窗口的管理功能。有些指令只能在指定窗口执行,而有些指令是可以用于大多数窗口,称为窗口通用指令。输入一个指令,点击命令框前面的 “√” 或按下 “ENTER” 键,就可以执行该指令\n以下是几条比较常用的指令:\n\n\n\n指令\n作用\n\n\n\n\nBYE\n退出 SAS 系统\n\n\nCLEAR\n清除当前窗口中的内容\n\n\nEND\n退出当前窗口,返回编辑窗口\n\n\nENDSAS\n退出 SAS系统\n\n\nFILE ’filename\n将当前窗口的内容存储到指定文件\n\n\nHELP\n进入帮助窗口\n\n\nINCLUDE ’filename‘\n调用指定文件\n\n\nKEYS\n进入快捷键定义窗口\n\n\nLIBNAME\n确定 SAS 数据库的内容\n\n\nLOG\n进入日志窗口\n\n\nNUMS\n打开编辑窗口的数字区\n\n\nNUMS OFF\n关闭编辑窗口的数字区\n\n\nOPTIONS\n进入参数定义窗口\n\n\nOUTPUT\n进入输出窗口\n\n\nPROGRAM\n进入编辑窗口\n\n\nRECALL\n调用上次执行的程序\n\n\nSUBMIT\n提交编辑窗口编辑的程序代码",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-22-SAS-Base.html#功能键",
    "href": "Guide/SAS/25-05-22-SAS-Base.html#功能键",
    "title": "03-SAS Base",
    "section": "11 功能键",
    "text": "11 功能键\n上述有些指令可以通过SAS系统定义的功 能键来完成,这些功能键都是 SAS 系统预先设定的。在指令工具中键入“KEYS”,点击 “√”或按“ENTER”,或按“F9”,就可以查看和自定义功能键。用户可以自己定义功能,同时可以根据自己的习惯,改变功能键的位置。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "03-SAS Base"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-25-SAS-DATA-LIB.html",
    "href": "Guide/SAS/25-05-25-SAS-DATA-LIB.html",
    "title": "05-SAS 数据库与数据集",
    "section": "",
    "text": "import saspy\nsas = saspy.SASsession()\n%load_ext saspy.sas_magic\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 30068",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "05-SAS 数据库与数据集"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-25-SAS-DATA-LIB.html#数据库",
    "href": "Guide/SAS/25-05-25-SAS-DATA-LIB.html#数据库",
    "title": "05-SAS 数据库与数据集",
    "section": "1 数据库",
    "text": "1 数据库\n\n1.1 数据库与库标记\nSAS 数据库是一个包含数据集的文件夹。每个数据库都包含一个或多个数据集。SAS 数据库的扩展名为 .sas7bdat。SAS 数据库可以存储在本地计算机上，也可以存储在远程服务器上。\nSAS 数据库可以通过 SAS 程序或 SAS 界面创建和管理。SAS 数据库可以包含多种类型的数据集，包括 SAS 数据集、Excel 数据集、CSV 数据集等。SAS 数据库可以通过 SAS 程序或 SAS 界面进行访问和管理。\n数据库使得 SAS 系统能够在 SAS 程序中调用指定的文件。\n为了使用 SAS 数据库，需要为每个数据库制定一个库标记来识别。库标记又称库逻辑名或库关联名。库标记是一个 SAS 程序中的变量名，用于引用数据库中的数据集。库标记可以是任意合法的 SAS 变量名，但通常使用大写字母和下划线来命名。例如，WORK、SASUSER、MYDATA 等都是合法的库标记。\n\n\n1.2 数据库类型\nSAS 数据库有两种类型：永久数据库和临时数据库。\n\n永久数据库：永久数据库是指在 SAS 程序运行结束后仍然存在的数据库。永久数据库通常存储在本地计算机或远程服务器上。永久数据库可以通过 SAS 程序或 SAS 界面进行访问和管理；在安装 SAS 时，SAS 会自动创建一个永久数据库，名为 SASUSER，用于存储用户的个人数据集。永久数据库的库标记通常以大写字母和下划线命名，例如 MYDATA。\n临时数据库：临时数据库是指在 SAS 程序运行结束后自动删除的数据库。临时数据库通常存储在内存中。临时数据库只能通过 SAS 程序进行访问和管理，不能通过 SAS 界面进行访问和管理；临时数据库一般存储在 WORK 目录中，WORK 目录是一个临时数据库，用于存储临时数据集。临时数据库的库标记通常以小写字母和下划线命名，例如 work、temp 等。\n\n永久型数据库的库标记也可以由用户使用 libname 语句来自行定义，libname 语句的一般形式为：\nlibname libref 'path';\n其中，libref 是库标记，path 是数据库的路径。libname 语句可以在 SAS 程序的任何位置使用，但通常在程序的开头使用。libname 语句可以用于创建永久数据库，也可以用于访问已有的永久数据库。\n启动 SAS 后，除了 Sasuser 数据库外，还会自动生成另外两个永久型数据库，他们的库标记分别为 Sashelp 和 Maps 。\n另外，根据用户安装的模块，在启动 SAS 后，会自动生成一些特殊的永久型数据库。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "05-SAS 数据库与数据集"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-25-SAS-DATA-LIB.html#数据集",
    "href": "Guide/SAS/25-05-25-SAS-DATA-LIB.html#数据集",
    "title": "05-SAS 数据库与数据集",
    "section": "2 数据集",
    "text": "2 数据集\n\n2.1 数据集的类型\nSAS 数据集有两种类型: SAS 数据文件（SAS data files）和 SAS 数据视窗（SAS data views）。\nSAS 的数据文件不仅包括描述部分，也包括数据部分。SAS 的视窗文件只有描述部分，没有数据部分，它的描述部分包括的信息使 SAS 过程可以访问到实际上并不包含在数据视窗内部的数据。\n一般情况下所说的 SAS 数据集指的是 SAS 数据文件。\n\n\n2.2 变量类型\n数据集中的变量可以有两种类型:数值型和字符型。\n数值型变量只允许变量值为数字,SAS 过程可以对这些数字进行统计运算,如计算变量的均数、标准差等。一般情况下, SAS 默认数值型变量小数点后保留两位有效数值,而小数点前的位数就是该变量值所有数值中的最大位数。\n用户也可以用 length 语句或 attrib 语句自己定义变量长度。 数值型变量当数据缺失时, SAS 表示为 .。\n字符型变量允许变量值为中、英文字母、各种符号和数字,此时的数字被当作字符处理,无法进行统计运算。字符型变量的默认长度为8个字 节,然而 SAS 规定字符型变量的最大长度不能超过 200 个字节。字符型变量数据缺失时, SAS 表示为空格。\n\n\n2.3 数据集的命令\n每个 SAS 数据集都有一个两级文件名,第一级是库标记,第二级是文件名,两者之间用 . 分隔。在建立 SAS 数据集时,可以通过指定两级文 件名定义 SAS 数据集,便于以后用 SAS 过程来识别和处理。\n如例 data prg1-1,表示该数据集为临时数据集,临时数据集的第一级库标记应为 Work,也可以省略该库标记。 如果将该数据集存放在硬盘的另一个目录中如 Sasuser,则数据集名为 sasuser.prg1-1,其物理位置就是 Sasuser 这个库标记所指定的文件夹,该数据集将永久保留在硬盘上。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "05-SAS 数据库与数据集"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-25-SAS-DATA-LIB.html#数据集的建立",
    "href": "Guide/SAS/25-05-25-SAS-DATA-LIB.html#数据集的建立",
    "title": "05-SAS 数据库与数据集",
    "section": "3 数据集的建立",
    "text": "3 数据集的建立\n\n3.1 创建 SAS 数据集\n建立数据集一般在数据步中完成,可以通过直接录人数据或者导人其他格式的数据文件中的数据,来建立 SAS 可以识别的 SAS 数据集。主要有如下几种方法：\n\ninput 和 datalines 语句\n\n\n%%SAS\n/*程序2-1*/\n/*10例肾移植患者的部分数据*/\n/*M为男性,F为女性*/\ndata prg2_1;\n    input no sex $ age blood $ surt;\ndatalines;\n1 M 41 A 368\n2 M 26 B 745\n3 F 35 B 401\n4 M 37 AB 552\n5 F 37 A 478\n6 F 39 O 628\n7 M 28 O 549\n8 M 31 B 128\n9 M 43 AB 463\n10 M 29 A 512\n;\nrun;\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 25624\n\n\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\u00145                                                             SAS 系统                                    19:02 Monday, May 26, 202524         ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;24       ! ods graphics on / outputfmt=png;NOTE: 正在写入 HTML5(SASPY_INTERNAL) Body（主体）文件: _TOMODS125         26         /*程序2-1*/27         /*10例肾移植患者的部分数据*/28         /*M为男性,F为女性*/29         data prg2_1;30          input no sex $ age blood $ surt;31         datalines;NOTE: 数据集 WORK.PRG2_1 有 10 个观测和 5 个变量。NOTE: “DATA 语句”所用时间（总处理时间）:      实际时间          0.00 秒      CPU 时间          0.01 秒      42         ;43         run;44         45         46         47         ods html5 (id=saspy_internal) close;ods listing;48         \u00146                                                             SAS 系统                                    19:02 Monday, May 26, 202549         \n\n\n\n\n程序 2-1 中第一行 data prg2_1; 是要求建立一个文件名为 prg2_1 的数据集,该数据集是一个临时数据集,系统会自动将其存放在 Work目录中。第二行 input 语句定义了数据集的变量名和变量类型,其中 @ 符号表示在同一行中输入数据,而 datalines 语句则表示数据的输入结束。最后一行 run; 表示程序结束。\n第一行会建立一个名为 prg2_1 的数据集,该数据集是一个临时数据集,系统会自动将其存放在 Work 数据库目录中。文件的后缀名为 sas7bdat,所以从 Windows资源管理器中查看该文件,文件名为 prg2_1.sas7bdat。如果需建立永久型数据集,可在 prg2_1 前面加上库标记,如 sasuser.prg2_1,则该 数据集将保存 Sasuser 数据库中,退出 SAS 也不会将该数据集删除。\n第二行 input no sex $ age blood $ surt; 是要求在 prg2_1.sas7bdat 数据集中建立5个变量, 它们的变量名分别为 no、sex、age、blood 和 surt,其中sex和blood变量名后面加上了一个符号 $,表示这些变量为字符型变量,其他未加 $ 的变量则默认为数值型变量。\n第三行 datalines; 表明开始对变量进行赋值,它向 SAS 指示下一行开始是数据行,直到分号出现,数据行赋值结束。而该分号必须出现在所有数据的下一行,才表示结束数据行。数据行中不同变量的数据之间可用一个或多个空格 分开。\n最后一行 run; 表示 data 步的结束,当后面还有其他数据步或过程步语句,该语句可省略。\n打印数据集\n\n%%SAS\n/*程序2-2*/\n/*print过程将数据显示在输出窗口中*/\nproc print data = prg2_1;\nrun;\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\nno\nsex\nage\nblood\nsurt\n\n\n\n\n1\n1\nM\n41\nA\n368\n\n\n2\n2\nM\n26\nB\n745\n\n\n3\n3\nF\n35\nB\n401\n\n\n4\n4\nM\n37\nAB\n552\n\n\n5\n5\nF\n37\nA\n478\n\n\n6\n6\nF\n39\n0\n628\n\n\n7\n7\nM\n28\n0\n549\n\n\n8\n8\nM\n31\nB\n128\n\n\n9\n9\nM\n43\nAB\n463\n\n\n10\n10\nM\n29\nA\n512\n\n\n\n\n\n\n\n\n\n\n横向输入\n如果数据集中变量较少，而观测值较多，可以使用横向输入的方法来输入数据，具体方法是在 input 语句中的变量名加上两个 @@ 符号，表示在同一行中输入数据，则在数据行中的数据可以横行排列,每个数据之间用空格分隔。\n具体如下程序 2-3 所示：\n\n%%SAS\n/*程序2-3*/\n/*变量比较少，观测比较多，可以采用横向输入方法；具体方法是在input语句中的变量名后加上两个@@，则在数据行中的数据可以横行排列*/\ndata prg2_2;\n    input test @@;\ndatalines;\n41 26 35 47 37 39 28 31 43 29\n;\nrun;\n\nproc print data = prg2_2;\nrun;\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\ntest\n\n\n\n\n1\n41\n\n\n2\n26\n\n\n3\n35\n\n\n4\n47\n\n\n5\n37\n\n\n6\n39\n\n\n7\n28\n\n\n8\n31\n\n\n9\n43\n\n\n10\n29\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 if-then/else语句\n如果需要从已知的数据集中将部分观测的资料取出来,重新建立一个新数据集,可用 if-then 语句,如将上述程序 2-1 资料中所有男性的资料建立一个新数据集,可用如下程序:\n\n%%SAS\n/*程序2-4*/\n/*if-then/else语句*/\n/*从已知的数据集中将部分观测的资料取出来，重新建立一个数据集*/\n/*output表示将满足条件的观测值保存到新建的数据集中\ndelete则表示将数据集prg2_1所有的观测值保存到新的观测值中，并删除满足条件的观测值*/\ndata male;\n    set prg2_1;\n    if sex = 'M' then output;\n    /*if sex = 'F' then delete;*/\n/*run;*/\nproc print data = male;\nrun;\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\nno\nsex\nage\nblood\nsurt\n\n\n\n\n1\n1\nM\n41\nA\n368\n\n\n2\n2\nM\n26\nB\n745\n\n\n3\n4\nM\n37\nAB\n552\n\n\n4\n7\nM\n28\n0\n549\n\n\n5\n8\nM\n31\nB\n128\n\n\n6\n9\nM\n43\nAB\n463\n\n\n7\n10\nM\n29\nA\n512\n\n\n\n\n\n\n\n\n\n\n程序 2-4 中第一行的 data male; 表示将建立一个新的数据集,其文件名为 male.sas7bdat。\n第二行 set prg2_1; 表示将从数据集 prg2_1 中读取数据。\n第三行 if sex='M' then output; 表示当变量 sex 的值是 M 时,该观测将被保存在 male 数据集中,如果不满足该条件,则不会保存在 male 数 据集中。有时, then output 可以省略,或者该语句也可以用下面的语句表示:\nif sex = 'F' then delete;\noutput 与 delete 的区别\noutput 表示将满足条件的观测保存到新建的数据集中,而 delete 则表示将数据集 prg2_1 所有观测保存到新的数据集中,并删除满足条件的 观测。\n\n\n3.3 else 语句\n如果希望将满足条件的观测保存到一个新的数据集中,将不满足条件的观测保存到另一个新的数据集中,可用 else 语句。现在需将男性的资料保存在 male 数据集中,将女性的资料保存在 female 数据集中,可用如下程序:\n\n%%SAS\n/*程序2-5*/\n/*将满足的数据保存在一个数据集中，不满足的保存到另一个*/\ndata male female;\n    set prg2_1;\n    if sex = 'M' then output male;\n             else output female;\nrun;\n\nproc print data=male;\n    title 'Male Data';\nrun;\n\nproc print data=female;\n    title 'Female Data';\nrun;\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nMale Data \n\n\n\n\n\n\n观测\nno\nsex\nage\nblood\nsurt\n\n\n\n\n1\n1\nM\n41\nA\n368\n\n\n2\n2\nM\n26\nB\n745\n\n\n3\n4\nM\n37\nAB\n552\n\n\n4\n7\nM\n28\n0\n549\n\n\n5\n8\nM\n31\nB\n128\n\n\n6\n9\nM\n43\nAB\n463\n\n\n7\n10\nM\n29\nA\n512\n\n\n\n\n\n\n\n\n\nFemale Data \n\n\n\n\n\n\n观测\nno\nsex\nage\nblood\nsurt\n\n\n\n\n1\n3\nF\n35\nB\n401\n\n\n2\n5\nF\n37\nA\n478\n\n\n3\n6\nF\n39\n0\n628\n\n\n\n\n\n\n\n\n\n\n\n\n3.4 drop/keep 语句\n这两个语句允许用户根据原有数据集的内容,保留部分变量在新数据集中。\ndrop 语句规定在新数据集中将不保留哪些变量, keep 语句规定在新数据集中保留哪些变量。\n程序 2-6 和程序 2-7 都表示新数据集 new 中将分别保留原数据集 prg2_1 中的 no、sex 和 surt 三个变量和保留 age 和 blood 这两个变量，分别使用 drop 和 keep 语句。\n\n%%SAS\n/*drop/keep语句，drop规定新数据集中不保留那些变量\nkeep语句规定那些变量被保留*/\ndata new;\n    set prg2_1;\n    drop age blood;\nproc print data = new;\nrun;\n\n/*keep*/\ndata new;\n    set prg2_1;\n    keep age blood;\nproc print data = new;\nrun;\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nFemale Data \n\n\n\n\n\n\n观测\nno\nsex\nsurt\n\n\n\n\n1\n1\nM\n368\n\n\n2\n2\nM\n745\n\n\n3\n3\nF\n401\n\n\n4\n4\nM\n552\n\n\n5\n5\nF\n478\n\n\n6\n6\nF\n628\n\n\n7\n7\nM\n549\n\n\n8\n8\nM\n128\n\n\n9\n9\nM\n463\n\n\n10\n10\nM\n512\n\n\n\n\n\n\n\n\n\nFemale Data \n\n\n\n\n\n\n观测\nage\nblood\n\n\n\n\n1\n41\nA\n\n\n2\n26\nB\n\n\n3\n35\nB\n\n\n4\n37\nAB\n\n\n5\n37\nA\n\n\n6\n39\n0\n\n\n7\n28\n0\n\n\n8\n31\nB\n\n\n9\n43\nAB\n\n\n10\n29\nA",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "05-SAS 数据库与数据集"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-24-SAS-program.html#sas-帮助",
    "href": "Guide/SAS/25-05-24-SAS-program.html#sas-帮助",
    "title": "04-SAS 程序基础",
    "section": "2 SAS 帮助",
    "text": "2 SAS 帮助\n启动 SAS 帮助文档的方法有以下几种， 1. 在命令框中键人 HELP ,然后点击 √ 或按 ENTER 2. 点击主菜单中的 帮助 ,再点击 SAS帮助和文档 。 3. 点击工具栏上的 ? 图标。 4. 使用功能键F1。\n要访问特定窗口的帮助信息,请在此窗口处于活动状态时从命令框中发出 HELP 命令,或者点击工具栏上的,或者使用功能键 F1。\n要访问特定主题的帮助,可以通过在命令框中键入 help keyword,keyword 值对应需要帮助的值。例如,若需要关于 SAS/EIS 软件的帮助,则键入 help sas/eis;若需要关于 proc ttest 的帮助,则键入 help ttest; 关键字的值不区分大小写。另 外,也可将光标置于编辑器窗口中的关键字(如 ttest )上,再按 F1 键调用关于 PROC ttest 的帮助, 或者直接在 SAS帮助文档中键入该关键字进行搜索。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "04-SAS 程序基础"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-26-SAS-DATA-tidy.html",
    "href": "Guide/SAS/25-05-26-SAS-DATA-tidy.html",
    "title": "06-SAS 数据集的整理",
    "section": "",
    "text": "%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "06-SAS 数据集的整理"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#浏览与修改数据集",
    "href": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#浏览与修改数据集",
    "title": "06-SAS 数据集的整理",
    "section": "1 浏览与修改数据集",
    "text": "1 浏览与修改数据集\n使用软件窗口的 Table editor/browse(edit) 菜单选项进行数据集的浏览与修改。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "06-SAS 数据集的整理"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#产生新变量",
    "href": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#产生新变量",
    "title": "06-SAS 数据集的整理",
    "section": "2 产生新变量",
    "text": "2 产生新变量\n\n2.1 直接生成新变量\n在数据集中可以通过SAS 提供的运算符和函数直接产生新变量,如将 程序 2-1 中生存时间的单位由天改为年,可产生一个新变量 surt_y,\n该变量的值等于变量 surt 除以 365.25,所以变量 surl_y 将表示单位为年的生存时间。可用如下程序:\n/*程序2-9*/\n/*产生新变量，改天为年*/\ndata prg2_6;\n    infile 'C:\\Document\\SASData\\syz.txt';\n    input no sex $ age blood $ surt;\n    surt_y = surt/365.25;\nproc print data = prg2_6;\nrun;\n\n\n2.2 if-then/else 语句生成变量\n通过 if-then/else 语句也可以产生新变量。\n仍用 程序 2-1 的数据,程序 2-11 表示将产生一个新的分组变量,变量名为 group ,其中变量值为 1 表示年龄 &gt;40 岁的患者,变量值为 2 表示年龄 ≤40 岁的患者。\n/*程序2-1`*/\n/*使用if-then/else来产生新变量*/\ndata prg2_7;\n    infile 'C:\\Document\\SASData\\syz.txt';\n    input no sex $ age blood $ surt; \n    if age&gt;40 then group =1;\n    else group = 2;\ndatalines;\n......\n;\nrun;\n\n\n2.3 多条件下的 if-then 语句\n如果满足的条件超过两个,可以使用 and 和 or 语句来控制。and 语句表示同时满足几个条件,就执行 then 后面的语句; or 表示只需满足几个 条件中的一个条件,就执行 then 后面的语句。程序 2-12 表示将年龄 &gt;40 岁的男性患者归为一组, 将年龄 ≤40 岁的男性患者归为另一组。\n/*程序2-12*/\n/*满足条件超过2个，可以使用and和or语句来控制*/\n/*由于sex为字符型变量，所以在表示其变量值时，需要在变量值两侧加上引号，单引号、双引号均可*/\ndata prg2_8;\n    infile 'C:\\Document\\SASData\\syz.txt';\n    input no sex $ age blood $ surt; \n    if sex = 'M' and age&gt;40 then group =1;\n    if sex = 'M' and age&lt;=40 then group =2;\ndatalines;\n......\n;\nrun;",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "06-SAS 数据集的整理"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-25-SAS-DATA-LIB.html#其他格式文件转换为-sas-数据集",
    "href": "Guide/SAS/25-05-25-SAS-DATA-LIB.html#其他格式文件转换为-sas-数据集",
    "title": "05-SAS 数据库与数据集",
    "section": "4 其他格式文件转换为 SAS 数据集",
    "text": "4 其他格式文件转换为 SAS 数据集\n\n4.1 文本文件转换为 SAS 数据集\n文本文件是指以纯文本格式存储的数据文件，通常以 .txt 或 .csv 为扩展名。SAS 可以通过 infile 语句和 input 语句将文本文件转换为 SAS 数据集。\n形如如下程序：\n/*程序2-7*/\n/*文本文件转换为SAS数据集*/\n/*路径必须用单引号括起来，而且文件后缀名不能省略*/\ndata prg2_3;\n    infile 'C:\\Document\\SASData\\syz.txt';\n    input no sex $ age blood $ surt;\nrun;\n注意:文件中不能有变量名,至于具体哪个数值属于哪个变量用户必须自己清楚。\ninfile 语句必须在 data 语句的后面,在 input 语句的前面。\n由于纯文本文件中没有变量名称,所以第三行的语句 input no sex $ age blood $ surt; 就是定义数据集中的变量名,而且变量名的次序 必须和纯文本文件中所对应的数据值的次序相同。\n\n\n4.2 Excel 文件转换为 SAS 数据集\n\n使用 Import Menu 选项\n使用 proc import 过程\n\n选项 2 使用 proc import 过程可以将 Excel 文件转换为 SAS 数据集。proc import 过程的基本语法如下：\n/*程序2-8*/\n/*sheet = \"sheet1$\";是读取工作表 1 中的数据导入SAS数据集*/\nproc import\n    datafile = \"C:\\Document\\SASData\\syz.xlsx\"\n    out = work.prg2_5\n    dbms = excel replace;\n    sheet = \"sheet1$\";\nrun;\nproc 是过程步开始的关键词, import 是过程步中的过程名称,该过程中 datafile = \"C:\\Document\\SASData\\syz.xlsx\" 表示将 C:\\Document\\SASData\\syz.xlsx 文件中的数据转换为 SAS 数据集格式的数据, out = work.prg2_5 表示建立一个名称为 prg2_5.sas7bdat 的临时数据集, dbms = excel 表示原文件格式为 Excel 文件格式; replace 表示如果该数据集文件已经存在,则该文件中原来的数据将被新的数据替代。\n以上四个选项都是 import 语句的选项,彼此之间用空格分隔不能用 ; 隔开。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "05-SAS 数据库与数据集"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据集的排序",
    "href": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据集的排序",
    "title": "06-SAS 数据集的整理",
    "section": "3 数据集的排序",
    "text": "3 数据集的排序\n将数据集中的所有观测按照一个或几个变 量的数值大小进行排序,可以使用 sort 过程。sort过程的语法结构如下:\nproc sort options;\nby [descending] variable-list;\nrun;\n语句中的 options 是表示 sort 过程可以使用的某些选项,其中一些常用的选项如下:\n\ndata = 数据集: 表示 sort 过程将对哪个数据集进行排序,如缺省该项,则使用最新创建的数据集;\nout = 数据集: 表示 sort 过程将排好序的数据输出到哪个数据集,如缺省该项,则将数据存放原来的数据集中,将原来数据集中的内容替换掉。\nby 语句: 表示 sort 过程将按照哪个变量对数据集进行排序。\n\ndescending: by语句 中的选项,如果选择该项,则表示按变量值的下降次序排序,如缺省该项,则按上升次序排序。该选项只决定紧随其后的一个变量的排序次序。\n\nvariable-list: 用来排序的变量名。当有多个变量时,sort 过程首先按 by 语句的第一个变量的次序重新排列观测,然后在此基础上,按第二变量 的次序重新排列观测,即当第一个变量的观测值相同时,再按第二变量的观测值排序,依次类推。 sort 过程对相同by变量值的那些观测保持原来的相对顺序.\n如果变量是数值型, sort 过程则按数值的大小排序;如果变量是字符型的，sort 过程先按每个变量值的第一个字母排序,如果第一个字母相同,则按第二个字母排序,依此类推。\n\n\n%%SAS\n/*程序2-12*/\ndata prg2_1;\n    input no sex $ age blood $ surt;\ndatalines;\n1 M 41 A 368\n2 M 26 B 745\n3 F 35 B 401\n4 M 37 AB 552\n5 F 37 A 478\n6 F 39 O 628\n7 M 28 O 549\n8 M 31 B 128\n9 M 43 AB 463\n10 M 29 A 512\n;\nrun;\n\nproc sort data = prg2_1 out = prg2_9;\n    by descending sex surt;\nrun;\n\nproc print data =prg2_9;\n    title '程序2-12';\nrun;\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n程序2-12 \n\n\n\n\n\n\n观测\nno\nsex\nage\nblood\nsurt\n\n\n\n\n1\n8\nM\n31\nB\n128\n\n\n2\n1\nM\n41\nA\n368\n\n\n3\n9\nM\n43\nAB\n463\n\n\n4\n10\nM\n29\nA\n512\n\n\n5\n7\nM\n28\nO\n549\n\n\n6\n4\nM\n37\nAB\n552\n\n\n7\n2\nM\n26\nB\n745\n\n\n8\n3\nF\n35\nB\n401\n\n\n9\n5\nF\n37\nA\n478\n\n\n10\n6\nF\n39\nO\n628",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "06-SAS 数据集的整理"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据集的连接",
    "href": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据集的连接",
    "title": "06-SAS 数据集的整理",
    "section": "4 数据集的连接",
    "text": "4 数据集的连接\n数据集的连接是把几个数据集中的数据纵向相加,组成一个新的数据集,新数据集中的观测数量是原来几个数据集中观测的总和。set 语句可以完成数据集的连接,语法结构为:\ndata newname;\nset name1 name2:\nrun;\n其中 newname 是新数据集的名称, namel 和 name2 是要连接的原数据集的名称,如果有多个数据集,则可以用逗号分隔开。 如果要连接的数据集的变量名相同,则新数据集中的变量名与原数据集中的变量名相同,如果原数据集中的变量名不同,则新数据集中的变量名为原数据集中所有变量名的并集;其中原数据集中没有的变量,在新数据集中将表示为缺省值.",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "06-SAS 数据集的整理"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据集的合并",
    "href": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据集的合并",
    "title": "06-SAS 数据集的整理",
    "section": "5 数据集的合并",
    "text": "5 数据集的合并\n数据集的合并是将几个数据集中的观测横向合并成一个新的数据集,合并数据集可使用 merge 语句, merge 语句的语法结构为:\ndata newname;\nmerge name1 name2.\nby keyvar;\nrun;\ndata 步中的 newname 为新数据集的名称, name1 和 name2 为原数据集的名称,还可以有多个数据集名,彼此之间用空格分隔,合并前需对原数据集按 keyvar 排序。\nby 语句表示可以根据 keyvar 所规定的关键变量进行合并,原数据集必须都有 keyvar 变量。\n如果没有 by 语句,合并时将一个数据集的第一个观测值和另一个数据集中第一个观测值合并成新数据集中的第一个观测值,第二个观测值和另一个数据集中的第二个观测值合并成新数据集的第二观测值,依次类推。\n\n%%SAS\n/*程序2-13*/\n/*数据的合并，merge语句的使用*/\n/*创建两个数据集*/\ndata prg_a;\n    input no sex $ age;\ndatalines;\n1 M 41\n2 M 26\n3 F 35\n4 M 37\n5 F 37\n6 F 39\n7 M 28\n8 M 31\n9 M 43\n10 M 29\n;\nproc print data = prg_a;\n    title '程序2-13';\nrun;\n\ndata prg_b;\n    input num blood $ surt;\ndatalines;\n41 A 368\n26 B 745\n35 B 401\n37 AB 552\n37 A 478\n39 O 628\n28 O 549\n31 B 128\n43 AB 463\n29 A 512\n;\nproc print data = prg_b;\n    title '程序2-14';\nrun;\n\n/*程序2-15*/\n/*对两个数据集进行合并*/\ndata prg2_11;\n    merge prg_a prg_b;\nrun;\nproc print;\n    title '程序2-15';\nrun;\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n程序2-13 \n\n\n\n\n\n\n观测\nno\nsex\nage\n\n\n\n\n1\n1\nM\n41\n\n\n2\n2\nM\n26\n\n\n3\n3\nF\n35\n\n\n4\n4\nM\n37\n\n\n5\n5\nF\n37\n\n\n6\n6\nF\n39\n\n\n7\n7\nM\n28\n\n\n8\n8\nM\n31\n\n\n9\n9\nM\n43\n\n\n10\n10\nM\n29\n\n\n\n\n\n\n\n\n\n程序2-14 \n\n\n\n\n\n\n观测\nnum\nblood\nsurt\n\n\n\n\n1\n41\nA\n368\n\n\n2\n26\nB\n745\n\n\n3\n35\nB\n401\n\n\n4\n37\nAB\n552\n\n\n5\n37\nA\n478\n\n\n6\n39\nO\n628\n\n\n7\n28\nO\n549\n\n\n8\n31\nB\n128\n\n\n9\n43\nAB\n463\n\n\n10\n29\nA\n512\n\n\n\n\n\n\n\n\n\n程序2-15 \n\n\n\n\n\n\n观测\nno\nsex\nage\nnum\nblood\nsurt\n\n\n\n\n1\n1\nM\n41\n41\nA\n368\n\n\n2\n2\nM\n26\n26\nB\n745\n\n\n3\n3\nF\n35\n35\nB\n401\n\n\n4\n4\nM\n37\n37\nAB\n552\n\n\n5\n5\nF\n37\n37\nA\n478\n\n\n6\n6\nF\n39\n39\nO\n628\n\n\n7\n7\nM\n28\n28\nO\n549\n\n\n8\n8\nM\n31\n31\nB\n128\n\n\n9\n9\nM\n43\n43\nAB\n463\n\n\n10\n10\nM\n29\n29\nA\n512\n\n\n\n\n\n\n\n\n\n\n\n5.1 含有缺失值的合并\n\n%%SAS\n/*程序2-16*/\n/*缺失值*/\ndata prg_c;\n    input num blood $ surt;\ndatalines;\n41 A 368\n26 B 745\n35 B 401\n37 AB 552\n37 A 478\n39 O 628\n28 O 549\n;\nrun;\n\ndata prg2_12;\n    merge prg_a prg_c;\nproc print data = prg2_12;\n    title '程序2-16';\nrun;\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n程序2-16 \n\n\n\n\n\n\n观测\nno\nsex\nage\nnum\nblood\nsurt\n\n\n\n\n1\n1\nM\n41\n41\nA\n368\n\n\n2\n2\nM\n26\n26\nB\n745\n\n\n3\n3\nF\n35\n35\nB\n401\n\n\n4\n4\nM\n37\n37\nAB\n552\n\n\n5\n5\nF\n37\n37\nA\n478\n\n\n6\n6\nF\n39\n39\nO\n628\n\n\n7\n7\nM\n28\n28\nO\n549\n\n\n8\n8\nM\n31\n.\n \n.\n\n\n9\n9\nM\n43\n.\n \n.\n\n\n10\n10\nM\n29\n.\n \n.\n\n\n\n\n\n\n\n\n\n\n\n\n5.2 by 语句\nby 语句可以对数据集进行分组,并对每个组进行处理。\n\n%%SAS\n/*程序2-17*/\n/*使用by语句来进行排序在合并*/\ndata prg_f;\n    input no sex $ age;\ndatalines;\n1 M 41\n2 M 26\n4 M 37\n5 F 37\n7 M 28\n8 M 31\n;\nrun;\ndata prg_g;\n    input no blood $ surt;\ndatalines;\n2 A 368\n3 B 745\n6 AB 552\n8 A 478\n9 O 549\n;\nrun;\n\nproc sort data = prg_f;\n    by no;\nrun;\nproc sort data = prg_g;\n    by no;\nrun;\ndata prg2_14;\n    merge prg_f prg_g;\n    by no;\nrun;\nproc print;\n    title '程序2-17';\nrun;\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n程序2-17 \n\n\n\n\n\n\n观测\nno\nsex\nage\nblood\nsurt\n\n\n\n\n1\n1\nM\n41\n \n.\n\n\n2\n2\nM\n26\nA\n368\n\n\n3\n3\n \n.\nB\n745\n\n\n4\n4\nM\n37\n \n.\n\n\n5\n5\nF\n37\n \n.\n\n\n6\n6\n \n.\nAB\n552\n\n\n7\n7\nM\n28\n \n.\n\n\n8\n8\nM\n31\nA\n478\n\n\n9\n9\n \n.\nO\n549",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "06-SAS 数据集的整理"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据集的求秩",
    "href": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据集的求秩",
    "title": "06-SAS 数据集的整理",
    "section": "6 数据集的求秩",
    "text": "6 数据集的求秩\n利用 rank 过程可以对数据集中的一个或者多个变量进行求秩。rank 过程把未缺失的数值从最小值到最大值排列,对最小值赋予秩1,对第二小值赋 予秩 2,等等,一直赋予秩n,即未缺失的观测个数出现数值相同的观测值时,其秩可以取两者的平均秩或最高秩或最低秩。\nrank 过程的语法结构如下:\nproc rank options;\n    var variable-list:\n    ranks variable-list;\nrun;\n\n6.1 语句中的 options\n语句中的 options 是表示 rank 过程可以使用的某些选项,其中一些常用的选项如下: - data = 数据集: 表示 rank 过程将对哪个数据集进行排秩,如缺省该项,则使用最新创建的数据集。 - out = 数据集: 表示 rank 过程将排好秩的数据输出到哪个数据集,如缺省该项,则将数据存放原来的数据集中,将原来数据集中的内容替换掉。 - var 语句: 表示 rank 过程将对哪个变量进行排秩,如果有多个变量,则用逗号分隔开。 - descending: 求秩顺序选项,如果选择该项,则表示按变量值的下降次序求秩,如缺省该项,则按上升次序求秩。 - ties=mean|high|low:表示对数值相同的观测值如何取秩。ties 等于 mean 表示数值相同的观测值取平均秩,等于 high 表示取相应秩中的最大值,等于 low 表示取相应秩中的最小值。 ### var语句\n该语句表示 rank 过程对哪个变量求秩,对多个变量求秩时,变量名以空格分开。如果省略 var 语句,则对数据集中所有数值变量计算秩。\n\n\n6.2 ranks语句\n如果希望在输出数据集中除了秩变量外还包括原始变量,使用 ranks 语句对求秩变量分配求秩后的名称。命名的次序和 var 语句变量列表中的次序相对应。如果省略 ranks 语句,则输出数据集中只有求秩后的结果而没有原始变量。ranks 语句必须与 var 语句同时使用。\n程序 2-18 表示将数据集 prg2_1 中的 age 和 surt 变量按从小到大的顺序求秩,相应的秩变量分别为 age_rank 和 surt_rank,将结果输出到数据集 prg2_14 中,并将求秩后的结果显示到输出窗口中:\n\n%%SAS\n/*程序2-18*/\n/*数据集的求秩过程*/\nproc rank data = prg2_1 out = prg2_15 ties = mean;\n    var age surt;\n    ranks age_rank surt_rank;\nrun;\nproc print;\n    title '程序2-18';\nrun;\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n程序2-18 \n\n\n\n\n\n\n观测\nno\nsex\nage\nblood\nsurt\nage_rank\nsurt_rank\n\n\n\n\n1\n1\nM\n41\nA\n368\n9.0\n2\n\n\n2\n2\nM\n26\nB\n745\n1.0\n10\n\n\n3\n3\nF\n35\nB\n401\n5.0\n3\n\n\n4\n4\nM\n37\nAB\n552\n6.5\n8\n\n\n5\n5\nF\n37\nA\n478\n6.5\n5\n\n\n6\n6\nF\n39\nO\n628\n8.0\n9\n\n\n7\n7\nM\n28\nO\n549\n2.0\n7\n\n\n8\n8\nM\n31\nB\n128\n4.0\n1\n\n\n9\n9\nM\n43\nAB\n463\n10.0\n4\n\n\n10\n10\nM\n29\nA\n512\n3.0\n6",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "06-SAS 数据集的整理"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据集的转置过程",
    "href": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据集的转置过程",
    "title": "06-SAS 数据集的整理",
    "section": "7 数据集的转置过程",
    "text": "7 数据集的转置过程\n对数据集进行转置,即行变成列,列变成行可以使用 transpose 过程。transpose 过程的基本语法结构如下:\nproc transpose options;\n    var variable-list:\n    id variable;\n    by variable-list;\nrun;\n\n7.1 语句中的 options\n是表示 transpose 过程可以使用的某些选项,其中一些常用的选项如下:\n\ndata = 数据集: 表示 transpose 过程将对哪个数据集进行转置,如缺省该项,则使用最新创建的数据集。\nout = 数据集: 表示 transpose 过程将转置后的数据输出到哪个数据集,如缺省该项,则将数据存放于名称为 datan 的数据集中( datan 中的 n 可取1,2,3…n).\n\n\n\n7.2 var语句\n该语句表示 transpose 过程对哪个变量进行转置,如果有多个变量,则用逗号分隔开。如果省略 var 语句,则对数据集中所有数值变量进行转置。\n\n\n7.3 id语句\n该语句表示 transpose 过程将哪个变量的值作为转置后数据集中的列名,如果省略 id 语句,则转置后的数据集中的列名为 col1, col2, col3 等。\nid 变量的值在输入数据集中只能出现一次;或者使用 by 语句,则在 by 组中只能出现一次。id 变量缺失的观测将从输出数据集中删去。\n\n\n7.4 by语句\n该语句表示 transpose 过程将对哪个变量进行分组转置,如果有多个变量,则用逗号分隔开。如果省略 by 语句,则对整个数据集进行转置。\n使用by语句可以对每个 by 组进行转置, by 变量包含在输出数据集中但没有被转置。使用 by 语句,要求数据集按照 by 语句后面的变量进行排序\n\n%%SAS\n/*程序2-19*/\n/*数据集的转置过程*/\nproc transpose data = prg2_1 out = prg2_16;\n    var sex age surt;\n    id no;\nrun;\nproc print;\n    title '程序2-19';\nrun;\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n程序2-19 \n\n\n\n\n\n\n观测\n_NAME_\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1\nsex\nM\nM\nF\nM\nF\nF\nM\nM\nM\nM\n\n\n2\nage\n41\n26\n35\n37\n37\n39\n28\n31\n43\n29\n\n\n3\nsurt\n368\n745\n401\n552\n478\n628\n549\n128\n463\n512",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "06-SAS 数据集的整理"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据的输出",
    "href": "Guide/SAS/25-05-26-SAS-DATA-tidy.html#数据的输出",
    "title": "06-SAS 数据集的整理",
    "section": "8 数据的输出",
    "text": "8 数据的输出\nSAS还可以将SAS数据集转换成其他格式的数据文件,如 *.dbf、*.xls、*.csv、*.txt 等,供其他软件使用。 “File(F)” 菜单中 “Export Data(E)” 选项可以完成这项功能，",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "06-SAS 数据集的整理"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-28-SAS-function.html",
    "href": "Guide/SAS/25-05-28-SAS-function.html",
    "title": "07-SAS 运算符与常用函数",
    "section": "",
    "text": "算术运算符\n\n\n符号\n含义\n\n\n\n\n**\n乘方\n\n\n*\n乘\n\n\n+\n加\n\n\n-\n减\n\n\n/\n除\n\n\n\n\n\n\n\n比较运算符\n\n\n符号\n含义\n\n\n\n\n= 或 EQ\n等于\n\n\n^ 或 NE\n不等于\n\n\n&gt; 或 GT\n大于\n\n\n&gt;= 或 GE\ng\n\n\n&lt; 或 LT\n小于\n\n\n&lt;= 或 LE\n小于等于\n\n\n\n\n\n\n\n逻辑运算符\n\n\n符号\n含义\n\n\n\n\n& 或 AND\n逻辑与\n\n\n| 或 OR\n逻辑或\n\n\n^ 或 NOT\n逻辑非",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "07-SAS 运算符与常用函数"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-28-SAS-function.html#sas-运算符",
    "href": "Guide/SAS/25-05-28-SAS-function.html#sas-运算符",
    "title": "07-SAS 运算符与常用函数",
    "section": "",
    "text": "算术运算符\n\n\n符号\n含义\n\n\n\n\n**\n乘方\n\n\n*\n乘\n\n\n+\n加\n\n\n-\n减\n\n\n/\n除\n\n\n\n\n\n\n\n比较运算符\n\n\n符号\n含义\n\n\n\n\n= 或 EQ\n等于\n\n\n^ 或 NE\n不等于\n\n\n&gt; 或 GT\n大于\n\n\n&gt;= 或 GE\ng\n\n\n&lt; 或 LT\n小于\n\n\n&lt;= 或 LE\n小于等于\n\n\n\n\n\n\n\n逻辑运算符\n\n\n符号\n含义\n\n\n\n\n& 或 AND\n逻辑与\n\n\n| 或 OR\n逻辑或\n\n\n^ 或 NOT\n逻辑非",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "07-SAS 运算符与常用函数"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-28-SAS-function.html#常用函数",
    "href": "Guide/SAS/25-05-28-SAS-function.html#常用函数",
    "title": "07-SAS 运算符与常用函数",
    "section": "2 常用函数",
    "text": "2 常用函数\n\n2.1 算术函数\n\n\n\n\n\n\n\n函数\n含义\n\n\n\n\n\\(ABS(x)\\)\nx 的绝对值\n\n\n\\(MAX(x_1,x_2…,x_n)\\)\nx 中的最大值\n\n\n\\(MIN(x_1,x_2…,x_n)\\)\nx 中的最小值\n\n\n\\(MOD(x,y)\\)\nxy 的余数\n\n\n\\(SIGN(x)\\)\n当 x &lt; 0 时,取值为 -1;\n当 x &gt; 0 时,取值为 1;\nx = 0 时,取值为 0.\n\n\n\\(SORT(x)\\)\nx 的平方根\n\n\n\n\n\n2.2 截取函数\n\n\n\n\n\n\n\n函数\n含义\n\n\n\n\n\\(CEIL(x)\\)\n大于等于 x 的最小整数\n\n\n\\(FLOOR(x)\\)\n小于等于 x 的最大整数\n\n\n\\(INT(x)\\)\nx 的整数部分\n\n\n\\(ROUND(x,n)\\)\nx 按 n 指定的精度取舍入值\n\n\n\\(SUBSTR( s,p,n)\\)\n从字符串 s 中的第 p 个字符开始抽取 n 个字符的子串\n\n\n\n\n\n2.3 数组函数\n\n\n\n\n\n\n\n函数\n含义\n\n\n\n\nDIM( x )\n计算数组 x 第一维的元素个数\n\n\nDIMk( x)\n计算数组 x 第 k 维的元素个数\n\n\nHBOUND( x )\n计算数组 x 第一维的上界\n\n\nHBOUNDk( x )\n计算数组 x 第 k 维的上界\n\n\nLBOUND( x )\n计算数组 x 第一维的下界\n\n\nLBOUNDk( x )\n计算数组 x 第 k 维的下界\n\n\n\n\n\n2.4 数学函数\n\n\n\n\n\n\n\n函数\n含义\n\n\n\n\nDIGAMMA( x )\n对 x 计算 GAMMA 函数对数的导数\n\n\nERF( x )\nx 的误差函数\n\n\nERFC( x )\nx 的误差函数的余函数\n\n\nEXP( x )\ne 的 x 次方\n\n\nGAMMA( x )\n对 x 计算完全 GAMMA 函数\n\n\nLGMAMMA( x )\n对 x 计算 GAMMA 函 数的自然对数\n\n\nL0G( x )\n以 e 为底的对数\n\n\nLOG2( x )\n以 2 为底的对数\n\n\nLOG10( x )\n以 10 为底的对数\n\n\n\\(ORDINAL(k,x_1,...)\\)\n返回由 k 确定的部分序列的最大值\n\n\nTRIGAMMA( x )\n计算 DIGAMMA(x) 函数的导数\n\n\nAIRY( x )\n计算微分方程的解\n\n\nDAIRY( x )\n求 AIRY(x) 的导数\n\n\nJBESSEL( nu,x )\n计算 BESSEL 函数\n\n\nIBESSEL( nu, x, kode )\n计算修正 BESSEL 函数\n\n\n\n\n\n2.5 三角函数\n\n三角函数\n\n\n\n\n\n\n函数\n含义\n\n\n\n\nARCOS( x )\n反余弦函数\n\n\nARSIN( x )\n反正弦函数\n\n\nATAN( x )\n反正切函数\n\n\nCOS( x )\n余弦函数\n\n\nCOSH( x )\n双曲余弦函数\n\n\nSIN( x )\n正弦函数\n\n\nSINH( x )\n双曲正弦函数\n\n\nTAN( x )\n正切函数\n\n\nTANH( x )\n双曲正切函数\n\n\n\n\n\n2.6 概率函数\n\n概率函数\n\n\n\n\n\n\n函数\n含义\n\n\n\n\nPOISSON( lambda,n )\n泊松分布的概率分布函数\n\n\nPROBBETA( x,a,b )\n\\(\\beta\\) 分布的分布函数\n\n\nPROBBNML( p,n,m )\n二项分布的概率分布函数\n\n\nPROBCHI( x, df, nc )\n\\(\\chi^2\\) 分布的分布函数\n\n\nPROBF( x, ndf, ddf, nc )\nF分布的分布函数\n\n\nPROBGAM( x,a )\n\\(\\gamma\\) 分布的分布函数\n\n\nPROBHYPR( nn,k,n,x, or )\n超几何分布的概率分布函数\n\n\nPROBNEGB( p,n,m )\n负二项分布的概率分布函数\n\n\nPROBNORM( x )\n标准正态分布函数\n\n\nPROBT( x, df, nc )\nt 分布函数\n\n\n\n\n\n2.7 随机函数\n\n随机函数\n\n\n\n\n\n\n函数\n含义\n\n\n\n\nNORMAL( seed )\n产生标准正态分布随机数\n\n\nRANBIN( seed, n, p)\n产生二项分布随机数\n\n\nRANCAU( seed )\n产生柯西分布随机数\n\n\nRANEXP( seed )\n产生指数分布随机数\n\n\nRANGAM( seed, alpha )\n产生 \\(\\gamma\\) 分布的随机数\n\n\nRANNOR( seed )\n产生标准正态离差随机数\n\n\nRANPOI( seed , lambda )\n产生泊松分布随机数\n\n\nRANTBL( seed, p1 , p2…, pn)\n产生离散分布随机数\n\n\nRANTRI( seed, h )\n产生三角分布随机数\n\n\nRANUNI( seed )\n产生均匀离差随机数\n\n\nUNIFORM( seed )\n产生均匀分布随机数\n\n\n\n\n\n2.8 样本统计函数\n\n样本统计函数\n\n\n\n\n\n\n函数\n含义\n\n\n\n\nCV( x,y,…)\n产生一组数据的变异系数\n\n\nKURTOSIS( x,y,…)\n产生一组数据的峰度系数\n\n\nMAX( x,y,…)\n产生一组数据的最大值\n\n\nMIN( x,y,…)\n产生一组数据的最小值\n\n\nMEAN( x,y,…)\n产生一组数据的均数\n\n\nN( x,y,…)\n产生一组数据的例数\n\n\nNMISS( x,y,…)\n产生一组数据的缺失例数\n\n\nRANGE( x,y,…..)\n产生一组数据的极差\n\n\nSKEWNESS( x,y,….)\n产生一组数据的偏度系数\n\n\nSTD( x,y,…. )\n产生一组数据的标准差\n\n\nSTDERR( x,y,….. )\n产生一组数据的标准误\n\n\nSUM( x,y,…..)\n产生一组数据的总和\n\n\nVAR( x,y,….)\n产生一组数据的方差\n\n\nCSS( x,y,….)\n产生一组数据的离均差平方和\n\n\nUSS( x,y,….)\n产生一组数据的平方和\n\n\n\n\n\n2.9 分位函数\n\n分位函数\n\n\n函数\n含义\n\n\n\n\nCINV( p, df, ne )\n\\(\\chi^2\\) 分布的分位数\n\n\nBETAINV( p, a, b)\n\\(\\beta\\) 分布的分位数\n\n\nFINV ( p, ndf, ddf, nc )\nF 分布的分位数\n\n\nTINV( p, df, nc )\nT 分布的分位数\n\n\nPROBIT(p)\n正态分布的分位数\n\n\nGAMINV(p,a)\n\\(\\gamma\\) 分布的分位数\n\n\n\n\n\n2.10 日期与时间函数\n\n日期与时间函数\n\n\n\n\n\n\n函数\n含义\n\n\n\n\nDATE( )\n取电脑系统今天的日期作为 SAS 日期值\n\n\nDATEPART( dt )\n抽取 SAS 日期时间值的日期部分\n\n\nDATETIME( )\n取当前日期和时间\n\n\nDAY( date )\n从 SAS 日期值计算出某月的那一天\n\n\nDHMS( d,h,m,s )\n从日期、小时、分钟、秒得到 SAS 日期时间值\n\n\nHMS( h,m,s )\n从小时、分钟和秒得到 SAS 时间值\n\n\nHOUR ( dt | time )\n从 SAS 日期时间值或时间值或文字计算小 时(点钟)\n\n\nINTCK( in, from, to )\n取时间间隔数字\n\n\nINTNX( in,from,nu )\n按给定间隔推算日期时间或日期时间值\n\n\nMDY( m, d, yr )\n从年、月和日得到 SAS 日期值\n\n\nMINUTE ( t | dt )\n从 SAS 时间或日期时间值或文字得到分钟数\n\n\nQ( date )\n从 SAS 日期值或文字得到月份\n\n\nSECOND( t | dt )\n从 SAS 时间或日期时间值或文字得到秒数\n\n\nTIME( )\n取当日的时间\n\n\nTIMEPART ( dt )\n抽取 SAS 日期时间值或文字的时间部分\n\n\nTODAY( )\n取当前日期作为 SAS 日期值\n\n\nWEEKDAY( date )\n从 SAS 日期值或文字得到星期几\n\n\nYEAR( date )\n从 SAS 日期值得到年\n\n\nYYQ( yr,q )\n从年和季节得到 SAS 日期值",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "07-SAS 运算符与常用函数"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-function-example.html",
    "href": "Guide/SAS/25-05-29-SAS-function-example.html",
    "title": "08-SAS 运算符与常用函数示例",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "08-SAS 运算符与常用函数示例"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-function-example.html#正态分布随机数的产生",
    "href": "Guide/SAS/25-05-29-SAS-function-example.html#正态分布随机数的产生",
    "title": "08-SAS 运算符与常用函数示例",
    "section": "1 正态分布随机数的产生",
    "text": "1 正态分布随机数的产生\n利用 rannor(seed) 函数生成两组各15个来自于均数为 170、方差为 30 的正态分布的随机数。\n\n\n代码\n%%SAS\n/*程序3-1*/\n/*利用rannor(seed)函数生成两组各15个来自于均数为170、方差为30的正态分布的随机数*/\n\ndata prg3_1;\n    do seed = 1 to 15;\n    x = 170 + sqrt(30)*rannor(seed);\n    y = 170 + sqrt(30)*rannor(seed);\n    output;\n    end;\nrun;\nproc print;\n    title '程序3-1';\nrun;\n\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 17612\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n程序3-1 \n\n\n\n\n\n\n观测\nseed\nx\ny\n\n\n\n\n1\n1\n179.885\n169.562\n\n\n2\n2\n172.172\n164.066\n\n\n3\n3\n182.260\n166.581\n\n\n4\n4\n172.813\n169.526\n\n\n5\n5\n166.746\n170.175\n\n\n6\n6\n165.959\n168.630\n\n\n7\n7\n173.752\n165.595\n\n\n8\n8\n165.923\n165.643\n\n\n9\n9\n171.866\n168.354\n\n\n10\n10\n162.607\n172.370\n\n\n11\n11\n177.152\n177.806\n\n\n12\n12\n167.723\n178.842\n\n\n13\n13\n164.207\n164.806\n\n\n14\n14\n175.223\n172.147\n\n\n15\n15\n169.583\n176.685\n\n\n\n\n\n\n\n\n\n\n程序说明：\n\n创建 数据集 prg3_1，包含变量 x 和 y。\n利用函数 SORT(x) 和 RANNOR(seed) 产生两个随机变量 x 和 y ,均来自均数为 170,方差为 30 的正态分布。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "08-SAS 运算符与常用函数示例"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-function-example.html#单一总体均数的置信区间的估计",
    "href": "Guide/SAS/25-05-29-SAS-function-example.html#单一总体均数的置信区间的估计",
    "title": "08-SAS 运算符与常用函数示例",
    "section": "2 单一总体均数的置信区间的估计",
    "text": "2 单一总体均数的置信区间的估计\n从某市随机抽取 10 例 18 岁男生,测得他们的身高的均数为 166.95cm,标准差为 3.64cm,试求其总体均数的 95% 置信区间：\n\n\n代码\n%%SAS\n/*程序3-2*/\n/*10例18岁的男生，身高均数为166.95cm，标准差为3.64cm，试求其总体均数的95%置信区间*/\ndata prg3_2;\n    n = 10;\n    mean = 166.95;\n    std = 3.64;\n    t = tinv(0.975,n-1);\n    /*95%的置信区间，双侧，右侧的界值为97.5%*/\n    in = t*std/sqrt(n);\n    lclm = mean-in;\n    uclm = mean+in;\nrun;\nproc print;\n    var lclm uclm;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n程序3-1 \n\n\n\n\n\n\n观测\nlclm\nuclm\n\n\n\n\n1\n164.346\n169.554\n\n\n\n\n\n\n\n\n\n\n程序说明:\n\n创建数据集 prg3_2;\n变量 n、mean 和 std 分别代表样本的例数、均数和标准差；\ntinv(0.975,n-1) 表示自由度为 n-1 时双侧 0.05 水平的 t 界值；\nin 的值等于 t 界值乘以标准误, lclm 为置信区间的下限, uclm 为置信区间的上限；\n最后调用 print 过程将双侧 95% 置信区间的上限和下限输出到输出到窗口。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "08-SAS 运算符与常用函数示例"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-function-example.html#两总体均数之差的置信区间估计",
    "href": "Guide/SAS/25-05-29-SAS-function-example.html#两总体均数之差的置信区间估计",
    "title": "08-SAS 运算符与常用函数示例",
    "section": "3 两总体均数之差的置信区间估计",
    "text": "3 两总体均数之差的置信区间估计\n为了解氨甲蝶呤( MTX)对外周血白细胞介素 -2(IL-2) 水平的影响,某医生将 61 例哮喘患 者随机分为两组。其中对照组 29 例,采用安慰剂治疗;试验组 32 例,采用小剂量 MTX 进行治疗。 测得对照组治疗前 IL-2 的平均数为 20.10IU/ml.标准差为 7.02IU/ml; 试验组治疗前 IL-2 的平均数 为 16.89IU/ml,标准差为 8.46IU/ml。问两组治疗前基线的 IL-2 总体均数相差有多大?\n\n\n代码\n%%SAS\n/*程序3-3*/\n/*61名患者随机分为两组，对照组29，实验组32\n对照组治疗前均数为20.10，标准差为7.02\n试验组治疗前均数为16.89，标准差为8.46\n问两组治疗前基线的总体均数相差有多大*/\ndata prg3_3;\n    n1 = 29;\n    n2 = 32;\n    m1 = 20.10;\n    m2 = 16.89;\n    s1 = 7.02;\n    s2 = 8.46;\n    sc2 = (s1**2*(n1-1)+s2**2*(n2-1))/(n1+n2-2);\n    st = sqrt(sc2*(1/n1+1/n2));\n    t = tinv(0.975,n1+n2-2);\n    in = t*st;\n    lclm = abs(m1-m2)-in;\n    uclm = abs(m1-m2)+in;\nproc print;\n    var lclm uclm;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n程序3-1 \n\n\n\n\n\n\n观测\nlclm\nuclm\n\n\n\n\n1\n-0.79660\n7.21660\n\n\n\n\n\n\n\n\n\n\n程序说明:\n\n创建数据集 prg3_3;\n变量 n1 和 n2 分别表示两组数据的例数,m1 和 m2 分别表示两组数据的均数, s1 和 s2 分别表示两组数据的标准差, sc2 为合并方差, st 为两均数相差的标准误\nt 为双侧 0.05 的界值;\nin 为 t 界值和标准误的乘积;\nlclm 和 uclm 分别是两均数相差 95% 置信区间的下限和上限;\n再调用print 过程将置信区间的结果输出到输出窗口。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "08-SAS 运算符与常用函数示例"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-plot.html",
    "href": "Guide/SAS/25-05-29-SAS-plot.html",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-plot.html#频数表的编制",
    "href": "Guide/SAS/25-05-29-SAS-plot.html#频数表的编制",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.1 频数表的编制",
    "text": "1.1 频数表的编制\n频数表是对分类变量进行单变量分析的常用方法,可以显示每个类别的频数和百分比。\n\n检查数据集，选择变量\n\n\n\n代码\n%%SAS\n/*check the dataset*/\nproc print data = sashelp.cars(obs=5) label;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\nMake\nModel\nType\nOrigin\nDriveTrain\nMSRP\nInvoice\nEngine Size (L)\nCylinders\nHorsepower\nMPG (City)\nMPG (Highway)\nWeight (LBS)\nWheelbase (IN)\nLength (IN)\n\n\n\n\n1\nAcura\nMDX\nSUV\nAsia\nAll\n$36,945\n$33,337\n3.5\n6\n265\n17\n23\n4451\n106\n189\n\n\n2\nAcura\nRSX Type S 2dr\nSedan\nAsia\nFront\n$23,820\n$21,761\n2.0\n4\n200\n24\n31\n2778\n101\n172\n\n\n3\nAcura\nTSX 4dr\nSedan\nAsia\nFront\n$26,990\n$24,647\n2.4\n4\n200\n22\n29\n3230\n105\n183\n\n\n4\nAcura\nTL 4dr\nSedan\nAsia\nFront\n$33,195\n$30,299\n3.2\n6\n270\n20\n28\n3575\n108\n186\n\n\n5\nAcura\n3.5 RL 4dr\nSedan\nAsia\nFront\n$43,755\n$39,014\n3.5\n6\n225\n18\n24\n3880\n115\n197\n\n\n\n\n\n\n\n\n\n\n\n使用 cars 数据集中的 MPG(City) 变量进行频数表的编制\n\n\n\n代码\n%%SAS\n/*frequency table for MPG_City*/\nproc freq data=sashelp.cars;\n    tables MPG_City;;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nFREQ 过程\n\n\n\n\n\n\n\nMPG (City)\n\n\nMPG_City\n频数\n百分比\n累积\n频数\n累积\n百分比\n\n\n\n\n10\n2\n0.47\n2\n0.47\n\n\n12\n4\n0.93\n6\n1.40\n\n\n13\n12\n2.80\n18\n4.21\n\n\n14\n13\n3.04\n31\n7.24\n\n\n15\n17\n3.97\n48\n11.21\n\n\n16\n31\n7.24\n79\n18.46\n\n\n17\n41\n9.58\n120\n28.04\n\n\n18\n69\n16.12\n189\n44.16\n\n\n19\n37\n8.64\n226\n52.80\n\n\n20\n57\n13.32\n283\n66.12\n\n\n21\n38\n8.88\n321\n75.00\n\n\n22\n18\n4.21\n339\n79.21\n\n\n23\n7\n1.64\n346\n80.84\n\n\n24\n22\n5.14\n368\n85.98\n\n\n25\n9\n2.10\n377\n88.08\n\n\n26\n22\n5.14\n399\n93.22\n\n\n27\n1\n0.23\n400\n93.46\n\n\n28\n5\n1.17\n405\n94.63\n\n\n29\n7\n1.64\n412\n96.26\n\n\n31\n1\n0.23\n413\n96.50\n\n\n32\n7\n1.64\n420\n98.13\n\n\n33\n1\n0.23\n421\n98.36\n\n\n35\n2\n0.47\n423\n98.83\n\n\n36\n1\n0.23\n424\n99.07\n\n\n38\n1\n0.23\n425\n99.30\n\n\n46\n1\n0.23\n426\n99.53\n\n\n59\n1\n0.23\n427\n99.77\n\n\n60\n1\n0.23\n428\n100.00",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-plot.html#单变量描述",
    "href": "Guide/SAS/25-05-29-SAS-plot.html#单变量描述",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.2 单变量描述",
    "text": "1.2 单变量描述\n单变量描述是对数值变量进行统计分析的过程,可以计算均值、标准差、最小值、最大值等统计量。\n\n1.2.1 means 过程\n\n\n代码\n%%SAS\n/*simple descriptive statistics for MPG_City*/\nproc means data = sashelp.cars;\n    var MPG_City;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: MPG_City MPG (City)\n\n\n数目\n均值\n标准差\n最小值\n最大值\n\n\n\n\n428\n20.0607477\n5.2382176\n10.0000000\n60.0000000\n\n\n\n\n\n\n\n\n\n\nmeans 过程默认给出均值、标准差、最小值和最大值，但是用户可以指定其他统计量，如下：\n\nStderr:均数的标准差,即标准误;\nSum:合计值;\nVariance:方差\nCV:变异系数;\nNmiss:缺失变量值的观测例数;\nRange:极差;\nUSS:平方和;\nCSS:离均差平方和;\nT:检验假设为总体均数为 0 的 student-t 检验的检验统计量 t 值;\nProbt:总体均数为0的检验假设中,值所对应的概率值(P值);\nSumweight:权重变量值的和;\nSkewness:偏度系数;\nKurtosis:峰度系数;\nCLM:双侧 95%置信区间的下限( lclm)和上限( uclm );\nMedian P50:中位数或 50% 分位数;\nP1:1%分位数;\nP5:5%分位数;\nP10:10% 分位数;\nQ1|P25:下四分位数或 25%分位数;\nQ3|P75:上四分位数或 75% 分位数:\nP90:90%分位数;\nP95:95% 分位数:\nP99:99%分位数:\nQrange:四分位数间距\n\n多个统计量输出示例：\n\n\n代码\n%%SAS\n/*complex descriptive statistics for MPG_City*/\nproc means data = sashelp.cars n mean std stderr cv clm Qrange;\n    var MPG_City;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: MPG_City MPG (City)\n\n\n数目\n均值\n标准差\n标准误差\n变异系数\n均值的95% 置信下限\n均值的95% 置信上限\n四分位间距\n\n\n\n\n428\n20.0607477\n5.2382176\n0.2531988\n26.1117767\n19.5630765\n20.5584188\n4.5000000\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.2 保留小数\nmeans 过程给出的结果中,每个统计量均在小数点后保留七位有效数字,可以通过 maxdec 语句改变有效位数,该语句是 means 过程的一个选 项,可加在 proc means 的后面。示例如下：\n\n\n代码\n%%SAS\n/*design the decimal*/\nproc means maxdec = 2 data = sashelp.cars n mean std stderr cv clm;\n    var MPG_City;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: MPG_City MPG (City)\n\n\n数目\n均值\n标准差\n标准误差\n变异系数\n均值的95% 置信下限\n均值的95% 置信上限\n\n\n\n\n428\n20.06\n5.24\n0.25\n26.11\n19.56\n20.56\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.3 频数表资料的描述性统计\n如果数据已经被整理成频数表资料，means 过程通过 freq 语句定义频数变量,用 var 语句定义组中值变量,同样可以计算简单的描述性统计量。\n\n\n代码\n%%SAS\n/*程序4-4 freq*/\ndata prg4_4;\n    input x f @@;\ndatalines;\n3.17 2 3.37 3 3.57 9 3.77 22 4.17 30\n4.37 21 4.57 15 4.77 10 4.97 6 5.17 4 5.37 2\n;\nrun;\nproc means;\n    freq f;\n    var x;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: x\n\n\n数目\n均值\n标准差\n最小值\n最大值\n\n\n\n\n124\n4.2409677\n0.4738313\n3.1700000\n5.3700000",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-plot.html#univariate-过程",
    "href": "Guide/SAS/25-05-29-SAS-plot.html#univariate-过程",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.3 univariate 过程",
    "text": "1.3 univariate 过程\nunivariate 过程能够给出的描述性统计量比较多,除了上述 means 过程给出的统计量外,它还能输出符号统计量、正态性检验的统计量以及用户自己定义的百分位数,而且可以生成若干个描述变量分布的茎叶图、箱式图、正态概率图等统计图。\n\n\n代码\n%%SAS\n/*程序4-6 univariate过程*/\ndata prg4_6;\n    input x f @@;\ndatalines;\n18 4 30 17 42 32 54 24 66 18 78 12 90 5 102 4 114 2\n;\nrun;\nproc univariate;\n    var x;\n    freq f;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n变量:  x\n \n频数:  f\n\n\n\n\n\n\n\n矩\n\n\n\n\n数目\n118\n权重总和\n118\n\n\n均值\n54.5084746\n观测总和\n6432\n\n\n标准差\n21.0724212\n方差\n444.046936\n\n\n偏度\n0.69543247\n峰度\n0.17594659\n\n\n未校平方和\n402552\n校正平方和\n51953.4915\n\n\n变异系数\n38.6589817\n标准误差均值\n1.93987361\n\n\n\n\n\n\n\n\n\n\n基本统计测度\n\n\n位置\n变异性\n\n\n\n\n均值\n54.50847\n标准差\n21.07242\n\n\n中位数\n54.00000\n方差\n444.04694\n\n\n众数\n42.00000\n极差\n96.00000\n\n\n \n \n四分位间距\n24.00000\n\n\n\n\n\n\n\n\n\n\n位置检验: Mu0=0\n\n\n检验\n统计量\np 值\n\n\n\n\nStudent t\nt\n28.09898\nPr &gt; |t|\n&lt;.0001\n\n\n符号\nM\n59\nPr &gt;= |M|\n&lt;.0001\n\n\n符号秩\nS\n3510.5\nPr &gt;= |S|\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n分位数（定义 5）\n\n\n水平\n分位数\n\n\n\n\n100% 最大值\n114\n\n\n99%\n114\n\n\n95%\n102\n\n\n90%\n78\n\n\n75% Q3\n66\n\n\n50% 中位数\n54\n\n\n25% Q1\n42\n\n\n10%\n30\n\n\n5%\n30\n\n\n1%\n18\n\n\n0% 最小值\n18\n\n\n\n\n\n\n\n\n\n\n极值观测\n\n\n最小值\n最大值\n\n\n值\n频数\n观测\n值\n频数\n观测\n\n\n\n\n18\n4\n1\n66\n18\n5\n\n\n30\n17\n2\n78\n12\n6\n\n\n42\n32\n3\n90\n5\n7\n\n\n54\n24\n4\n102\n4\n8\n\n\n66\n18\n5\n114\n2\n9\n\n\n\n\n\n\n\n\n\n\n\n程序说明:\n\n数据集 prg4_6 中的变量为 x 和 f ;\n调用 univariate 过程时,var x;语句指明 x 为分析变量;\nfreq f,语句表示 f 为频数变量。\n\n整个分析结果输出的统计量分为五个部分:\n\n矩(Moments)、\n基本统计测度( Basic Statistical Measures)、\n位置检验( Tests for Location:Mu0=0)、\n分位数[Quantiles(Definition 5)]\n和极值观测(Extreme Observations )\n\n\n矩(Moments) 部分的统计量包括:\n\n\n非缺失值的例数(N)\n权重总和(Sum Weights)\n均数(Mean)\n观测总和(Sum Observations)\n标准差(Std Deviation)\n方差(Variance)\n偏度(Skewness,即偏度系数)\n峰度(Kurtosis,即峰度系数)\n未校正平方和(Uncorrected SS,即平方和)\n校正平方和(Corrected SS,即离均差平方和)\n变异系数(Coeff Variation)\n标准误差均数(Std Error Mean,即标准误)。\n\n\n基本统计测度(Basic Statistical Measures)部分统计量包括:\n\n\n均数(Mean)\n标准差(StdDeviation)\n中位数(Median)\n方差(Variance)\n众数(Mode)\n极差(Range)\n四分位极差(Interquartile Range,即四分位数间距)。\n\n\n位置检验(Tests for Location: Mu0=0)部分的统计量包括:\n\n\nStudent’s t: 总体均数为0的 student-t 检验的检验统计量u值;\nPr &gt; |t|: 总体均数为0的t检验中,检验统计量所对应的概率值(P值);\nSign M: 总体中位数为0的符号检验的检验统计量 M值;\nPr &gt;= |M|: 总体中位数为0的符号检验中检验统计量所对应的概率值(P值);\nSigned Ranks: 总体中位数为0的符号秩检验的检验统计量S值;\nPr &gt;= |S|: 总体中位数为0的符号秩检验中，检验统计量所对应的概率值(P值)。\n\n\n分位数[Quantiles(Definition 5)] 部分的统计量包括:\n\n\n100% 分位数( 100% Max，即最大值)\n99% 分位数\n95% 分位数\n90% 分位数\n75% (即 Q3,上四分位数)\n50% 分位数(即 Median,中位数)\n25% 分位数(即 Q1,下四分位数)\n10% 分位数\n5% 分位数\n1% 分位数和 0% 分位数( 0% Min,即最小值)。\n\n\n极值观测(Extreme Observations) 部分列出了五个最小值和五个最大值以及这些值分别对应的频数和观测号。\n\numivariate 过程除了能够给出几个特定的百分位数,还能输出用户自己定义的百分位数。此时在过程中要使用 output 语句.\n\n\n代码\n%%SAS\ndata prg4_6;\n    input x f @@;\ndatalines;\n18 4 30 17 42 32 54 24 66 18 78 12 90 5 102 4 114 2\n;\nrun;\n/*程序4-7 univariate 自定义百分数，需要使用 output 语句*/\n/**/\nproc univariate data = prg4_6;\n    var x;\n    freq f;\n    output out = pct pctlpre = p pctlpts = 2.5 97.5;\nrun;\nproc print data = pct;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n变量:  x\n \n频数:  f\n\n\n\n\n\n\n\n矩\n\n\n\n\n数目\n118\n权重总和\n118\n\n\n均值\n54.5084746\n观测总和\n6432\n\n\n标准差\n21.0724212\n方差\n444.046936\n\n\n偏度\n0.69543247\n峰度\n0.17594659\n\n\n未校平方和\n402552\n校正平方和\n51953.4915\n\n\n变异系数\n38.6589817\n标准误差均值\n1.93987361\n\n\n\n\n\n\n\n\n\n\n基本统计测度\n\n\n位置\n变异性\n\n\n\n\n均值\n54.50847\n标准差\n21.07242\n\n\n中位数\n54.00000\n方差\n444.04694\n\n\n众数\n42.00000\n极差\n96.00000\n\n\n \n \n四分位间距\n24.00000\n\n\n\n\n\n\n\n\n\n\n位置检验: Mu0=0\n\n\n检验\n统计量\np 值\n\n\n\n\nStudent t\nt\n28.09898\nPr &gt; |t|\n&lt;.0001\n\n\n符号\nM\n59\nPr &gt;= |M|\n&lt;.0001\n\n\n符号秩\nS\n3510.5\nPr &gt;= |S|\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n分位数（定义 5）\n\n\n水平\n分位数\n\n\n\n\n100% 最大值\n114\n\n\n99%\n114\n\n\n95%\n102\n\n\n90%\n78\n\n\n75% Q3\n66\n\n\n50% 中位数\n54\n\n\n25% Q1\n42\n\n\n10%\n30\n\n\n5%\n30\n\n\n1%\n18\n\n\n0% 最小值\n18\n\n\n\n\n\n\n\n\n\n\n极值观测\n\n\n最小值\n最大值\n\n\n值\n频数\n观测\n值\n频数\n观测\n\n\n\n\n18\n4\n1\n66\n18\n5\n\n\n30\n17\n2\n78\n12\n6\n\n\n42\n32\n3\n90\n5\n7\n\n\n54\n24\n4\n102\n4\n8\n\n\n66\n18\n5\n114\n2\n9\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\np2_5\np97_5\n\n\n\n\n1\n18\n102\n\n\n\n\n\n\n\n\n\n\n程序说明：\n\nunivariate 产生的部分统计量输出到新建的数据集中,数据集的名称由 out= 来定义,本例 out=pct 就是表示将要新建的数据集名称定为 pct。\noutput 语句中的选项 pctlpts 表示需要计算的百分位数,本例需要输出第 2.5% 和第 97.5%分位数;\npctlpre 表示在新数据集中的变量中百分位数的前缀,本例表示百分位数前缀为 p。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-plot.html#正态性检验",
    "href": "Guide/SAS/25-05-29-SAS-plot.html#正态性检验",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.4 正态性检验",
    "text": "1.4 正态性检验\n正态性检验是检验数据是否符合正态分布的统计方法,常用的正态性检验方法有 Shapiro-Wilk 检验、Kolmogorov-Smirnov 检验等。\n在 SAS 中,可以使用 univariate 过程进行正态性检验，其语法只需要在 proc univariate 语句中加上 normal 选项即可，normal 和 plot 选项,就能输出该组数据正态性检验的结果和茎叶图、箱式图及正态概率图。\n\n\n代码\n%%SAS\nproc univariate normal plot data = sashelp.cars;\n    var MPG_City;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n变量:  MPG_City  (MPG (City))\n\n\n\n\n\n\n\n矩\n\n\n\n\n数目\n428\n权重总和\n428\n\n\n均值\n20.0607477\n观测总和\n8586\n\n\n标准差\n5.23821764\n方差\n27.438924\n\n\n偏度\n2.7820718\n峰度\n15.7911473\n\n\n未校平方和\n183958\n校正平方和\n11716.4206\n\n\n变异系数\n26.1117767\n标准误差均值\n0.25319881\n\n\n\n\n\n\n\n\n\n\n基本统计测度\n\n\n位置\n变异性\n\n\n\n\n均值\n20.06075\n标准差\n5.23822\n\n\n中位数\n19.00000\n方差\n27.43892\n\n\n众数\n18.00000\n极差\n50.00000\n\n\n \n \n四分位间距\n4.50000\n\n\n\n\n\n\n\n\n\n\n位置检验: Mu0=0\n\n\n检验\n统计量\np 值\n\n\n\n\nStudent t\nt\n79.22923\nPr &gt; |t|\n&lt;.0001\n\n\n符号\nM\n214\nPr &gt;= |M|\n&lt;.0001\n\n\n符号秩\nS\n45903\nPr &gt;= |S|\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n正态性检验\n\n\n检验\n统计量\np 值\n\n\n\n\nShapiro-Wilk\nW\n0.80784\nPr &lt; W\n&lt;0.0001\n\n\nKolmogorov-Smirnov\nD\n0.178848\nPr &gt; D\n&lt;0.0100\n\n\nCramer-von Mises\nW-Sq\n2.686108\nPr &gt; W-Sq\n&lt;0.0050\n\n\nAnderson-Darling\nA-Sq\n14.67298\nPr &gt; A-Sq\n&lt;0.0050\n\n\n\n\n\n\n\n\n\n\n分位数（定义 5）\n\n\n水平\n分位数\n\n\n\n\n100% 最大值\n60.0\n\n\n99%\n36.0\n\n\n95%\n29.0\n\n\n90%\n26.0\n\n\n75% Q3\n21.5\n\n\n50% 中位数\n19.0\n\n\n25% Q1\n17.0\n\n\n10%\n15.0\n\n\n5%\n14.0\n\n\n1%\n12.0\n\n\n0% 最小值\n10.0\n\n\n\n\n\n\n\n\n\n\n极值观测\n\n\n最小值\n最大值\n\n\n值\n观测\n值\n观测\n\n\n\n\n10\n167\n36\n156\n\n\n10\n119\n38\n405\n\n\n12\n413\n46\n150\n\n\n12\n217\n59\n374\n\n\n12\n216\n60\n151\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.1 正态性检验部分”Tests for Normality”\n正态性检验部分包括 Shapiro-Wilk 检验以及基于经验分布函数的拟合优度检验:Kolmogorov-Smirnov 检验、Cramer-von Mises 检验、 Anderson-Darling 检验。\n样本量\n\n当 n≤2000 时,选用 Shapiro-Wilks 检验的检验统计量;\n当 n&gt;2000 时则选用 Kolmogorov-Smirnov 检验的检验统计量。\n\n总体参数 根据总体参数是否已知来选用不同的拟合优度检验及其对应的检验统计量\n\n正态分布总体均数和标准差都已知或都未知时上述三种基于经验分布函数的拟合优度检验都可选用;\n正态分布总体均数和标准差有一者未知时,选用 Cramer-von Mises 检验或 Anderson-Darling 检验。\n\n本例由于样本例数仅为 428,所以选用 Shapiro-Wilks 检验统计量 W=0.80784 所对应的 P= &lt; 0.001,说明该资料不服从正态分布。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-plot.html#几何均数的计算",
    "href": "Guide/SAS/25-05-29-SAS-plot.html#几何均数的计算",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.5 几何均数的计算",
    "text": "1.5 几何均数的计算",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-univariate.html",
    "href": "Guide/SAS/25-05-29-SAS-univariate.html",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-univariate.html#频数表的编制",
    "href": "Guide/SAS/25-05-29-SAS-univariate.html#频数表的编制",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.1 频数表的编制",
    "text": "1.1 频数表的编制\n频数表是对分类变量进行单变量分析的常用方法,可以显示每个类别的频数和百分比。\n\n检查数据集，选择变量\n\n\n\n代码\n%%SAS\n/*check the dataset*/\nproc print data = sashelp.cars(obs=5) label;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\nMake\nModel\nType\nOrigin\nDriveTrain\nMSRP\nInvoice\nEngine Size (L)\nCylinders\nHorsepower\nMPG (City)\nMPG (Highway)\nWeight (LBS)\nWheelbase (IN)\nLength (IN)\n\n\n\n\n1\nAcura\nMDX\nSUV\nAsia\nAll\n$36,945\n$33,337\n3.5\n6\n265\n17\n23\n4451\n106\n189\n\n\n2\nAcura\nRSX Type S 2dr\nSedan\nAsia\nFront\n$23,820\n$21,761\n2.0\n4\n200\n24\n31\n2778\n101\n172\n\n\n3\nAcura\nTSX 4dr\nSedan\nAsia\nFront\n$26,990\n$24,647\n2.4\n4\n200\n22\n29\n3230\n105\n183\n\n\n4\nAcura\nTL 4dr\nSedan\nAsia\nFront\n$33,195\n$30,299\n3.2\n6\n270\n20\n28\n3575\n108\n186\n\n\n5\nAcura\n3.5 RL 4dr\nSedan\nAsia\nFront\n$43,755\n$39,014\n3.5\n6\n225\n18\n24\n3880\n115\n197\n\n\n\n\n\n\n\n\n\n\n\n使用 cars 数据集中的 MPG(City) 变量进行频数表的编制\n\n\n\n代码\n%%SAS\n/*frequency table for MPG_City*/\nproc freq data=sashelp.cars;\n    tables MPG_City;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nFREQ 过程\n\n\n\n\n\n\n\nMPG (City)\n\n\nMPG_City\n频数\n百分比\n累积\n频数\n累积\n百分比\n\n\n\n\n10\n2\n0.47\n2\n0.47\n\n\n12\n4\n0.93\n6\n1.40\n\n\n13\n12\n2.80\n18\n4.21\n\n\n14\n13\n3.04\n31\n7.24\n\n\n15\n17\n3.97\n48\n11.21\n\n\n16\n31\n7.24\n79\n18.46\n\n\n17\n41\n9.58\n120\n28.04\n\n\n18\n69\n16.12\n189\n44.16\n\n\n19\n37\n8.64\n226\n52.80\n\n\n20\n57\n13.32\n283\n66.12\n\n\n21\n38\n8.88\n321\n75.00\n\n\n22\n18\n4.21\n339\n79.21\n\n\n23\n7\n1.64\n346\n80.84\n\n\n24\n22\n5.14\n368\n85.98\n\n\n25\n9\n2.10\n377\n88.08\n\n\n26\n22\n5.14\n399\n93.22\n\n\n27\n1\n0.23\n400\n93.46\n\n\n28\n5\n1.17\n405\n94.63\n\n\n29\n7\n1.64\n412\n96.26\n\n\n31\n1\n0.23\n413\n96.50\n\n\n32\n7\n1.64\n420\n98.13\n\n\n33\n1\n0.23\n421\n98.36\n\n\n35\n2\n0.47\n423\n98.83\n\n\n36\n1\n0.23\n424\n99.07\n\n\n38\n1\n0.23\n425\n99.30\n\n\n46\n1\n0.23\n426\n99.53\n\n\n59\n1\n0.23\n427\n99.77\n\n\n60\n1\n0.23\n428\n100.00",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-univariate.html#单变量描述",
    "href": "Guide/SAS/25-05-29-SAS-univariate.html#单变量描述",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.2 单变量描述",
    "text": "1.2 单变量描述\n单变量描述是对数值变量进行统计分析的过程,可以计算均值、标准差、最小值、最大值等统计量。\n\n1.2.1 means 过程\n\n\n代码\n%%SAS\n/*simple descriptive statistics for MPG_City*/\nproc means data = sashelp.cars;\n    var MPG_City;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: MPG_City MPG (City)\n\n\n数目\n均值\n标准差\n最小值\n最大值\n\n\n\n\n428\n20.0607477\n5.2382176\n10.0000000\n60.0000000\n\n\n\n\n\n\n\n\n\n\nmeans 过程默认给出均值、标准差、最小值和最大值，但是用户可以指定其他统计量，如下：\n\nStderr:均数的标准差,即标准误;\nSum:合计值;\nVariance:方差\nCV:变异系数;\nNmiss:缺失变量值的观测例数;\nRange:极差;\nUSS:平方和;\nCSS:离均差平方和;\nT:检验假设为总体均数为 0 的 student-t 检验的检验统计量 t 值;\nProbt:总体均数为0的检验假设中,值所对应的概率值(P值);\nSumweight:权重变量值的和;\nSkewness:偏度系数;\nKurtosis:峰度系数;\nCLM:双侧 95%置信区间的下限( lclm)和上限( uclm );\nMedian | P50:中位数或 50% 分位数;\nP1:1%分位数;\nP5:5%分位数;\nP10:10% 分位数;\nQ1 | P25:下四分位数或 25%分位数;\nQ3 | P75:上四分位数或 75% 分位数:\nP90:90%分位数;\nP95:95% 分位数:\nP99:99%分位数:\nQrange:四分位数间距\n\n多个统计量输出示例：\n\n\n代码\n%%SAS\n/*complex descriptive statistics for MPG_City*/\nproc means data = sashelp.cars n mean std stderr cv clm Qrange;\n    var MPG_City;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: MPG_City MPG (City)\n\n\n数目\n均值\n标准差\n标准误差\n变异系数\n均值的95% 置信下限\n均值的95% 置信上限\n四分位间距\n\n\n\n\n428\n20.0607477\n5.2382176\n0.2531988\n26.1117767\n19.5630765\n20.5584188\n4.5000000\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.2 保留小数\nmeans 过程给出的结果中,每个统计量均在小数点后保留七位有效数字,可以通过 maxdec 语句改变有效位数,该语句是 means 过程的一个选 项,可加在 proc means 的后面。示例如下：\n\n\n代码\n%%SAS\n/*design the decimal*/\nproc means maxdec = 2 data = sashelp.cars n mean std stderr cv clm;\n    var MPG_City;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: MPG_City MPG (City)\n\n\n数目\n均值\n标准差\n标准误差\n变异系数\n均值的95% 置信下限\n均值的95% 置信上限\n\n\n\n\n428\n20.06\n5.24\n0.25\n26.11\n19.56\n20.56\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.3 频数表资料的描述性统计\n如果数据已经被整理成频数表资料，means 过程通过 freq 语句定义频数变量,用 var 语句定义组中值变量,同样可以计算简单的描述性统计量。\n\n\n代码\n%%SAS\n/*程序4-4 freq*/\ndata prg4_4;\n    input x f @@;\ndatalines;\n3.17 2 3.37 3 3.57 9 3.77 22 4.17 30\n4.37 21 4.57 15 4.77 10 4.97 6 5.17 4 5.37 2\n;\nrun;\nproc means;\n    freq f;\n    var x;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: x\n\n\n数目\n均值\n标准差\n最小值\n最大值\n\n\n\n\n124\n4.2409677\n0.4738313\n3.1700000\n5.3700000",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-univariate.html#univariate-过程",
    "href": "Guide/SAS/25-05-29-SAS-univariate.html#univariate-过程",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.3 univariate 过程",
    "text": "1.3 univariate 过程\nunivariate 过程能够给出的描述性统计量比较多,除了上述 means 过程给出的统计量外,它还能输出符号统计量、正态性检验的统计量以及用户自己定义的百分位数,而且可以生成若干个描述变量分布的茎叶图、箱式图、正态概率图等统计图。\n\n\n代码\n%%SAS\n/*程序4-6 univariate过程*/\ndata prg4_6;\n    input x f @@;\ndatalines;\n18 4 30 17 42 32 54 24 66 18 78 12 90 5 102 4 114 2\n;\nrun;\nproc univariate;\n    var x;\n    freq f;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n变量:  x\n \n频数:  f\n\n\n\n\n\n\n\n矩\n\n\n\n\n数目\n118\n权重总和\n118\n\n\n均值\n54.5084746\n观测总和\n6432\n\n\n标准差\n21.0724212\n方差\n444.046936\n\n\n偏度\n0.69543247\n峰度\n0.17594659\n\n\n未校平方和\n402552\n校正平方和\n51953.4915\n\n\n变异系数\n38.6589817\n标准误差均值\n1.93987361\n\n\n\n\n\n\n\n\n\n\n基本统计测度\n\n\n位置\n变异性\n\n\n\n\n均值\n54.50847\n标准差\n21.07242\n\n\n中位数\n54.00000\n方差\n444.04694\n\n\n众数\n42.00000\n极差\n96.00000\n\n\n \n \n四分位间距\n24.00000\n\n\n\n\n\n\n\n\n\n\n位置检验: Mu0=0\n\n\n检验\n统计量\np 值\n\n\n\n\nStudent t\nt\n28.09898\nPr &gt; |t|\n&lt;.0001\n\n\n符号\nM\n59\nPr &gt;= |M|\n&lt;.0001\n\n\n符号秩\nS\n3510.5\nPr &gt;= |S|\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n分位数（定义 5）\n\n\n水平\n分位数\n\n\n\n\n100% 最大值\n114\n\n\n99%\n114\n\n\n95%\n102\n\n\n90%\n78\n\n\n75% Q3\n66\n\n\n50% 中位数\n54\n\n\n25% Q1\n42\n\n\n10%\n30\n\n\n5%\n30\n\n\n1%\n18\n\n\n0% 最小值\n18\n\n\n\n\n\n\n\n\n\n\n极值观测\n\n\n最小值\n最大值\n\n\n值\n频数\n观测\n值\n频数\n观测\n\n\n\n\n18\n4\n1\n66\n18\n5\n\n\n30\n17\n2\n78\n12\n6\n\n\n42\n32\n3\n90\n5\n7\n\n\n54\n24\n4\n102\n4\n8\n\n\n66\n18\n5\n114\n2\n9\n\n\n\n\n\n\n\n\n\n\n\n程序说明:\n\n数据集 prg4_6 中的变量为 x 和 f ;\n调用 univariate 过程时,var x;语句指明 x 为分析变量;\nfreq f,语句表示 f 为频数变量。\n\n整个分析结果输出的统计量分为五个部分:\n\n矩(Moments)、\n基本统计测度( Basic Statistical Measures)、\n位置检验( Tests for Location:Mu0=0)、\n分位数[Quantiles(Definition 5)]\n和极值观测(Extreme Observations )\n\n\n矩(Moments) 部分的统计量包括:\n\n\n非缺失值的例数(N)\n权重总和(Sum Weights)\n均数(Mean)\n观测总和(Sum Observations)\n标准差(Std Deviation)\n方差(Variance)\n偏度(Skewness,即偏度系数)\n峰度(Kurtosis,即峰度系数)\n未校正平方和(Uncorrected SS,即平方和)\n校正平方和(Corrected SS,即离均差平方和)\n变异系数(Coeff Variation)\n标准误差均数(Std Error Mean,即标准误)。\n\n\n基本统计测度(Basic Statistical Measures)部分统计量包括:\n\n\n均数(Mean)\n标准差(StdDeviation)\n中位数(Median)\n方差(Variance)\n众数(Mode)\n极差(Range)\n四分位极差(Interquartile Range,即四分位数间距)。\n\n\n位置检验(Tests for Location: Mu0=0)部分的统计量包括:\n\n\nStudent’s t: 总体均数为0的 student-t 检验的检验统计量u值;\nPr &gt; |t|: 总体均数为0的t检验中,检验统计量所对应的概率值(P值);\nSign M: 总体中位数为0的符号检验的检验统计量 M值;\nPr &gt;= |M|: 总体中位数为0的符号检验中检验统计量所对应的概率值(P值);\nSigned Ranks: 总体中位数为0的符号秩检验的检验统计量S值;\nPr &gt;= |S|: 总体中位数为0的符号秩检验中，检验统计量所对应的概率值(P值)。\n\n\n分位数[Quantiles(Definition 5)] 部分的统计量包括:\n\n\n100% 分位数( 100% Max，即最大值)\n99% 分位数\n95% 分位数\n90% 分位数\n75% (即 Q3,上四分位数)\n50% 分位数(即 Median,中位数)\n25% 分位数(即 Q1,下四分位数)\n10% 分位数\n5% 分位数\n1% 分位数和 0% 分位数( 0% Min,即最小值)。\n\n\n极值观测(Extreme Observations) 部分列出了五个最小值和五个最大值以及这些值分别对应的频数和观测号。\n\numivariate 过程除了能够给出几个特定的百分位数,还能输出用户自己定义的百分位数。此时在过程中要使用 output 语句.\n\n\n代码\n%%SAS\ndata prg4_6;\n    input x f @@;\ndatalines;\n18 4 30 17 42 32 54 24 66 18 78 12 90 5 102 4 114 2\n;\nrun;\n/*程序4-7 univariate 自定义百分数，需要使用 output 语句*/\n/**/\nproc univariate data = prg4_6;\n    var x;\n    freq f;\n    output out = pct pctlpre = p pctlpts = 2.5 97.5;\nrun;\nproc print data = pct;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n变量:  x\n \n频数:  f\n\n\n\n\n\n\n\n矩\n\n\n\n\n数目\n118\n权重总和\n118\n\n\n均值\n54.5084746\n观测总和\n6432\n\n\n标准差\n21.0724212\n方差\n444.046936\n\n\n偏度\n0.69543247\n峰度\n0.17594659\n\n\n未校平方和\n402552\n校正平方和\n51953.4915\n\n\n变异系数\n38.6589817\n标准误差均值\n1.93987361\n\n\n\n\n\n\n\n\n\n\n基本统计测度\n\n\n位置\n变异性\n\n\n\n\n均值\n54.50847\n标准差\n21.07242\n\n\n中位数\n54.00000\n方差\n444.04694\n\n\n众数\n42.00000\n极差\n96.00000\n\n\n \n \n四分位间距\n24.00000\n\n\n\n\n\n\n\n\n\n\n位置检验: Mu0=0\n\n\n检验\n统计量\np 值\n\n\n\n\nStudent t\nt\n28.09898\nPr &gt; |t|\n&lt;.0001\n\n\n符号\nM\n59\nPr &gt;= |M|\n&lt;.0001\n\n\n符号秩\nS\n3510.5\nPr &gt;= |S|\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n分位数（定义 5）\n\n\n水平\n分位数\n\n\n\n\n100% 最大值\n114\n\n\n99%\n114\n\n\n95%\n102\n\n\n90%\n78\n\n\n75% Q3\n66\n\n\n50% 中位数\n54\n\n\n25% Q1\n42\n\n\n10%\n30\n\n\n5%\n30\n\n\n1%\n18\n\n\n0% 最小值\n18\n\n\n\n\n\n\n\n\n\n\n极值观测\n\n\n最小值\n最大值\n\n\n值\n频数\n观测\n值\n频数\n观测\n\n\n\n\n18\n4\n1\n66\n18\n5\n\n\n30\n17\n2\n78\n12\n6\n\n\n42\n32\n3\n90\n5\n7\n\n\n54\n24\n4\n102\n4\n8\n\n\n66\n18\n5\n114\n2\n9\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\np2_5\np97_5\n\n\n\n\n1\n18\n102\n\n\n\n\n\n\n\n\n\n\n程序说明：\n\nunivariate 产生的部分统计量输出到新建的数据集中,数据集的名称由 out= 来定义,本例 out=pct 就是表示将要新建的数据集名称定为 pct。\noutput 语句中的选项 pctlpts 表示需要计算的百分位数,本例需要输出第 2.5% 和第 97.5%分位数;\npctlpre 表示在新数据集中的变量中百分位数的前缀,本例表示百分位数前缀为 p。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-univariate.html#正态性检验",
    "href": "Guide/SAS/25-05-29-SAS-univariate.html#正态性检验",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.4 正态性检验",
    "text": "1.4 正态性检验\n正态性检验是检验数据是否符合正态分布的统计方法,常用的正态性检验方法有 Shapiro-Wilk 检验、Kolmogorov-Smirnov 检验等。\n在 SAS 中,可以使用 univariate 过程进行正态性检验，其语法只需要在 proc univariate 语句中加上 normal 选项即可，normal 和 plot 选项,就能输出该组数据正态性检验的结果和茎叶图、箱式图及正态概率图。\n\n\n代码\n%%SAS\nproc univariate normal plot data = sashelp.cars;\n    var MPG_City;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n变量:  MPG_City  (MPG (City))\n\n\n\n\n\n\n\n矩\n\n\n\n\n数目\n428\n权重总和\n428\n\n\n均值\n20.0607477\n观测总和\n8586\n\n\n标准差\n5.23821764\n方差\n27.438924\n\n\n偏度\n2.7820718\n峰度\n15.7911473\n\n\n未校平方和\n183958\n校正平方和\n11716.4206\n\n\n变异系数\n26.1117767\n标准误差均值\n0.25319881\n\n\n\n\n\n\n\n\n\n\n基本统计测度\n\n\n位置\n变异性\n\n\n\n\n均值\n20.06075\n标准差\n5.23822\n\n\n中位数\n19.00000\n方差\n27.43892\n\n\n众数\n18.00000\n极差\n50.00000\n\n\n \n \n四分位间距\n4.50000\n\n\n\n\n\n\n\n\n\n\n位置检验: Mu0=0\n\n\n检验\n统计量\np 值\n\n\n\n\nStudent t\nt\n79.22923\nPr &gt; |t|\n&lt;.0001\n\n\n符号\nM\n214\nPr &gt;= |M|\n&lt;.0001\n\n\n符号秩\nS\n45903\nPr &gt;= |S|\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n正态性检验\n\n\n检验\n统计量\np 值\n\n\n\n\nShapiro-Wilk\nW\n0.80784\nPr &lt; W\n&lt;0.0001\n\n\nKolmogorov-Smirnov\nD\n0.178848\nPr &gt; D\n&lt;0.0100\n\n\nCramer-von Mises\nW-Sq\n2.686108\nPr &gt; W-Sq\n&lt;0.0050\n\n\nAnderson-Darling\nA-Sq\n14.67298\nPr &gt; A-Sq\n&lt;0.0050\n\n\n\n\n\n\n\n\n\n\n分位数（定义 5）\n\n\n水平\n分位数\n\n\n\n\n100% 最大值\n60.0\n\n\n99%\n36.0\n\n\n95%\n29.0\n\n\n90%\n26.0\n\n\n75% Q3\n21.5\n\n\n50% 中位数\n19.0\n\n\n25% Q1\n17.0\n\n\n10%\n15.0\n\n\n5%\n14.0\n\n\n1%\n12.0\n\n\n0% 最小值\n10.0\n\n\n\n\n\n\n\n\n\n\n极值观测\n\n\n最小值\n最大值\n\n\n值\n观测\n值\n观测\n\n\n\n\n10\n167\n36\n156\n\n\n10\n119\n38\n405\n\n\n12\n413\n46\n150\n\n\n12\n217\n59\n374\n\n\n12\n216\n60\n151\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.1 正态性检验部分”Tests for Normality”\n正态性检验部分包括 Shapiro-Wilk 检验以及基于经验分布函数的拟合优度检验:Kolmogorov-Smirnov 检验、Cramer-von Mises 检验、 Anderson-Darling 检验。\n样本量\n\n当 n≤2000 时,选用 Shapiro-Wilks 检验的检验统计量;\n当 n&gt;2000 时则选用 Kolmogorov-Smirnov 检验的检验统计量。\n\n总体参数 根据总体参数是否已知来选用不同的拟合优度检验及其对应的检验统计量\n\n正态分布总体均数和标准差都已知或都未知时上述三种基于经验分布函数的拟合优度检验都可选用;\n正态分布总体均数和标准差有一者未知时,选用 Cramer-von Mises 检验或 Anderson-Darling 检验。\n\n本例由于样本例数仅为 428,所以选用 Shapiro-Wilks 检验统计量 W=0.80784 所对应的 P= &lt; 0.001,说明该资料不服从正态分布。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-univariate.html#几何均数的计算",
    "href": "Guide/SAS/25-05-29-SAS-univariate.html#几何均数的计算",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.5 几何均数的计算",
    "text": "1.5 几何均数的计算\nSAS 无法直接计算几何均数,必须通过 SAS 语句编写一段程序,用 means 过程或 univariate 过程间接计算出几何均数。\n例 4-3 69例类风湿关节炎(RA)患者血清 EBV-VCA-lgG 抗体滴度的分布结果如下,求其平均抗体滴度\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n抗体滴度\n1:10\n1:20\n1:40\n1:80\n1:160\n1:320\n1:640\n1:1280\n\n\n\n\n人数\n4\n3\n10\n10\n11\n15\n14\n2\n\n\n\n\n\n代码\n%%SAS\ndata prg4 _9;\n    input x f @@;\n    y = log10(x);\ndatalines;\n10 4 20 3 40 10 80 10 160 11 320 15 640 14 1280 2\nrun;\nproc means noprint;\n    var y;\n    freq f;\n    output out = b mean = logmean;\nrun;\ndata c;\n    set b;\n    g = 10**logmean;\nrun;\nproc print data = c;\n    var g;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\ng\n\n\n\n\n1\n150.641\n\n\n\n\n\n\n\n\n\n\n\n1.5.1 程序说明:\n\n第一步创建数据集 prg4_9;,它有三个变量 x、f 和 y, x 为抗体滴度的倒数, f 为某抗体滴度所对应的频数, y 是 x 的对数(以10为底)。\n第二步是用 means 过程计算y的描述性统计量,将计算所得到的均数输出到数据集 b 中用变量名 logmean 表示,则数据集 b 有一个变量 logmean,一个观测,其值为y的均数。\nnoprint 语句表示不在 Output 窗口显示 means 过程的结果;\n第三步新建数据集 c ,调用数据集 b 中的内容,新产生变量 g ,该变量的值为变量 logmean 的反对数。\n第四步将数据集 c 的结果显示在 Output 窗口内。\n\n得出这组数据的几何均数为：\\(1:150.641\\)",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-univariate.html#means-过程常用选项和语句",
    "href": "Guide/SAS/25-05-29-SAS-univariate.html#means-过程常用选项和语句",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.6 means 过程常用选项和语句",
    "text": "1.6 means 过程常用选项和语句\n运用 means 和 univariate 过程进行计量资料的统计描述时,可根据需求增加一些选项或语句,以满足用户的输入/输出要求。\n\n1.6.1 means 过程的基本格式\nproc means &lt;options&gt; &lt;统计量关键字&gt;;\n    by &lt;descending&gt; 变量名1 &lt;变量名 2&gt; &lt;notsorted&gt;;\n    class 变量名1 &lt;变量名 2&gt;...;\n    var 变量名 1 &lt;变量名 2&gt;...;\n    freq 变量名;\n    output out = 数据集名 统计量名(变量列表) = 新列名;\nrun;\n\n\n1.6.2 means 过程常用选项\n\nnoprint：不在 Output 窗口输出结果,但仍然将结果输出到数据集中;\nmaxdec=: 指定输出结果的小数点后有效位数，默认是 7 位，常用有 maxdec=2、maxdec=4 等;\nalpha=value: 用于指定均数置信区间的置信水平,默认值为 0.05.\nmissing: 将 class 语句所指定变量的缺失值作为合法的水平用以创建代表分组的组合,否则 class 语句所指定变量为缺失值的观测将会被排除在分析过程之外.\n\n\n\n1.6.3 means 过程的常用语句\n\nby: 用于指定分组变量,以便按照该变量将输人数据集分割为多个子数据集，从而在各子数据集内分别执行相应的分析过程，使用该语句前需使用 sort 过程对输人数据集进行排序。可以在 by 语句中设置 notsorted 或 descending 选项,前者表示数据未按照 by 语句所指定变量进行排列,后者是在输人数据集时先按照 by 语句所指定变量进行降序排列时使用.\nclass: 也用于指定分组变量,但其作用与 by 语句稍有不同。每一个 class 语句所指定变量的水平或多个 class 语句所指定变量的每一个水平组合均定义一个分组,有关全体样本和各分组内样本的相应统计量均会被计算并显示.",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-29-SAS-univariate.html#univariate-过程常用选项和语句",
    "href": "Guide/SAS/25-05-29-SAS-univariate.html#univariate-过程常用选项和语句",
    "title": "09-SAS 计量资料的单变量分析",
    "section": "1.7 univariate 过程常用选项和语句",
    "text": "1.7 univariate 过程常用选项和语句\n\n1.7.1 univariate 过程的基本格式\nproc univariate &lt;options&gt;;\n    by &lt;descending&gt; 变量名1 &lt;变量名 2&gt; &lt;notsorted&gt;;\n    class 变量名1 &lt;变量名 2&gt;...;\n    var 变量名 1 &lt;变量名 2&gt;...;\n    freq 变量名;\n    histogram 变量名1 &lt;变量名 2&gt;/&lt;选项&gt;;\n    probplot 变量名1 &lt;变量名 2&gt;/&lt;选项&gt;;\n    qqplot 变量名1 &lt;变量名 2&gt;/&lt;选项&gt;;\n    output out=数据集名称 &lt;选项&gt;;\nrun;\n\n\n1.7.2 univariate 过程常用选项\n\ndata = 输入资料文件名称 指明到底对那一个资料文件进行分析，若省略此选项则 SAS 会自动找出在本程序之前最后形成的资料文件并对它进行分析。\nnoprint 使用此选项分析结果将不在 result 里输出。\nplot 使用此选项 UNIVARIATE 过程将产生三种图形：茎叶图 (Stem-And-Leaf Plot) 、平行条状图 (Horizontal Bar Chart)、盒状图(Box Plot)、正态分布拟合图 (Normal Probability Plot)\ncibasic 选项 以正态分布为基础,为均数、标准差、方差等计算置信区间,该选项还可以设定次级选项设定置信区间类型及置信区间的置信水平。\ncipctldf 选项以非参数方法为各分位数计算置信区间,该选项的用法和功能与 cibasic 类似。\ncipctdfnormal 选项以正态分布假设为基础为各分位数计算置信区间,该选项的用法和功能与 cibasic 类似\n\n\n\n1.7.3 univariate 过程常用语句\n\nhistogram语句 该语句用于对指定的变量绘制高分辨率的直方图,同时还可以为直方图添加分布密度曲线。在一个 univariate 过程中可以同时调用多条 histogram 语句,同时还可以为 histogram 语句设定相应的变量及选项来对生成的图形进行相应的调整。\nprobplot 语句 该语句用于对指定变量绘制高分辨率的概率图。与 histogram 语句一样，该语句也可以指定对应的分析变量及控制选项来执行不同的控制功能。\nqqplot 语句 该语句用于对指定变量绘制高分辨率的 q-q 图,用于判断数据是否符合所指定的理论分布。该语句的用法与 probplot 语句类似,也可设定相应的变量及控制选项。\nppplot 语句 该语句用于对指定变量绘制高分辨率的 p-p 图,用于判断数据是否符合所指定的理论分布。该语句的用法与 probplot 语句类似,也可设定相应的变量及控制选项。\ncdfplot 语句 该语句用于对指定变量绘制高分辨率的经验分布函数图,用于判断数据是否符合所指定的理论分布。该语句的用法与 probplot 语句类似,也可设定相应的变量及控制选项。\n\n\n\n1.7.4 直方图示例\n\n\n代码\n%%SAS\nPROC UNIVARIATE DATA=SASHELP.FISH NOPRINT;\n    WHERE SPECIES='Bream';\n    VAR HEIGHT;\n    HISTOGRAM;\nRUN;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.7.5 正态拟合曲线\n加一个 NORMAL 选项则在直方图上面加了一根拟合后的正态分布图，并且还增加了拟合正太分布的参数估计、拟合优度、分位数，这里的括号中的意思可理解为均值（MU）和标准差（SIGMA）的值为原始数据本身的均值和标准差.\n\n\n代码\n%%SAS\nPROC UNIVARIATE DATA=SASHELP.FISH NOPRINT;\n    WHERE SPECIES='Bream';\n    VAR HEIGHT;\n    HISTOGRAM / NORMAL(MU=EST SIGMA=EST);\nRUN;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n“Height”的拟合正态分布\n\n\n\n\n\n\n\n“正态”分布的参数\n\n\n参数\n符号\n估计\n\n\n\n\n均值\nMu\n15.18321\n\n\n标准差\nSigma\n1.964707\n\n\n\n\n\n\n\n\n\n\n“正态”分布的拟合优度检验\n\n\n检验\n统计量\np 值\n\n\n\n\nKolmogorov-Smirnov\nD\n0.06967244\nPr &gt; D\n&gt;0.150\n\n\nCramer-von Mises\nW-Sq\n0.03567178\nPr &gt; W-Sq\n&gt;0.250\n\n\nAnderson-Darling\nA-Sq\n0.29075434\nPr &gt; A-Sq\n&gt;0.250\n\n\n\n\n\n\n\n\n\n\n“正态”分布的分位数\n\n\n百分比\n分位数\n\n\n观测\n估计\n\n\n\n\n1.0\n11.5200\n10.6126\n\n\n5.0\n12.3778\n11.9516\n\n\n10.0\n12.4800\n12.6653\n\n\n25.0\n13.9129\n13.8580\n\n\n50.0\n14.9544\n15.1832\n\n\n75.0\n16.3618\n16.5084\n\n\n90.0\n18.0840\n17.7011\n\n\n95.0\n18.7542\n18.4149\n\n\n99.0\n18.9570\n19.7538\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.7.6 实际数据核分布的密度曲线\n\n\n代码\n%%SAS\nPROC UNIVARIATE DATA=SASHELP.FISH NOPRINT;\n    WHERE SPECIES = 'Bream';\n    VAR HEIGHT;\n    HISTOGRAM / NORMAL(MU=EST SIGMA=EST) KERNEL;\nRUN;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n“Height”的拟合正态分布\n\n\n\n\n\n\n\n“正态”分布的参数\n\n\n参数\n符号\n估计\n\n\n\n\n均值\nMu\n15.18321\n\n\n标准差\nSigma\n1.964707\n\n\n\n\n\n\n\n\n\n\n“正态”分布的拟合优度检验\n\n\n检验\n统计量\np 值\n\n\n\n\nKolmogorov-Smirnov\nD\n0.06967244\nPr &gt; D\n&gt;0.150\n\n\nCramer-von Mises\nW-Sq\n0.03567178\nPr &gt; W-Sq\n&gt;0.250\n\n\nAnderson-Darling\nA-Sq\n0.29075434\nPr &gt; A-Sq\n&gt;0.250\n\n\n\n\n\n\n\n\n\n\n“正态”分布的分位数\n\n\n百分比\n分位数\n\n\n观测\n估计\n\n\n\n\n1.0\n11.5200\n10.6126\n\n\n5.0\n12.3778\n11.9516\n\n\n10.0\n12.4800\n12.6653\n\n\n25.0\n13.9129\n13.8580\n\n\n50.0\n14.9544\n15.1832\n\n\n75.0\n16.3618\n16.5084\n\n\n90.0\n18.0840\n17.7011\n\n\n95.0\n18.7542\n18.4149\n\n\n99.0\n18.9570\n19.7538\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.7.7 plot\nplot 选项，在结果中增加了分析变量数据的分布图、盒形图、以及概率图.\n\n\n代码\n%%SAS\nPROC UNIVARIATE DATA=SASHELP.FISH PLOT;\n    WHERE SPECIES = 'Bream';\n    VAR HEIGHT;\n    HISTOGRAM / NORMAL(MU=EST SIGMA=EST) KERNEL;\nRUN;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n变量:  Height\n\n\n\n\n\n\n\n矩\n\n\n\n\n数目\n35\n权重总和\n35\n\n\n均值\n15.1832114\n观测总和\n531.4124\n\n\n标准差\n1.96470673\n方差\n3.86007253\n\n\n偏度\n0.24174068\n峰度\n-0.5914026\n\n\n未校平方和\n8199.78929\n校正平方和\n131.242466\n\n\n变异系数\n12.9399945\n标准误差均值\n0.33209605\n\n\n\n\n\n\n\n\n\n\n基本统计测度\n\n\n位置\n变异性\n\n\n\n\n均值\n15.18321\n标准差\n1.96471\n\n\n中位数\n14.95440\n方差\n3.86007\n\n\n众数\n.\n极差\n7.43700\n\n\n \n \n四分位间距\n2.44890\n\n\n\n\n\n\n\n\n\n\n位置检验: Mu0=0\n\n\n检验\n统计量\np 值\n\n\n\n\nStudent t\nt\n45.71934\nPr &gt; |t|\n&lt;.0001\n\n\n符号\nM\n17.5\nPr &gt;= |M|\n&lt;.0001\n\n\n符号秩\nS\n315\nPr &gt;= |S|\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n分位数（定义 5）\n\n\n水平\n分位数\n\n\n\n\n100% 最大值\n18.9570\n\n\n99%\n18.9570\n\n\n95%\n18.7542\n\n\n90%\n18.0840\n\n\n75% Q3\n16.3618\n\n\n50% 中位数\n14.9544\n\n\n25% Q1\n13.9129\n\n\n10%\n12.4800\n\n\n5%\n12.3778\n\n\n1%\n11.5200\n\n\n0% 最小值\n11.5200\n\n\n\n\n\n\n\n\n\n\n极值观测\n\n\n最小值\n最大值\n\n\n值\n观测\n值\n观测\n\n\n\n\n11.5200\n1\n18.0369\n31\n\n\n12.3778\n3\n18.0840\n32\n\n\n12.4440\n5\n18.6354\n34\n\n\n12.4800\n2\n18.7542\n33\n\n\n12.6700\n8\n18.9570\n30\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n“Height”的拟合正态分布\n\n\n\n\n\n\n\n“正态”分布的参数\n\n\n参数\n符号\n估计\n\n\n\n\n均值\nMu\n15.18321\n\n\n标准差\nSigma\n1.964707\n\n\n\n\n\n\n\n\n\n\n“正态”分布的拟合优度检验\n\n\n检验\n统计量\np 值\n\n\n\n\nKolmogorov-Smirnov\nD\n0.06967244\nPr &gt; D\n&gt;0.150\n\n\nCramer-von Mises\nW-Sq\n0.03567178\nPr &gt; W-Sq\n&gt;0.250\n\n\nAnderson-Darling\nA-Sq\n0.29075434\nPr &gt; A-Sq\n&gt;0.250\n\n\n\n\n\n\n\n\n\n\n“正态”分布的分位数\n\n\n百分比\n分位数\n\n\n观测\n估计\n\n\n\n\n1.0\n11.5200\n10.6126\n\n\n5.0\n12.3778\n11.9516\n\n\n10.0\n12.4800\n12.6653\n\n\n25.0\n13.9129\n13.8580\n\n\n50.0\n14.9544\n15.1832\n\n\n75.0\n16.3618\n16.5084\n\n\n90.0\n18.0840\n17.7011\n\n\n95.0\n18.7542\n18.4149\n\n\n99.0\n18.9570\n19.7538",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "09-SAS 计量资料的单变量分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-31-two-sample-univariate.html",
    "href": "Guide/SAS/25-05-31-two-sample-univariate.html",
    "title": "10-SAS 两样本均数的比较分析",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "10-SAS 两样本均数的比较分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-31-two-sample-univariate.html#两样本均数的比较的概述",
    "href": "Guide/SAS/25-05-31-two-sample-univariate.html#两样本均数的比较的概述",
    "title": "10-SAS 两样本均数的比较分析",
    "section": "1 两样本均数的比较的概述",
    "text": "1 两样本均数的比较的概述\n两均数比较有三种情况: 样本均数和总体均数比较﹑配对设计资料两样本均数的比较和非配对设计资料的两样本均数比较。\n\n前两种比较除了可用 means 和 univariate 过程完成外,还能用 ttest 过程完成;\n而后者一般可用 ttest 过程完成.\n置信区间估计和假设检验在原理上无根本区别,只是考虑问题的角度不同,故也可以利用置信区间估计得到假设检验的结论.\n单一总体均数的置信区间估计可运用 means 过程完成.\n两独立样本总体均数的置信区间估计可运用 ttest 过程完成.\n\n\n1.1 什么是 t test\n用于评估来自至多两个样本的变量的均值间差异的统计学手段。变量必须是可量化的， 比如身高，产量，体重变化等等指标。\nt test反映了基于期望的显著性水平，你所观察到的差异是否 “显著”。t test使用t分布来评估期望变异程度。\n如果你的样本数量大于30，那么假定样本符合正态分布，也就是 z-test 的检验模型。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "10-SAS 两样本均数的比较分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-31-two-sample-univariate.html#单样本均数的-t-检验",
    "href": "Guide/SAS/25-05-31-two-sample-univariate.html#单样本均数的-t-检验",
    "title": "10-SAS 两样本均数的比较分析",
    "section": "2 单样本均数的 t 检验",
    "text": "2 单样本均数的 t 检验\n例 5-1 某医生测量 30 例从事铅作业男性工人的血红蛋白含量( g/L),具体数值如下:\n\n\n\n171\n79\n135\n78\n118\n175\n122\n105\n111\n140\n\n\n\n\n138\n132\n142\n140\n168\n113\n131\n145\n128\n124\n\n\n134\n116\n129\n155\n135\n134\n136\n113\n119\n132\n\n\n\n问这批工人的血红蛋白是否不同于正常成年男性平均数140g/L?\n\n\n代码\n%%SAS\ndata prg5_1;\n    input x @@;\ndatalines;\n171 79 135 78 118 175 122 105 111 140\n138 132 142 140 168 113 131 145 128 124\n134 116 129 155 135 134 136 113 119 132\n;\nrun;\nproc ttest h0=140;\nvar x;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n\nSAS 系统 \n\n\nTTEST 过程\n \n变量:   x\n\n\n\n\n\n\n数目\n均值\n标准差\n标准误差\n最小值\n最大值\n\n\n\n\n30\n129.9\n21.9481\n4.0072\n78.0000\n175.0\n\n\n\n\n\n\n\n\n\n\n均值\n95% 置信均值\n标准差\n95% 置信限标准差\n\n\n\n\n129.9\n121.7\n138.1\n21.9481\n17.4796\n29.5052\n\n\n\n\n\n\n\n\n\n\n自由度\nt 值\nPr &gt; |t|\n\n\n\n\n29\n-2.51\n0.0178\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1 结果说明\n\n第一部分结果显示了分析变量 x 的一些描述性统计量,包括例数(N),均数(Mean),标准差(Std Dev),标准误( Std Err),最小值( Minimum )和最大值(Maximum )。\n第二部分结果显示了分析变量 x 置信区间的情况,包括均数(Mean)及其95%置信区间(95%CL Mean)、标准差(Std Dev)及其置信区间(95%CL Std Dev)。\n第三部分结果显示了 t 检验的结果,内容包括自由度(DF)检验统计量t值(t Value)和该值所对应的概率值(Pr&gt; |t|)。\n本例 t 检验的检验统计量等于 -2.51 ,所对应的P值为 0.0178&lt;0.05 ,说明该样本均数和总体均数的差异有统计学意义,即从事铅工作的男性工人血红蛋白的含量要低于正常成人。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "10-SAS 两样本均数的比较分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-31-two-sample-univariate.html#配对资料两样本均数比较的t检验",
    "href": "Guide/SAS/25-05-31-two-sample-univariate.html#配对资料两样本均数比较的t检验",
    "title": "10-SAS 两样本均数的比较分析",
    "section": "3 配对资料两样本均数比较的t检验",
    "text": "3 配对资料两样本均数比较的t检验\n当配对资料原始变量值是已知的,可用 means、univariate 和 ttest 过程完成t检验。\n例5-2 为比较两种方法对乳酸饮料中脂肪含量测定结果是否不同,某人随机抽取了10份乳酸饮料制品,分别用哥特里-罗紫法和脂肪酸水解法测定,其结果见表5-1。问两法测定结果是否不同?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n哥特里-罗紫法\n0.840\n0.591\n0.674\n0.632\n0.687\n0.978\n0.750\n0.730\n1.200\n0.870\n\n\n脂肪酸水解法\n0.580\n0.509\n0.500\n0.316\n0.337\n0.517\n0.454\n0.512\n0.997\n0.506\n\n\n\n\n3.1 means 过程\n\n\n代码\n%%SAS\n/* programme 5-2*/\ndata prg5_2;\n    input x1 x2 @@;\n    d = x1 - x2;\ndatalines;\n0.840 0.580 0.591 0.509 0.674 0.500 0.632 0.316 0.687 0.337 \n0.978 0.517 0.750 0.454 0.730 0.512 1.200 0.997 0.870 0.506\n;\nrun;\nproc means n mean std stderr t prt;\n    var d;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: d\n\n\n数目\n均值\n标准差\n标准误差\nt 值\nPr &gt; |t|\n\n\n\n\n10\n0.2724000\n0.1086812\n0.0343680\n7.93\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 univariate 过程\n\n\n代码\n%%SAS\n/* programme 5-3*/\nproc univariate data = prg5_2;\n    var d;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n变量:  d\n\n\n\n\n\n\n\n矩\n\n\n\n\n数目\n10\n权重总和\n10\n\n\n均值\n0.2724\n观测总和\n2.724\n\n\n标准差\n0.10868119\n方差\n0.0118116\n\n\n偏度\n-0.0337503\n峰度\n0.06565577\n\n\n未校平方和\n0.848322\n校正平方和\n0.1063044\n\n\n变异系数\n39.8976451\n标准误差均值\n0.03436801\n\n\n\n\n\n\n\n\n\n\n基本统计测度\n\n\n位置\n变异性\n\n\n\n\n均值\n0.272400\n标准差\n0.10868\n\n\n中位数\n0.278000\n方差\n0.01181\n\n\n众数\n.\n极差\n0.37900\n\n\n \n \n四分位间距\n0.14700\n\n\n\n\n\n\n\n\n\n\n位置检验: Mu0=0\n\n\n检验\n统计量\np 值\n\n\n\n\nStudent t\nt\n7.925976\nPr &gt; |t|\n&lt;.0001\n\n\n符号\nM\n5\nPr &gt;= |M|\n0.0020\n\n\n符号秩\nS\n27.5\nPr &gt;= |S|\n0.0020\n\n\n\n\n\n\n\n\n\n\n分位数（定义 5）\n\n\n水平\n分位数\n\n\n\n\n100% 最大值\n0.4610\n\n\n99%\n0.4610\n\n\n95%\n0.4610\n\n\n90%\n0.4125\n\n\n75% Q3\n0.3500\n\n\n50% 中位数\n0.2780\n\n\n25% Q1\n0.2030\n\n\n10%\n0.1280\n\n\n5%\n0.0820\n\n\n1%\n0.0820\n\n\n0% 最小值\n0.0820\n\n\n\n\n\n\n\n\n\n\n极值观测\n\n\n最小值\n最大值\n\n\n值\n观测\n值\n观测\n\n\n\n\n0.082\n2\n0.296\n7\n\n\n0.174\n3\n0.316\n4\n\n\n0.203\n9\n0.350\n5\n\n\n0.218\n8\n0.364\n10\n\n\n0.260\n1\n0.461\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3 ttest 过程\n\n\n代码\n%%SAS\n/* programme 5-4*/\nproc ttest data = prg5_2;\n    var d;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n\nSAS 系统 \n\n\nTTEST 过程\n \n变量:   d\n\n\n\n\n\n\n数目\n均值\n标准差\n标准误差\n最小值\n最大值\n\n\n\n\n10\n0.2724\n0.1087\n0.0344\n0.0820\n0.4610\n\n\n\n\n\n\n\n\n\n\n均值\n95% 置信均值\n标准差\n95% 置信限标准差\n\n\n\n\n0.2724\n0.1947\n0.3501\n0.1087\n0.0748\n0.1984\n\n\n\n\n\n\n\n\n\n\n自由度\nt 值\nPr &gt; |t|\n\n\n\n\n9\n7.93\n&lt;.0001",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "10-SAS 两样本均数的比较分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-31-two-sample-univariate.html#两样本均数比较的-t-检验",
    "href": "Guide/SAS/25-05-31-two-sample-univariate.html#两样本均数比较的-t-检验",
    "title": "10-SAS 两样本均数的比较分析",
    "section": "4 两样本均数比较的 t 检验",
    "text": "4 两样本均数比较的 t 检验\n两样本均数比较的t检验—般用ttest过程。\n例 5-3 欲考察牙周炎患者和正常人血清中肿瘤坏死因子-α(TNF-a,U/ml)平均含量是否不同。研究者随机选取了牙周炎患者和正常人各12例,所测定TNF-α的含量见表5-2。问牙周炎患者和正常人间血清中TNF-α平均含量是否不同。\n\n\n\n分组\n1\n2\n3\n4\n5\n6\n\n\n\n\n牙周炎患者 x₁\n9.71\n10.58\n11.00\n7.10\n7.55\n8.65\n\n\n（n₁ = 12）\n8.87\n9.02\n9.88\n8.68\n10.52\n11.02\n\n\n正常人 x₂\n6.52\n6.80\n7.12\n5.50\n4.89\n7.03\n\n\n（n₂ = 12）\n8.00\n4.55\n5.67\n6.77\n6.89\n7.05\n\n\n\n\n\n代码\n%%SAS\n/* programme 5-5*/\ndata prg5_3;\n    input x num @@;\ndatalines;\n9.71 1 10.58 1 11.00 1 7.10 1 7.55 1 8.65 1 \n8.87 1 9.02 1 9.88 1 8.68 1 10.52 1 11.02 1 \n6.52 2 6.80 2 7.12 2 5.50 2 4.89 2 7.03 2 \n8.00 2 4.55 2 5.67 2 6.77 2 6.89 2 7.05 2\n;\nrun;\nproc ttest;\n    var x;\n    class num;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n\nSAS 系统 \n\n\nTTEST 过程\n \n变量:   x\n\n\n\n\n\n\nnum\n方法\n数目\n均值\n标准差\n标准误差\n最小值\n最大值\n\n\n\n\n1\n \n12\n9.3817\n1.2924\n0.3731\n7.1000\n11.0200\n\n\n2\n \n12\n6.3992\n1.0222\n0.2951\n4.5500\n8.0000\n\n\n差 (1-2)\n汇总\n \n2.9825\n1.1652\n0.4757\n \n \n\n\n差 (1-2)\nSatterthwaite\n \n2.9825\n \n0.4757\n \n \n\n\n\n\n\n\n\n\n\n\nnum\n方法\n均值\n95% 置信均值\n标准差\n95% 置信限标准差\n\n\n\n\n1\n \n9.3817\n8.5605\n10.2028\n1.2924\n0.9155\n2.1943\n\n\n2\n \n6.3992\n5.7497\n7.0487\n1.0222\n0.7241\n1.7356\n\n\n差 (1-2)\n汇总\n2.9825\n1.9960\n3.9690\n1.1652\n0.9011\n1.6491\n\n\n差 (1-2)\nSatterthwaite\n2.9825\n1.9930\n3.9720\n \n \n \n\n\n\n\n\n\n\n\n\n\n方法\n方差\n自由度\nt 值\nPr &gt; |t|\n\n\n\n\n汇总\n等于\n22\n6.27\n&lt;.0001\n\n\nSatterthwaite\n不等于\n20.892\n6.27\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n方差齐性\n\n\n方法\n分子自由度\n分母自由度\nF 值\nPr &gt; F\n\n\n\n\n折叠的 F\n11\n11\n1.60\n0.4491\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1 结果说明\n\n第一部分是两个组的一些简单描述性统计量,包括每组的例数、均数、标准差、标准误、最小值和最大值,以及两组均数差值的均数、标准差和标准误。\n第二部分是两组的均数和标准差及其置信区间,还分别用两种方法( Pooled 法和 Satterthwaite 法)计算的两组均数差值及其95%置信区间，以及用Pooled 方法计算两组标准差的差值及其95%置信区间。\n第三部分是 t 检验的结果,该结果包括方差齐性和方差不齐两种情况下的结果。由于本例两组方差齐性(见第四部分说明),故在考察t检验的结果时,应选择方差齐性条件下( method 为 Pooled)的检验结果,本例为检验统计量 t=6.27,所对应的 P&lt;0.000 1 ,可以认为两样本均数的差异有统计学意义,可认为牙周炎患者与正常人血清中TNF-α平均含量不同。\n第四部分方差齐性检验的结果。本例方差齐性检验的检验统计量 F=1.60 ,其对应的 P=0.449 1 ,可以认为两方差是齐性的。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "10-SAS 两样本均数的比较分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-31-two-sample-univariate.html#ttest-过程常用选项和语句",
    "href": "Guide/SAS/25-05-31-two-sample-univariate.html#ttest-过程常用选项和语句",
    "title": "10-SAS 两样本均数的比较分析",
    "section": "5 ttest 过程常用选项和语句",
    "text": "5 ttest 过程常用选项和语句\n运用 ttest 过程进行 t 检验时,可根据需求增加一些选项或语句,使得到的结果更加符合用户的要求。\n\n5.1 ttest过程的基本格式\nproc ttest &lt;选项&gt;;\n    BOOTSTRAP &lt;/ options&gt;;\n    class 变量名;\n    by 变量名1 &lt;变量名2&gt;...;\n    var 变量名1 &lt;变量名2&gt;...&lt;选项&gt;;\n    paired 变量名 &lt;变量名2&gt;...;\n    freq 变量名;\n    weight 变量名;\nrun;\n\n\n5.2 ttest过程的常用选项\n\nalpha=value 选项将设置一个为 0~1 之间的任意值作为概率值( value ),也可用于指定统计量置信区间的置信水平,默认值为0.05。当此选项设置为0~1区间之外的值时,SAS将提示出错。\nCI = type 指定标准差的置信区间的类型，EQUAL（默认）为 equal-tailed 的置信区间，UMPU 为基于一致最优无偏检验的置信区间，NONE 不输出置信区间；\nSIDES = type 指定单侧、双侧检验，默认 type=2 为双侧检验，L 为左侧检验，U 右侧检验；\ncochran 选项用于指定在方差不齐情况下进行近似 t 检验时,使用 Cochran-Cox 近似法计算近似 t 统计量对应的概率值。\n\n\n\n5.3 ttest过程的常用语句\n\nclass 语句用于指定分组变量,该变量的取值将决定样本的分组情况。该语句后面只能跟一个变量名,且该变量必须是分类变量。\nby 语句用于按照某个变量的不同取值,分别进行 ttest 过程分析。\npaired 语句用于指定配对 t 检验中要进行比较的变量对。组成变量对的变量或变量列表之间可用 * 或 : 连接。仅在配对 t 检验时使用,不能和 class 语句同时使用,数据格式为将要检验的变量对分成两列(即设置为两个变量)。对于每一个变量对, ttest 过程用 * 或 : 左侧的变量减去右侧的变量,将所得的差值当做新的变量,执行单组样本均数比较的 t 检验。\nvar 语句用于指定要进行 t 检验的变量,该语句后面可以跟一个或多个变量名,且这些变量必须是连续变量。\nfreq 语句用于指定频数变量,该变量的值将作为每个观测值的权重,即每个观测值的频数。该语句后面只能跟一个变量名,且该变量必须是整数型。\nweight 语句用于指定权重变量,该变量的值将作为每个观测值的权重,即每个观测值的权重。该语句后面只能跟一个变量名,且该变量必须是连续型。\n\n\n\n5.4 ttest 绘图\nPROC TTEST DATA = DATASET PLOTS = (绘图类型);\n可选的绘图类型：\n\nALL 或 NONE: 绘制全部图形或不绘制任何图形；\nBOXPLOT: 箱型图；\nHISTOGRAM: 直方图（包括正态分布、核密度线）；\nINTERVALPLOT: 均值的置信区间的图形；\nQQPLOT: QQ图；\nSUMMARYPLOT: 在一张图中绘制直方图和盒形图；\nAGREEMENTPLOT: AGREEMENT图；\nPROFILESPLOT: PROFILESPLOT图.\n\n注意：TTEST 过程步都默认绘制 QQ 图和 SUMMARYPLOT 图，配对 T 检验还默认绘制 AGREEMENTPLOT 图和 PROFILESPLOT 图。 另外，指定绘制图形类型之后，那些默认图仍然会绘制，除非加上( ONLY ):\nPROC TTEST data = 数据集 PLOTS(ONLY) = (绘图类型);\nRUN;\n\n\n代码\n%%SAS\n/*test programme only plot*/\nproc ttest data = prg5_2 plots(only) = boxplot;\n    var d;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n\nSAS 系统 \n\n\nTTEST 过程\n \n变量:   d\n\n\n\n\n\n\n数目\n均值\n标准差\n标准误差\n最小值\n最大值\n\n\n\n\n10\n0.2724\n0.1087\n0.0344\n0.0820\n0.4610\n\n\n\n\n\n\n\n\n\n\n均值\n95% 置信均值\n标准差\n95% 置信限标准差\n\n\n\n\n0.2724\n0.1947\n0.3501\n0.1087\n0.0748\n0.1984\n\n\n\n\n\n\n\n\n\n\n自由度\nt 值\nPr &gt; |t|\n\n\n\n\n9\n7.93\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%SAS\n/*test programme multiple plots*/\nPROC TTEST data = prg5_2 PLOTS = (BOXPLOT HISTOGRAM QQPLOT);\n   var d;\nRUN;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n\nSAS 系统 \n\n\nTTEST 过程\n \n变量:   d\n\n\n\n\n\n\n数目\n均值\n标准差\n标准误差\n最小值\n最大值\n\n\n\n\n10\n0.2724\n0.1087\n0.0344\n0.0820\n0.4610\n\n\n\n\n\n\n\n\n\n\n均值\n95% 置信均值\n标准差\n95% 置信限标准差\n\n\n\n\n0.2724\n0.1947\n0.3501\n0.1087\n0.0748\n0.1984\n\n\n\n\n\n\n\n\n\n\n自由度\nt 值\nPr &gt; |t|\n\n\n\n\n9\n7.93\n&lt;.0001",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "10-SAS 两样本均数的比较分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-01-multi-sample-ANOVA.html",
    "href": "Guide/SAS/25-06-01-multi-sample-ANOVA.html",
    "title": "11-SAS 多个样本均数比较的方差分析",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "11-SAS 多个样本均数比较的方差分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#方差分析概述",
    "href": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#方差分析概述",
    "title": "11-SAS 多个样本均数比较的方差分析",
    "section": "1 方差分析概述",
    "text": "1 方差分析概述\n方差分析可用于多个样本均数的比较,当然也可以用于两个样本均数的比较;还可以分析因素间的交互作用和进行回归方程的线性假设检验等。\n方差分析能够分析的实验设计类型包括完全随机设计、随机区组设计、拉丁方设计、析因设计、正交设计、系统设计、裂区设计和重复测量设计等。\nSAS系统提供的有关方差分析的过程有 anova、glm、lattice、nested、genmod、mixed 和 varcomp 等,其中以 anova 和 glm 过程最为常用。\n相对于 anova 过程, glm 过程的适用范围更广,适用于平衡和不平衡的方差分析。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "11-SAS 多个样本均数比较的方差分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#anova-过程做完全随机设计资料的方差分析",
    "href": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#anova-过程做完全随机设计资料的方差分析",
    "title": "11-SAS 多个样本均数比较的方差分析",
    "section": "2 anova 过程做完全随机设计资料的方差分析",
    "text": "2 anova 过程做完全随机设计资料的方差分析\nANOVA过程步主要处理均衡数据（分类变量的每个水平的观察数是相等），该过程考虑到均衡设计的特殊构造，处理起来速度更快更省内存，也可以处理拉丁方设计、若干不完全的均衡区组设计数据等。\n若试验设计不均衡，也不是前面几种实验设计数据，则应该使用 GLM 过程。\n\n2.1 基本语法\nPROC ANOVA data = 数据集 &lt;可选项&gt; ;\n    CLASS 分类变量列表;\n    MODEL 因变量 = 效应变量列表 &lt;/可选项&gt;;\n    &lt;MEANS 效应变量列表 &lt;/可选项&gt; ;&gt;\n    &lt;TEST &lt;H=效应变量列表&gt; E=效应变量列表;&gt;\nrun;\n\n\n2.2 说明\n\nCLASS 语句是必不可少的，必须放在 MODEL 语句之前，用来指定分类、区组变量（单因素方差分析只有一个变量）；\nMODEL 语句也是必不可少的，该语句用来规定因变量和自变量效应（单因素方差分析的自变量就是分类变量）。若没有规定自变量的效应，则只拟合截距，假设检验为因变量的均值是否为0. Model语句的主要形式有4种：\n\n主效应模型: model y=a b c;\n含有交叉因素的模型: model y=a b c a*b a*c b*c a*b*c;\n嵌套模型: model y=a b c(a b);\n包含嵌套、交叉和主效应的模型: model y=a b(a) c(a) b*c(a);\n\nMEANS 语句必须出现在 MODEL 语句之后，用来计算在效应变量所对应的因变量均值，但这些均值没有针对模型中的效应进行修正。若要计算修正的均值需要用GLM过程步的LSMEANS语句；\nMEANS 语句的可选项主要有两个内容，一是选择多重比较的检验方法，二是设定这些检验的参数（只能用于主效应）；\n\nbon: 对所有主效应均值之差进行 Bonferroni 的 t 检验；\nduncan: 对所有主效应均值进行 Duncan 的多重极差检验；\nsmm|gt2: 当样本量不等时，基于学生化最大模和 Sidak 不相关 t 不等式，等到 Hochberg 的 GT2 方法，对主效应均值进行两两对比检验；\nsnk: 对所有主效应均值进行 Student-Newman-Keuls 的多重极差检验；\nt|lsd: 对所有主效应均值进行两两 t 检验，它相当于在单元观察数相等时 Fisher 的最小显著差检验；\ntukey: 对所有主效应均值进行 Tukey 的学生化极差检验；\nwaller: 对所有主效应均值进行 Waller-Duncan 的 k 比率检验；\nalpha=p : 设置显著水平；\nclm: 对变量的每个水平的均值按置信区间形式输出；\ne=效应变量: 指定在多重对比检验中所使用的误差均方。默认使用残差均方。指定的效应变量必须是在model语句中出现过的；\nkratio=值: 给出 Waller-Duncan 检验的类型1/类型2的误差限制比例。Kratio 的合理值为50、100（默认）、500，大约相当于两水平时 alpha 值为0.1、0.05、0.01.\nhovtest: 要求输出组间方差齐性的 Levene 检验；\n\nTEST 语句指定效应变量（H=）和误差变量（E=）做 F 检验，误差变量必须要指定且只能指定1个效应变量。默认是用残差均方作为误差项对所有平方和（SS）计算F值。\n默认情况下，结果中包括盒形图、均值图和最小二乘均值差图。也可以指定包括可在面板中显示或作为单个图显示的任何诊断图。也可指定包含于这些图中的最大点数。\n\n\n\n2.3 示例：按汽车类型划分的 MPG_Highway 均值差值的检验\n代码说明：\n\nCLASS 语句：指定分类变量 Type，该变量有 6 个水平（SUV、轿车、跑车等）\nMODEL 语句：\n\n分析变量为 MPG_Highway（公路油耗）\n效应变量为 Type（汽车类型）\n\nMEANS 语句：计算不同 Type 水平下 MPG_Highway 的均值\n\n/ SNK 选项指定使用 SNK 方法进行多重比较\nALPHA=0.05 设置显著性水平\n\n\n\n\n代码\n%%SAS\nPROC ANOVA data = sashelp.cars;\n    CLASS Type;\n    MODEL MPG_Highway = Type;\n    MEANS Type / SNK ALPHA=0.05;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n\n\nSAS 系统 \n\n\nANOVA 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\nType\n6\nHybrid SUV Sedan Sports Truck Wagon\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n428\n\n\n使用的观测数\n428\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nANOVA 过程\n \n因变量: MPG_Highway   MPG (Highway)\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n5\n6743.47900\n1348.69580\n77.64\n&lt;.0001\n\n\n误差\n422\n7331.03268\n17.37212\n \n \n\n\n校正合计\n427\n14074.51168\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nMPG_Highway 均值\n\n\n\n\n0.479127\n15.52701\n4.167987\n26.84346\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nAnova SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nType\n5\n6743.478998\n1348.695800\n77.64\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nANOVA 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nANOVA 过程\n \nStudent-Newman-Keuls Test for MPG_Highway\n\n\n注意:This test controls the Type I experimentwise error rate under the complete null hypothesis but not under partial null hypotheses.\n\n\n\n\n\n\nAlpha\n0.05\n\n\nError Degrees of Freedom\n422\n\n\nError Mean Square\n17.37212\n\n\nHarmonic Mean of Cell Sizes\n13.35634\n\n\n\n\n\n\n注意:Cell sizes are not equal.\n\n\n\n\n\n\nNumber of Means\n2\n3\n4\n5\n6\n\n\n\n\nCritical Range\n3.1702475\n3.7934753\n4.1600539\n4.4186173\n4.6174103",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "11-SAS 多个样本均数比较的方差分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#glm-过程做完全随机设计资料的方差分析",
    "href": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#glm-过程做完全随机设计资料的方差分析",
    "title": "11-SAS 多个样本均数比较的方差分析",
    "section": "3 glm 过程做完全随机设计资料的方差分析",
    "text": "3 glm 过程做完全随机设计资料的方差分析\nGLM 过程步是一个通用的线性模型过程步，适用于平衡和不平衡的方差分析。它可以处理多种实验设计，包括完全随机设计、随机区组设计、拉丁方设计、析因设计等。\n\n3.1 基本语法\nPROC GLM data = 数据集 &lt;可选项&gt; ;\n    CLASS 分类变量列表;\n    MODEL 因变量 = 效应变量列表 &lt;/可选项&gt;;\n    &lt;MEANS 效应变量列表 &lt;/可选项&gt; ;&gt;\n    &lt;LSMEANS 效应变量列表 &lt;/可选项&gt; ;&gt;\n    &lt;TEST &lt;H=效应变量列表&gt; E=效应变量列表;&gt;\n    &lt;OUTPUT OUT=输出数据集 &lt;可选项&gt; ;&gt;\nrun;\n\nclass 语句是定义处理因素中不同水平的分组情况,称为分组变量,本例为变量 Type。\nmodel 语句用来指定分析变量(效应)和分组变量(自变量),两者用 = 相连,效应在 = 左侧,本例为变量 MPG_Highway,自变量在 = 右侧,本例为变量 Type。\nmeans 语句表示需要计算处理因素不同水平组中分析变量的均数和标准差,后面可加选项,表示对均数进行多重两两比较,并确定两两比较的方法。\nSAS 系统提供了多种两两比较的方法可供选择,如 SNK(Student-Newman-Keuls) 检验、最小显著差异 (least significant difference,LSD) t 检验、Scheffe 检验、Dunnett 法、Tukey 检验、Duncan 检验等,本例选择 LSD t 检验。\n\n\n\n代码\n%%SAS\nPROC GLM data = sashelp.cars;\n    CLASS Type;\n    MODEL MPG_Highway = Type;\n    MEANS Type;\n    MEANS Type / LSD ALPHA=0.05;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\nType\n6\nHybrid SUV Sedan Sports Truck Wagon\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n428\n\n\n使用的观测数\n428\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \n因变量: MPG_Highway   MPG (Highway)\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n5\n6743.47900\n1348.69580\n77.64\n&lt;.0001\n\n\n误差\n422\n7331.03268\n17.37212\n \n \n\n\n校正合计\n427\n14074.51168\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nMPG_Highway 均值\n\n\n\n\n0.479127\n15.52701\n4.167987\n26.84346\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nI 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nType\n5\n6743.478998\n1348.695800\n77.64\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nIII 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nType\n5\n6743.478998\n1348.695800\n77.64\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Type”的\n水平\n数目\nMPG_Highway\n\n\n均值\n标准差\n\n\n\n\nHybrid\n3\n56.0000000\n8.66025404\n\n\nSUV\n60\n20.5000000\n3.33700363\n\n\nSedan\n262\n28.6297710\n4.46745909\n\n\nSports\n49\n25.4897959\n2.90202838\n\n\nTruck\n24\n21.0000000\n3.87859230\n\n\nWagon\n30\n27.9000000\n4.41275580\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \nt Tests (LSD) for MPG_Highway\n\n\n注意:This test controls the Type I comparisonwise error rate, not the experimentwise error rate.\n\n\n\n\n\n\nAlpha\n0.05\n\n\nError Degrees of Freedom\n422\n\n\nError Mean Square\n17.37212\n\n\nCritical Value of t\n1.96560\n\n\n\n\n\n\n\n\n\n\nComparisons significant at the 0.05 level are indicated by ***.\n\n\nType\n比较\n均值\n间\n差值\n95% 置信限\n \n\n\n\n\nHybrid - Sedan\n27.3702\n22.6132\n32.1272\n***\n\n\nHybrid - Wagon\n28.1000\n23.1391\n33.0609\n***\n\n\nHybrid - Sports\n30.5102\n25.6376\n35.3828\n***\n\n\nHybrid - Truck\n35.0000\n29.9831\n40.0169\n***\n\n\nHybrid - SUV\n35.5000\n30.6532\n40.3468\n***\n\n\nSedan - Hybrid\n-27.3702\n-32.1272\n-22.6132\n***\n\n\nSedan - Wagon\n0.7298\n-0.8493\n2.3088\n \n\n\nSedan - Sports\n3.1400\n1.8648\n4.4151\n***\n\n\nSedan - Truck\n7.6298\n5.8825\n9.3770\n***\n\n\nSedan - SUV\n8.1298\n6.9572\n9.3023\n***\n\n\nWagon - Hybrid\n-28.1000\n-33.0609\n-23.1391\n***\n\n\nWagon - Sedan\n-0.7298\n-2.3088\n0.8493\n \n\n\nWagon - Sports\n2.4102\n0.5110\n4.3094\n***\n\n\nWagon - Truck\n6.9000\n4.6564\n9.1436\n***\n\n\nWagon - SUV\n7.4000\n5.5681\n9.2319\n***\n\n\nSports - Hybrid\n-30.5102\n-35.3828\n-25.6376\n***\n\n\nSports - Sedan\n-3.1400\n-4.4151\n-1.8648\n***\n\n\nSports - Wagon\n-2.4102\n-4.3094\n-0.5110\n***\n\n\nSports - Truck\n4.4898\n2.4486\n6.5310\n***\n\n\nSports - SUV\n4.9898\n3.4123\n6.5673\n***\n\n\nTruck - Hybrid\n-35.0000\n-40.0169\n-29.9831\n***\n\n\nTruck - Sedan\n-7.6298\n-9.3770\n-5.8825\n***\n\n\nTruck - Wagon\n-6.9000\n-9.1436\n-4.6564\n***\n\n\nTruck - Sports\n-4.4898\n-6.5310\n-2.4486\n***\n\n\nTruck - SUV\n0.5000\n-1.4787\n2.4787\n \n\n\nSUV - Hybrid\n-35.5000\n-40.3468\n-30.6532\n***\n\n\nSUV - Sedan\n-8.1298\n-9.3023\n-6.9572\n***\n\n\nSUV - Wagon\n-7.4000\n-9.2319\n-5.5681\n***\n\n\nSUV - Sports\n-4.9898\n-6.5673\n-3.4123\n***\n\n\nSUV - Truck\n-0.5000\n-2.4787\n1.4787\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 两两比较采用 Dunnett 法\n如果两两比较采用的是 Dunnett 检验,将程序中 MEANS Type / LSD ALPHA=0.05; 部分中的的 LSD 改为 Dunnett 即可.\nSAS 默认分组变量中的变量值最小的组为对照组。如果在 Dunnett 后面加条件,可以将任何一组定义为对照组。这里选择 Sedan 为对照组,语句为: means c/dunnett ('Sedan');,上述 dunnett 语句后面的括号内为作为对照组的变量值,该变量值需用单引号表示。\n\n\n代码\n%%SAS\nPROC GLM data = sashelp.cars;\n    CLASS Type;\n    MODEL MPG_Highway = Type;\n    MEANS Type;\n    MEANS Type / dunnett ('Sedan') ALPHA=0.05;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\nType\n6\nHybrid SUV Sedan Sports Truck Wagon\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n428\n\n\n使用的观测数\n428\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \n因变量: MPG_Highway   MPG (Highway)\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n5\n6743.47900\n1348.69580\n77.64\n&lt;.0001\n\n\n误差\n422\n7331.03268\n17.37212\n \n \n\n\n校正合计\n427\n14074.51168\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nMPG_Highway 均值\n\n\n\n\n0.479127\n15.52701\n4.167987\n26.84346\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nI 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nType\n5\n6743.478998\n1348.695800\n77.64\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nIII 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nType\n5\n6743.478998\n1348.695800\n77.64\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Type”的\n水平\n数目\nMPG_Highway\n\n\n均值\n标准差\n\n\n\n\nHybrid\n3\n56.0000000\n8.66025404\n\n\nSUV\n60\n20.5000000\n3.33700363\n\n\nSedan\n262\n28.6297710\n4.46745909\n\n\nSports\n49\n25.4897959\n2.90202838\n\n\nTruck\n24\n21.0000000\n3.87859230\n\n\nWagon\n30\n27.9000000\n4.41275580\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \nDunnett's t Tests for MPG_Highway\n\n\n注意:This test controls the Type I experimentwise error for comparisons of all treatments against a control.\n\n\n\n\n\n\nAlpha\n0.05\n\n\nError Degrees of Freedom\n422\n\n\nError Mean Square\n17.37212\n\n\nCritical Value of Dunnett's t\n2.57769\n\n\n\n\n\n\n\n\n\n\nComparisons significant at the 0.05 level are indicated by ***.\n\n\nType\n比较\n均值\n间\n差值\nSimultaneous 95% 置信限\n \n\n\n\n\nHybrid - Sedan\n27.3702\n21.1319\n33.6086\n***\n\n\nWagon - Sedan\n-0.7298\n-2.8006\n1.3410\n \n\n\nSports - Sedan\n-3.1400\n-4.8122\n-1.4678\n***\n\n\nTruck - Sedan\n-7.6298\n-9.9211\n-5.3385\n***\n\n\nSUV - Sedan\n-8.1298\n-9.6674\n-6.5921\n***",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "11-SAS 多个样本均数比较的方差分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#随机区组设计资料的方差分析",
    "href": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#随机区组设计资料的方差分析",
    "title": "11-SAS 多个样本均数比较的方差分析",
    "section": "4 随机区组设计资料的方差分析",
    "text": "4 随机区组设计资料的方差分析\n随机区组设计又称为配伍组设计,是先将除处理因素外其他条件相同或相近的受试对象归人一个区组,再将一个区组内的受试对象随机分配 到不同的实验组内,从而保证同一个区组内的受试对象接受的处理是不同的。由于区组内各受试 对象的其他条件相同或相近,因此彼此间实验效应的差异主要是由处理因素引起的,而且处理因素和区组因素没有交互作用。\n\n4.1 基本语法\n其语法与完全随机设计的方差分析类似,只是在 CLASS 语句中增加了区组变量,并在 MODEL 语句中指定区组变量。\n\n\n4.2 示例\n在一项“纤维蛋白胶干粉在肝损伤模型上的止血作用”的研究中，某研究者对9个窝别的27只大鼠建立肝损伤模型，每个窝别中的3只大鼠随机接受不同的处理，即空白组、凝血酶组和纤维蛋白胶组。观测大鼠在30分钟内的失血量，数据集如下：\n\n\n\n窝别\n空白组\n凝血酶组\n纤维蛋白胶组\n\n\n\n\n1\n3.85\n2.14\n0.87\n\n\n2\n3.88\n1.98\n0.36\n\n\n3\n4.56\n1.21\n0.42\n\n\n4\n3.33\n1.45\n0.51\n\n\n5\n2.22\n0.76\n0.83\n\n\n6\n4.44\n1.78\n0.82\n\n\n7\n4.29\n1.55\n0.44\n\n\n8\n6.23\n4.22\n1.23\n\n\n9\n5.55\n2.22\n0.23\n\n\n\n\n\n4.3 程序说明\n\na：代表区组\nb：代表处理因素\ndo：利用一个循环语句建立数据集\n这里不同的止血效果相互比较用 SNK 方法\n\n\n\n代码\n%%SAS\ndata prg11_1;\n    do a = 1 to 9;\n    do b = 1 to 3;\n        input x @@;\n        output;\n    end;\nend;\ndatalines;\n3.85 2.14 0.87 3.88 1.98 0.36\n4.56 1.21 0.42 3.33 1.45 0.51\n2.22 0.76 0.83 4.44 1.78 0.82\n4.29 1.55 0.44 6.23 4.22 1.23\n5.55 2.22 0.23\n;\nrun;\nproc glm;\n    class a b;\n    model x= a b;\n    means b/snk alpha=0.05;\nrun;\nquit;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\na\n9\n1 2 3 4 5 6 7 8 9\n\n\nb\n3\n1 2 3\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n27\n\n\n使用的观测数\n27\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \n因变量: x\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n10\n73.34128148\n7.33412815\n16.80\n&lt;.0001\n\n\n误差\n16\n6.98348148\n0.43646759\n \n \n\n\n校正合计\n26\n80.32476296\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nx 均值\n\n\n\n\n0.913059\n29.06589\n0.660657\n2.272963\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nI 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\na\n8\n12.50382963\n1.56297870\n3.58\n0.0143\n\n\nb\n2\n60.83745185\n30.41872593\n69.69\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nIII 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\na\n8\n12.50382963\n1.56297870\n3.58\n0.0143\n\n\nb\n2\n60.83745185\n30.41872593\n69.69\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \nStudent-Newman-Keuls Test for x\n\n\n注意:This test controls the Type I experimentwise error rate under the complete null hypothesis but not under partial null hypotheses.\n\n\n\n\n\n\nAlpha\n0.05\n\n\nError Degrees of Freedom\n16\n\n\nError Mean Square\n0.436468\n\n\n\n\n\n\n\n\n\n\nNumber of Means\n2\n3\n\n\n\n\nCritical Range\n0.6601857\n0.8036097\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4 结果说明\n\n结果的形式与完全随机设计资料的结果是一样的。由于随机区组设计有两个分组变量,所以在针对分组变量的方差分析表中,有两个分组变量的情况需分析。本例模型的方差分析表中的 \\(F= 16.80,P&lt;0.0001\\),说明模型有统计学意义。\n区组和处理变量中,选择 Type Ⅲ SS 的结果,区组变量 a 的 \\(F=3.58 ,P=0.0143\\),说明各区组均数之间的差异有统计学意义。\n处理组变量 b 的 \\(F=69.69,P&lt;0.0001\\),说明各处理组均数之间的差异也有统计学意义。\n多个均数两两比较 SNK 检验的结果与 LSD t 检验有些类似,只是在确定界值时的表达方式有所不同。SNK 检验比较中列出了按均数大小排序时的组数及其检验统计量的界值,分别为: Number of Means 表示组数, Critical Range 表示不同组数时的统计量的界值。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "11-SAS 多个样本均数比较的方差分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#拉丁方设计资料的方差分析",
    "href": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#拉丁方设计资料的方差分析",
    "title": "11-SAS 多个样本均数比较的方差分析",
    "section": "5 拉丁方设计资料的方差分析",
    "text": "5 拉丁方设计资料的方差分析\n拉丁方设计是有三个因素的设计类型,是在随机区组设计的基础上,又增加了一个已知的对实验结果有影响的因素,增加了均衡性,减少了误差,提高了实验效率。\n主要用于用于控制两个额外变量（blocking factors）的影响，同时研究一个处理因素的效应。\n不过,在拉丁方设计中,三个因素的水平数必须相同,而且这三个因素不存在交互作用。假设水平数为 r ,整个设计可以组成一个由 r 个拉丁字母排成 r 行 r 列的方阵,使得每行每列的每个字母都只出现一次,这样的方阵叫 r 阶拉丁方或 r*r 拉丁方。\n\n5.1 拉丁方设计资料示例\n\n\n\n\n\n\n\n\n\n\n\n\n编号（行区组）\n注射部位编号（列区组）1\n注射部位编号（列区组）2\n注射部位编号（列区组）3\n注射部位编号（列区组）4\n注射部位编号（列区组）5\n注射部位编号（列区组）6\n\n\n\n\n1\nC(87)\nB(75)\nE(81)\nD(75)\nA(84)\nF(66)\n\n\n2\nB(73)\nA(81)\nD(87)\nC(85)\nF(64)\nE(79)\n\n\n3\nF(73)\nE(73)\nB(74)\nA(78)\nD(73)\nC(77)\n\n\n4\nA(77)\nF(68)\nC(69)\nB(74)\nE(76)\nD(73)\n\n\n5\nD(64)\nC(64)\nF(72)\nE(76)\nB(70)\nA(81)\n\n\n6\nE(75)\nD(77)\nA(82)\nF(61)\nC(82)\nB(61)\n\n\n\n字母 A、B、C、D、E、F 分别代表不同的药物,括号内的数字代表每个注射部位注射药物后得到的测量值。\n\n\n代码\n%%SAS\ndata Latin_Square;\n    do r= 1 to 6;\n        do c =1 to 6;\n            input z $ x @@;\n            output;\n        end;\n    end;\ndatalines;\nC 87 B 75 E 81 D 75 A 84 F 66\nB 73 A 81 D 87 C 85 F 64 E 79\nF 73 E 73 B 74 A 78 D 73 C 77\nA 77 F 68 C 69 B 74 E 76 D 73\nD 68 C 70 F 72 E 76 B 70 A 81\nE 75 D 77 A 82 F 61 C 82 B 61\n;\nrun;\nproc glm;\n    class r c z;\n    model x = r c z;\nrun;\nquit;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\nr\n6\n1 2 3 4 5 6\n\n\nc\n6\n1 2 3 4 5 6\n\n\nz\n6\nA B C D E F\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n36\n\n\n使用的观测数\n36\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \n因变量: x\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n15\n975.083333\n65.005556\n2.37\n0.0362\n\n\n误差\n20\n547.666667\n27.383333\n \n \n\n\n校正合计\n35\n1522.750000\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nx 均值\n\n\n\n\n0.640344\n6.984973\n5.232909\n74.91667\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nI 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nr\n5\n194.9166667\n38.9833333\n1.42\n0.2587\n\n\nc\n5\n73.2500000\n14.6500000\n0.53\n0.7474\n\n\nz\n5\n706.9166667\n141.3833333\n5.16\n0.0033\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nIII 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nr\n5\n194.9166667\n38.9833333\n1.42\n0.2587\n\n\nc\n5\n73.2500000\n14.6500000\n0.53\n0.7474\n\n\nz\n5\n706.9166667\n141.3833333\n5.16\n0.0033\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2 程序说明\n\n使用两个 do-end 循环语句；\n该数据集有四个变量, r、c 和 z 都是分组变量\nr 为行区组变量,表示编号;\nc 为列区组变量表示注射部位;\nz 为处理变量,表示注射药物;\nx 为分析变量,表示测量值大小；\nglm 过程中将三个分组因素变量放在 = 右侧,彼此之间用空格分隔.\n\n\n\n5.3 结果说明\n\n各个分组因素的方差分析表部分都有三个分组变量的情况,分别说明三个因素的各个水平之间均数的差异是否有统计学意义。\n这里模型的方差分析表中的 F=2.37.P=0.0362，说明模型有统计学意义。\n选择查看 Type Ⅲ SS 下的结果：\n\n\n本例 r 所对应的 F=1.42,P=0.2587,说明 6 个样本的总体均数之间差异无统计学意义;\nc 所对应的 F=0.53,P=0.747 4,说明6个注射部位皮肤疱疹大小的总体均数差异无统计学意义;\nz 所对应的 F=5.16,P=0.0033,说明6 种药物产生皮肤疱疹大小的总体均数之间差异有统计学意义。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "11-SAS 多个样本均数比较的方差分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#两阶段交叉设计资料的方差分析",
    "href": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#两阶段交叉设计资料的方差分析",
    "title": "11-SAS 多个样本均数比较的方差分析",
    "section": "6 两阶段交叉设计资料的方差分析",
    "text": "6 两阶段交叉设计资料的方差分析\n交叉设计是指受试对象在不同的实验阶段分别接受不同的处理因素,则实验效应受到三个因素的影响,一个是处理因素,一个是个体区组因素,还有一个是实验阶段因素。\n虽然交叉实验的处理是单因素,但影响实验结果的因素还有非人为控制的受试者的个体差异和实验阶段这两个因素。\n因此,该设计不仅平衡了处理顺序的影响而且能把处理方法间的差别、时间先后之间的差别和受试者之间的差别分别进行分析。最简单的交叉设计是处理因素的水平数为 2,而处理顺序因素和实验阶段因素的水平数都为 2。\n\n6.1 两阶段交叉设计资料示例\n某研究者为研究A、B两种方案治疗12名高血压患者的疗效，随机让1、2、5、8、9、10号患者第一阶段先用A 法治疗，第二阶段后用B法治疗；3、4、6、7、11、12号患者第一阶段先用B法治疗，第二阶段后用A法治疗,记录治疗后收缩压的下降值。\n试分析A、B 两种方案治疗高血压的疗效有无差异。\nA、B两种方案收缩压下降值（mmHg）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n阶段\n患者编号1\n患者编号2\n患者编号3\n患者编号4\n患者编号5\n患者编号6\n患者编号7\n患者编号8\n患者编号9\n患者编号10\n患者编号11\n患者编号12\n\n\n\n\nI\nA\nA\nB\nB\nA\nB\nB\nA\nA\nA\nB\nB\n\n\nI\n17.6\n28.0\n10.3\n14.0\n24.4\n17.0\n15.1\n26.0\n30.2\n31.8\n14.0\n17.5\n\n\nII\nB\nB\nA\nA\nB\nA\nA\nB\nB\nB\nA\nA\n\n\nII\n13.0\n12.6\n20.0\n27.0\n13.4\n23.0\n26.0\n13.0\n18.9\n28.0\n19.4\n21.2\n\n\n\n\n\n代码\n%%SAS\ndata cross_over;\n    do time = 1 to 2;\n        do r = 1 to 12;\n            input treat $ x @@;\n            output;\n        end;\n    end;\ndatalines;\nA 17.6 A 28.0 B 10.3 B 14.0 A 24.4 B 17.0 B 15.1 A 26.0 A 30.2 A 31.8 B 14.0 B 17.5\nB 13.0 B 12.6 A 20.0 A 27.0 B 13.4 A 23.0 A 26.0 B 13.0 B 18.9 B 28.0 A 19.4 A 21.2\n;\nrun;\nproc glm;\n    class r time treat;\n    model x = r time treat;\nrun;\nquit;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\nr\n12\n1 2 3 4 5 6 7 8 9 10 11 12\n\n\ntime\n2\n1 2\n\n\ntreat\n2\nA B\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n24\n\n\n使用的观测数\n24\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \n因变量: x\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n13\n844.1066667\n64.9312821\n7.47\n0.0016\n\n\n误差\n10\n86.8916667\n8.6891667\n \n \n\n\n校正合计\n23\n930.9983333\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nx 均值\n\n\n\n\n0.906668\n14.69583\n2.947739\n20.05833\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nI 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nr\n11\n355.3983333\n32.3089394\n3.72\n0.0238\n\n\ntime\n1\n4.5066667\n4.5066667\n0.52\n0.4879\n\n\ntreat\n1\n484.2016667\n484.2016667\n55.72\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nIII 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nr\n11\n355.3983333\n32.3089394\n3.72\n0.0238\n\n\ntime\n1\n4.5066667\n4.5066667\n0.52\n0.4879\n\n\ntreat\n1\n484.2016667\n484.2016667\n55.72\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.2 程序说明\n\n数据集 cross_over 中的变量 r 代表受试者编号;\n变量 time 为实验阶段,1 表示第 I 阶段,2表示第 Ⅱ 阶段;\ntreat 为治疗方法编号,A 表示 A 疗法,B 表示 B 疗法。\n\n\n\n6.3 结果说明\n\n交叉设计的方差分析结果与拉丁方的结果完全一样。\n本例模型的方差分析结果为: F=7.47,P&lt;0.0016,说明模型有统计学意义。\n选择 Type Ⅲ SS 结果显示,r 所对应的 F=3.72,P&lt;0.0238,说明不同受试者的治疗后收缩压下降值的均数之间的差异有统计学意义;\ntime 所对应的 F=0.52,P=0.4879,说明不同试验阶段之间的差异无统计学意义;\nteat 所对应的 F=55.72,P&lt;0.0001,说明两种治疗方式所造成的收缩压下降值之间的差异有统计学意义。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "11-SAS 多个样本均数比较的方差分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#析因设计资料的方差分析",
    "href": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#析因设计资料的方差分析",
    "title": "11-SAS 多个样本均数比较的方差分析",
    "section": "7 析因设计资料的方差分析",
    "text": "7 析因设计资料的方差分析\n前四种设计方法都是只涉及每一个处理因素对实验效应的影响,而在析因设计中不仅可以考虑每一个处理因素对实验效应的主效应,还可以对两个或更多处理因素的交互作用进行分析。\n在析因设计实验中将各因素的所有水平相互交叉进行组合,每种组合看作一种处理,然后在每种处理中进行实验;对研究中的每个因素只需要较少的实验对象，就可以提供较多的信息以提高试验的效率。\n\n7.1 两因素两水平的析因分析\n在两因素两水平的析因设计中,每个因素有两个水平,每个处理组有两个因素的所有组合,即 2*2=4 个处理组。\n\n\n7.2 示例\n\n\n\n\n\n\n\n\n\n\nA（饲料盐含量）\n高盐饲料（\\(a_1\\)）\n高盐饲料（\\(a_1\\)）\n正常盐饲料（\\(a_2\\)）\n正常盐饲料（\\(a_2\\)）\n\n\n\n\nB（CSSN 变性）\n是（\\(b_1\\)）\n否（\\(b_2\\)）\n是（\\(b_1\\)）\n否（\\(b_2\\)）\n\n\n1\n0.47\n0.16\n1.09\n0.31\n\n\n2\n0.51\n0.12\n1.15\n0.54\n\n\n3\n0.23\n0.17\n0.51\n0.50\n\n\n4\n0.36\n0.11\n0.51\n0.38\n\n\n5\n0.32\n0.15\n0.20\n0.66\n\n\n6\n0.42\n0.28\n0.68\n0.93\n\n\n7\n0.18\n0.09\n1.38\n0.65\n\n\n8\n0.28\n0.27\n0.57\n0.41\n\n\n9\n0.26\n0.16\n1.38\n0.28\n\n\n\n\n\n代码\n%%SAS\ndata factorial;\n    input x a b @@;\ndatalines;\n0.47 1 1 0.51 1 1 0.23 1 1\n0.36 1 1 0.32 1 1 0.42 1 1\n0.18 1 1 0.28 1 1 0.26 1 1\n0.16 1 2 0.12 1 2 0.17 1 2\n0.11 1 2 0.15 1 2 0.28 1 2\n0.09 1 2 0.27 1 2 0.16 1 2\n1.09 2 1 1.15 2 1 0.51 2 1\n0.51 2 1 0.2 2 1 0.68 2 1\n1.38 2 1 0.57 2 1 1.38 2 1\n0.31 2 2 0.54 2 2 0.5 2 2\n0.38 2 2 0.66 2 2 0.93 2 2\n0.65 2 2 0.41 2 2 0.28 2 2\n;\nproc anova;\n    class a b;\n    model x = a b a*b;\nquit;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nANOVA 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\na\n2\n1 2\n\n\nb\n2\n1 2\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n36\n\n\n使用的观测数\n36\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nANOVA 过程\n \n因变量: x\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n3\n2.16725278\n0.72241759\n11.91\n&lt;.0001\n\n\n误差\n32\n1.94131111\n0.06066597\n \n \n\n\n校正合计\n35\n4.10856389\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nx 均值\n\n\n\n\n0.527496\n53.19116\n0.246305\n0.463056\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nAnova SS\n均方\nF 值\nPr &gt; F\n\n\n\n\na\n1\n1.60022500\n1.60022500\n26.38\n&lt;.0001\n\n\nb\n1\n0.52080278\n0.52080278\n8.58\n0.0062\n\n\na*b\n1\n0.04622500\n0.04622500\n0.76\n0.3892\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.3 程序说明\n\n数据集 factorial 中变量 a 表示饲料盐含量,其变量值有两个水平,1 代表高盐饲料,2 代表正常盐饲料;\n变量 b 表示CSSN 变性,其变量值 1 代表变性,2 代表未变性;\n变量 x 表示不同组合下每次试验的 ALD 浓度。\n在 glm 过程中定义模型类型时,用 * 表示两因素的交互作用,析因设计考虑 a 因素和 b 因素的交互作用。\n\n\n\n7.4 结果说明\n\n本例模型的方差分析结果为 F=11.91,P&lt;0.0001,说明模型有统计学意义。\n根据 Type Ⅲ SS 结果显示,A 因素所对应的 F=26.38,P&lt;0.0001,说明饲料盐含量对大鼠血浆中ALD浓度有影响；\nB 因素对应的 F=8.58,P=0.0062,说明CSSN 变性与否对大鼠血浆中 ALD 浓度有影响;\n两种因素的交互项 A*B 的 F=0.76,P=0.3892,尚不能认为两种因素有交互作用。\n\n\n\n7.5 两因素三水平的析因分析\n在两因素三水平的析因设计中,每个因素有三个水平,每个处理组有两个因素的所有组合,即 3*3=9 个处理组。\n\n\n7.6 示例\n\n\n\nA药物剂量\nB药物剂量5μg\nB药物剂量15μg\nB药物剂量30μg\n\n\n\n\n1.0 mg\n105\n115\n75\n\n\n1.0\n80\n105\n95\n\n\n2.0\n65\n80\n85\n\n\n2.5 mg\n75\n125\n135\n\n\n2.5\n115\n130\n120\n\n\n2.5\n80\n90\n150\n\n\n5.0 mg\n85\n65\n180\n\n\n2.5\n120\n120\n190\n\n\n2.5\n125\n100\n160\n\n\n\n\n\n代码\n%%SAS\ndata prg233;\n    input x a b @@;\ncards;\n65 1 1 80 1 1 65 1 1 75 2 1\n115 2 1 80 2 1 85 3 1 120 3 1\n125 3 1 115 1 2 105 1 2 80 1 2\n125 2 2 130 2 2 90 2 2 65 3 2\n120 3 2 100 3 2 75 1 3 95 1 3\n85 1 3 135 2 3 120 2 3 150 2 3\n180 3 3 190 3 3 160 3 3\n;\nproc anova;\n    class a b;\n    model x=a b a*b;\nquit;\n\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 2836\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nANOVA 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\na\n3\n1 2 3\n\n\nb\n3\n1 2 3\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n27\n\n\n使用的观测数\n27\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nANOVA 过程\n \n因变量: x\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n8\n24074.07407\n3009.25926\n8.58\n&lt;.0001\n\n\n误差\n18\n6316.66667\n350.92593\n \n \n\n\n校正合计\n26\n30390.74074\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nx 均值\n\n\n\n\n0.792152\n17.26251\n18.73302\n108.5185\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nAnova SS\n均方\nF 值\nPr &gt; F\n\n\n\n\na\n2\n8335.185185\n4167.592593\n11.88\n0.0005\n\n\nb\n2\n8385.185185\n4192.592593\n11.95\n0.0005\n\n\na*b\n4\n7353.703704\n1838.425926\n5.24\n0.0056\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.7 三因素不同水平的析因设计\n\n\n代码\n%%SAS\ndata prg522;\ndo a = 1 to 5;\n    do b = 1 to 2;\n        do c = 1 to 2;\n            do i = 1 to 5;\n                input x @@;\n                output;\n            end;\n        end;\n    end;\nend;\ndatalines;\n0.25 -0.25 1.25 -0.75 0.40 4.75 3.45 4.00 4.85 4.55 \n0.50 2.10 2.75 1.00 2.35 3.75 4.00 4.00 4.25 4.60 \n0.30 0.10 0.50 -0.35 0.05 4.60 4.80 4.00 5.20 4.30 \n1.50 1.50 1.25 1.37 2.55 4.00 4.05 4.15 4.10 4.25 \n0.75 -0.50 0.60 0.40 -0.20 4.55 3.50 4.25 4.10 4.40 \n0.75 2.65 3.00 0.05 1.17 4.10 5.00 4.20 4.15 4.17 \n0.20 -1.00 0.85 -0.50 0.90 4.25 3.10 4.00 5.00 4.20 \n-0.75 0.90 0.95 0.62 1.05 3.27 4.25 4.00 4.25 4.25 \n-0.10 0.00 2.50 0.10 -0.10 4.72 4.30 4.10 4.80 3.60 \n1.75 2.40 1.75 3.05 2.75 4.80 4.02 4.15 4.75 4.80\n;\nrun;\nproc glm;\n    class a b c;\n    model x = a b c a*b a*c b*c a*b*c;\nrun;\nquit;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\na\n5\n1 2 3 4 5\n\n\nb\n2\n1 2\n\n\nc\n2\n1 2\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n100\n\n\n使用的观测数\n100\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \n因变量: x\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n19\n316.1767440\n16.6408813\n38.71\n&lt;.0001\n\n\n误差\n80\n34.3884400\n0.4298555\n \n \n\n\n校正合计\n99\n350.5651840\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nx 均值\n\n\n\n\n0.901906\n25.50707\n0.655634\n2.570400\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nI 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\na\n4\n5.2133940\n1.3033485\n3.03\n0.0221\n\n\nb\n1\n9.9225000\n9.9225000\n23.08\n&lt;.0001\n\n\nc\n1\n283.3162240\n283.3162240\n659.10\n&lt;.0001\n\n\na*b\n4\n1.9472300\n0.4868075\n1.13\n0.3472\n\n\na*c\n4\n1.4813260\n0.3703315\n0.86\n0.4908\n\n\nb*c\n1\n12.6878440\n12.6878440\n29.52\n&lt;.0001\n\n\na*b*c\n4\n1.6082260\n0.4020565\n0.94\n0.4479\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nIII 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\na\n4\n5.2133940\n1.3033485\n3.03\n0.0221\n\n\nb\n1\n9.9225000\n9.9225000\n23.08\n&lt;.0001\n\n\nc\n1\n283.3162240\n283.3162240\n659.10\n&lt;.0001\n\n\na*b\n4\n1.9472300\n0.4868075\n1.13\n0.3472\n\n\na*c\n4\n1.4813260\n0.3703315\n0.86\n0.4908\n\n\nb*c\n1\n12.6878440\n12.6878440\n29.52\n&lt;.0001\n\n\na*b*c\n4\n1.6082260\n0.4020565\n0.94\n0.4479",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "11-SAS 多个样本均数比较的方差分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#正交设计资料的方差分析",
    "href": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#正交设计资料的方差分析",
    "title": "11-SAS 多个样本均数比较的方差分析",
    "section": "8 正交设计资料的方差分析",
    "text": "8 正交设计资料的方差分析\n析因设计的缺点是当因素比较多或者各个因素中的水平数较多时,所需的实验单位数、处理组数、方差分析的计算量剧增,实现起来会很困难。此时可选择正交设计。正交设计并不考虑所有水平的交互作用,只考虑部分重要因素的一级交互作用。在作正交设计时,要根据具体情况选择合适的正交表。\n\n8.1 正交设计资料示例\n在某研究中,研究者对影响某种鱼类产卵数量的因素进行实验,设计了一个正交实验。实验因素包括 A、B、C、D 四个因素,每个因素有两个水平。\n\n\n\n\n\n\n\n\n\n\n\n实验序号\nA因素温度/℃\nB因素含氧量/%\nC因素含水量/%\nD因素pH\n产卵数量\n\n\n\n\n1\n5\n0.5\n10\n6.0\n86\n\n\n2\n5\n0.5\n30\n8.0\n95\n\n\n3\n5\n5.0\n10\n8.0\n91\n\n\n4\n5\n5.0\n30\n6.0\n94\n\n\n5\n25\n0.5\n10\n8.0\n91\n\n\n6\n25\n0.5\n30\n6.0\n96\n\n\n7\n25\n5.0\n10\n6.0\n83\n\n\n8\n25\n5.0\n30\n8.0\n90\n\n\n\n\n\n代码\n%%SAS\ndata Orthogonal;\n    input a b c d x @@;\ndatalines;\n5 0.5 10 6.0 86 \n5 0.5 30 8.0 95 \n5 5.0 10 8.0 91 \n5 5.0 30 6.0 94 \n25 0.5 10 8.0 91 \n25 0.5 30 6.0 96 \n25 5.0 10 6.0 83 \n25 5.0 30 8.0 90\n;\nrun;\nproc glm;\n    class a b c d;\n    model x = a b c d a*b;\nrun;\nquit;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\na\n2\n5 25\n\n\nb\n2\n0.5 5\n\n\nc\n2\n10 30\n\n\nd\n2\n6 8\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n8\n\n\n使用的观测数\n8\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \n因变量: x\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n5\n137.5000000\n27.5000000\n27.50\n0.0355\n\n\n误差\n2\n2.0000000\n1.0000000\n \n \n\n\n校正合计\n7\n139.5000000\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nx 均值\n\n\n\n\n0.985663\n1.101928\n1.000000\n90.75000\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nI 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\na\n1\n4.50000000\n4.50000000\n4.50\n0.1679\n\n\nb\n1\n12.50000000\n12.50000000\n12.50\n0.0715\n\n\nc\n1\n72.00000000\n72.00000000\n72.00\n0.0136\n\n\nd\n1\n8.00000000\n8.00000000\n8.00\n0.1056\n\n\na*b\n1\n40.50000000\n40.50000000\n40.50\n0.0238\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nIII 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\na\n1\n4.50000000\n4.50000000\n4.50\n0.1679\n\n\nb\n1\n12.50000000\n12.50000000\n12.50\n0.0715\n\n\nc\n1\n72.00000000\n72.00000000\n72.00\n0.0136\n\n\nd\n1\n8.00000000\n8.00000000\n8.00\n0.1056\n\n\na*b\n1\n40.50000000\n40.50000000\n40.50\n0.0238",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "11-SAS 多个样本均数比较的方差分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#glm-语句的常用选项和过程",
    "href": "Guide/SAS/25-06-01-multi-sample-ANOVA.html#glm-语句的常用选项和过程",
    "title": "11-SAS 多个样本均数比较的方差分析",
    "section": "9 glm 语句的常用选项和过程",
    "text": "9 glm 语句的常用选项和过程\n\n9.1 glm 过程的基本格式\nproc glm&lt;选项&gt;;\n    class 变量;\n    model 应变量 = 自变量;\n    absorb 变量;\n    by 变量;\n    freq 变量;\n    id 变量;\n    weight 变量;\n    contrast '标签' 效应值 &lt;…效应值&gt; &lt;/ 选项 &gt;;\n    estimate '标签' 效应值 &lt;…效应值&gt; &lt;/ 选项 &gt;\n    Ismeans 分类或处理变量 &lt;/ 选项&gt;;\n    manova &lt;检验方法选项&gt; &lt;/ 其他细节选项 &gt;:\n    means 分类或处理变量 &lt;/ 选项&gt;;\n    output &lt;out=数据集名称&gt; 输出变量 = 定义变量名称 &lt;...输出变量 = 定义变量名称&gt; &lt;/ 选项 &gt;;\n    random 随机效应变量 &lt;/ 选项 &gt;;\n    repeated 重复因子 &lt;/ 选项 &gt;;\n    test &lt;h= 效应变量 &gt; t = 效应误差项 &lt;/ 选项 &gt;;\nrun;\n\n\n9.2 glm 过程的常用选项\n\nalpha=value 选项 将设置一个为 0~1 之间的任意值作为概率值(value),也可用于指定统计量置信区间的置信水平,默认值为 0.05.\nplots=boxplot 选项 用于产生箱式图(boxplot),使用时必需先通过 ods graphics on 语句启用 ods 图形,才能进行绘图。完整语句可参考如下:\n\nods graphics on;\nproc glm plots = boxplot;\n    class c;\n    model x = c;\n    means c;\n    means c/lsd;\nrun;\nquit;\nods graphics off;\n\n\n代码\n%%SAS\nods graphics on;\n\n/* 修正数据集名称和数据步语法 */\nproc glm data=sashelp.cars plots(only)=boxplot;\n    class Origin;\n    model Invoice = Origin;\n    \n    /* 仅保留一种多重比较方法（SNK法） */\n    means Origin / SNK ALPHA=0.05;\n    \n    /* 输出方差分析表 */\n    output out=residuals predicted=pred residual=resid;\nrun;\nquit;\nods graphics off;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n残差图 \n\n\nGLM 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\nOrigin\n3\nAsia Europe USA\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n428\n\n\n使用的观测数\n428\n\n\n\n\n\n\n\n\n残差图 \n\n\nGLM 过程\n \n因变量: Invoice\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n2\n36546709291\n18273354645\n80.60\n&lt;.0001\n\n\n误差\n425\n96354614801\n226716740.71\n \n \n\n\n校正合计\n427\n132901324092\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nInvoice 均值\n\n\n\n\n0.274991\n50.16580\n15057.12\n30014.70\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nI 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nOrigin\n2\n36546709291\n18273354645\n80.60\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nIII 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nOrigin\n2\n36546709291\n18273354645\n80.60\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n残差图 \n\n\nGLM 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n残差图 \n\n\nGLM 过程\n \nStudent-Newman-Keuls Test for Invoice\n\n\n注意:This test controls the Type I experimentwise error rate under the complete null hypothesis but not under partial null hypotheses.\n\n\n\n\n\n\nAlpha\n0.05\n\n\nError Degrees of Freedom\n425\n\n\nError Mean Square\n2.2672E8\n\n\nHarmonic Mean of Cell Sizes\n141.0973\n\n\n\n\n\n\n注意:Cell sizes are not equal.\n\n\n\n\n\n\nNumber of Means\n2\n3\n\n\n\n\nCritical Range\n3523.5793\n4216.2475\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.3 glm 过程的常用语句\n\nby 语句 用于按照某个变量的不同取值,分别进行 glm 过程分析。\nmeans 变量/hovtest选项 该选型可用于检验方差齐性,可以指定以下方法用于检验方差产性: hovtest=bartlett, hovtest=bf, hovtest=levene,hovtest=obrien,默认的检验方法为 levene。注意,该选项一般用于完全随机设计的方差分析中。\nmeans 变量/bon选项 除文中所提及的多重比较方法之外,常用的还有 bonferroni t 检验,须在 means 后面选项中指明。\nmeans 变量/cldif选项 该选项用于两两比较的结果以置信区间的形式展现。可在 means c/lsd 后面加上 cldiff,变为 means c/lsd cldif,可以得到如下两两比较的结果展现形式:\n\n\n\n代码\n%%SAS\nods graphics on;\n\n/* 修正数据集名称和数据步语法 */\nproc glm data=sashelp.cars plots(only)=boxplot;\n    class Origin;\n    model Invoice = Origin;\n    \n    /* 仅保留一种多重比较方法（SNK法） */\n    means Origin / lsd cldif ALPHA=0.05;\n    \n    /* 输出方差分析表 */\n    output out=residuals predicted=pred residual=resid;\nrun;\nquit;\nods graphics off;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n残差图 \n\n\nGLM 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\nOrigin\n3\nAsia Europe USA\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n428\n\n\n使用的观测数\n428\n\n\n\n\n\n\n\n\n残差图 \n\n\nGLM 过程\n \n因变量: Invoice\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n2\n36546709291\n18273354645\n80.60\n&lt;.0001\n\n\n误差\n425\n96354614801\n226716740.71\n \n \n\n\n校正合计\n427\n132901324092\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\nInvoice 均值\n\n\n\n\n0.274991\n50.16580\n15057.12\n30014.70\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nI 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nOrigin\n2\n36546709291\n18273354645\n80.60\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nIII 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nOrigin\n2\n36546709291\n18273354645\n80.60\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n残差图 \n\n\nGLM 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n残差图 \n\n\nGLM 过程\n \nt Tests (LSD) for Invoice\n\n\n注意:This test controls the Type I comparisonwise error rate, not the experimentwise error rate.\n\n\n\n\n\n\nAlpha\n0.05\n\n\nError Degrees of Freedom\n425\n\n\nError Mean Square\n2.2672E8\n\n\nCritical Value of t\n1.96556\n\n\n\n\n\n\n\n\n\n\nComparisons significant at the 0.05 level are indicated by ***.\n\n\nOrigin\n比较\n均值\n间\n差值\n95% 置信限\n \n\n\n\n\nEurope - USA\n18446\n14829\n22062\n***\n\n\nEurope - Asia\n21793\n18234\n25352\n***\n\n\nUSA - Europe\n-18446\n-22062\n-14829\n***\n\n\nUSA - Asia\n3347\n-44\n6739\n \n\n\nAsia - Europe\n-21793\n-25352\n-18234\n***\n\n\nAsia - USA\n-3347\n-6739\n44",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "11-SAS 多个样本均数比较的方差分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-04-SAS-corr-reg.html",
    "href": "Guide/SAS/25-06-04-SAS-corr-reg.html",
    "title": "12-SAS 相关与回归分析",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "12-SAS 相关与回归分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-04-SAS-corr-reg.html#直线相关分析",
    "href": "Guide/SAS/25-06-04-SAS-corr-reg.html#直线相关分析",
    "title": "12-SAS 相关与回归分析",
    "section": "1 直线相关分析",
    "text": "1 直线相关分析\n两个变量之间的相关关系分析可以使用 corr过程。如两个变量都来自正态分布的总体,可以作直线相关分析,计算 Pearson 相关系数。\n以下数据和程序用来说明如何计算直线相关系数。\n数据集可从 Data-collection 处下载。\n如可以选择如下方式载入数据集：\n\n数据集载入 libname 目录下：\n\nlibname libname ‘C:\\syp\\data’;\ndata = dataset;\n    set libname.spec-data;\n    where spec-condition;\n    keep var1 var2...;\n    drop var1 var2...;\n    format var1 dollar12. var2 monyy.;\nrun;\n程序说明：\n\nlibname 语句定义了将在 SAS 的环境中的 SAS 资源管理器下新建一个工作目录；\n‘C:\\syp\\data’ 是数据集所在的路径，指定了你想要导入的数据；\ndata = dataset 是创建一个新的数据集，dataset 是数据集的名称；\nset libname.spec-data 是从指定的库中读取数据集；\nwhere spec-condition 是指定数据集的条件筛选；\nkeep var1 var2... 是指定保留的数据变量；\ndrop var1 var2... 是指定删除的数据变量；\nformat var1 dollar12. var2 monyy. 是指定变量的格式。\nrun; 语句结束数据步。\n\n\n或者直接使用 infile 语句读取数据：\n\ndata 数据集名;\n    infile ‘文件路径+文件名’ &lt;可选参数&gt;;\n    input 变量1 变量2...;\nrun;\n\n使用 input 语句和 datalines 语句直接输入数据：\n\n\n\n代码\n%%SAS\n/*程序12-1*/\ndata prg7_1;\n    input x y @@;\ncards;\n12.81 10.23\n11.89 10.01\n13.51 11.02\n10.82 9.08\n14.12 12.89\n12.53 11.28\n11.94 10.59\n11.23 10.88\n14.72 12.6\n12.45 11.26\n11.30 10.02\n12.08 11.55\n;\nproc corr;\n    var x y;\nrun;\n\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 17040\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nCORR 过程\n\n\n\n\n\n\n2 变量:\nx y\n\n\n\n\n\n\n\n\n\n\n简单统计量\n\n\n变量\n数目\n均值\n标准差\n总和\n最小值\n最大值\n\n\n\n\nx\n12\n12.45000\n1.18244\n149.40000\n10.82000\n14.72000\n\n\ny\n12\n10.95083\n1.08520\n131.41000\n9.08000\n12.89000\n\n\n\n\n\n\n\n\n\n\nPearson 相关系数, N = 12\nProb &gt; |r|, H0: Rho=0\n\n\n \nx\ny\n\n\n\n\nx\n\n\n1.00000\n\n\n \n\n\n\n\n0.81599\n\n\n0.0012\n\n\n\n\ny\n\n\n0.81599\n\n\n0.0012\n\n\n\n\n1.00000\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n1.1 结果说明\ncorr 过程首先给出两个变量的一些简单统计量,如例数、均数、标准差、总和、最小值和最大值。\n随后输出相关分析的结果,结果中有 Pearson 相关系数,即直线相关系数,还有判断该相关系数是否来自总体相关系数为0的总体假设检验的 P 值(当 \\(H_0:Rho=0\\) 时,\\(Prob&gt;r\\)),这两个值位于两个变量名所交叉处,相关系数位于上方,检验结果的 P 值位于下方。\n这里 Pearson 相关系数为 r=0.815 99,所对应的 P=0.001 2&lt;0.05说明两个变量之间存在正相关关系,即一个变量的值增大时,另一个变量的值也相应增大。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "12-SAS 相关与回归分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-04-SAS-corr-reg.html#直线回归分析",
    "href": "Guide/SAS/25-06-04-SAS-corr-reg.html#直线回归分析",
    "title": "12-SAS 相关与回归分析",
    "section": "2 直线回归分析",
    "text": "2 直线回归分析\n直线回归分析是研究一个因变量与一个或多个自变量之间的线性关系。SAS 中可以使用 reg 过程和 nlin 过程进行直线回归分析。\n直线回归分析是回归分析中较为简单的一种,即两个变量的数值在散点图上呈直线变化,完成直线回归可用 reg 过程。\n\n\n代码\n%%SAS\n/*程序12-2*/\ndata prg_reg;\n    input x y @@;\ncards;\n56 5.32 32 3.21 41 4.67 51 5.03 25 3.01\n35 3.57 21 2.98 47 3.93 62 5.62\n;\nproc reg;\n    model y = x;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n\n读取的观测数\n9\n\n\n使用的观测数\n9\n\n\n\n\n\n\n\n\n\n\n方差分析\n\n\n源\n自由度\n平方\n和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n1\n7.61077\n7.61077\n59.15\n0.0001\n\n\n误差\n7\n0.90072\n0.12867\n \n \n\n\n校正合计\n8\n8.51149\n \n \n \n\n\n\n\n\n\n\n\n\n\n均方根误差\n0.35871\nR 方\n0.8942\n\n\n因变量均值\n4.14889\n调整 R 方\n0.8791\n\n\n变异系数\n8.64598\n \n \n\n\n\n\n\n\n\n\n\n\n参数估计\n\n\n变量\n自由度\n参数\n估计\n标准\n误差\nt 值\nPr &gt; |t|\n\n\n\n\nIntercept\n1\n1.29098\n0.39037\n3.31\n0.0130\n\n\nx\n1\n0.06952\n0.00904\n7.69\n0.0001\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1 model 语句常用选项\n\nstb 选项：model y = x / stb 计算标准化回归系数,即将自变量和因变量都标准化后进行回归分析,得到的回归系数可以直接比较各个自变量对因变量的影响大小。\np 选项：model y = x / p 计算预测值和残差,即在回归分析中计算每个观测值的预测值和残差,可以用于评估模型的拟合效果。\nclb 选项：model y = x / clb 计算回归系数的置信区间,即在回归分析中计算每个回归系数的置信区间,可以用于评估回归系数的可靠性。\ncli 输出每个观测预测值的双侧 95%容许区间。\nclm 输出每个观测预测值均数的双侧95% 置信区间。\nr 输出残差分析的结果,除了输出p选项要求的内容外,还包括预测值和残差的标准误、student 残差和 Conk 的 D 统计量。如果使用了 cli、clm 和 r 选项,p 选项就可以省略。语句为 model y=x/cli clm r;\n\n\n\n代码\n%%SAS\n/*程序12-3*/\ndata prg_reg;\n    input x y @@;\ncards;\n56 5.32 32 3.21 41 4.67 51 5.03 25 3.01\n35 3.57 21 2.98 47 3.93 62 5.62\n;\nproc reg;\n    model y = x / cli clm r;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n\n读取的观测数\n9\n\n\n使用的观测数\n9\n\n\n\n\n\n\n\n\n\n\n方差分析\n\n\n源\n自由度\n平方\n和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n1\n7.61077\n7.61077\n59.15\n0.0001\n\n\n误差\n7\n0.90072\n0.12867\n \n \n\n\n校正合计\n8\n8.51149\n \n \n \n\n\n\n\n\n\n\n\n\n\n均方根误差\n0.35871\nR 方\n0.8942\n\n\n因变量均值\n4.14889\n调整 R 方\n0.8791\n\n\n变异系数\n8.64598\n \n \n\n\n\n\n\n\n\n\n\n\n参数估计\n\n\n变量\n自由度\n参数\n估计\n标准\n误差\nt 值\nPr &gt; |t|\n\n\n\n\nIntercept\n1\n1.29098\n0.39037\n3.31\n0.0130\n\n\nx\n1\n0.06952\n0.00904\n7.69\n0.0001\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n输出统计量\n\n\n观测\n因\n变量\n预测\n值\n标准\n误差\n均值\n预测\n95% 置信均值\n95% 置信预测\n残差\n标砖误差\n残差\nStudent\n残差\nCook D\n\n\n\n\n1\n5.32\n5.1839\n0.1800\n4.7582\n5.6096\n4.2349\n6.1330\n0.1361\n0.310\n0.439\n0.032\n\n\n2\n3.21\n3.5155\n0.1452\n3.1722\n3.8588\n2.6005\n4.4306\n-0.3055\n0.328\n-0.931\n0.085\n\n\n3\n4.67\n4.1412\n0.1196\n3.8584\n4.4239\n3.2471\n5.0353\n0.5288\n0.338\n1.564\n0.153\n\n\n4\n5.03\n4.8363\n0.1493\n4.4833\n5.1893\n3.9176\n5.7551\n0.1937\n0.326\n0.594\n0.037\n\n\n5\n3.01\n3.0289\n0.1884\n2.5833\n3.4745\n2.0708\n3.9870\n-0.0189\n0.305\n-0.062\n0.001\n\n\n6\n3.57\n3.7241\n0.1317\n3.4126\n4.0355\n2.8205\n4.6277\n-0.1541\n0.334\n-0.462\n0.017\n\n\n7\n2.98\n2.7508\n0.2176\n2.2363\n3.2653\n1.7588\n3.7429\n0.2292\n0.285\n0.804\n0.188\n\n\n8\n3.93\n4.5583\n0.1309\n4.2488\n4.8678\n3.6553\n5.4612\n-0.6283\n0.334\n-1.881\n0.272\n\n\n9\n5.62\n5.6010\n0.2235\n5.0725\n6.1295\n4.6016\n6.6004\n0.0190\n0.281\n0.068\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n残差和\n0\n\n\n残差平方和\n0.90072\n\n\n预测残差 SS (PRESS)\n1.26549\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n结果说明\n\n第 5、6 列为 clm 选项的结果,为预测值均数的 95% 置信区间(95% CL Mean );\n第 7、8 列为 cli选项的结果,为预测值的 95%容许区间(95%CPredict);第9列为残差值(Residual);\n第 10~12 列为选项r的结果,分别为残差标准误(Sld ErrorResidual)、student 残差值( Student Residual)、和Cook D统计量(Cook’sD),从 Cook’s D统计量可以看出第8观测与其他观测有很大差异,在实际工作中应仔细考察这个观测的情况。\n\n\n\n\n2.2 两条回归直线的比较\n使用 glm 过程可以比较两条回归直线,即比较两个不同组别的回归直线是否有显著差异。\n\n\n代码\n%%SAS\n/*程序12-4*/\ndata two_liner;\n    input x y c @@;\ndatalines;\n13 3.54 1 11 3.01 1 9 3.09 1 6 2.48 1 8 2.56 1 10 3.36 1 12 3.18 1 7 2.65 1\n10 3.01 2 9 2.83 2 11 2.92 2 12 3.09 2 15 3.98 2 16 3.89 2 8 2.21 2 7 2.39 2\n10 2.74 2 15 3.36 2\n;\nproc glm data = two_liner;\n    class c;\n    model y= x c x*c;\nrun;\nproc glm data = two_liner;\n    class c;\n    model y = x c;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\nc\n2\n1 2\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n18\n\n\n使用的观测数\n18\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \n因变量: y\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n3\n3.44201996\n1.14733999\n27.18\n&lt;.0001\n\n\n误差\n14\n0.59100782\n0.04221484\n \n \n\n\n校正合计\n17\n4.03302778\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\ny 均值\n\n\n\n\n0.853458\n6.812167\n0.205463\n3.016111\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nI 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nx\n1\n3.19449983\n3.19449983\n75.67\n&lt;.0001\n\n\nc\n1\n0.21641720\n0.21641720\n5.13\n0.0400\n\n\nx*c\n1\n0.03110293\n0.03110293\n0.74\n0.4052\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nIII 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nx\n1\n2.75799086\n2.75799086\n65.33\n&lt;.0001\n\n\nc\n1\n0.08377358\n0.08377358\n1.98\n0.1807\n\n\nx*c\n1\n0.03110293\n0.03110293\n0.74\n0.4052\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n\n\n\n\n\n\n\n分类水平信息\n\n\n分类\n水平\n值\n\n\n\n\nc\n2\n1 2\n\n\n\n\n\n\n\n\n\n\n读取的观测数\n18\n\n\n使用的观测数\n18\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nGLM 过程\n \n因变量: y\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n2\n3.41091703\n1.70545851\n41.12\n&lt;.0001\n\n\n误差\n15\n0.62211075\n0.04147405\n \n \n\n\n校正合计\n17\n4.03302778\n \n \n \n\n\n\n\n\n\n\n\n\n\nR 方\n变异系数\n均方根误差\ny 均值\n\n\n\n\n0.845746\n6.752131\n0.203652\n3.016111\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nI 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nx\n1\n3.19449983\n3.19449983\n77.02\n&lt;.0001\n\n\nc\n1\n0.21641720\n0.21641720\n5.22\n0.0373\n\n\n\n\n\n\n\n\n\n\n源\n自由度\nIII 型 SS\n均方\nF 值\nPr &gt; F\n\n\n\n\nx\n1\n3.39583675\n3.39583675\n81.88\n&lt;.0001\n\n\nc\n1\n0.21641720\n0.21641720\n5.22\n0.0373\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n程序说明\n\n进行两条回归直线比较可调用 glm 过程,在 glm 过程中需定义分组变量,本例为 c。在定义模型时,将变量 x 和 c 同时加入模型中,并考察两者的交互作用,该交互作用就是检验两条直线的回归系数之间差异是否有统计学意义。\n第二个 glm 过程没有变量 x 和 c 的交互项,则考察两条回归直线的截距之间的差异是否有统计学意义。\n\n\n\n结果说明\n\n第一个 glm 过程的结果中，主要考察 x*c 的交互作用项的 F 值和 P 值。F 值为 0.74，P 值为 0.4052，说明两条回归直线的斜率没有显著差异。\n第二个 glm 过程的结果中，主要考察 c 的 F 值和 P 值。F 值为 5.22，P 值为 0.0373，说明两条回归直线的截距有显著差异。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "12-SAS 相关与回归分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-04-SAS-corr-reg.html#秩相关",
    "href": "Guide/SAS/25-06-04-SAS-corr-reg.html#秩相关",
    "title": "12-SAS 相关与回归分析",
    "section": "3 秩相关",
    "text": "3 秩相关\n当样本资料不服从正态分布时,用直线相关不能正确描述两个变量之间的相关关系,此时可考虑用秩相关进行分析。\n主要可以使用 proc corr 过程中的 spearman 选项来计算 Spearman 秩相关系数。\n数据集可以从 Data-collection 处下载。\n\n\n代码\n%%SAS\n/*程序12-5*/\ndata rank_corr;\n    set 'Data\\prg7_4.sas7bdat';  /* 从已有数据集读取 */\nrun;\n\nproc corr data = rank_corr spearman;\n    var x y;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nCORR 过程\n\n\n\n\n\n\n2 变量:\nx y\n\n\n\n\n\n\n\n\n\n\n简单统计量\n\n\n变量\n数目\n均值\n标准差\n中位数\n最小值\n最大值\n\n\n\n\nx\n18\n5.55556\n8.66889\n0.95500\n0.03000\n27.96000\n\n\ny\n18\n5.55556\n8.31028\n2.89500\n0.05000\n33.95000\n\n\n\n\n\n\n\n\n\n\nSpearman 相关系数, N = 18\nProb &gt; |r|, H0: Rho=0\n\n\n \nx\ny\n\n\n\n\nx\n\n\n1.00000\n\n\n \n\n\n\n\n0.90506\n\n\n&lt;.0001\n\n\n\n\ny\n\n\n0.90506\n\n\n&lt;.0001\n\n\n\n\n1.00000\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1 结果说明\n该结果的结构与两变量直线相关分析的结构很相似,只是最后计算的相关系数为 Spearman 相关系数。本例秩相关系数为 0.905 06，所对应的 P&lt;0.0001,说明 x 与 y 构成的关系有统计学意义。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "12-SAS 相关与回归分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-04-SAS-corr-reg.html#加权直线回归",
    "href": "Guide/SAS/25-06-04-SAS-corr-reg.html#加权直线回归",
    "title": "12-SAS 相关与回归分析",
    "section": "4 加权直线回归",
    "text": "4 加权直线回归\n前述直线回归方程的最小二乘估计方法对于模型中的每个观测点是同等看待的,反映在确定回归直线时每个点的残差平方和要最小。然而在某些情况下,根据一定的专业知识考虑并结合实际数据,某些观察值对于估计回归方程显得更“重要”而有些并不是很“重要”此时可以采用加权最小二乘估计,以减少方差较大的观测对回归结果的影响。\n\n加权最小二乘法（WLS）：通过观测量方差的倒数对观测进行加权，从而拟合线性回归模型。\n权重的选择：理想的权重是误差方差的倒数，但通常难以计算，因此需要使用其他方法来确定。\n加权回归在处理数据时能够提高模型的准确性，尤其是在数据存在异方差性时。\n\n\n4.1 示例\n\n\n\n\n\n\n\n\n\n\n\n\n\n序号\n年龄/岁（X）\nIgG抗体水平（Y）\n\\(W=\\frac{1}{X^2}\\)\n\\(WX=\\frac{1}{X}\\)\n\\(WY\\)\n\\(WXY=\\frac{Y}{X}\\)\n\\(WY^2=\\frac{Y^2}{X^2}\\)\n\n\n\n\n1\n0.11\n4.00\n82.64\n9.09\n330.58\n36.36\n1322.31\n\n\n2\n0.12\n5.10\n69.44\n8.33\n354.17\n42.50\n1806.25\n\n\n3\n0.21\n9.50\n22.68\n4.76\n215.42\n45.24\n2046.49\n\n\n4\n0.30\n9.00\n11.11\n3.33\n100.00\n30.00\n900.00\n\n\n5\n0.34\n17.20\n8.65\n2.94\n148.79\n50.59\n2559.17\n\n\n6\n0.44\n14.00\n5.17\n2.27\n72.31\n31.82\n1012.40\n\n\n7\n0.56\n18.90\n3.19\n1.79\n60.27\n33.75\n1139.06\n\n\n8\n0.60\n29.40\n2.78\n1.67\n81.67\n49.00\n2401.00\n\n\n9\n0.69\n22.10\n2.10\n1.45\n46.42\n32.03\n1025.86\n\n\n10\n0.80\n41.50\n1.56\n1.25\n64.84\n51.87\n2691.02\n\n\n合计\n4.17\n170.70\n209.32\n36.89\n1474.46\n403.16\n16903.55\n\n\n\n\n\n代码\n%%SAS\n/*程序12-6*/\ndata prg7_5;\n    input x y @@;\n    w = 1/(x*x);\ncards;\n0.11 4.00 0.12 5.10 0.21 9.50 0.30 9.00 0.34 17.20 0.44 14.00 0.56 18.90 0.60 29.40\n0.69 22.10 0.80 41.50\n;\nproc reg data = prg7_5;\n    weight w;\n    model y=x;\nrun;\n\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 33664\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n\n读取的观测数\n10\n\n\n使用的观测数\n10\n\n\n\n\n\n\n\n权重: w\n\n\n\n\n\n方差分析\n\n\n源\n自由度\n平方\n和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n1\n5869.96312\n5869.96312\n72.53\n&lt;.0001\n\n\n误差\n8\n647.41204\n80.92650\n \n \n\n\n校正合计\n9\n6517.37516\n \n \n \n\n\n\n\n\n\n\n\n\n\n均方根误差\n8.99592\nR 方\n0.9007\n\n\n因变量均值\n7.04403\n调整 R 方\n0.8882\n\n\n变异系数\n127.70979\n \n \n\n\n\n\n\n\n\n\n\n\n参数估计\n\n\n变量\n自由度\n参数\n估计\n标准\n误差\nt 值\nPr &gt; |t|\n\n\n\n\nIntercept\n1\n-0.17197\n1.05095\n-0.16\n0.8741\n\n\nx\n1\n40.95053\n4.80825\n8.52\n&lt;.0001\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "12-SAS 相关与回归分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-04-SAS-corr-reg.html#指数曲线会回归",
    "href": "Guide/SAS/25-06-04-SAS-corr-reg.html#指数曲线会回归",
    "title": "12-SAS 相关与回归分析",
    "section": "5 指数曲线会回归",
    "text": "5 指数曲线会回归\n\n5.1 示例\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n编号\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\n住院天数(x)\n2\n5\n7\n10\n14\n19\n26\n31\n34\n38\n45\n52\n53\n60\n65\n\n\n预后指数(y)\n54\n50\n45\n37\n35\n25\n20\n16\n18\n13\n8\n11\n8\n4\n6\n\n\n\n\n\n代码\n%%SAS\n/*程序12-7*/\ndata prg12_7;\n    input x y @@;\ncards;\n2 54 5 50 7 45 10 37 14 35 19 25 26\n20 31 16 34 18 38 13 45 8 52 11 53 8\n60 4 65 6\n;\nproc nlin data = prg12_7;\n    parms a=0 b=0;\n    model y=exp (a+b*x);\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nNLIN 过程\n因变量“y”\n方法: Gauss-Newton\n\n\n\n\n\n\n迭代阶段\n\n\n迭代\na\nb\n平方和\n\n\n\n\n0\n0\n0\n11425.0\n\n\n1\n2.8413\n-0.0470\n6455.2\n\n\n2\n3.7352\n-0.0365\n879.2\n\n\n3\n4.1136\n-0.0402\n66.2189\n\n\n4\n4.0719\n-0.0396\n49.4669\n\n\n5\n4.0709\n-0.0396\n49.4593\n\n\n6\n4.0708\n-0.0396\n49.4593\n\n\n\n\n\n\n\n\n\n\nNOTE: Convergence criterion met.\n\n\n\n\n\n\n\n\n\n\n估计汇总\n\n\n\n\n方法\nGauss-Newton\n\n\n迭代\n6\n\n\n子迭代\n4\n\n\n平均子迭代\n0.666667\n\n\nR\n6.724E-6\n\n\nPPC(b)\n1.048E-6\n\n\nRPC(b)\n0.000032\n\n\n对象\n4.399E-8\n\n\n目标\n49.4593\n\n\n读取的观测\n15\n\n\n使用的观测\n15\n\n\n缺失的观测\n0\n\n\n\n\n\n\n\n注意:An intercept was not specified for this model.\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\n近似\nPr &gt; F\n\n\n\n\n模型\n2\n12060.5\n6030.3\n1585.01\n&lt;.0001\n\n\n误差\n13\n49.4593\n3.8046\n \n \n\n\n未校正合计\n15\n12110.0\n \n \n \n\n\n\n\n\n\n\n\n\n\n参数\n估计\n近似\n标准误差\n近似 95% 置信限\n\n\n\n\na\n4.0708\n0.0251\n4.0166\n4.1251\n\n\nb\n-0.0396\n0.00171\n-0.0433\n-0.0359\n\n\n\n\n\n\n\n\n\n\n近似相关矩阵\n\n\n \na\nb\n\n\n\n\na\n1.0000000\n-0.7071474\n\n\nb\n-0.7071474\n1.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n5.2 结果说明\n整个结果可以分为五个部分\n\n第一部分为用高斯-牛顿方法进行迭代,并输出每次迭代确定的参数估计值和残差平方和，本例表明经过6次迭代,误差平方和从 11425.0减小到 49.4593,满足收敛准则,即提示 NOTE:Convergence criterion met,停止迭代。得到回归方程的参数a为 4.0708,b为 -0.039 6.\n第二部分是迭代过程中的一些统计量。R是参数的初步收敛值,PPC是期望参数变化值，RPC是回顾参数变化值,Object是迭代过程中目标函数值的相关改变值。\n第三部分是对模型进行方差分析的结果,方差来源包括回归部分(Model)、残差部分(Error)和未校正平方和( Uncorrected total )。\n第四部分为参数估计的结果,包括参数的估计结果、相应的渐近标准误差和渐近 95% 置信区间,由此得到回归方程为: \\(\\hat y = e^{4.0708-0.0396 x}\\)\n第五部分是方程中的两个参数 a 和 b 的渐近相关矩阵。\n\n\n\n5.3 关于高斯-牛顿迭代法\n高斯-牛顿迭代法是一种用于非线性回归模型中求解回归参数的迭代方法。其基本思想是使用泰勒级数展开式近似代替非线性回归模型，通过多次迭代修正回归系数，使其不断逼近最佳回归系数，最终使残差平方和达到最小。\n基本步骤\n\n初始值选择：可以根据经验选定初始值，或使用分段法求出初始值，或者通过线性变换后施行最小平方法求出初始值。\n泰勒级数展开：设非线性回归模型为 \\(f(X, \\beta)\\)，在初始值附近作泰勒展开，忽略二阶及以上的项。\n估计修正因子：用最小平方法估计修正因子，得到新的迭代值。\n精确度检验：计算残差平方和，若满足给定的误差率则停止迭代，否则继续。\n重复迭代：重复上述步骤，直到满足精度要求。\n重要考虑事项：高斯-牛顿迭代法在迭代点附近区域线性度较好时，收敛速度较快，但在线性度较差时效果不佳。此外，求解过程中涉及矩阵求逆，计算量较大。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "12-SAS 相关与回归分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-04-SAS-corr-reg.html#对数曲线回归",
    "href": "Guide/SAS/25-06-04-SAS-corr-reg.html#对数曲线回归",
    "title": "12-SAS 相关与回归分析",
    "section": "6 对数曲线回归",
    "text": "6 对数曲线回归\n对数回归是一种在方程中包含对数的回归模型。具体来说，在对数回归中，取自变量的对数。\n当样本数据形成对数曲线时，对数回归对于拟合回归模型非常有用，从而使回归模型更好地拟合样本数据。\n因此，对数回归是非线性回归的一种，就像指数回归和多项式回归一样。\n对数回归方程的公式为:\n\\[y=a+b\\cdot \\ln(x)\\]\n\n6.1 示例\n\n\n\n编号\n1\n2\n3\n4\n5\n\n\n\n\nCRF/(nmol/L)\n0.005\n0.050\n0.500\n5.000\n25.000\n\n\nACTH/(pmol/L)\n34.11\n57.99\n94.49\n128.50\n169.98\n\n\n\n\n\n代码\n%%SAS\n/*程序12-8*/\ndata prg12_8;\n    input x y @@;\ncards;\n0.005 34.11 0.050 57.99\n0.500 94.49 5.000 128.50\n25.000 169.98\n;\nproc nlin data=prg12_8;\n    parms a=0 b=0;\n    model y=a+b*log10(x);\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nNLIN 过程\n因变量“y”\n方法: Gauss-Newton\n\n\n\n\n\n\n迭代阶段\n\n\n迭代\na\nb\n平方和\n\n\n\n\n0\n0\n0\n58860.1\n\n\n1\n110.1\n36.1154\n234.3\n\n\n\n\n\n\n\n\n\n\nNOTE: Convergence criterion met.\n\n\n\n\n\n\n\n\n\n\n估计汇总\n\n\n\n\n方法\nGauss-Newton\n\n\n迭代\n1\n\n\nR\n0\n\n\nPPC\n0\n\n\nRPC(a)\n1.1006E8\n\n\n对象\n0.996019\n\n\n目标\n234.3347\n\n\n读取的观测\n5\n\n\n使用的观测\n5\n\n\n缺失的观测\n0\n\n\n\n\n\n\n\n\n\n\n源\n自由度\n平方和\n均方\nF 值\n近似\nPr &gt; F\n\n\n\n\n模型\n1\n11567.2\n11567.2\n148.09\n0.0012\n\n\n误差\n3\n234.3\n78.1116\n \n \n\n\n校正合计\n4\n11801.6\n \n \n \n\n\n\n\n\n\n\n\n\n\n参数\n估计\n近似\n标准误差\n近似 95% 置信限\n\n\n\n\na\n110.1\n4.0953\n97.0270\n123.1\n\n\nb\n36.1154\n2.9678\n26.6705\n45.5603\n\n\n\n\n\n\n\n\n\n\n近似相关矩阵\n\n\n \na\nb\n\n\n\n\na\n1.0000000\n0.2617813\n\n\nb\n0.2617813\n1.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n6.2 回归方程\n\\[\\hat y = 110.1 + 36.1154 log_{10} x\\]",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "12-SAS 相关与回归分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-04-SAS-corr-reg.html#多元线性回归",
    "href": "Guide/SAS/25-06-04-SAS-corr-reg.html#多元线性回归",
    "title": "12-SAS 相关与回归分析",
    "section": "7 多元线性回归",
    "text": "7 多元线性回归\n多元线性回归是分析一个应变量和多个自变量之间的依存关系,逐步回归则能判断哪些自变量对应变量有影响,哪些没有。\nSAS系统可以用 reg、nlin 、glm、stepwise、rsreg 和 rsquare 等过程来完成,由于每个过程各具特点,因此在使用过程中,应根据需要选择合适的过程。这里介绍最常用的 reg 过程。\n\n7.1 多元线性回归过程的特点介绍\n以下是 SAS 中与回归分析相关的 reg、nlin、glm、stepwise、rsreg 等过程的主要特点和应用场景介绍（注：SAS 中无独立的 rsquare 过程，可能是对回归结果中决定系数的误写，故不单独介绍）：\n\nREG 过程（线性回归）\n\n\n主要特点\n\n基础线性回归：用于拟合多元线性回归模型（响应变量为连续变量），支持自变量为连续变量或分类变量（需提前转换为虚拟变量）。\n变量选择方法：内置多种变量选择策略，包括：\n\nSELECTION=STEPWISE（逐步回归）：自动向前引入和向后剔除变量，基于显著性水平（如 SLENTRY 和 SLSTAY）。\nSELECTION=FORWARD（向前选择）：逐步添加显著变量。\nSELECTION=BACKWARD（向后剔除）：逐步删除不显著变量。\nSELECTION=MAXR（最大 \\(R^2\\) 增量）、SELECTION=MINR（最小均方误差）等。\n\n输出统计量：提供回归系数、显著性检验（t 检验、F 检验）、决定系数（\\(R^2\\)）、调整 \\(R^2\\)、残差分析等。\n支持交互项和多项式项：可通过 MODEL 语句手动指定复杂项（如 y = x1 x2 x1*x2 x1**2）。\n\n应用场景\n\n分析连续响应变量与多个自变量的线性关系（如房价预测、销售额影响因素分析）。\n需要灵活选择变量筛选方法时（逐步回归是最常用的场景）。\n示例代码：\n\nproc reg data=mydata;\n   model y = x1 x2 x3 / selection=stepwise slentry=0.05 slstay=0.10;\nrun;\n\n\nNLIN 过程（非线性回归）\n\n\n主要特点\n\n拟合非线性模型：用于响应变量与自变量存在非线性关系的场景（如指数增长、Logistic 曲线、Michaelis-Menten 动力学等）。\n需指定模型形式：用户需手动定义非线性函数形式（通过 PARMS 语句设定初始参数值），SAS 通过迭代算法（如高斯-牛顿法）优化参数。\n输出统计量：提供参数估计值、标准误、拟合优度（如残差平方和、决定系数），但解释性弱于线性模型。\n\n应用场景\n\n生物学、化学、工程等领域的非线性关系建模（如药物浓度衰减、酶促反应动力学）。\n示例代码（拟合指数模型 y = a*exp(b*x)）：\n\nproc nlin data=mydata;\n   parms a=1 b=0.1; /* 初始参数值 */\n   model y = a*exp(b*x);\nrun;\n\n\nGLM 过程（广义线性模型）\n\n\n主要特点\n\n扩展线性回归：允许响应变量服从非正态分布（如二项分布、泊松分布、伽马分布等），通过链接函数将线性预测值与响应变量的均值关联。\n支持分类变量：可直接分析因子变量（分类变量）及其交互作用，自动生成方差分析（ANOVA）表。\n常见模型：\n\n二项分布 + logit 链接：逻辑回归（响应变量为二分类，如患病/未患病）。\n泊松分布 + 对数链接：计数数据回归（如网页点击量）。\n正态分布 + 恒等链接：等同于传统线性回归（但 GLM 更灵活）。\n\n输出统计量：提供参数估计（优势比、发生率比等）、偏差检验、拟合优度（如 Pearson 卡方、Deviance）。\n\n应用场景\n\n响应变量为分类变量（二分类、多分类）或计数数据时（如客户流失预测、疾病发病率影响因素）。\n示例代码（逻辑回归）：\n\nproc glm data=mydata;\n   class gender; /* 声明分类变量 */\n   model outcome = gender age / dist=binomial link=logit;\nrun;\n\n\nSTEPWISE 过程（逐步回归，已弃用）\n\n\n主要特点\n\n历史过程：早期 SAS 中用于逐步回归的独立过程，目前已被 REG 过程的 SELECTION=STEPWISE 选项替代。\n功能局限：仅支持逐步回归（向前引入 + 向后剔除），无法使用其他变量选择方法（如向前选择、向后剔除单独使用）。\n输出内容：与 REG 过程的逐步回归结果类似，包括变量进入/剔除步骤、显著性水平、最终模型参数等。\n\n应用场景\n\n仅建议用于兼容旧代码，新场景推荐使用 REG 过程的 SELECTION 选项。\n\n\n\nRSREG 过程（响应面回归）\n\n\n主要特点\n\n实验设计与优化：用于拟合响应面模型（Response Surface Methodology, RSM），分析响应变量与多个自变量的非线性关系（通常为二次模型），寻找最优工艺条件。\n模型形式：自动拟合包含一次项、二次项和交互项的模型（如 ( y = 0 + i x_i + {ii}x_i^2 + {ij}x_i x_j )）。\n支持设计类型：中心复合设计（CCD）、Box-Behnken 设计等，内置实验设计功能。\n输出结果：提供模型参数、响应面图、等高线图、最优点估计（如最大值、最小值）。\n\n应用场景\n\n工业优化（如化工反应条件优化、产品配方设计）、生物医药实验（如药物剂量-效应关系）。\n示例代码：\n\nproc rsreg data=mydata;\n   model y = x1 x2 / design=ccd; /* 使用中心复合设计 */\nrun;\n\n\n\n7.2 选择建议\n\n\n\n\n\n\n\n\n需求场景\n推荐过程\n理由\n\n\n\n\n连续响应变量的线性回归\nREG\n基础线性模型，支持多种变量选择方法\n\n\n非线性关系建模\nNLIN\n需自定义非线性函数形式\n\n\n非正态响应变量（分类/计数数据）\nGLM\n广义线性模型，支持二项式、泊松等分布\n\n\n自动变量选择（逐步回归）\nREG（SELECTION）\n替代传统 STEPWISE 过程，更灵活\n\n\n响应面分析与优化\nRSREG\n内置实验设计，支持二次模型和交互项，适用于寻优\n\n\n\n\n\n7.3 注意事项\n\nSTEPWISE 过程在新版 SAS 中可能被标记为弃用，建议优先使用 REG 过程的 SELECTION 选项。\n回归分析前需验证假设（如线性关系、正态性、方差齐性、无多重共线性等），可通过 REG 或 GLM 过程的残差分析实现。\n对于高维数据（自变量数量多），可结合 REG 过程的正则化方法（如 LASSO、岭回归，需加载 SAS/STAT 模块）。\n\n\n\n7.4 多元线性回归示例\n1个反应变量 y ，和3个自变量 \\(x_1\\),\\(x_2\\),\\(x_3\\)。\n\n\n代码\n%%SAS\n/*程序12-9*/\ndata multi_reg;\n    set 'Data\\prg7_8.sas7bdat';  /* 从已有数据集读取 */\nrun;\n\nproc reg;\n    model y = x1-x3;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n\n读取的观测数\n16\n\n\n使用的观测数\n16\n\n\n\n\n\n\n\n\n\n\n方差分析\n\n\n源\n自由度\n平方\n和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n3\n1883479\n627826\n14.31\n0.0003\n\n\n误差\n12\n526521\n43877\n \n \n\n\n校正合计\n15\n2410000\n \n \n \n\n\n\n\n\n\n\n\n\n\n均方根误差\n209.46778\nR 方\n0.7815\n\n\n因变量均值\n2275.00000\n调整 R 方\n0.7269\n\n\n变异系数\n9.20738\n \n \n\n\n\n\n\n\n\n\n\n\n参数估计\n\n\n变量\n自由度\n参数\n估计\n标准\n误差\nt 值\nPr &gt; |t|\n\n\n\n\nIntercept\n1\n-2922.22620\n1459.15441\n-2.00\n0.0683\n\n\nx1\n1\n68.89352\n23.86651\n2.89\n0.0137\n\n\nx2\n1\n31.95682\n16.11436\n1.98\n0.0707\n\n\nx3\n1\n84.69245\n78.11936\n1.08\n0.2996\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.5 逐步法\n筛选的方法有前进法、后退法、逐步法等,最常用是逐步法。现仍参照上例数据,讨论如何用SAS 完成逐步回归。\n程序中增加 /selection = stepwise sle =0.10 sls = 0.15 stb 语句，其中 selection=stepwise 表示将用逐步法筛选变量;stb 表示将输出标准化偏回归系数,sle=0.10 和 sls=0.15 选项表示在筛选变量时入选标准为 0.10,剔除标准为 0.15.\n\n\n代码\n%%SAS\n/*程序12-10*/\nproc reg data = multi_reg;\n    model y= x1 x2 x3/selection = stepwise sle =0.10 sls = 0.15 stb;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n\n读取的观测数\n16\n\n\n使用的观测数\n16\n\n\n\n\n\n\n \n\n\n\n逐步选择: 第 1 步\n\n\n\n变量 x1 已输入: R 方 = 0.6110 和 C(p) = 9.3658\n\n\n\n\n\n\n\n\n\n方差分析\n\n\n源\n自由度\n平方\n和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n1\n1472539\n1472539\n21.99\n0.0003\n\n\n误差\n14\n937461\n66962\n \n \n\n\n校正合计\n15\n2410000\n \n \n \n\n\n\n\n\n\n\n\n\n\n变量\n参数\n估计\n标准\n误差\nII 型 SS\nF 值\nPr &gt; F\n\n\n\n\nIntercept\n-1783.27616\n867.82268\n282749\n4.22\n0.0590\n\n\nx1\n100.51458\n21.43426\n1472539\n21.99\n0.0003\n\n\n\n\n\n\n条件数字的边界: 1, 1\n\n\n\n\n逐步选择: 第 2 步\n\n\n\n变量 x2 已输入: R 方 = 0.7601 和 C(p) = 3.1754\n\n\n\n\n\n\n\n\n\n方差分析\n\n\n源\n自由度\n平方\n和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n2\n1831908\n915954\n20.60\n&lt;.0001\n\n\n误差\n13\n578092\n44469\n \n \n\n\n校正合计\n15\n2410000\n \n \n \n\n\n\n\n\n\n\n\n\n\n变量\n参数\n估计\n标准\n误差\nII 型 SS\nF 值\nPr &gt; F\n\n\n\n\nIntercept\n-4019.79958\n1057.87260\n642090\n14.44\n0.0022\n\n\nx1\n85.75822\n18.22215\n984932\n22.15\n0.0004\n\n\nx2\n40.38947\n14.20772\n359369\n8.08\n0.0138\n\n\n\n\n\n\n条件数字的边界: 1.0883, 4.3533\n\n\n\n留在模型中的所有变量的显著性水平都为 0.1500。\n\n\n没有其他变量满足 0.1000 显著性水平，无法输入该模型。\n\n\n\n\n\n\n\n\n\n\n\n“逐步选择”的汇总\n\n\n步\n进入的\n变量\n删除的\n变量\n进入的\n变量数\n偏\nR 方\n模型\nR 方\nC(p)\nF 值\nPr &gt; F\n\n\n\n\n1\nx1\n \n1\n0.6110\n0.6110\n9.3658\n21.99\n0.0003\n\n\n2\nx2\n \n2\n0.1491\n0.7601\n3.1754\n8.08\n0.0138\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n读取的观测数\n16\n\n\n使用的观测数\n16\n\n\n\n\n\n\n\n\n\n\n方差分析\n\n\n源\n自由度\n平方\n和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n2\n1831908\n915954\n20.60\n&lt;.0001\n\n\n误差\n13\n578092\n44469\n \n \n\n\n校正合计\n15\n2410000\n \n \n \n\n\n\n\n\n\n\n\n\n\n均方根误差\n210.87586\nR 方\n0.7601\n\n\n因变量均值\n2275.00000\n调整 R 方\n0.7232\n\n\n变异系数\n9.26927\n \n \n\n\n\n\n\n\n\n\n\n\n参数估计\n\n\n变量\n自由度\n参数\n估计\n标准\n误差\nt 值\nPr &gt; |t|\n标准化\n估计\n\n\n\n\nIntercept\n1\n-4019.79958\n1057.87260\n-3.80\n0.0022\n0\n\n\nx1\n1\n85.75822\n18.22215\n4.71\n0.0004\n0.66692\n\n\nx2\n1\n40.38947\n14.20772\n2.84\n0.0138\n0.40285\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n过程\n\n\n\n\n\n7.6 后退法\n如果用前进法和后退法来筛选变量,可以在 model 后加上选项 forward 和 backward。\n程序说明: 该程序和逐步回归程序的不同点在于 selection= 语句后面的方法有所不同,本程序选择的是 backward,表示后退法。\n\n\n代码\n%%SAS\n/*程序12-11*/\nproc reg data = multi_reg;\n    model y= x1 x2 x3/selection = backward;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n\n读取的观测数\n16\n\n\n使用的观测数\n16\n\n\n\n\n\n\n \n\n\n\n向后消除: 第 0 步\n\n\n\n所有变量已输入: R 方 = 0.7815 和 C(p) = 4.0000\n\n\n\n\n\n\n\n\n\n方差分析\n\n\n源\n自由度\n平方\n和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n3\n1883479\n627826\n14.31\n0.0003\n\n\n误差\n12\n526521\n43877\n \n \n\n\n校正合计\n15\n2410000\n \n \n \n\n\n\n\n\n\n\n\n\n\n变量\n参数\n估计\n标准\n误差\nII 型 SS\nF 值\nPr &gt; F\n\n\n\n\nIntercept\n-2922.22620\n1459.15441\n175979\n4.01\n0.0683\n\n\nx1\n68.89352\n23.86651\n365606\n8.33\n0.0137\n\n\nx2\n31.95682\n16.11436\n172558\n3.93\n0.0707\n\n\nx3\n84.69245\n78.11936\n51571\n1.18\n0.2996\n\n\n\n\n\n\n条件数字的边界: 2.4281, 17.217\n\n\n\n\n向后消除: 第 1 步\n\n\n\n变量 x3 已删除: R 方 = 0.7601 和 C(p) = 3.1754\n\n\n\n\n\n\n\n\n\n方差分析\n\n\n源\n自由度\n平方\n和\n均方\nF 值\nPr &gt; F\n\n\n\n\n模型\n2\n1831908\n915954\n20.60\n&lt;.0001\n\n\n误差\n13\n578092\n44469\n \n \n\n\n校正合计\n15\n2410000\n \n \n \n\n\n\n\n\n\n\n\n\n\n变量\n参数\n估计\n标准\n误差\nII 型 SS\nF 值\nPr &gt; F\n\n\n\n\nIntercept\n-4019.79958\n1057.87260\n642090\n14.44\n0.0022\n\n\nx1\n85.75822\n18.22215\n984932\n22.15\n0.0004\n\n\nx2\n40.38947\n14.20772\n359369\n8.08\n0.0138\n\n\n\n\n\n\n条件数字的边界: 1.0883, 4.3533\n\n\n\n留在模型中的所有变量的显著性水平都为 0.1000。\n\n\n\n\n\n\n\n\n\n\n\n“向后消除”的汇总\n\n\n步\n删除的\n变量\n进入的\n变量数\n偏\nR 方\n模型\nR 方\nC(p)\nF 值\nPr &gt; F\n\n\n\n\n1\nx3\n2\n0.0214\n0.7601\n3.1754\n1.18\n0.2996\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nREG 过程\n模型: MODEL1\n因变量: y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n过程",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "12-SAS 相关与回归分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-04-SAS-corr-reg.html#corr-过程和-reg-过程常用选项和语句详解",
    "href": "Guide/SAS/25-06-04-SAS-corr-reg.html#corr-过程和-reg-过程常用选项和语句详解",
    "title": "12-SAS 相关与回归分析",
    "section": "8 CORR 过程和 REG 过程常用选项和语句详解",
    "text": "8 CORR 过程和 REG 过程常用选项和语句详解\n在使用 SAS 进行相关分析和回归分析时，CORR 过程和 REG 过程是最常用的工具。通过灵活配置选项和语句，可以更精确地控制分析过程并获取所需结果。\n\n8.1 CORR 过程（相关分析）\n\n基本格式\nproc corr &lt;选项&gt;;\n    var 变量名1 变量名2 &lt;变量名3&gt;...;    /* 指定参与分析的变量 */\n    with 变量名1 &lt;变量名2&gt;...;         /* 指定与var变量计算相关系数的变量 */\nrun;\n\n\n常用选项\n\nPEARSON\n\n功能：计算皮尔逊积矩相关系数（默认选项），适用于两个连续变量呈线性关系的场景。\n\n示例：proc corr pearson;\n\nSPEARMAN\n\n功能：计算斯皮尔曼等级相关系数，基于变量的秩次而非原始值，适用于非正态分布或非线性关系的数据。\n\n示例：proc corr spearman;\n\nNOMISS\n\n功能：排除任何包含缺失值的观测（行），确保计算基于完整数据。\n\n对比：默认行为是 NOMISS 的列版本（NOMISSCOL），仅排除特定变量有缺失值的观测。\n\n其他实用选项\n\nKENDALL：计算肯德尔 τ 相关系数（适用于有序分类变量）。\n\nCOV：输出协方差矩阵而非相关系数矩阵。\n\nPVALUE：显示相关系数的 p 值（默认包含）。\n\n\n\n\n\n8.2 REG 过程（回归分析）\n\n基本格式\nproc reg data=数据集名 &lt;选项&gt;;\n    model 应变量 = 自变量1 &lt;自变量2&gt;... / &lt;选项&gt;;  /* 定义回归模型 */\n    plot 变量表达式;                             /* 绘制残差图等诊断图 */\n    var 变量名1 &lt;变量名2&gt;...;                   /* 指定分析变量范围 */\n    freq 变量名;                                /* 指定频数变量 */\n    weight 变量名;                              /* 指定权重变量 */\n    by 变量名1 &lt;变量名2&gt;...;                    /* 分组分析 */\nrun;\n\n\nMODEL 语句常用选项\n\nNOINT\n\n功能：不拟合截距项（强制回归直线过原点）。\n\n注意：慎用此选项，可能导致模型解释困难或违反统计假设。\n\n共线性诊断选项\n\nCOLIN：计算方差膨胀因子（VIF）和条件指数，诊断自变量间的多重共线性。\n\nCOLLINOINT：在 COLIN 基础上包含截距项，适用于检验截距是否受共线性影响。\n\n变量选择方法\n\nSELECTION=STEPWISE：逐步回归（向前引入+向后剔除）。\n\nSELECTION=FORWARD：向前选择法。\n\nSELECTION=BACKWARD：向后剔除法。\n\n控制参数：SLENTRY=0.05（引入变量的显著性水平）、SLSTAY=0.10（保留变量的显著性水平）。\n\n残差分析选项\n\nR：输出残差、预测值和学生化残差。\n\nP：生成预测值和残差数据集。\n\nDW：计算 Durbin-Watson 统计量（检验残差自相关）。\n\n其他实用选项\n\nSTB：输出标准化回归系数（β 系数）。\n\nCLM 和 CLI：分别计算均值和个体预测值的置信区间。\n\nOUTPUT OUT=新数据集名：将结果保存到新数据集。\n\n\n\n\n\n8.3 三、对比与最佳实践\n\n\n\n\n\n\n\n\n场景\n推荐选项/语句\n说明\n\n\n\n\n检验非正态分布变量的相关性\nproc corr spearman;\n斯皮尔曼等级相关系数不依赖分布假设。\n\n\n处理缺失值\nproc corr nomiss;\n确保所有变量无缺失值时进行计算。\n\n\n诊断多重共线性\nmodel y = x1 x2 x3 / colin;\nVIF &gt; 10 或条件指数 &gt; 30 提示严重共线性。\n\n\n自动筛选显著变量\nmodel y = x1-x10 / selection=stepwise slentry=0.05;\n逐步引入 p &lt; 0.05 的变量。\n\n\n绘制残差图\nplot r.*p. 或 plot residual.*predicted.\n检测残差是否符合正态分布和等方差性假设。\n\n\n\n\n\n8.4 五、注意事项\n\n共线性处理：若发现严重共线性，可尝试删除冗余变量、合并变量或使用主成分回归。\n\n缺失值策略：NOMISS 可能导致样本量大幅减少，建议先使用 PROC MI 进行多重插补。\n\n模型验证：始终检查残差图和诊断统计量（如 Durbin-Watson、Shapiro-Wilk 检验）。\n\n逐步回归的局限性：变量选择结果可能受输入顺序影响，最终模型需结合专业知识验证。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "12-SAS 相关与回归分析"
    ]
  },
  {
    "objectID": "navbar/translation/medicine.html",
    "href": "navbar/translation/medicine.html",
    "title": "",
    "section": "",
    "text": "代码\n\n\n\n\nFundamentals of Machine Learning and Deep Learning in Medicine"
  },
  {
    "objectID": "navbar/small_talk/book.html",
    "href": "navbar/small_talk/book.html",
    "title": "公共卫生相关的书籍",
    "section": "",
    "text": "读的书有点乱，主要都在微信读书看，部分是纸质书，介绍主要以我看过或在看的书为主。\n\n1 Maybe you will be interested\n对于刚接触统计的朋友，可以先看看《女士品茶：统计学如何变革了科学和生活》这本书，也会他能激发你的一些对于统计学（家们）的兴趣，通过一些有趣的小故事，带你走进统计学的世界。\n2025-03-30，出于对 THE LADY TASTING TEA 的兴趣，后续将会整理一下出现的主要统计学家和他的主要成果（公式）。\n\n\n2 Basic\n考研的话，除了人卫第八版是一个通用教材，其他的院校各有不同，比如说赵耐青、方积乾和贺佳，根据院校给的参考书目来选择，这几个人的书都买了看过一下，个人主要用过人卫版和姜晶梅的《医学统计学》，各有见长。\n\n\n3 Advance\n提升部分可以看看 Springer 出版的 Mathematical Models in Epidemiology ，2023年科学出版社出版了中文版 《流行病学中的数学模型》\n\nThe book is a comprehensive, self-contained introduction to the mathematical modeling and analysis of disease transmission models. It includes (i) an introduction to the main concepts of compartmental models including models with heterogeneous mixing of individuals and models for vector-transmitted diseases, (ii) a detailed analysis of models for important specific diseases, including tuberculosis, HIV/AIDS, influenza, Ebola virus disease, malaria, dengue fever and the Zika virus, (iii) an introduction to more advanced mathematical topics, including age structure, spatial structure, and mobility, and (iv) some challenges and opportunities for the future.\nThere are exercises of varying degrees of difficulty, and projects leading to new research directions. For the benefit of public health professionals whose contact with mathematics may not be recent, there is an appendix covering the necessary mathematical background. There are indications which sections require a strong mathematical background so that the book can be useful for both mathematical modelers and public health professionals.\n\nend."
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-2.html",
    "href": "Guide/SAS/25-06-06-chi2-2.html",
    "title": "14-SAS 卡方检验2-四格表资料卡方检验",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "14-SAS 卡方检验2-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-2.html#rxc表资料的chi2检验",
    "href": "Guide/SAS/25-06-06-chi2-2.html#rxc表资料的chi2检验",
    "title": "14-SAS 卡方检验2-四格表资料卡方检验",
    "section": "1 RxC表资料的\\(\\chi^2\\)检验",
    "text": "1 RxC表资料的\\(\\chi^2\\)检验\n行x列表资料根据行变量和列变量的类型,可以分成3种类型:双向无序列联表资料、单向有序列联表资料和双向有序列联表资料。\n对于这三种类型的资料可用 SAS 提供的 CMH 统计量( Cochran-Mantel-Haenszel Statisitic)进行分析,其包括3个统计量即:\n\nNonzero Correlation:行变量和列变量为非零相关,可用于双向有序的资料;\nRow Mean Scores Difer:行均数得分差值,可用于列变量为有序变量的资料;\nGeneral Association:行、列变量为一般关联,可用于双向无序的资料。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "14-SAS 卡方检验2-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-2.html#双向无序资料",
    "href": "Guide/SAS/25-06-06-chi2-2.html#双向无序资料",
    "title": "14-SAS 卡方检验2-四格表资料卡方检验",
    "section": "2 双向无序资料",
    "text": "2 双向无序资料\n双向无序资料的行变量和列变量都是名义变量,如职业、血型、疾病的类型等,这种变量的各水平间无内在的有序关联。分析这种资料,目的在于检验两变量的关系是否独立。\n\n2.1 示例\n以下是提取后的 Markdown 表格：\n\n\n\nABO 血型\nMN 血型-M\nMN 血型-N\nMN 血型-MN\n合计\n\n\n\n\nO\n431\n490\n902\n1823\n\n\nA\n388\n410\n800\n1598\n\n\nB\n495\n587\n950\n2032\n\n\nAB\n137\n179\n32\n348\n\n\n合计\n1451\n1666\n2684\n5801\n\n\n\n\n\n代码\n%%SAS\n/*程序14-1*/\ndata chi14_1;\n    input r c f @@;\ndatalines;\n1 1 431 1 2 490 1 3 902\n2 1 388 2 2 410 2 3 800\n3 1 495 3 2 587 3 3 950\n4 1 137 4 2 179 4 3 32\n;\nproe freq;\n    weight f;\n    tables r*c/cmh norow nocol nopercent;\nrun;\n\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 5264\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nFREQ 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n频数\n\n\n\n\n\n\n\n\nr-c表\n\n\nr\nc\n\n\n1\n2\n3\n合计\n\n\n\n\n1\n\n\n431\n\n\n\n\n490\n\n\n\n\n902\n\n\n\n\n1823\n\n\n\n\n2\n\n\n388\n\n\n\n\n410\n\n\n\n\n800\n\n\n\n\n1598\n\n\n\n\n3\n\n\n495\n\n\n\n\n587\n\n\n\n\n950\n\n\n\n\n2032\n\n\n\n\n4\n\n\n137\n\n\n\n\n179\n\n\n\n\n32\n\n\n\n\n348\n\n\n\n\n合计\n\n\n1451\n\n\n\n\n1666\n\n\n\n\n2684\n\n\n\n\n5801\n\n\n\n\n\n\n\n\n\n\n\n“c-r”的汇总统计量\n\n\n\n\n\n\nCochran-Mantel-Haenszel 统计量 （基于表评分）\n\n\n统计量\n备择假设\n自由度\n值\n概率\n\n\n\n\n1\n非零相关\n1\n51.3356\n&lt;.0001\n\n\n2\n行评分均值差异\n3\n148.8630\n&lt;.0001\n\n\n3\n一般关联\n6\n213.1248\n&lt;.0001\n\n\n\n\n\n\n总样本大小 = 5801",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "14-SAS 卡方检验2-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-2.html#单向有序资料",
    "href": "Guide/SAS/25-06-06-chi2-2.html#单向有序资料",
    "title": "14-SAS 卡方检验2-四格表资料卡方检验",
    "section": "3 单向有序资料",
    "text": "3 单向有序资料\n单向有序资料表示行变量是名义变量,列变量是有序变量,如疗效分为治愈、显效、好转、无效等。\nSAS对列变量为有序变量的资料,将列变量的各水平依次进行评分,再比较行变量的各水平间的平均得分的差别是否有统计学意义。\n对行变量为有序变量的资料则按双向有序资料进行处理。\n\n3.1 示例\n\n\n\n地区\n轻度（1）\n中度（2）\n较重度（3）\n严重（4）\n合计\n\n\n\n\n城市（1）\n2211\n949\n296\n71\n3527\n\n\n农村（2）\n670\n330\n115\n52\n1167\n\n\n合计\n2881\n1279\n411\n123\n4694\n\n\n\n\n\n代码\n%%SAS\n/*程序14-2*/\ndata chi14_2;\n    do r =1 to 2;\n        do c =1 to 4;\n            input f @@;\n            output;\n        end;\n    end;\ndatalines;\n2211 949 296 71 670 330 115 52\n;\nproe freq;\n    tables r*c/cmh nopercent nocol;\n    weight f;\nrun;\n\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 37228\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nFREQ 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n频数\n\n\n行百分比\n\n\n\n\n\n\n\n\nr-c表\n\n\nr\nc\n\n\n1\n2\n3\n4\n合计\n\n\n\n\n1\n\n\n2211\n\n\n62.69\n\n\n\n\n949\n\n\n26.91\n\n\n\n\n296\n\n\n8.39\n\n\n\n\n71\n\n\n2.01\n\n\n\n\n3527\n\n\n \n\n\n\n\n2\n\n\n670\n\n\n57.41\n\n\n\n\n330\n\n\n28.28\n\n\n\n\n115\n\n\n9.85\n\n\n\n\n52\n\n\n4.46\n\n\n\n\n1167\n\n\n \n\n\n\n\n合计\n\n\n2881\n\n\n\n\n1279\n\n\n\n\n411\n\n\n\n\n123\n\n\n\n\n4694\n\n\n\n\n\n\n\n\n\n\n\n“c-r”的汇总统计量\n\n\n\n\n\n\nCochran-Mantel-Haenszel 统计量 （基于表评分）\n\n\n统计量\n备择假设\n自由度\n值\n概率\n\n\n\n\n1\n非零相关\n1\n20.3626\n&lt;.0001\n\n\n2\n行评分均值差异\n1\n20.3626\n&lt;.0001\n\n\n3\n一般关联\n3\n26.6849\n&lt;.0001\n\n\n\n\n\n\n总样本大小 = 4694\n\n\n\n\n\n\n\n\n\n3.2 结果说明\n选择 2   行评分均值差异 1   20.3626 &lt;.0001 作为此处单向有序资料卡方检验的结果，说明差异具有统计学意义。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "14-SAS 卡方检验2-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-2.html#双向有序资料",
    "href": "Guide/SAS/25-06-06-chi2-2.html#双向有序资料",
    "title": "14-SAS 卡方检验2-四格表资料卡方检验",
    "section": "4 双向有序资料",
    "text": "4 双向有序资料\n双向有序资料表示行变量和列变量都是有序变量,SAS在处理这种资料时,对行变量和列变量分别依次进行评分,然后检验这两个变量之间是否有相关关系。\n\n4.1 示例\n\n\n\n年龄\n-\n+\n++\n+++\n合计\n\n\n\n\n20~\n70\n22\n4\n2\n98\n\n\n30~\n27\n24\n9\n3\n63\n\n\n40~\n16\n23\n13\n7\n59\n\n\n≥50\n9\n20\n15\n14\n58\n\n\n合计\n122\n89\n41\n26\n278\n\n\n\n\n\n代码\n%%SAS\n/*程序14-3*/\ndata chi14_3;\n    do r = 1 to 4;\n        do c =1 to 4;\n            input f@@;\n            output;\n        end;\n    end;\ndatalines;\n70 22 4 2 27 24 9 3 16 23 13 7 9 20 15 14\n;\nproc freq;\n    weight f;\n    tables r*c/cmh nopercent norow nocol;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nFREQ 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n频数\n\n\n\n\n\n\n\n\nr-c表\n\n\nr\nc\n\n\n1\n2\n3\n4\n合计\n\n\n\n\n1\n\n\n70\n\n\n\n\n22\n\n\n\n\n4\n\n\n\n\n2\n\n\n\n\n98\n\n\n\n\n2\n\n\n27\n\n\n\n\n24\n\n\n\n\n9\n\n\n\n\n3\n\n\n\n\n63\n\n\n\n\n3\n\n\n16\n\n\n\n\n23\n\n\n\n\n13\n\n\n\n\n7\n\n\n\n\n59\n\n\n\n\n4\n\n\n9\n\n\n\n\n20\n\n\n\n\n15\n\n\n\n\n14\n\n\n\n\n58\n\n\n\n\n合计\n\n\n122\n\n\n\n\n89\n\n\n\n\n41\n\n\n\n\n26\n\n\n\n\n278\n\n\n\n\n\n\n\n\n\n\n\n“c-r”的汇总统计量\n\n\n\n\n\n\nCochran-Mantel-Haenszel 统计量 （基于表评分）\n\n\n统计量\n备择假设\n自由度\n值\n概率\n\n\n\n\n1\n非零相关\n1\n63.3895\n&lt;.0001\n\n\n2\n行评分均值差异\n3\n63.4505\n&lt;.0001\n\n\n3\n一般关联\n9\n71.1755\n&lt;.0001\n\n\n\n\n\n\n总样本大小 = 278\n\n\n\n\n\n\n\n\n\n4.2 结果说明\n这里是双向有序资料，所以选用 1   非零相关    1   63.3895 &lt;.0001 作为这里的结果，\\(\\chi^2_{CMH}=63.3895,P&lt;0.0001\\)，差异有统计学意义，说明年龄和严重等级有相关关系。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "14-SAS 卡方检验2-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-2.html#分层资料",
    "href": "Guide/SAS/25-06-06-chi2-2.html#分层资料",
    "title": "14-SAS 卡方检验2-四格表资料卡方检验",
    "section": "5 分层资料",
    "text": "5 分层资料\nCMH 统计量也可用于多层行x列表资料的卡方检验,即按一个或多个因素分层后,研究行变量和列变量间的联系。可通过控制分层变量的影响后,检验研究行变量和列变量的关系。\n\n5.1 示例\n\n\n\n性别\n组别\n使用(c=1)\n未使用(c=2)\n\n\n\n\n男(sex=1)\n病例组(r=1)\n5\n36\n\n\n男\n对照组(r=2)\n33\n645\n\n\n女(sex=2)\n病例组(r=1)\n10\n58\n\n\n女\n对照组(r=2)\n19\n518\n\n\n\n\n\n代码\n%%SAS\n/*程序14-4*/\ndata chi14_4;\n    do c=1 to 2;\n        do sex=1 to 2;\n            do r=1 to 2;\n                input f@@;\n                output;\n            end;\n        end;\n    end;\ndatalines;\n5 33 10 19 36 645 58 518\n;\nproc freq;\n    weight f;\n    tables sex*r*c/cmh nopercent norow nocol;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nFREQ 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n频数\n\n\n\n\n\n\n\n\nr-c的表 1\n\n\n控制“ sex=1”\n\n\nr\nc\n\n\n1\n2\n合计\n\n\n\n\n1\n\n\n5\n\n\n\n\n36\n\n\n\n\n41\n\n\n\n\n2\n\n\n33\n\n\n\n\n645\n\n\n\n\n678\n\n\n\n\n合计\n\n\n38\n\n\n\n\n681\n\n\n\n\n719\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n频数\n\n\n\n\n\n\n\n\nr-c的表 2\n\n\n控制“ sex=2”\n\n\nr\nc\n\n\n1\n2\n合计\n\n\n\n\n1\n\n\n10\n\n\n\n\n58\n\n\n\n\n68\n\n\n\n\n2\n\n\n19\n\n\n\n\n518\n\n\n\n\n537\n\n\n\n\n合计\n\n\n29\n\n\n\n\n576\n\n\n\n\n605\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\nFREQ 过程\n\n\n“c-r”的汇总统计量控制“sex”\n\n\n\n\n\n\nCochran-Mantel-Haenszel 统计量 （基于表评分）\n\n\n统计量\n备择假设\n自由度\n值\n概率\n\n\n\n\n1\n非零相关\n1\n19.5130\n&lt;.0001\n\n\n2\n行评分均值差异\n1\n19.5130\n&lt;.0001\n\n\n3\n一般关联\n1\n19.5130\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n普通优比和相对风险\n\n\n统计量\n方法\n值\n95% 置信限\n\n\n\n\n优比\nMantel-Haenszel\n3.7560\n2.0158\n6.9983\n\n\n \nLogit\n3.7775\n2.0114\n7.0941\n\n\n相对风险（第 1 列）\nMantel-Haenszel\n3.3831\n1.9403\n5.8987\n\n\n \nLogit\n3.3953\n1.9390\n5.9452\n\n\n相对风险（第 2 列）\nMantel-Haenszel\n0.8992\n0.8336\n0.9700\n\n\n \nLogit\n0.9007\n0.8351\n0.9713\n\n\n\n\n\n\n\n\n\n\n优比齐性的\nBreslow-Day 检验\n\n\n\n\n卡方\n0.7029\n\n\n自由度\n1\n\n\nPr &gt; 卡方\n0.4018\n\n\n\n\n\n\n总样本大小 = 1324\n\n\n\n\n\n\n\n\n\n5.2 结果说明\n整个结果共分三个部分：\n\n第一部分输出的是输出两张列联表,分别是 sex=1 和 sex=2 时的二维列联表。\n第二部分输出在控制分层变量sex后总的检验 Cochran-Mantel-Haenszel的统计结果,由于行变量和列变量都只有两个,可以看作双向无序资料,三种结果是一致的,结果为: \\(\\chi^2_{CMH}=19.5130\\) ,所对应的 P&lt;0.0001,说明控制了性别因素后,使用别嘌呤醇与发生药物性皮疹是有关系的。\n第三部分是一些相对数统计量的比值,第一个是病例对照(优比),即病例-对照研究中的优势比(又称比值比,OR),有两个统计结果,个是用 Mantel-Haenszel 方法计算的值,另一个是用 Logit 方法计算的值,在病例-对照研究中,优势比是非常重要的指标,本例说明使用别嘌呤醇发生药物性皮疹的危险性是不使用该药危险性的4倍(3.7560或3.7775)。后面两个是 OR的 95%置信区间,本例为(2.015 8,6.9983)或(2.011 4,7.094 1),其结论与 CMH 结论相同。下面两个都是队列研究所用的统计指标(相对危险度,RR),区别在于分别将第1列或第2列作为发病情况。\n第四部分为对优势比进行 Breslow-Day 齐性检验,本例 \\(\\chi^2=0.7029,P=0.401 8&gt;0.05\\),说明不同分层之间的优势比具有齐性，可认为它们一致。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "14-SAS 卡方检验2-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-2.html#proc-freq-过程常用选项与语句详解",
    "href": "Guide/SAS/25-06-06-chi2-2.html#proc-freq-过程常用选项与语句详解",
    "title": "14-SAS 卡方检验2-四格表资料卡方检验",
    "section": "6 PROC FREQ 过程常用选项与语句详解",
    "text": "6 PROC FREQ 过程常用选项与语句详解\n\n6.1 PROC FREQ 过程的基本格式\nproc freq &lt;选项&gt;;\n    by 变量名1 &lt;变量名2&gt;...;\n    tables 表格定义项 &lt;选项&gt;;\n    output out=输出数据集 &lt;关键词&gt;;\n    test &lt;关键词&gt;;\n    weight 权重变量;\nrun;\n\n各语句说明：\n\nby：用于对分组变量进行分层分析，需先排序。\ntables：用于指定交叉表分析的变量及表格结构，是最常用的语句。\noutput：将分析结果输出为数据集，便于后续处理。\ntest：用于指定统计检验，如卡方检验、Fisher精确检验等。\nweight：指定频数变量，适用于加权样本数据。\n\n\n\n\n6.2 TABLES 语句常用选项详解\nTABLES 是 PROC FREQ 中的核心语句之一，除了基础的交叉表功能，还可通过附加选项实现更复杂的统计分析：\n\nriskdiff\n用于二分类四格表（2×2表）中，计算两组间的事件发生率差异（即风险差 Risk Difference, RD）及其置信区间。\n提供的指标包括：\n\n每行的事件发生率（即比例或风险）\n风险差（两行比例之差）\n渐近标准误（依据正态分布近似）\n渐近 95% 置信区间\n精确 95% 置信区间（基于二项分布）\n\n应用场景：队列研究、临床试验中对比暴露组与对照组的疾病发生率。\ntables exposure*outcome / riskdiff;\n\n\nrelrisk\n用于四格表中，计算相对风险（Relative Risk, RR）或比值比（Odds Ratio, OR），及其置信区间。\n\n若数据源自队列研究：计算 RR（相对风险）\n若数据源自病例对照研究：计算 OR（比值比）\n\n返回值包括：\n\nRR 或 OR 的点估计\n渐近 95% 置信区间\n\n使用示例：\ntables exposure*disease / relrisk;\n\n\n\n6.3 其他常用选项：\n\nchisq：卡方检验（包括Pearson卡方、Yates校正和Fisher精确检验）\nnocol / norow / nopercent：抑制列百分比、行百分比、总百分比的显示\nexpected：显示期望频数（用于判断适用卡方检验的前提）\nmeasures：输出列联表的各种关联性度量指标，如 Phi、Cramer’s V、Contingency Coefficient 等\n\n\n\n6.4 补充说明：OUTPUT 与 TEST 语句\n\noutput 语句常用关键字：\n\nout=：指定输出结果数据集\nn nlevel：输出各变量频数和水平数\npct：输出比例\nchisq：输出卡方值\nrelrisk：输出相对风险及置信区间\n\n例如：\noutput out=result_freq chisq relrisk;\n\n\ntest 语句常用于非参数检验，如：\ntest chisq;\n\n\n\n\n6.5 示例代码\nproc freq data=clinic;\n    tables treatment*response / chisq riskdiff relrisk expected;\n    output out=freq_results chisq relrisk;\nrun;\n\n\n\n6.6 适用场景总结\n\n\n\n场景\n推荐选项\n\n\n\n\n检验两个分类变量是否独立\nchisq\n\n\n计算列联表中比例差异（风险差）\nriskdiff\n\n\n计算 RR/OR 及其区间\nrelrisk\n\n\n临床或流行病学数据分层分析\nby, tables",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "14-SAS 卡方检验2-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-1.html",
    "href": "Guide/SAS/25-06-06-chi2-1.html",
    "title": "13-SAS 卡方检验1-四格表资料卡方检验",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "13-SAS 卡方检验1-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-1.html#chi2概述",
    "href": "Guide/SAS/25-06-06-chi2-1.html#chi2概述",
    "title": "13-SAS 卡方检验1-四格表资料卡方检验",
    "section": "1 \\(\\chi^2\\)概述",
    "text": "1 \\(\\chi^2\\)概述\n卡方检验是一种基于卡方分布的假设检验方法，主要用于检验两个离散变量之间是否存在关联（独立性检验）或理论分布与观测分布是否一致（适配度检验）。其基本思想是比较实际观测值与期望值之间的偏离程度，卡方值越小，表示实际值越接近期望值。\n关于卡方检验的具体论述可查看本网站的 Learn/Basic 下的 卡方检验的章节。\n在 SAS 中卡方检验主要使用 freq 语句来完成。freq 语句主要用于产生一维或多维的频数表和列联表,并对列联表资料计算各种统计量。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "13-SAS 卡方检验1-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-1.html#基本公式",
    "href": "Guide/SAS/25-06-06-chi2-1.html#基本公式",
    "title": "13-SAS 卡方检验1-四格表资料卡方检验",
    "section": "2 基本公式",
    "text": "2 基本公式\n\n\n代码\n%%SAS\n/*程序13-1*/\ndata chi13_1;\n    set 'Data\\prg8_1.sas7bdat';  /* 从已有数据集读取 */\nrun;\n\nproc freq;\n    tables group*LDL/chisq expected;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nFREQ 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n频数\n\n\n期望\n\n\n百分比\n\n\n行百分比\n\n\n列百分比\n\n\n\n\n\n\n\n\ngroup-LDL表\n\n\ngroup\nLDL\n\n\n1\n2\n合计\n\n\n\n\n1\n\n\n41\n\n\n25.166\n\n\n16.60\n\n\n48.81\n\n\n55.41\n\n\n\n\n43\n\n\n58.834\n\n\n17.41\n\n\n51.19\n\n\n24.86\n\n\n\n\n84\n\n\n \n\n\n34.01\n\n\n \n\n\n \n\n\n\n\n2\n\n\n33\n\n\n48.834\n\n\n13.36\n\n\n20.25\n\n\n44.59\n\n\n\n\n130\n\n\n114.17\n\n\n52.63\n\n\n79.75\n\n\n75.14\n\n\n\n\n163\n\n\n \n\n\n65.99\n\n\n \n\n\n \n\n\n\n\n合计\n\n\n74\n\n\n29.96\n\n\n\n\n173\n\n\n70.04\n\n\n\n\n247\n\n\n100.00\n\n\n\n\n\n\n\n\n\n\n\n表“LDL-group”的统计量\n\n\n\n\n\n\n统计量\n自由度\n值\n概率\n\n\n\n\n卡方\n1\n21.5540\n&lt;.0001\n\n\n似然比卡方检验\n1\n20.9621\n&lt;.0001\n\n\n连续调整卡方\n1\n20.2142\n&lt;.0001\n\n\nMantel-Haenszel 卡方\n1\n21.4667\n&lt;.0001\n\n\nPhi 系数\n \n0.2954\n \n\n\n列联系数\n \n0.2833\n \n\n\nCramer V\n \n0.2954\n \n\n\n\n\n\n\n\n\n\n\nFisher 的精确检验\n\n\n\n\n单元格 (1,1) 频数 (F)\n41\n\n\n左侧 Pr &lt;= F\n1.0000\n\n\n右侧 Pr &gt;= F\n&lt;.0001\n\n\n \n \n\n\n表概率 (P)\n&lt;.0001\n\n\n双侧 Pr &lt;= P\n&lt;.0001\n\n\n\n\n\n\n样本大小 = 247\n\n\n\n\n\n\n\n\n2.1 结果说明\n\n表“LDL-group”的统计量 部分为，检验的结果及3个分析行列变量的关联性统计量。首先是检验的结果,其中第一列列出了各种进行，检验的方法,从上至下分别为卡方(基本公式计算法)似然比法连续性校正法、Mantel-Haenszel法,第二列为自由度,第三列为各种方法计算所得的值,第四列为x值所对应的概率值(P值)。根据不同的资料可选择不同的结果。本例由于总频数&gt;40所有的理论频数均&gt;5,所以选用基本公式法,则 \\(\\chi^2=21.5540,P&lt;0.0001\\),说明两组异常率差别有统计学意义,冠心病患者的低密度脂蛋白异常率高 于正常人。\n检验结果的下方列出的3个分析行列变量的关联性统计量,分别为:Phi 系数、列联系数和 Cramer V 统计量,它们的值都在 -1 和 1 之间,绝对值越大说明关系越密切。\nFisher精确概率法结果,输出结果包括了四格表中第1行第1列所在单元格的实际频数及左侧、右侧概率、表概率和双侧概率其中左侧概率为四格表边缘合计数固定条件下表中第1行第1列所在单元格的实际频数(本例为 41)不断减少而至不可能再减少所构成的所有四格表(包括41)的概率之和,为1.0000。右侧慨率则为所有第1行第1列单元格的实际频数 ≥99 的所有四格表的概率之和,其&lt;0.0001。表概率即为当第1行第1列单元格的实际频数等于 41 时四格表的概率,其 &lt;0.0001。双侧概率则以本四格表出现的概率为 P,而将 \\(P≤P\\) ,的所有四格表的概率相加所得,本例为&lt;0.0001,结论与 \\(\\chi^2\\) 检验一致。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "13-SAS 卡方检验1-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-1.html#连续性校正公式",
    "href": "Guide/SAS/25-06-06-chi2-1.html#连续性校正公式",
    "title": "13-SAS 卡方检验1-四格表资料卡方检验",
    "section": "3 连续性校正公式",
    "text": "3 连续性校正公式\n当 \\(1≤T&lt;5\\) ,且 \\(n≥40\\) 时,需选用连续性校正公式计算的结果.\n\n\n代码\n%%SAS\n/*程序13-2*/\ndata chi13_2;\n    set 'Data\\prg8_2.sas7bdat';  /* 从已有数据集读取 */\nrun;\n\nproc freq;\n    tables group*eff/chisq expected norow nocol nopercent;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nFREQ 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n频数\n\n\n期望\n\n\n\n\n\n\n\n\ngroup-eff表\n\n\ngroup\neff\n\n\n1\n2\n合计\n\n\n\n\n1\n\n\n31\n\n\n28.5\n\n\n\n\n1\n\n\n3.5\n\n\n\n\n32\n\n\n \n\n\n\n\n2\n\n\n26\n\n\n28.5\n\n\n\n\n6\n\n\n3.5\n\n\n\n\n32\n\n\n \n\n\n\n\n合计\n\n\n57\n\n\n\n\n7\n\n\n\n\n64\n\n\n\n\n\n\n\n\n\n\n\n表“eff-group”的统计量\n\n\n\n\n\n\n统计量\n自由度\n值\n概率\n\n\n\n\n卡方\n1\n4.0100\n0.0452\n\n\n似然比卡方检验\n1\n4.4016\n0.0359\n\n\n连续调整卡方\n1\n2.5664\n0.1092\n\n\nMantel-Haenszel 卡方\n1\n3.9474\n0.0469\n\n\nPhi 系数\n \n0.2503\n \n\n\n列联系数\n \n0.2428\n \n\n\nCramer V\n \n0.2503\n \n\n\n\nWARNING: 50% 的单元格包含小于\n5 的期望计数。卡方不是有效的检验。\n\n\n\n\n\n\n\n\n\n\n\nFisher 的精确检验\n\n\n\n\n单元格 (1,1) 频数 (F)\n31\n\n\n左侧 Pr &lt;= F\n0.9946\n\n\n右侧 Pr &gt;= F\n0.0521\n\n\n \n \n\n\n表概率 (P)\n0.0467\n\n\n双侧 Pr &lt;= P\n0.1042\n\n\n\n\n\n\n样本大小 = 64",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "13-SAS 卡方检验1-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-1.html#配对四格表资料的chi2检验",
    "href": "Guide/SAS/25-06-06-chi2-1.html#配对四格表资料的chi2检验",
    "title": "13-SAS 卡方检验1-四格表资料卡方检验",
    "section": "4 配对四格表资料的\\(\\chi^2\\)检验",
    "text": "4 配对四格表资料的\\(\\chi^2\\)检验\n配对四格表的主要目的是为了判断行列变量的一致性。\n\n\n代码\n%%SAS\n/*程序13-3*/\ndata chi13_3;\n    set 'Data\\prg8_3.sas7bdat';  /* 从已有数据集读取 */\nrun;\n\nproc freq;\n    tables A*B/agree;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nFREQ 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n频数\n\n\n百分比\n\n\n行百分比\n\n\n列百分比\n\n\n\n\n\n\n\n\nA-B表\n\n\nA\nB\n\n\n1\n2\n合计\n\n\n\n\n1\n\n\n15\n\n\n30.00\n\n\n62.50\n\n\n93.75\n\n\n\n\n9\n\n\n18.00\n\n\n37.50\n\n\n26.47\n\n\n\n\n24\n\n\n48.00\n\n\n \n\n\n \n\n\n\n\n2\n\n\n1\n\n\n2.00\n\n\n3.85\n\n\n6.25\n\n\n\n\n25\n\n\n50.00\n\n\n96.15\n\n\n73.53\n\n\n\n\n26\n\n\n52.00\n\n\n \n\n\n \n\n\n\n\n合计\n\n\n16\n\n\n32.00\n\n\n\n\n34\n\n\n68.00\n\n\n\n\n50\n\n\n100.00\n\n\n\n\n\n\n\n\n\n\n\n表“B-A”的统计量\n\n\n\n\n\n\nMcNemar 检验\n\n\n卡方\n自由度\nPr &gt; 卡方\n\n\n\n\n6.4000\n1\n0.0114\n\n\n\n\n\n\n\n\n\n\n简单 Kappa 系数\n\n\n估计\n标准\n误差\n95% 置信限\n\n\n\n\n0.5942\n0.1085\n0.3815\n0.8068\n\n\n\n\n\n\n样本大小 = 50\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1 结果说明\n\n第一部分是 McNemar 检验的统计量、自由度和P值,本例为x=6.400 0,P=0.011 4&lt;0.05,说明两种培养基的阳性率的差异有统计学意义,甲培养基的阳性率高于乙培养基。\n第二部分是一致性检验的Kappa值、渐近标准误和 95% 置信区间,本例 Kappa 值为 0.594 2,其 95% 置信区间为(0.3815,0.8068)。\n根据经验 Kappa≥0.75 表明两者一致性较好,0.4≤Kappa&lt;0.75 表明一致性一般,Kappa&lt;0.4 则表明一致性较差。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "13-SAS 卡方检验1-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-06-chi2-1.html#fisher确切概率法",
    "href": "Guide/SAS/25-06-06-chi2-1.html#fisher确切概率法",
    "title": "13-SAS 卡方检验1-四格表资料卡方检验",
    "section": "5 Fisher确切概率法",
    "text": "5 Fisher确切概率法\n当四格表任一格子理论频数 T&lt;1,或n&lt;40时,需使用连续性校正公式（Fisher确切概率法）计算的结果。\n\n\n代码\n%%SAS\n/*程序13-4*/\ndata chi13_4;\n    set 'Data\\prg8_4.sas7bdat';  /* 从已有数据集读取 */\nrun;\n\nproc freq;\n    tables group*eff/exact;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nFREQ 过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n频数\n\n\n百分比\n\n\n行百分比\n\n\n列百分比\n\n\n\n\n\n\n\n\ngroup-eff表\n\n\ngroup\neff\n\n\n1\n2\n合计\n\n\n\n\n1\n\n\n17\n\n\n44.74\n\n\n94.44\n\n\n47.22\n\n\n\n\n1\n\n\n2.63\n\n\n5.56\n\n\n50.00\n\n\n\n\n18\n\n\n47.37\n\n\n \n\n\n \n\n\n\n\n2\n\n\n19\n\n\n50.00\n\n\n95.00\n\n\n52.78\n\n\n\n\n1\n\n\n2.63\n\n\n5.00\n\n\n50.00\n\n\n\n\n20\n\n\n52.63\n\n\n \n\n\n \n\n\n\n\n合计\n\n\n36\n\n\n94.74\n\n\n\n\n2\n\n\n5.26\n\n\n\n\n38\n\n\n100.00\n\n\n\n\n\n\n\n\n\n\n\n表“eff-group”的统计量\n\n\n\n\n\n\n统计量\n自由度\n值\n概率\n\n\n\n\n卡方\n1\n0.0059\n0.9390\n\n\n似然比卡方检验\n1\n0.0059\n0.9390\n\n\n连续调整卡方\n1\n0.0000\n1.0000\n\n\nMantel-Haenszel 卡方\n1\n0.0057\n0.9398\n\n\nPhi 系数\n \n-0.0124\n \n\n\n列联系数\n \n0.0124\n \n\n\nCramer V\n \n-0.0124\n \n\n\n\nWARNING: 50% 的单元格包含小于\n5 的期望计数。卡方不是有效的检验。\n\n\n\n\n\n\n\n\n\n\n\nFisher 的精确检验\n\n\n\n\n单元格 (1,1) 频数 (F)\n17\n\n\n左侧 Pr &lt;= F\n0.7297\n\n\n右侧 Pr &gt;= F\n0.7824\n\n\n \n \n\n\n表概率 (P)\n0.5121\n\n\n双侧 Pr &lt;= P\n1.0000\n\n\n\n\n\n\n样本大小 = 38\n\n\n\n\n\n\n\n\n5.1 结果说明\n这里使用的Fisher确切概率法，所以应该查看 Fisher 的精确检验 中的结果，这里以双侧为例，得到 P 值为 1.0000，P&gt;0.05，即说明尚不能认为两种疗法的的总体有效率有不同。\n\n\n5.2 P=1.0000\n这里的 P=1.0000 有一点反直觉，理论上，Fisher 确切概率法的 P 值范围在 0 到 1 之间，但在某些极端情况下可能出现 P=1 的结果。\n当实际观测数据完全符合零假设（即两组完全无差异）时，所有可能的排列组合中，只有当前观测数据一种情况满足零假设，且其他所有可能的排列均与零假设偏离更大（即概率更低）。此时：\n\n单侧检验：若零假设为 “组 1≤组 2”，而观测数据显示两组率完全相等（如 a/(a+b) = c/(c+d)），则所有偏离零假设的情况（如组 1 &gt; 组 2）的概率之和为 0，因此 P=1（表示 “观测数据与零假设完全一致的概率为 100%”）。\n双侧检验：若考虑两组率相等的情况，且所有偏离零假设的排列（无论是组 1 &gt; 组 2 还是组 1 &lt; 组 2）的概率之和为 0，则 P=1。\n\n注意事项\n\nP=1 的本质：并非表示 “零假设一定成立”，而是在当前样本量和数据下，没有任何证据显示两组存在差异。\n小样本局限性：Fisher 确切概率法适用于小样本，但当样本量较大时，可能因计算量过大而不适用，此时可改用卡方检验或连续性校正卡方检验。\n实际应用场景：P=1 通常出现在两组数据完全一致（如发生率相同）或极端稀疏数据（如事件发生数为 0）的情况下，属于理论上的特殊情况，实际研究中较为罕见。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "13-SAS 卡方检验1-四格表资料卡方检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-07-binomial.html",
    "href": "Guide/SAS/25-06-07-binomial.html",
    "title": "15-SAS 对二项分布的概率计算与假设检验",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "15-SAS 对二项分布的概率计算与假设检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-07-binomial.html#sas-与-binomial",
    "href": "Guide/SAS/25-06-07-binomial.html#sas-与-binomial",
    "title": "15-SAS 对二项分布的概率计算与假设检验",
    "section": "1 SAS 与 Binomial",
    "text": "1 SAS 与 Binomial\n在 SAS 中,与二项分布有关的函数为 probbnm(p,n,r),函数中 p 为某事件的发生概率,n 为样本含量，为阳性事件的例数,该函数可以计算出发生阳性事件的例数从 0 到 r 的累计概率。利用该函数可以对服从二项分布的数据进行概率计算和假设检验。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "15-SAS 对二项分布的概率计算与假设检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-07-binomial.html#阳性事件发生的概率",
    "href": "Guide/SAS/25-06-07-binomial.html#阳性事件发生的概率",
    "title": "15-SAS 对二项分布的概率计算与假设检验",
    "section": "2 阳性事件发生的概率",
    "text": "2 阳性事件发生的概率\n某种医学技能测试的通过率为0.80。今有10例学生参加测试,试分别计算这10例学生中有6、7、8人获得通过的概率。本例 π=0.8,n=10,计算 r=6、7、8的概率.\n\n\n代码\n%%SAS\n/*程序15-1*/\ndata binomial15_1;\n    do r=6 to 8;\n        p1=probbnml(0.8,10,r);\n        p2=probbnml(0.8,10,r-1);\n        p=p1-p2;\n        output;\n    end;\nproc print;\n    var r p;\nrun;\n\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 30048\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\nr\np\n\n\n\n\n1\n6\n0.08808\n\n\n2\n7\n0.20133\n\n\n3\n8\n0.30199",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "15-SAS 对二项分布的概率计算与假设检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-07-binomial.html#总体率的区间估计正态近似法",
    "href": "Guide/SAS/25-06-07-binomial.html#总体率的区间估计正态近似法",
    "title": "15-SAS 对二项分布的概率计算与假设检验",
    "section": "3 总体率的区间估计(正态近似法)",
    "text": "3 总体率的区间估计(正态近似法)\n根据数理统计学的中心极限定理可得,当 n 较大、π 不接近 0 也不接近 1 时,二项分布 \\(B(n,π)\\) 近似正态分布 \\(N[n \\pi,nπ(1-π)]\\),相应的样本率 p 的分布也近似正态分布 \\(N(\\pi,\\sigma_{p}^2)\\)。为此,当 n 较大、p 和 1-p 均不太小,如 np 和 n(1-p) 均&gt;5时,可利用样本率 p 的分布近似正态分布来估计总体率的 1-α 置信区间。\n\n\n代码\n%%SAS\ndata binomial15_2;\n    n=100;\n    p=0.45;\n    sp=sqrt(p*(1-p)/n);\n    u=probit(0.975);\n    usp=u*sp;\n    lclm=p-usp;\n    uclm=p+usp;\nproc print;\n    var n p sp lclm uclm;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\nn\np\nsp\nlclm\nuclm\n\n\n\n\n1\n100\n0.45\n0.049749\n0.35249\n0.54751\n\n\n\n\n\n\n\n\n\n\n\n3.1 程序说明\n数据集中 n 为观察的患者人数 p 为样本率,驴为率的标准误,u 为置信水准为 0.05 时标准正态分布的双侧界值, lclm 为 95% 置信区间的下限,uclm 为 95% 置信区间的上限。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "15-SAS 对二项分布的概率计算与假设检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-07-binomial.html#程序说明",
    "href": "Guide/SAS/25-06-07-binomial.html#程序说明",
    "title": "15-SAS 对二项分布的概率计算与假设检验",
    "section": "3.1 程序说明",
    "text": "3.1 程序说明\n数据集中 n 为观察的患者人数 p 为样本率,驴为率的标准误,u 为置信水准为 0.05 时标准正态分布的双侧界值, lclm 为 95% 置信区间的下限,uclm 为 95% 置信区间的上限。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "15-SAS 对二项分布的概率计算与假设检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-07-binomial.html#示例1",
    "href": "Guide/SAS/25-06-07-binomial.html#示例1",
    "title": "15-SAS 对二项分布的概率计算与假设检验",
    "section": "4.1 示例1",
    "text": "4.1 示例1\n已知输卵管结扎的育龄妇女实施壶腹部-壶腹部吻合术后的受孕率为 0.55。今对 10例输卵管结扎的育龄妇女实施峡部-峡部吻合术,结果有9 人受孕。问实施峡部-峡部吻合术妇女的受孕率是否高于壶腹部-壶腹部吻合术的受孕率?\n本例 π=0.55,n=10,r=9。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "15-SAS 对二项分布的概率计算与假设检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-07-binomial.html#程序说明-1",
    "href": "Guide/SAS/25-06-07-binomial.html#程序说明-1",
    "title": "15-SAS 对二项分布的概率计算与假设检验",
    "section": "4.2 程序说明",
    "text": "4.2 程序说明\n本例为单侧检验,首先用函数计算发生例数≤8的累计概率d,再计算1-d就是发生例数&gt;9的概率。\n\n\n代码\n%%SAS\n/*程序15-3*/\ndata binomial15_3;\n    d= probbnml(0.55,10,8);\n    p= 1-d;\nrun;\nproc print;\n    var p;\nrun;\n\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 11632\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\np\n\n\n\n\n1\n0.023257",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "15-SAS 对二项分布的概率计算与假设检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-07-binomial.html#结果说明",
    "href": "Guide/SAS/25-06-07-binomial.html#结果说明",
    "title": "15-SAS 对二项分布的概率计算与假设检验",
    "section": "4.3 结果说明",
    "text": "4.3 结果说明\n由于 P=0.023 257&lt;0.05,说明样本率与总体率的差别有统计学意义,可认为行峡部-峡部吻合术的受孕率要高于壶腹部-壶腹部吻合术。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "15-SAS 对二项分布的概率计算与假设检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-07-binomial.html#示例2",
    "href": "Guide/SAS/25-06-07-binomial.html#示例2",
    "title": "15-SAS 对二项分布的概率计算与假设检验",
    "section": "4.4 示例2",
    "text": "4.4 示例2\n已知某高校临床医学专业一年级女生100m短跑的达标率为0.70。现在该校一年级的预防医学专业中随机测试了10例女生,有9人达标。问该校这两个专业一年级女生 100m 短跑的达标率是否不同?\n本例 π=0.70,n=10,r=9。\n\n4.4.1 解题\n这是一个小样本单个比例的假设检验问题，可以使用二项分布检验（Binomial test）。\n\n原假设与备择假设\n\n\n\\(H_0\\)：预防医学专业的达标率 = 0.70（与临床医学相同）\n\\(H_1\\)：预防医学专业的达标率 ≠ 0.70（与临床医学不同）\n\n这是一个双侧检验（two-sided test）。\n\n检验统计量\n\n我们用 二项分布 进行精确检验：\n已知：\n\n样本容量 \\(n = 10\\)\n达标人数 \\(r = 9\\)\n总体比例 \\(\\pi = 0.70\\)\n\n我们计算在 \\(n=10\\), \\(\\pi=0.70\\) 时，得到“像 9 人或比 9 人更极端”的概率（双尾 p 值）：\n\n二项分布计算公式\n\n单个值的概率为：\n\\[\nP(R = r) = \\binom{n}{r} \\cdot \\pi^r \\cdot (1 - \\pi)^{n - r}\n\\]\n具体来说：\n\\[\nP(R = 9) = \\binom{10}{9} \\cdot 0.7^9 \\cdot 0.3^1\n\\]\n\\[\nP(R = 10) = \\binom{10}{10} \\cdot 0.7^{10}\n\\]\n\n双尾 p 值的计算\n\n我们要计算：\n\\[\np = P(R \\geq 9) + P(R \\leq x)，\n\\]\n其中 \\(P(R \\leq x)\\) 使得 \\(P(R \\leq x) \\leq P(R \\geq 9)\\)。我们可以查二项分布表或用软件计算。\n\n用 R 语言求解示例\n\n# 双侧二项检验\nbinom.test(x = 9, n = 10, p = 0.70, alternative = \"two.sided\")\n\n用 Pytho 求解示例\n\nbinomtest(k=9, n=10, p=0.70, alternative='two-sided')\n具体运行代码及结果如下所示。\n\n\n4.4.2 总结公式\n\n\\(H_0: p = 0.70\\)\n使用二项分布：\n\\[\nP(r) = \\binom{n}{r} \\cdot p^r \\cdot (1 - p)^{n - r}\n\\]\n双侧 p 值为：\n\\[\np = P(X \\geq r) + P(X \\leq x), \\text{ 使得 } P(X \\leq x) \\leq P(X \\geq r)\n\\]\n\n\n\n代码\n%%SAS\n/*程序15-4*/\ndata binomial15_4;\n    p01=probbnml(0.7,10,9);\n    p02=probbnml(0.7,10,8);\n    p0=p01-p02;\n    do i=0 to 10;\n        p11=probbnml(0.7,10,i);\n        p12=probbnml(0.7,10,i-1);\n    pl=p11-p12;\n        if i=0 then pl=p11;\n    if pl&lt;=p0 then output;\n    end;\nrun;\nproc means sum;\n    var pl;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: pl\n\n\n总和\n\n\n\n\n0.2995767\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.3 程序说明\n首先用函数计算发生例数≤9的累计概率 p01,以及发生例数&lt;8的累计概率 p02，p0 就是发生例数 =9 的概率,由于本例是双侧检验,还需要分别计算发生例数=i(i=0,1，…,10)的概率,考虑比发生例数 =9更背离无效假设(即两个专业达标率相同)的事件,即满足p1≤p0,计算这些事件的概率之和,所得即为无效假设成立的概率。\n\n\n4.4.4 结果说明\n由于 P=0.299 576 7&gt;0.05,说明尚不能认为两个样本率的差别有统计学意义,即不能认为两个专业一年级女生 100m 短跑的达标率不同\n\n\n代码\n#程序15-5\nfrom scipy.stats import binomtest\n\n# 参数设置\nr = 9        # 达标人数\nn = 10       # 总人数\np0 = 0.70    # 临床医学专业达标率\n\n# 执行双侧二项检验\nresult = binomtest(k=r, n=n, p=p0, alternative='two-sided')\n\n# 输出结果\nprint(\"p值 =\", result.pvalue)\nprint(\"是否显著差异（alpha=0.05）:\", \"显著\" if result.pvalue &lt; 0.05 else \"不显著\")\n\n\np值 = 0.2995766784999999\n是否显著差异（alpha=0.05）: 不显著",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "15-SAS 对二项分布的概率计算与假设检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-07-binomial.html#示例",
    "href": "Guide/SAS/25-06-07-binomial.html#示例",
    "title": "15-SAS 对二项分布的概率计算与假设检验",
    "section": "5.1 示例",
    "text": "5.1 示例\n已知某疾病采用常规治疗的治愈率约为45%。现随机抽取180例该疾病患者改用新的治疗方法进行治疗,治愈117人。问新治疗方法是否比常规治疗方法的效果好?\n本例 π=0.45,n=180,x=117。\n\n5.1.1 求解\n使用正态近似法（Z 检验对单个比例进行假设检验的公式如下：\n\n假设检验设定\n\n我们要检验的原假设和备择假设是：\n\n\\(H_0: p = p_0\\)（治愈率与常规治疗相同，即 0.45）\n\\(H_1: p &gt; p_0\\)（新治疗方法更好，即治愈率更高）\n\n这是一个右尾单侧检验。\n\n正态近似检验的统计量公式\n\n当样本量较大时（np 和 n(1−p) 都 ≥ 5），可以用正态分布近似二项分布。检验统计量为：\n\\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\n\\]\n其中：\n\n\\(\\hat{p} = \\frac{x}{n}\\)：样本比例（新疗法下的治愈率）\n\\(p_0\\)：原假设下的比例（常规疗法治愈率）\n\\(n\\)：样本容量\n\\(x\\)：样本中成功次数\n\n\n代入数据计算\n\n已知：\n\n\\(x = 117\\)\n\\(n = 180\\)\n\\(\\hat{p} = \\frac{117}{180} = 0.65\\)\n\\(p_0 = 0.45\\)\n\n计算标准误差（SE）：\n\\[\nSE = \\sqrt{\\frac{0.45 \\times (1 - 0.45)}{180}} = \\sqrt{\\frac{0.2475}{180}} \\approx 0.03713\n\\]\n计算Z值：\n\\[\nZ = \\frac{0.65 - 0.45}{0.03713} \\approx \\frac{0.20}{0.03713} \\approx 5.39\n\\]\n\n计算P值，查表或使用函数计算。\n\n\n\n代码\n%%SAS\n/*程序15-6*/\ndata binomial15_6;\n    n=180;\n    x=117;\n    pai= 0.45;\n    p = x/n;\n    u = (p-pai)/sqrt(pai*(1-pai)/n);\n    pro = (1-probnorm(abs(u)))*2;\nrun;\nproc print;\n    var u pro;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\nu\npro\n\n\n\n\n1\n5.39360\n6.906E-8\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 程序说明\n数据集中 n 为样本例数,x 为治愈例数,pai为总体率,p 为样本率,u 为检验统计量，pro 为u所对应的概率值。这里用到了标准正态函数 probnorm。\n\n\n5.1.3 结果说明\n“检验的检验统计量的值为 5.393 60,所对应的 P值为 6 906x10-,远远&lt;0.05,说明样本率和总体率之间的差异有统计学意义,可以认为新治疗方法比常规疗法的效果好",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "15-SAS 对二项分布的概率计算与假设检验"
    ]
  },
  {
    "objectID": "Guide/R/25-06-09-ggplot2-base.html",
    "href": "Guide/R/25-06-09-ggplot2-base.html",
    "title": "02-ggplot2 绘图基础",
    "section": "",
    "text": "1 实践参考\nR 可视化的内容主要来源于 [R Graphics Cookbook, 2nd edition](https://r-graphics.org/) 和 [Beautiful-Visualization-with-R](https://github.com/EasyChart/Beautiful-Visualization-with-R)",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Learn/Advance/01-negative-binomial.html",
    "href": "Learn/Advance/01-negative-binomial.html",
    "title": "01-负二项分布",
    "section": "",
    "text": "负二项分布（Negative Binomial Distribution）是离散型概率分布，描述的是：\n\n“在一系列独立的伯努利试验中，直到出现第 kk 次成功为止，所需的失败次数（或总次数）。”\n\n你可以把它看作是二项分布的“逆过程”：\n\n二项分布：给定试验次数 nn，观察“成功”次数；\n负二项分布：给定成功次数 kk，观察要达成这些成功，需要进行多少次试验。",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "01-负二项分布"
    ]
  },
  {
    "objectID": "Learn/Advance/01-negative-binomial.html#基本概念什么是负二项分布",
    "href": "Learn/Advance/01-negative-binomial.html#基本概念什么是负二项分布",
    "title": "01-负二项分布",
    "section": "",
    "text": "负二项分布（Negative Binomial Distribution）是离散型概率分布，描述的是：\n\n“在一系列独立的伯努利试验中，直到出现第 kk 次成功为止，所需的失败次数（或总次数）。”\n\n你可以把它看作是二项分布的“逆过程”：\n\n二项分布：给定试验次数 nn，观察“成功”次数；\n负二项分布：给定成功次数 kk，观察要达成这些成功，需要进行多少次试验。",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "01-负二项分布"
    ]
  },
  {
    "objectID": "Learn/Advance/01-negative-binomial.html#举个例子",
    "href": "Learn/Advance/01-negative-binomial.html#举个例子",
    "title": "01-负二项分布",
    "section": "2 举个例子",
    "text": "2 举个例子\n\n比如你在投篮，命中率是 0.3，你想看看你需要投几次，才能命中 3 个球。这时就用负二项分布。\n\n假设：\n\n每投一次是一次“伯努利试验”（成/败）；\n投中是“成功”，不中是“失败”；\n你想知道，为了投中 3 次，可能要投几次（或者会有几个“失败”）？\n\n这时候就服从负二项分布。",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "01-负二项分布"
    ]
  },
  {
    "objectID": "Learn/Advance/01-negative-binomial.html#负二项分布的定义与公式",
    "href": "Learn/Advance/01-negative-binomial.html#负二项分布的定义与公式",
    "title": "01-负二项分布",
    "section": "3 负二项分布的定义与公式",
    "text": "3 负二项分布的定义与公式\n\n3.1 参数：\n\nk：成功次数（固定的）\np：单次成功的概率\nx：失败次数（我们想知道的随机变量）\n\n\n\n3.2 概率质量函数（PMF）：\n\\(P(X=x)=(x+k−1k−1)pk(1−p)x,x=0,1,2,...P(X = x) = \\binom{x + k - 1}{k - 1} p^k (1-p)^x, \\quad x = 0, 1, 2, ...\\)\n这表示：在出现第 k 次成功之前，有 x 次失败。",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "01-负二项分布"
    ]
  },
  {
    "objectID": "Learn/Advance/01-negative-binomial.html#负二项分布的均值和方差",
    "href": "Learn/Advance/01-negative-binomial.html#负二项分布的均值和方差",
    "title": "01-负二项分布",
    "section": "4 负二项分布的均值和方差",
    "text": "4 负二项分布的均值和方差\n记住两个重要的公式：\n\n期望（均值）：\n\\(μ= \\frac{k(1-p)}{p}\\)\n方差：\n\\(σ^2= \\frac{k(1-p)}{p^2}\\)\n\n它的方差 &gt; 均值，也就是 “过度离散”（overdispersion），这点很重要，是很多统计模型选择它的原因。",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "01-负二项分布"
    ]
  },
  {
    "objectID": "Learn/Advance/01-negative-binomial.html#与二项分布的比较",
    "href": "Learn/Advance/01-negative-binomial.html#与二项分布的比较",
    "title": "01-负二项分布",
    "section": "5 与二项分布的比较",
    "text": "5 与二项分布的比较\n\n\n\n特征\n二项分布\n负二项分布\n\n\n固定参数\n总试验次数 nn\n成功次数 kk\n\n\n随机变量\n成功次数 xx\n失败次数 xx 或试验总次数\n\n\n常用于\n有限次数内成功次数\n达成几次成功需失败多少次\n\n\n期望\nnp\n\\(k(1−p)/p\\)\n\n\n方差\nnp(1−p)\n\\(k(1−p)/p^2\\)\n\n\n\n\n生物统计学：如动物聚集数目建模；\n毒理学试验：比如记录小白鼠死亡个数；\n保险/风险建模：建模事故、疾病发生次数；\n回归分析中的离散因变量建模（如负二项回归），适用于计数数据方差 &gt; 均值的情况。",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "01-负二项分布"
    ]
  },
  {
    "objectID": "Learn/Advance/01-negative-binomial.html#负二项分布的两种形式",
    "href": "Learn/Advance/01-negative-binomial.html#负二项分布的两种形式",
    "title": "01-负二项分布",
    "section": "6 负二项分布的两种形式",
    "text": "6 负二项分布的两种形式\n在统计学里，负二项分布还有两种理解方式：\n\n\n\n\n\n\n\n\n类型\n描述\n用于\n\n\n失败数模型\n固定成功次数 k，求失败数 x\n生物试验、理论建模\n\n\n计数模型（r, μ形式）\n固定均值 μ 和形状参数 k\n回归分析中常用\n\n\n\n第二种形式用的是“均值-方差”形式，更适用于建模。",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "01-负二项分布"
    ]
  },
  {
    "objectID": "Learn/Advance/01-negative-binomial.html#总结记忆法",
    "href": "Learn/Advance/01-negative-binomial.html#总结记忆法",
    "title": "01-负二项分布",
    "section": "7 总结记忆法",
    "text": "7 总结记忆法\n\n“负二项 = 重复试验直到成功k次”\n\n它解决的是：“我得失败几次，才能成功这么多次？”",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "01-负二项分布"
    ]
  },
  {
    "objectID": "Learn/Advance/00-advance.html",
    "href": "Learn/Advance/00-advance.html",
    "title": "00-医学统计学（伪）高级部分",
    "section": "",
    "text": "“高级医学统计学”（Advanced Medical Statistics）是医学统计学中的高阶分支，主要研究和应用复杂统计理论与方法，以解决医学研究中更具挑战性的问题。它通常面向具有一定统计基础和医学背景的研究人员和研究生，是医学科研方法论的重要组成部分。\n\n\n高级医学统计学涵盖以下几类主要内容：\n\n\n\n广义线性模型（GLM）：如 Logistic 回归、Poisson 回归；\n广义估计方程（GEE）：用于相关数据（如重复测量）；\n混合效应模型（Mixed Models）：线性/非线性，适用于多层级数据。\n\n\n\n\n\nCox 比例风险模型：分析事件发生时间；\n竞争风险模型；\n多状态模型；\n加速失效时间模型（AFT）。\n\n\n\n\n\n先验信息的引入；\nMCMC 模拟；\n贝叶斯参数估计与预测。\n\n\n\n\n\n倾向得分匹配（PSM）、加权（IPTW）；\n工具变量（IV）；\n双重稳健估计；\n结构方程模型（SEM）；\n鲁宾因果模型（RCM）与结构因果模型（SCM）。\n\n\n\n\n\nBonferroni、Holm 方法；\nFDR（False Discovery Rate）；\n多重检验在组学/影像/大数据中的应用。\n\n\n\n\n\n多重插补（MI）；\n最大似然估计（MLE）；\n蒙特卡洛模拟。\n\n\n\n\n\n纵向数据分析；\n时间序列分析；\n空间统计分析；\n网络医学统计（如脑连接网络）；\n高维数据统计学（如组学、大数据中的变量选择）。\n\n\n\n\n\n随机对照试验（RCT）；\n自适应设计；\n非劣效设计；\n样本量估计、功效分析。\n\n\n\n\n\n\nR / Python：用于建模、可视化、高维数据分析；\nSAS：广泛用于临床试验数据管理与分析；\nStata：广泛应用于经管类的数据建模与分析，适用于医疗政策评估等内容的应用；\nWinBUGS / JAGS / Stan：贝叶斯建模；\nSPSS：较适用于传统统计分析。\n\n\n\n\n高级医学统计学的应用包括但不限于：\n\n复杂临床试验设计与分析；\n公共卫生大数据分析；\n预测模型开发（如疾病风险模型）；\n基因组学、蛋白质组学等组学研究；\n医疗政策评估；\n医学人工智能建模（统计学习、机器学习的融合）。\n\n\n\n\n\n[因果推断：从概念到实践](https://github.com/xieliaing/CausalInferenceIntro)",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "00-医学统计学（伪）高级部分"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-10-poisson-reg.html",
    "href": "Guide/SAS/25-06-10-poisson-reg.html",
    "title": "19-SAS Poisson 回归与负二项回归",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic\n\n\n\n1 什么是 Poisson 回归\nPossion分布是一种描述和分析稀有事件发生次数的概率分析方法。泊松分布是偏态分布，线性回归不能解决相关问题，一般推荐使用Poisson回归(Poisson Regression)。Poisson回归主要用于分析服从Poisson分布的因变量与影响其取值的自变量之间变化关系的一种模型，即单位时间(或空间)内某稀有事件发生数的影响因素分析，如某罕见疾病的发病率的影响因素分析。\n医学研究中有不少现象可使用泊松回归进行分析，对于以人群为基础的稀有疾病如肿瘤或卫生事件等资料,宜用 Poisson 回归分析，比如对浅表性胃炎病人长期随访一段时间后的胃癌发生数。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "19-SAS Poisson 回归与负二项回归"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-09-negative-binomial.html",
    "href": "Guide/SAS/25-06-09-negative-binomial.html",
    "title": "17-SAS 与负二项分布",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "17-SAS 与负二项分布"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-09-negative-binomial.html#动差法示例",
    "href": "Guide/SAS/25-06-09-negative-binomial.html#动差法示例",
    "title": "17-SAS 与负二项分布",
    "section": "1.1 动差法示例",
    "text": "1.1 动差法示例\n在研究某种毒物的致死作用时,对 60 只小白鼠进行了显性致死试验,得到数据资料见表。若该样本计数服从负二项分布,试估计其参数 μ 和 k。\n\n\n\n胚胎死亡数\n0\n1\n2\n3\n4\n5\n6\n合计\n\n\n\n\n观察雌鼠数\n30\n14\n8\n4\n2\n0\n2\n60\n\n\n\n\n1.1.1 动差法原理（Method of Moments）\n动差法是一种参数估计方法，通过令样本矩（如样本均值、样本方差）等于理论分布的对应矩，解方程得到参数估计。\n对于负二项分布：\n\n参数：\n\n\\(\\mu = \\text{E}(X)\\)\n\\(\\text{Var}(X) = \\mu + \\mu^2/k\\)\n\n\n由样本数据可得样本均值 \\(\\bar{x}\\) 和样本方差 \\(s^2\\)，则可解出：\n\n\\(\\hat{k} = \\mu^2 / (s^2 - \\mu)\\)\n\n这个估计在 \\(s^2 &gt; \\mu\\) 时成立（即数据存在过度离散）。\n\n\n1.1.2 其他参数估计方法\n\n\n\n\n\n\n\n\n\n方法\n思路\n优点\n缺点\n\n\n\n\n频数法（Frequency Method）\n利用频数分布构造估计量，如通过最大频数位置反推参数\n直观，适用于整数型数据\n精度差，参数间依赖强\n\n\n零频数法（Zero Frequency Method）\n利用观测中“零”的频率推断参数\n简便，只需零频数\n精度有限，需大量样本支持\n\n\n最大似然估计（MLE）\n构造似然函数，以数值优化求得参数估计\n统计效率高，常用于建模\n需要迭代计算，依赖软件\n\n\n\n\n\n代码\n%%SAS\n/*程序17-1*/\ndata nbinomial;\n    input x f @@;\ndatalines;\n0 30 1 14 2 8 3 4 4 2 5 0 6 2\n;\nrun;\nproc univariate;\n    var x;\n    freq f;\noutput out= mv2 mean = mu var = v;\nrun;\ndata k;\n    set mv2;\n    k = mu**2/(v-mu);\nproc print;\n    var mu k;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nUNIVARIATE 过程\n变量:  x\n \n频数:  f\n\n\n\n\n\n\n\n矩\n\n\n\n\n数目\n60\n权重总和\n60\n\n\n均值\n1.03333333\n观测总和\n62\n\n\n标准差\n1.43759058\n方差\n2.06666667\n\n\n偏度\n1.78111198\n峰度\n3.3122114\n\n\n未校平方和\n186\n校正平方和\n121.933333\n\n\n变异系数\n139.121669\n标准误差均值\n0.18559215\n\n\n\n\n\n\n\n\n\n\n基本统计测度\n\n\n位置\n变异性\n\n\n\n\n均值\n1.033333\n标准差\n1.43759\n\n\n中位数\n0.500000\n方差\n2.06667\n\n\n众数\n0.000000\n极差\n6.00000\n\n\n \n \n四分位间距\n2.00000\n\n\n\n\n\n\n\n\n\n\n位置检验: Mu0=0\n\n\n检验\n统计量\np 值\n\n\n\n\nStudent t\nt\n5.567764\nPr &gt; |t|\n&lt;.0001\n\n\n符号\nM\n15\nPr &gt;= |M|\n&lt;.0001\n\n\n符号秩\nS\n232.5\nPr &gt;= |S|\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n分位数（定义 5）\n\n\n水平\n分位数\n\n\n\n\n100% 最大值\n6.0\n\n\n99%\n6.0\n\n\n95%\n4.0\n\n\n90%\n3.0\n\n\n75% Q3\n2.0\n\n\n50% 中位数\n0.5\n\n\n25% Q1\n0.0\n\n\n10%\n0.0\n\n\n5%\n0.0\n\n\n1%\n0.0\n\n\n0% 最小值\n0.0\n\n\n\n\n\n\n\n\n\n\n极值观测\n\n\n最小值\n最大值\n\n\n值\n频数\n观测\n值\n频数\n观测\n\n\n\n\n0\n30\n1\n1\n14\n2\n\n\n1\n14\n2\n2\n8\n3\n\n\n2\n8\n3\n3\n4\n4\n\n\n3\n4\n4\n4\n2\n5\n\n\n4\n2\n5\n6\n2\n7\n\n\n\n\n\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\nmu\nk\n\n\n\n\n1\n1.03333\n1.03333\n\n\n\n\n\n\n\n\n\n\n\n\n1.1.3 程序说明\n数据集中的x和f分别表示胚胎死亡数和雌鼠数,首先通过 univariate 过程计算均数和方差,并将该两项指标输出到 mv2 数据集中,再用数据集k调用mv2的内容,用专用公式计算k的值。\n\n\n1.1.4 结果说明\nunivariate 过程的输出结果不再叙述,最后输出的两个参数分别为u=1.033 33,k=1.033 33.",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "17-SAS 与负二项分布"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-09-negative-binomial.html#零频数法",
    "href": "Guide/SAS/25-06-09-negative-binomial.html#零频数法",
    "title": "17-SAS 与负二项分布",
    "section": "1.2 零频数法",
    "text": "1.2 零频数法\n理论上，\n\\[\nP(X=0) = \\left( \\frac{k}{k+\\mu} \\right)^k\n\\]\n设观察零频率 \\(f_0 = 30/60 = 0.5\\)，解此方程估计 k。\n\n\n代码\n%%SAS\ndata zerofreq;\n    f0 = 0.5;\n    mu = 1.0;  /* 可以先用动差法得出的均值 */\n    do k = 0.01 to 20 by 0.01;\n        p0 = (k/(k+mu))**k;\n        diff = abs(p0 - f0);\n        output;\n    end;\nrun;\n\nproc sort data=zerofreq;\n    by diff;\nrun;\n\nproc print data=zerofreq(obs=1);\n    var mu k p0;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\nmu\nk\np0\n\n\n\n\n1\n1\n1\n0.5",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "17-SAS 与负二项分布"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html",
    "href": "Guide/R/02-ggplot2-base.html",
    "title": "02-ggplot2 绘图基础",
    "section": "",
    "text": "R 可视化的内容主要来源于 [R Graphics Cookbook, 2nd edition](https://r-graphics.org/) 和 [Beautiful-Visualization-with-R](https://github.com/EasyChart/Beautiful-Visualization-with-R)(Zhang 2019)",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-10-Goodness-of-Fit.html",
    "href": "Guide/SAS/25-06-10-Goodness-of-Fit.html",
    "title": "18-SAS 拟合优度检验",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "18-SAS 拟合优度检验"
    ]
  },
  {
    "objectID": "Guide/SAS/25-06-10-Goodness-of-Fit.html#示例",
    "href": "Guide/SAS/25-06-10-Goodness-of-Fit.html#示例",
    "title": "18-SAS 拟合优度检验",
    "section": "1.1 示例",
    "text": "1.1 示例\n观察某克山病区克山病患者的空间分布情况。调查者将该地区划分为279个取样单位,统计各取样单位历年累计病例数,资料见下表。问此资料是否服从 Poisson 分布?\n\n\n\n取样单位内病例数\n观察频数\n\n\n\n\n0\n26\n\n\n1\n51\n\n\n2\n75\n\n\n3\n63\n\n\n4\n38\n\n\n5\n17\n\n\n6\n5\n\n\n7\n3\n\n\n≥8\n1\n\n\n合计\n279\n\n\n\n\n\n代码\n%%SAS\n/*程序18-1*/\ndata prg9_12;\n    input x f @@;\n    t= x*f;\ndatalines;\n0 26 1 51 2 75 3 63 4 38 5 17 6 5 7 3 8 1\n;\nrun;\nproc means sum noprint;\n    var t f;\noutput out = b sum = sumt sumf;\nrun;\ndata c;\n    set b;\n        do x=0 to 8;\n            lamda=sumt/sumf;\n            output;\n        end;\nrun;\ndata d;\n    merge prg9_12 c;\n    by x;\nrun;\ndata e;\n    set d;\n    if x = 0 then p = poisson(lamda,0);\n    if x &gt; 0 and x &lt; 8 then p = poisson(lamda,x)- poisson(lamda,x-1);\n    if x = 8 then p = 1-poisson(amda,x-1);\n    retain chisq o p 0;\n    t1 = sumf*p;\n    chisq =((f-t1)**2)/t1;\nrun;\nproc means sum noprint;\n    var chisq;\n    output out =f sum = sumchi;\nrun;\ndata g;\n    set f;\n    p_chi= 1-probchi(sumchi,8);\nrun;\nproc print;\n    var sumchi p_chi;\nrun;\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\nsumchi\np_chi\n\n\n\n\n1\n2.48975\n0.96221\n\n\n\n\n\n\n\n\n\n\n\n1.1.1 程序说明\n这里的程序有点复杂，主要可以拆分为 7 个步骤：\n\n首先建立数据集 prg9_12,其中拥有两个变量 x 和 f ,变量 x 表示取样单位内的病例数,变量 f 为发生病例数的频数。另外产生一个变量,该变量表示取样单位内的总发病人数。\n然后对该数据集用 means 过程计算总发病人数和总人数,并将这两个统计量输出到数据集 b 中。\n创建数据集 c ,调用数据集 b ,计算总发病率(lamda),并用 do-end 语句产生 x 变量,该变量的值从 0 到 8 ,所以此时数据集中有四个变量:总发病人数(sumt)、总人数(sumf)、总发病率(lamda)和x(单位病例数),观测数有9例这9例观测变量x的值从 0 到 8,其他变量值都相同。\n将数据集 prg9_12 和 c 以 x 为关键变量进行合并,产生数据集 d。再建数据集 e,调用数据集 d,根据数据集 d 的变量x的数值产生每个 x 值的 Poisson 的概率值(P)。\n然后通过 Poisson 的概率值计算每个单位病例数的理论发病人数(t),从而计算出各单位病例数所对应的 x 值( chisq )。\n用 means 过程计算，值的总和( sumchi )。并将结果输出到数据集 f。\n另建立数据集 g,调用数据集 f,根据 X 值的合计,用 probchi 函数计算出该 \\(\\chi^2\\) 值所对应的 P 值(p_chi),最后将结果输出到 Outpu 窗口。\n\n\n\n1.1.2 结果说明\n本例 \\(\\chi^2=2.48975,P=0.96221&gt;0.05\\),说明该克山病区克山病患者的空间分布服从 Poisson 分布.",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "18-SAS 拟合优度检验"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#安装package",
    "href": "Guide/R/02-ggplot2-base.html#安装package",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.1 安装package",
    "text": "2.1 安装package\n可以通过如下方式安装：\n\n代码install.packages(\"tidyverse\")\ninstall.packages(\"gcookbook\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#载入package",
    "href": "Guide/R/02-ggplot2-base.html#载入package",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.2 载入package",
    "text": "2.2 载入package\n在每个R会话中，需要再运行代码之前加载这几个包：\n{r，eval=false} library(tidyverse) library(gcookbook)\n\n2.2.1 注意\n运行library(tidyverse)会加载 ggplot2,dplyr等许多其他包，如果想要R会话更加流畅高效，可以分别加载ggplot2,dplyr,gcookbook等。",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#更新package",
    "href": "Guide/R/02-ggplot2-base.html#更新package",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.3 更新package",
    "text": "2.3 更新package\n运行 update.packages() 可以查看那些包可以被更新，如果想要不加提示的更新说有的package，可以加入参数：ask=FALSE。\n{r，eval=FALSE} update.packages(ask=FALSE)\n\n2.3.1 注意\n一般来说package的作者会发布一些新版本来修复旧版本中的问题，并提供一些新特征或功能，通常是建议将packages更新到最新版。但是有时候可能会出现一些bug或package之间的冲突，可以通过版本控制/构建虚拟环境来进行解决。",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#加载以符号分隔的文本文件",
    "href": "Guide/R/02-ggplot2-base.html#加载以符号分隔的文本文件",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.4 加载以符号分隔的文本文件",
    "text": "2.4 加载以符号分隔的文本文件\n\n一般的加载语法\n\ndata &lt;- read.csv(\"dataname.csv\")\n\nrender 包中的 read_csv() 函数，这个函数的运行速度比 read.csv() 快很多，且更适合处理字符串、日期和时间数据。\n如果数据文件首列没有列明：\n\ndata &lt;- read.csv(\"dataname.csv\",header = FALSE)\n# 手动为列名赋值\nnames(data) &lt;- c(\"colum_1\",\"colum_2\",\"colum_3\"...)",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#从excel文件中加载数据",
    "href": "Guide/R/02-ggplot2-base.html#从excel文件中加载数据",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.5 从Excel文件中加载数据",
    "text": "2.5 从Excel文件中加载数据\n可以使用 readxl 包中的 read_excel() 函数用于读取 .xls、.xlsx等Excel文件。\n# just install once\n# install.packages(\"readxl\")\n\nlibrary(readxl)\ndata &lt;- read_excel(\"data_name.xlsx\",1)\n\n# designate the sheet\ndata &lt;- read_excel(\"data_name.xlsx\",sheet = sheet_number)\ndata &lt;- read_excel(\"data_name.xlsx\",sheet = sheet_name)\nread_excel() 默认使用工作标的第一行作为列名，如果不想以第一行作为列名，可以设置参数 col_names = FALSE ，相应的，各列会被命名为 X1,X2 等。",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#从spsssasstata文件中加载数据",
    "href": "Guide/R/02-ggplot2-base.html#从spsssasstata文件中加载数据",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.6 从SPSS/SAS/Stata文件中加载数据",
    "text": "2.6 从SPSS/SAS/Stata文件中加载数据\n\n2.6.1 使用 haven 包中的函数\n# just install once\n# install.packages(\"haven\")\n\nlibrary(haven)\n#for SPSS\ndata &lt;- read_sav(\"data_name.sav\")\n#for SAS\ndata &lt;- read_sas(\"data_name.sas7bdat\")\n#for Stata\ndata &lt;- read_dta(\"data_name.dta\")\n\n2.6.2 使用 foreign 包替代\n这个同样支持 SPSS 和 Stata 数据，但是这个包更新缓慢。\n它还可以支持octave/MATLAB,SYSTAT,SAS XPORT等数据的读取。\n通过 ls(\"package:foreign\") 查看所有的函数列表。\n\n代码# install.packages(\"foreign\")\nlibrary(foreign)\nls(\"package:foreign\")\n\n [1] \"data.restore\"  \"lookup.xport\"  \"read.arff\"     \"read.dbf\"     \n [5] \"read.dta\"      \"read.epiinfo\"  \"read.mtp\"      \"read.octave\"  \n [9] \"read.S\"        \"read.spss\"     \"read.ssd\"      \"read.systat\"  \n[13] \"read.xport\"    \"write.arff\"    \"write.dbf\"     \"write.dta\"    \n[17] \"write.foreign\"",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#链接函数和管道操作符",
    "href": "Guide/R/02-ggplot2-base.html#链接函数和管道操作符",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.7 链接函数和管道操作符 %>%\n",
    "text": "2.7 链接函数和管道操作符 %&gt;%\n\n\n代码library(dplyr)\n\n# view the morley dataset\nmorley\n\n    Expt Run Speed\n001    1   1   850\n002    1   2   740\n003    1   3   900\n004    1   4  1070\n005    1   5   930\n006    1   6   850\n007    1   7   950\n008    1   8   980\n009    1   9   980\n010    1  10   880\n011    1  11  1000\n012    1  12   980\n013    1  13   930\n014    1  14   650\n015    1  15   760\n016    1  16   810\n017    1  17  1000\n018    1  18  1000\n019    1  19   960\n020    1  20   960\n021    2   1   960\n022    2   2   940\n023    2   3   960\n024    2   4   940\n025    2   5   880\n026    2   6   800\n027    2   7   850\n028    2   8   880\n029    2   9   900\n030    2  10   840\n031    2  11   830\n032    2  12   790\n033    2  13   810\n034    2  14   880\n035    2  15   880\n036    2  16   830\n037    2  17   800\n038    2  18   790\n039    2  19   760\n040    2  20   800\n041    3   1   880\n042    3   2   880\n043    3   3   880\n044    3   4   860\n045    3   5   720\n046    3   6   720\n047    3   7   620\n048    3   8   860\n049    3   9   970\n050    3  10   950\n051    3  11   880\n052    3  12   910\n053    3  13   850\n054    3  14   870\n055    3  15   840\n056    3  16   840\n057    3  17   850\n058    3  18   840\n059    3  19   840\n060    3  20   840\n061    4   1   890\n062    4   2   810\n063    4   3   810\n064    4   4   820\n065    4   5   800\n066    4   6   770\n067    4   7   760\n068    4   8   740\n069    4   9   750\n070    4  10   760\n071    4  11   910\n072    4  12   920\n073    4  13   890\n074    4  14   860\n075    4  15   880\n076    4  16   720\n077    4  17   840\n078    4  18   850\n079    4  19   850\n080    4  20   780\n081    5   1   890\n082    5   2   840\n083    5   3   780\n084    5   4   810\n085    5   5   760\n086    5   6   810\n087    5   7   790\n088    5   8   810\n089    5   9   820\n090    5  10   850\n091    5  11   870\n092    5  12   870\n093    5  13   810\n094    5  14   740\n095    5  15   810\n096    5  16   940\n097    5  17   950\n098    5  18   800\n099    5  19   810\n100    5  20   870\n\n代码morley %&gt;% filter(Expt == 1) %&gt;% summary()\n\n      Expt        Run            Speed     \n Min.   :1   Min.   : 1.00   Min.   : 650  \n 1st Qu.:1   1st Qu.: 5.75   1st Qu.: 850  \n Median :1   Median :10.50   Median : 940  \n Mean   :1   Mean   :10.50   Mean   : 909  \n 3rd Qu.:1   3rd Qu.:15.25   3rd Qu.: 980  \n Max.   :1   Max.   :20.00   Max.   :1070  \n\n\n\n2.7.1 普通函数\n\n代码summary(filter(morley,Expt == 1))\n\n      Expt        Run            Speed     \n Min.   :1   Min.   : 1.00   Min.   : 650  \n 1st Qu.:1   1st Qu.: 5.75   1st Qu.: 850  \n Median :1   Median :10.50   Median : 940  \n Mean   :1   Mean   :10.50   Mean   : 909  \n 3rd Qu.:1   3rd Qu.:15.25   3rd Qu.: 980  \n Max.   :1   Max.   :20.00   Max.   :1070",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#绘制散点图",
    "href": "Guide/R/02-ggplot2-base.html#绘制散点图",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.8 绘制散点图",
    "text": "2.8 绘制散点图\n\n代码plot(mtcars$wt,mtcars$mpg)\n\n\n\n\n\n\n\n使用 ggplot2 绘制\n\n代码library(ggplot2)\nggplot(mtcars,aes(x = wt,y = mpg))+\ngeom_point()",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#绘制折线图",
    "href": "Guide/R/02-ggplot2-base.html#绘制折线图",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.9 绘制折线图",
    "text": "2.9 绘制折线图\n\n代码plot(pressure$temperature,pressure$pressure,type = \"l\")\n\n\n\n\n\n\n\n\n2.9.1 增加数据点和多条折线\n\n代码plot(pressure$temperature,pressure$pressure,type = \"l\")\npoints(pressure$temperature,pressure$pressure)\n\nlines(pressure$temperature,pressure$pressure/2,col = \"red\")\npoints(pressure$temperature,pressure$pressure/2,col = \"red\")\n\n\n\n\n\n\n\n\n2.9.2 ggplot2 中的 geom_line()\n\n\n代码library(ggplot2)\nggplot(pressure,aes(x = temperature,y = pressure))+\ngeom_line()\n\n\n\n\n\n\n\n\n代码library(ggplot2)\nggplot(pressure,aes(x = temperature,y = pressure))+\ngeom_line()+\ngeom_point()",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/01-R-intro.html",
    "href": "Guide/R/01-R-intro.html",
    "title": "01-R 相关介绍与记录",
    "section": "",
    "text": "R语言是一种自由软件编程语言与操作环境，主要用于统计分析、绘图以及数据挖掘。R由新西兰奥克兰大学的统计学家罗斯·伊哈卡和罗伯特·杰特曼开发，现在由R核心小组负责开发，同时也有其他用户编写了诸多外挂的软件包。R以S语言为基础，其词法作用域语义来自Scheme。R的后台程序大多由C语言、FORTRAN语言和R自己写成。\nR 语言是为数学研究工作者设计的一种数学编程语言，主要用于统计分析、绘图、数据挖掘。\n如果你是一个计算机程序的初学者并且急切地想了解计算机的通用编程，R 语言不是一个很理想的选择，可以选择 Python、C 或 Java。\nR 语言与 C 语言都是贝尔实验室的研究成果，但两者有不同的侧重领域，R 语言是一种解释型的面向数学理论研究工作者的语言，而 C 语言是为计算机软件工程师设计的。\nR 语言是解释运行的语言（与 C 语言的编译运行不同），它的执行速度比 C 语言慢得多，不利于优化。但它在语法层面提供了更加丰富的数据结构操作并且能够十分方便地输出文字和图形信息，所以它广泛应用于数学尤其是统计学领域https://www.runoob.com/r/r-tutorial.html\n近些年 R 的发展也是极为迅速，在 Rstudio 改名为 Posit 后，R 的生态在快速发展。\n本网站就是其中一个分支的成果： Quarto\n\nAn open-source scientific and technical publishing system\n\n其他的还有诸如：\n\n0.1 Posit Workbench（数据科学家协同开发平台）\n\nJupyter, RStudio, and VS Code environments centrally maintained and ready to use\n\n\n\n0.2 MLOps（机器学习模型部署）\n\nMachine learning operations, or MLOps, is a set of practices to deploy and maintain machine learning models in production reliably and efficiently. The vetiver framework is for MLOps tasks in Python and R.\n\n\n\n0.3 Shiny for python\n\nEffortless Python web applications with the power of reactive programming.",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "01-R 相关介绍与记录"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#绘制条形图",
    "href": "Guide/R/02-ggplot2-base.html#绘制条形图",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.10 绘制条形图",
    "text": "2.10 绘制条形图\n\n2.10.1 使用 barplot() 函数\n向 barplot() 函数传递两个参数，第一个向量用来设定条形的高度，第二个向量用来设定每个条形对应的标签（可选）；如果向量中的元素已被命名，则系统会自动使用元素的名字作为条形标签。\n\n代码# check dataset\nBOD\n\n  Time demand\n1    1    8.3\n2    2   10.3\n3    3   19.0\n4    4   16.0\n5    5   15.6\n6    7   19.8\n\n代码barplot(BOD$demand,names.arg = BOD$Time)\n\n\n\n\n\n\n\n\n2.10.2 使用 ggplot2 中的 geom_col 函数\n\n代码library(ggplot2)\n\n# 变量值的频数表，使用 BOD 数据框，时间（Time）对应X值，需求（demand）对应Y值.\nggplot(BOD,aes(x = Time,y = demand))+\ngeom_col()\n\n\n\n\n\n\n代码# 将x转换为因子型变量，从而使系统将其视作离散值\nggplot(BOD,aes(x = factor(Time),y = demand))+\ngeom_col()",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#绘制直方图",
    "href": "Guide/R/02-ggplot2-base.html#绘制直方图",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.11 绘制直方图",
    "text": "2.11 绘制直方图\n使用 hist() 函数#| echo: true #| output: true #| cache: true\n\n代码hist(mtcars$mpg)\n\n\n\n\n\n\n代码# designate the interval\nhist(mtcars$mpg,breaks = 10)\n\n\n\n\n\n\n\n\n2.11.1 使用 ggplot2 中的 geom_histogram()\n\n\n代码library(ggplot2)\nggplot(mtcars,aes(x = mpg))+\ngeom_histogram()\n\n\n\n\n\n\n代码# use a bigger interval\nggplot(mtcars,aes(x = mpg))+\ngeom_histogram(binwidth = 4)",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#绘制箱型图",
    "href": "Guide/R/02-ggplot2-base.html#绘制箱型图",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.12 绘制箱型图",
    "text": "2.12 绘制箱型图",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#使用-plot-函数",
    "href": "Guide/R/02-ggplot2-base.html#使用-plot-函数",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.13 使用 plot() 函数",
    "text": "2.13 使用 plot() 函数\n当 X 为因子型变量（与数值型变量对应）时，会默认绘制线形图。\n\n代码plot(ToothGrowth$supp,ToothGrowth$len)\n\n\n\n\n\n\n\n当两个参数向量在同一个数据框时，也可以使用 boxplot() 函数和语法。公式语法允许用户在 x 轴上使用变量组合。\n\n代码# 公式语法\nboxplot(len ~ supp,data = ToothGrowth)\n\n\n\n\n\n\n\n引入交互,基于多组变量的箱型图。\n\n代码# 公式语法\nboxplot(len ~ supp + dose,data = ToothGrowth)\n\n\n\n\n\n\n\n\n2.13.1 使用 ggplot2 中的 geom_boxplot() 函数\n\n代码library(ggplot2)\nggplot(ToothGrowth,aes(x = supp,y = len))+\ngeom_boxplot()\n\n\n\n\n\n\n\n使用 interaction() 函数将分组变量组合在一起来绘制基于多组变量的箱型图。\n\n代码library(ggplot2)\nggplot(ToothGrowth,aes(x = interaction(supp,dose),y = len))+\ngeom_boxplot()",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/02-ggplot2-base.html#绘制函数图像",
    "href": "Guide/R/02-ggplot2-base.html#绘制函数图像",
    "title": "02-ggplot2 绘图基础",
    "section": "\n2.14 绘制函数图像",
    "text": "2.14 绘制函数图像\n\n2.14.1 使用 curve() 函数绘制函数图像\n\n代码curve(x^3 - 5*x,from = -4,to = 4)\n\n\n\n\n\n\n\n绘制用户自定义的函数图像\n\n代码myfun &lt;- function(xvar){1/(1+exp(-xvar+10))}\ncurve(myfun(x),from = 0,to = 20)\n# add straight line\ncurve(1 - myfun(x),add = TRUE,col = \"red\")\n\n\n\n\n\n\n\n\n2.14.2 使用 ggplot2 中的 stat_function(geom = \"Line\") 函数\n\n代码library(ggplot2)\nggplot(data.frame(x = c(0,20)),aes(x=x))+\nstat_function(fun = myfun,geom = \"line\")\n\n\n\n\n\n\n\nend.",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "02-ggplot2 绘图基础"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html",
    "href": "Guide/R/03-ggplot2-barplot.html",
    "title": "03-ggplot2 绘图——条形图",
    "section": "",
    "text": "代码library(gcookbook)  # Load gcookbook for the pg_mean data set\nlibrary(ggplot2)\nggplot(pg_mean, aes(x = group, y = weight)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n代码# There's no entry for Time == 6\nBOD\n\n  Time demand\n1    1    8.3\n2    2   10.3\n3    3   19.0\n4    4   16.0\n5    5   15.6\n6    7   19.8\n\n代码# Time is numeric (continuous)\nstr(BOD)\n\n'data.frame':   6 obs. of  2 variables:\n $ Time  : num  1 2 3 4 5 7\n $ demand: num  8.3 10.3 19 16 15.6 19.8\n - attr(*, \"reference\")= chr \"A1.4, p. 270\"\n\n代码#&gt; 'data.frame':    6 obs. of  2 variables:\n#&gt;  $ Time  : num  1 2 3 4 5 7\n#&gt;  $ demand: num  8.3 10.3 19 16 15.6 19.8\n#&gt;  - attr(*, \"reference\")= chr \"A1.4, p. 270\"\n\nggplot(BOD, aes(x = Time, y = demand)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n代码# There's no entry for Time == 6\nBOD\n\n  Time demand\n1    1    8.3\n2    2   10.3\n3    3   19.0\n4    4   16.0\n5    5   15.6\n6    7   19.8\n\n代码# Convert Time to a discrete (categorical) variable with factor()\nggplot(BOD, aes(x = factor(Time), y = demand)) +\n  geom_col()\n\n\n\n\n\n\n\n\n在默认的设置下，条形图的填充色为深灰色且条形图没有边框线，用户可以通过调整 fill 参数来改变填充色和调整 colour/color 参数为条形图添加边框线。\n将填充色设置为浅蓝色，边框现的颜色设置为黑色：\n\n代码library(gcookbook)  # Load gcookbook for the pg_mean data set\nlibrary(ggplot2)\nggplot(pg_mean, aes(x = group, y = weight)) +\n  geom_col(fill = \"lightblue\",colour = \"black\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#x-是离散型变量",
    "href": "Guide/R/03-ggplot2-barplot.html#x-是离散型变量",
    "title": "03-ggplot2 绘图——条形图",
    "section": "",
    "text": "代码library(gcookbook)  # Load gcookbook for the pg_mean data set\nlibrary(ggplot2)\nggplot(pg_mean, aes(x = group, y = weight)) +\n  geom_col()",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#x-是连续型变量",
    "href": "Guide/R/03-ggplot2-barplot.html#x-是连续型变量",
    "title": "03-ggplot2 绘图——条形图",
    "section": "",
    "text": "代码# There's no entry for Time == 6\nBOD\n\n  Time demand\n1    1    8.3\n2    2   10.3\n3    3   19.0\n4    4   16.0\n5    5   15.6\n6    7   19.8\n\n代码# Time is numeric (continuous)\nstr(BOD)\n\n'data.frame':   6 obs. of  2 variables:\n $ Time  : num  1 2 3 4 5 7\n $ demand: num  8.3 10.3 19 16 15.6 19.8\n - attr(*, \"reference\")= chr \"A1.4, p. 270\"\n\n代码#&gt; 'data.frame':    6 obs. of  2 variables:\n#&gt;  $ Time  : num  1 2 3 4 5 7\n#&gt;  $ demand: num  8.3 10.3 19 16 15.6 19.8\n#&gt;  - attr(*, \"reference\")= chr \"A1.4, p. 270\"\n\nggplot(BOD, aes(x = Time, y = demand)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n代码# There's no entry for Time == 6\nBOD\n\n  Time demand\n1    1    8.3\n2    2   10.3\n3    3   19.0\n4    4   16.0\n5    5   15.6\n6    7   19.8\n\n代码# Convert Time to a discrete (categorical) variable with factor()\nggplot(BOD, aes(x = factor(Time), y = demand)) +\n  geom_col()",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#调整配色",
    "href": "Guide/R/03-ggplot2-barplot.html#调整配色",
    "title": "03-ggplot2 绘图——条形图",
    "section": "",
    "text": "在默认的设置下，条形图的填充色为深灰色且条形图没有边框线，用户可以通过调整 fill 参数来改变填充色和调整 colour/color 参数为条形图添加边框线。\n将填充色设置为浅蓝色，边框现的颜色设置为黑色：\n\n代码library(gcookbook)  # Load gcookbook for the pg_mean data set\nlibrary(ggplot2)\nggplot(pg_mean, aes(x = group, y = weight)) +\n  geom_col(fill = \"lightblue\",colour = \"black\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#pastel1-调色盘",
    "href": "Guide/R/03-ggplot2-barplot.html#pastel1-调色盘",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n2.1 pastel1 调色盘",
    "text": "2.1 pastel1 调色盘\n\n代码library(gcookbook)  # Load gcookbook for the cabbage_exp data set\n# check dataset\ncabbage_exp\n\n  Cultivar Date Weight        sd  n         se\n1      c39  d16   3.18 0.9566144 10 0.30250803\n2      c39  d20   2.80 0.2788867 10 0.08819171\n3      c39  d21   2.74 0.9834181 10 0.31098410\n4      c52  d16   2.26 0.4452215 10 0.14079141\n5      c52  d20   3.11 0.7908505 10 0.25008887\n6      c52  d21   1.47 0.2110819 10 0.06674995\n\n代码# load ggplot2\nlibrary(ggplot2)\nggplot(cabbage_exp, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(position = \"dodge\", colour = \"black\") +\n  scale_fill_brewer(palette = \"Pastel1\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#缺失项",
    "href": "Guide/R/03-ggplot2-barplot.html#缺失项",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n2.2 缺失项",
    "text": "2.2 缺失项\n如果分类变量各水平的组合中有缺失项，那么绘图结果中的条形则相应地略去不绘，同时，临近的条形将自动扩充到相应的位置，示例如下：\n\n代码library(gcookbook)  # Load gcookbook for the cabbage_exp data set\n# check dataset\ncabbage_exp\n\n  Cultivar Date Weight        sd  n         se\n1      c39  d16   3.18 0.9566144 10 0.30250803\n2      c39  d20   2.80 0.2788867 10 0.08819171\n3      c39  d21   2.74 0.9834181 10 0.31098410\n4      c52  d16   2.26 0.4452215 10 0.14079141\n5      c52  d20   3.11 0.7908505 10 0.25008887\n6      c52  d21   1.47 0.2110819 10 0.06674995\n\n代码# delete last row\nce &lt;- cabbage_exp[1:5,]\nce\n\n  Cultivar Date Weight        sd  n         se\n1      c39  d16   3.18 0.9566144 10 0.30250803\n2      c39  d20   2.80 0.2788867 10 0.08819171\n3      c39  d21   2.74 0.9834181 10 0.31098410\n4      c52  d16   2.26 0.4452215 10 0.14079141\n5      c52  d20   3.11 0.7908505 10 0.25008887\n\n代码# load ggplot2\nlibrary(ggplot2)\nggplot(ce, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(position = \"dodge\", colour = \"black\") +\n  scale_fill_brewer(palette = \"Pastel1\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#使用-geom_bar-函数",
    "href": "Guide/R/03-ggplot2-barplot.html#使用-geom_bar-函数",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n3.1 使用 geom_bar() 函数",
    "text": "3.1 使用 geom_bar() 函数\n使用 geom_bar() 函数，同时不映射任何变量到y参数\n\n代码# Equivalent to using geom_bar(stat = \"bin\")\nggplot(diamonds, aes(x = cut)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#条形图着色",
    "href": "Guide/R/03-ggplot2-barplot.html#条形图着色",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n3.2 条形图着色",
    "text": "3.2 条形图着色\n\n代码library(gcookbook) # Load gcookbook for the uspopchange data set\nlibrary(dplyr)\n\n# select top 10 state with population growth\nupc &lt;- uspopchange %&gt;%\n  arrange(desc(Change)) %&gt;%\n  slice(1:10)\n\nupc\n\n            State Abb Region Change\n1          Nevada  NV   West   35.1\n2         Arizona  AZ   West   24.6\n3            Utah  UT   West   23.8\n4           Idaho  ID   West   21.1\n5           Texas  TX  South   20.6\n6  North Carolina  NC  South   18.5\n7         Georgia  GA  South   18.3\n8         Florida  FL  South   17.6\n9        Colorado  CO   West   16.9\n10 South Carolina  SC  South   15.3\n\n\n将 Region 映射到 fill 是上并绘制条形图：\n\n代码ggplot(upc, aes(x = Abb, y = Change, fill = Region)) +\n  geom_col()\n\n\n\n\n\n\n\n\n3.2.1 设定图形颜色\n借助函数 scale_fill_brewer() or scale_fill_manual() 重新设定图形颜色：\n\n代码ggplot(upc, aes(x = reorder(Abb, Change), y = Change, fill = Region)) +\n  geom_col(colour = \"black\") +\n  scale_fill_manual(values = c(\"#669933\", \"#FFCC66\")) +\n  xlab(\"State\")\n\n\n\n\n\n\n\n\n3.2.2 注意\n颜色的映射是在 aes() 内部完成的，但是颜色的设定是在 aes() 外部完成的。",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#pos-映射到-fill-中",
    "href": "Guide/R/03-ggplot2-barplot.html#pos-映射到-fill-中",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n4.1 pos 映射到 fill 中",
    "text": "4.1 pos 映射到 fill 中\n\n代码library(ggplot2)\nggplot(climate_sub, aes(x = Year, y = Anomaly10y, fill = pos)) +\n  geom_col(position = \"identity\")\n\n\n\n\n\n\n\n\n4.1.1 注意\n这里条形图的参数设定为 position = \"identity\" ，可以避免系统因对负值绘制堆积条形而发出的警告信息。",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#调整配色-1",
    "href": "Guide/R/03-ggplot2-barplot.html#调整配色-1",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n4.2 调整配色",
    "text": "4.2 调整配色\n\n4.2.1 scale_fill_manual() 参数\n设定 scale_fill_manual() 参数对图形进行调整，设定参数 guide = FALSE 可以删除图例；设定边框颜色（color/colour）和边框线宽度（size），这里边框线的单位是毫米。\n\n代码ggplot(climate_sub, aes(x = Year, y = Anomaly10y, fill = pos)) +\n  geom_col(position = \"identity\", colour = \"black\", size = 0.25) +\n  scale_fill_manual(values = c(\"#CCEEFF\", \"#FFDDDD\"), guide = FALSE)\n\n\n\n\n\n\n\n出现警告信息（Warning messages）\n调整后：\n\n代码ggplot(climate_sub, aes(x = Year, y = Anomaly10y, fill = pos)) +\n  geom_col(position = \"identity\", colour = \"black\", linewidth = 0.25) +\n  scale_fill_manual(values = c(\"#CCEEFF\", \"#FFDDDD\"), guide = \"none\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#标准宽度",
    "href": "Guide/R/03-ggplot2-barplot.html#标准宽度",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n5.1 标准宽度",
    "text": "5.1 标准宽度\n\n代码library(gcookbook) # Load gcookbook for the pg_mean data set\nlibrary(ggplot2)\nggplot(pg_mean, aes(x = group, y = weight)) +\n  geom_col()",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#更窄",
    "href": "Guide/R/03-ggplot2-barplot.html#更窄",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n5.2 更窄",
    "text": "5.2 更窄\n\n代码library(gcookbook) # Load gcookbook for the pg_mean data set\nlibrary(ggplot2)\nggplot(pg_mean, aes(x = group, y = weight)) +\n  geom_col(width = 0.5)",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#更宽",
    "href": "Guide/R/03-ggplot2-barplot.html#更宽",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n5.3 更宽",
    "text": "5.3 更宽\n条形宽度的最大宽度值为1.\n\n代码library(gcookbook) # Load gcookbook for the pg_mean data set\nlibrary(ggplot2)\nggplot(pg_mean, aes(x = group, y = weight)) +\n  geom_col(width = 1)",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#调整组内间距",
    "href": "Guide/R/03-ggplot2-barplot.html#调整组内间距",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n5.4 调整组内间距",
    "text": "5.4 调整组内间距\n簇状条形图默认组内间距为0，如果希望增加组内间距，可以通过将 width() 的值设定的小一些，并将 position_dodge() 的值设定大于 width() 来实现。\n\n5.4.1 条形更窄的簇状条形图\n\n代码ggplot(cabbage_exp, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(width = 0.5, position = \"dodge\")\n\n\n\n\n\n\n\n\n5.4.2 具有条形间距的簇状条形图\n\n代码ggplot(cabbage_exp, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(width = 0.5, position = position_dodge(0.7))\n\n\n\n\n\n\n\n\n5.4.3 position 语法\n以下四条命令是等价的：\ngeom_bar(position = \"dodge\")\ngeom_bar(width = 0.9, position = position_dodge())\ngeom_bar(position = position_dodge(0.9))\ngeom_bar(width = 0.9, position = position_dodge(width=0.9))",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#使用-geom_bar-函数-1",
    "href": "Guide/R/03-ggplot2-barplot.html#使用-geom_bar-函数-1",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n6.1 使用 geom_bar() 函数",
    "text": "6.1 使用 geom_bar() 函数\n使用 geom_bar() 函数，并映射一个变量给填充色参数 fill 即可，该命令会将 Date 对应到 x 轴上，并以 Cultivar 作为填充色\n\n代码library(gcookbook) # Load gcookbook for the cabbage_exp data set\nlibrary(ggplot2)\nggplot(cabbage_exp, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col()",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#反转堆积顺序和图例顺序",
    "href": "Guide/R/03-ggplot2-barplot.html#反转堆积顺序和图例顺序",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n6.2 反转堆积顺序和图例顺序",
    "text": "6.2 反转堆积顺序和图例顺序\n默认情况下，条形的堆积顺序和图例顺序是一致的，但是都某些数据集而言需要调整图例顺序，用户可以通过 guide() 函数来对图例顺序进行调整，并指定图例所对应的需要调整的图形属性（fill)，并使用 position_stack(reverse = TRUE) 参数来实现反转条形的堆积顺序，通过上述两种函数来保证图例顺序与条形顺序一致。\n\n代码library(gcookbook) # Load gcookbook for the cabbage_exp data set\nlibrary(ggplot2)\nggplot(cabbage_exp, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(position = position_stack(reverse = TRUE)) +\n  guides(fill = guide_legend(reverse = TRUE))",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#获得效果更好的条形图",
    "href": "Guide/R/03-ggplot2-barplot.html#获得效果更好的条形图",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n6.3 获得效果更好的条形图",
    "text": "6.3 获得效果更好的条形图\n使用 scale_fill_brewer() 函数得到一个新的调色板，最后设定 colour=\"black\" 为条形添加一个黑色边框线。\n\n代码library(gcookbook) # Load gcookbook for the cabbage_exp data set\nlibrary(ggplot2)\nggplot(cabbage_exp, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(colour = \"black\") +\n  scale_fill_brewer(palette = \"Pastel1\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#使用-geom_colposition-fill-实现",
    "href": "Guide/R/03-ggplot2-barplot.html#使用-geom_colposition-fill-实现",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n7.1 使用 geom_col(position =  \"fill\") 实现",
    "text": "7.1 使用 geom_col(position =  \"fill\") 实现\n使用 geom_col(position =  \"fill\") 可以将y的值调整为0到1之间。\n\n代码library(gcookbook) # Load gcookbook for the cabbage_exp data set\n\nggplot(cabbage_exp, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(position = \"fill\")\n\n\n\n\n\n\n\n\n7.1.1 让标签以百分比的形式展示\n\n代码ggplot(cabbage_exp, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent)",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#更换调色板并添加边框线",
    "href": "Guide/R/03-ggplot2-barplot.html#更换调色板并添加边框线",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n7.2 更换调色板并添加边框线",
    "text": "7.2 更换调色板并添加边框线\n\n代码ggplot(cabbage_exp, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(colour = \"black\", position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_brewer(palette = \"Pastel1\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#使用-geom_text-函数",
    "href": "Guide/R/03-ggplot2-barplot.html#使用-geom_text-函数",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n8.1 使用 geom_text() 函数",
    "text": "8.1 使用 geom_text() 函数\n需要分别制定一个变量映射给x、y和标签本身，通过设定 vjust （竖直调整数据标签位置）来将标签位置移动至条形图顶端的上方或下方\n\n代码library(gcookbook) # Load gcookbook for the cabbage_exp data set\n\n# Below the top\nggplot(cabbage_exp, aes(x = interaction(Date, Cultivar), y = Weight)) +\n  geom_col() +\n  geom_text(aes(label = Weight), vjust = 1.5, colour = \"white\")\n\n\n\n\n\n\n代码# Above the top\nggplot(cabbage_exp, aes(x = interaction(Date, Cultivar), y = Weight)) +\n  geom_col() +\n  geom_text(aes(label = Weight), vjust = -0.2)",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#给频数条形图添加标签",
    "href": "Guide/R/03-ggplot2-barplot.html#给频数条形图添加标签",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n8.2 给频数条形图添加标签",
    "text": "8.2 给频数条形图添加标签\n\n代码ggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar() +\n  geom_text(aes(label = after_stat(count)), stat = \"count\", \n            vjust = 1.5, colour = \"white\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#给簇状条形图添加标签",
    "href": "Guide/R/03-ggplot2-barplot.html#给簇状条形图添加标签",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n8.3 给簇状条形图添加标签",
    "text": "8.3 给簇状条形图添加标签\n需要设定 position = position_dodge() 并给其一个参数来设定分类间距，分类间距默认值是0.9，因为簇状图的条形更窄，所以需要使用字号 size 来匹配条形宽度，数据标签的默认字号是5，用户可以设定为 3 使其看起来更小（适配）。\n\n代码ggplot(cabbage_exp, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(position = \"dodge\") +\n  geom_text(\n    aes(label = Weight),\n    colour = \"white\", size = 3,\n    vjust = 1.5, position = position_dodge(.9)\n  )",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#堆积图添加数据标签",
    "href": "Guide/R/03-ggplot2-barplot.html#堆积图添加数据标签",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n8.4 堆积图添加数据标签",
    "text": "8.4 堆积图添加数据标签\n要对堆积图添加数据标签，先要对每组条形所对应的数据进行累计求和，有需要在此之前保证数据的合理排序，否则可能会计算出错误的累计和。\n使用 dplyr 包中的 arrange() 函数完成上述操作，使用 rev() 函数调整 Cultivar 的顺序。\n\n代码library(dplyr)\n\n# Sort by the Date and Cultivar columns\nce &lt;- cabbage_exp %&gt;%\n  arrange(Date, rev(Cultivar))\n\n# Get the cumulative sum\nce &lt;- ce %&gt;%\n  group_by(Date) %&gt;%\n  mutate(label_y = cumsum(Weight))\n\nce\n\n# A tibble: 6 × 7\n# Groups:   Date [3]\n  Cultivar Date  Weight    sd     n     se label_y\n  &lt;fct&gt;    &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 c52      d16     2.26 0.445    10 0.141     2.26\n2 c39      d16     3.18 0.957    10 0.303     5.44\n3 c52      d20     3.11 0.791    10 0.250     3.11\n4 c39      d20     2.8  0.279    10 0.0882    5.91\n5 c52      d21     1.47 0.211    10 0.0667    1.47\n6 c39      d21     2.74 0.983    10 0.311     4.21\n\n代码ggplot(ce, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col() +\n  geom_text(aes(y = label_y, label = Weight), vjust = 1.5, colour = \"white\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#堆积图数据标签放置在中部",
    "href": "Guide/R/03-ggplot2-barplot.html#堆积图数据标签放置在中部",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n8.5 堆积图数据标签放置在中部",
    "text": "8.5 堆积图数据标签放置在中部\n如果想把数据标签放在条形中部，需要对累计求和的结果加以调整，并同时略去 geom_bar() 函数对 y 偏移量 offset 的设置。\n\n代码library(dplyr)\n\nce &lt;- cabbage_exp %&gt;%\n  arrange(Date, rev(Cultivar))\n\n# Calculate y position, placing it in the middle\nce &lt;- ce %&gt;%\n  group_by(Date) %&gt;%\n  mutate(label_y = cumsum(Weight) - 0.5 * Weight)\n\nggplot(ce, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col() +\n  geom_text(aes(y = label_y, label = Weight), colour = \"white\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#添加要素",
    "href": "Guide/R/03-ggplot2-barplot.html#添加要素",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n8.6 添加要素",
    "text": "8.6 添加要素\n\n修改颜色\n将数据标签置于中间\n缩小标签字号 size\n\n调用 paste() 函数给标签添加后缀\n使用 format() 函数保留两位小数\n\n\n代码ggplot(ce, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(colour = \"black\") +\n  geom_text(aes(y = label_y, label = paste(format(Weight, nsmall = 2), \"kg\")), size = 4) +\n  scale_fill_brewer(palette = \"Pastel1\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#使用-geom_point-命令",
    "href": "Guide/R/03-ggplot2-barplot.html#使用-geom_point-命令",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n9.1 使用 geom_point() 命令",
    "text": "9.1 使用 geom_point() 命令\n\n代码library(gcookbook) # Load gcookbook for the tophitters2001 data set\ntophit &lt;- tophitters2001[1:25, ] # Take the top 25 from the tophitters data set\n\nggplot(tophit, aes(x = avg, y = name)) +\n  geom_point()",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#修改排序",
    "href": "Guide/R/03-ggplot2-barplot.html#修改排序",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n9.2 修改排序",
    "text": "9.2 修改排序\n尽管 tophit 函数的行排序恰好是根据 avg 变量进行排序的，但这并不意味着在图中的也是这样进行排序；在默认的点图设置下，坐标轴上的变量通畅会根据变量类型自动选取合适的排序方式。\n这里，变量 name 属于字符串类型，因此，点图根据字母先后顺序对其进行了排序；当变量是因子型变量时，点图会根据定义好的因子水平顺序对其进行排序。\n用户可以使用 reorder(name,avg) 函数实现这一过程，该过程会先将 name 变量转换为因子，然后，根据 avg 变量的大小对其进行排序。\n为了使图形效果更好，用户可以使用图形主题系统（theming system）删除垂直网格线，并将水平网格线的线性修改为虚线。\n\n代码library(gcookbook) # Load gcookbook for the tophitters2001 data set\ntophit &lt;- tophitters2001[1:25, ] # Take the top 25 from the tophitters data set\nlibrary(ggplot2)\nggplot(tophit, aes(x = avg, y = reorder(name, avg))) +\n  geom_point(size = 3) +  # Use a larger dot\n  theme_bw() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    panel.grid.major.y = element_line(colour = \"grey60\", linetype = \"dashed\")\n  )",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#旋转点图",
    "href": "Guide/R/03-ggplot2-barplot.html#旋转点图",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n9.3 旋转点图",
    "text": "9.3 旋转点图\n将点图的x轴和y轴互换，互换后，x轴对应名称，y轴对应数值，同时将标签旋转60°。\n\n代码ggplot(tophit, aes(x = reorder(name, avg), y = avg)) +\n  geom_point(size = 3) +  # Use a larger dot\n  theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.x = element_line(colour = \"grey60\", linetype = \"dashed\"),\n    axis.text.x = element_text(angle = 60, hjust = 1)\n  )",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#分组",
    "href": "Guide/R/03-ggplot2-barplot.html#分组",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n9.4 分组",
    "text": "9.4 分组\n因为前面已将 name 转为因子型变量，可以视作为一种分类变量，现在再根据因子 lg 对样本进行分组，因子 lg 有两个水平，分别是 NL 和 AL；一次依据 lg 和 avg 对变量进行排序。\n需要注意的是， reorder() 函数只能根据一个变量对因子水平进行排序，所以这里只能手动来实现 lg 和 avg 进行排序。\n\n代码# Get the names, sorted first by lg, then by avg\nnameorder &lt;- tophit$name[order(tophit$lg, tophit$avg)]\n\n# Turn name into a factor, with levels in the order of nameorder\ntophit$name &lt;- factor(tophit$name, levels = nameorder)\n\n\n将 lg 变量映射到点的颜色属性上，借助 geom_segment() 函数来实现“以数据点为端点的线段”代替贯通全图的网格线。\n需要注意的是， geom_segment() 函数需要设定 x、y、xend和 yend 四个参数.\n\n代码ggplot(tophit, aes(x = avg, y = name)) +\n  geom_segment(aes(yend = name), xend = 0, colour = \"grey50\") +\n  geom_point(size = 3, aes(colour = lg)) +\n  scale_colour_brewer(palette = \"Set1\", limits = c(\"NL\", \"AL\")) +\n  theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank(),   # 无水平网格线\n    legend.position = \"right\",              # 图例放在右侧\n    legend.justification = c(1, 0.5),       # 图例对齐方式\n    legend.box.just = \"right\"               # 图例框靠右对齐\n  )",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#分组2分面展示",
    "href": "Guide/R/03-ggplot2-barplot.html#分组2分面展示",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n9.5 分组2：分面展示",
    "text": "9.5 分组2：分面展示\n通过调整 lg 变量的因子水平来修改分面显示的堆叠顺序。\n\n代码ggplot(tophit, aes(x = avg, y = name)) +\n  geom_segment(aes(yend = name), xend = 0, colour = \"grey50\") +\n  geom_point(size = 3, aes(colour = lg)) +\n  scale_colour_brewer(palette = \"Set1\", limits = c(\"NL\", \"AL\"), guide = \"none\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank()) +\n  facet_grid(lg ~ ., scales = \"free_y\", space = \"free_y\")\n\n\n\n\n\n\n\nend.",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#idea",
    "href": "Guide/SAS/25-03-11-SAS-install.html#idea",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "2.1 idea？",
    "text": "2.1 idea？\n因为在jupyter中已经确定可以通过魔法命令来调用 Stata 和 SAS 的 engine 来实现 Pytho(jupyter) 和 Stata/SAS 的联合使用，但是 Quarto 中也可以直接使用 Python 的命令，实现的形式如下：",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#实现",
    "href": "Guide/SAS/25-03-11-SAS-install.html#实现",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "2.2 实现",
    "text": "2.2 实现\n这里首先按照基本的结构组装了一个程序，但是运行后报错\n\nimport saspy\n%load_ext saspy.sas_magic\n\n%%sas\ndata iris;\n    set sashelp.iris;\nrun;\n\nproc print data=iris(obs=10);\nrun;\n\n\nCell In[1], line 5\n    data iris;\n         ^\nSyntaxError: invalid syntax\n\n\n2.2.1 报错原因\n\n%load_ext 是一个单行 cell magic，不能和 %%sas 混在同一个 cell 里;\n%%sas 是 cell magic，它期望在 cell 顶部单独存在。\nQuarto 里的一个 chunk 是一个 cell。这里把所有东西都写进一个 chunk，相当于写进了一个 Python cell，解释器会试图把 SAS 语句当作 Python 执行，自然就报错了。\n\n\n\n2.2.2 修改\n\nimport saspy\n%load_ext saspy.sas_magic\n\n\n%%SAS\nproc print data=sashelp.iris(obs=10);\nrun;\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 29008\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\n\n\n\n\n观测\nSpecies\nSepalLength\nSepalWidth\nPetalLength\nPetalWidth\n\n\n\n\n1\nSetosa\n50\n33\n14\n2\n\n\n2\nSetosa\n46\n34\n14\n3\n\n\n3\nSetosa\n46\n36\n10\n2\n\n\n4\nSetosa\n51\n33\n17\n5\n\n\n5\nSetosa\n55\n35\n13\n2\n\n\n6\nSetosa\n48\n31\n16\n2\n\n\n7\nSetosa\n52\n34\n14\n2\n\n\n8\nSetosa\n49\n36\n14\n1\n\n\n9\nSetosa\n44\n32\n13\n2\n\n\n10\nSetosa\n50\n35\n16\n6\n\n\n\n\n\n\n\n\n\n\n顺利完成。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#forward",
    "href": "Guide/SAS/25-03-11-SAS-install.html#forward",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "2.3 forward？",
    "text": "2.3 forward？\n由于在 .ipynb 文件中运行 SAS 程序后得到的 .html 文件在编译为网站后会出现“畸变”，改用 .qmd 后也许会有所改进。\n同时 .qmd 对一些文档控制之处的支持更为便捷，且可以插入文献和脚注，更适合作为专业的文档编辑工具。\n2025.07.07 实测，在 .qmd 中插入 SAS 程序运行后编译网页还是有“畸变”，暂时不清楚是由 .qmd 还是 .ipynb 引起。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html",
    "href": "Guide/SAS/25-03-11-SAS-install.html",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "",
    "text": "[SAS9.4下载安装教程](https://blog.csdn.net/wzk4869/article/details/128893187)\n\n\n\n[Linux 环境下 SAS 软件安装与配置详细教程](https://my.oschina.net/emacs_8839324/blog/17390712)\nSAS 在 Linux 系统中的安装极为复杂，笔者曾在某短时间研究多日，最后还是没有安装成功。\n\n\n\n在 macOS 中安装SAS一般有两种途径：\n\n下载 SAS University Edition for mac（这个版本是免费版，目前还不了解哪些功能被阉割）\n在 macOS 上搭建 Windows 虚拟机，然后按照 Windows上装 SAS 的方法进行。\n\n如果你有可用的 SID 可以考虑安装虚拟机后运行 SAS，如果没有可用的 SID，则考虑使用免费的 免费使用 SAS 云软件，需要按照身份和需求进行注册，最后登录 SAS® OnDemand for Academics进行使用。\n\n\n\nSAS VS Code 扩展轻量级，可在任何地方运行，并允许您集成 SAS 和其他语言。该工具还提供直接连接到 SAS Viya 和 SAS 9 并运行代码的功能。\n\nSAS 语法突出显示和帮助、代码完成和代码片段\n用于连接 SAS 和运行代码的配置文件配置\n支持 SAS Viya 和 SAS 9 连接\n访问 SAS 内容和库\n为 SAS、SQL、Python 和其他语言创建笔记本\n\n扩展程序可在 GitHub 上找到仓库与原代码：[vscode-sas-extension](https://github.com/sassoftware/vscode-sas-extension)\n更多关于 SAS 与 vscode 的信息可以访问：[SAS Extension for Visual Studio Code](https://developer.sas.com/programming/vs_code_extension)\n\n\n在 vscode 的扩展页面搜索 “sas” ，第一个 “official SAS ···“ 即为正确扩展：\n\n\n\n\n\nBefore you can run SAS code, you must configure the SAS extension to access your SAS 9.4 (remote or local) server or a SAS Viya server and add a connection profile.\n在运行 SAS 代码之前，您必须配置 SAS 扩展以访问 SAS 9.4（远程或本地）服务器或 SAS Viya 服务器。您必须获得 SAS 9.4 或 SAS Viya 的许可才能运行 SAS 代码。\n\n打开 SAS 程序文件。\n单击 VS Code 窗口左下方状态栏中的“无配置文件”。 您还可以打开命令面板（F1，或Ctrl+Shift+P在 Windows 或 Linux 上，或Shift+CMD+P在 OSX 上）并找到SAS: Add New Connection Profile命令。\n按照“添加新连接配置文件”部分中的说明添加配置文件。\n创建配置文件后，状态栏项将从“无配置文件”更改为新配置文件的名称。\n\n\n更多设置可以查看[SAS Extension for Visual Studio Code Documentation](https://sassoftware.github.io/vscode-sas-extension/Configurations/Profiles/sas9local)\n\n\n\nSAS 文件右上角有一个 奔跑的小人 ，点击即可开始运行所选中的程序段落，并在右侧窗口输出结果。\n\n\n\n\n\n\n安装 Anaconda 集成环境或 Python 和 SAS 软件，其中要求Python3.4+；\nSAS 需要 SAS 9.4+ 或 SAS Viya 3.1+；\nPython在Jupyter Notebook和SAS之间起一个桥梁的作用，Jupyter Notebook中的SAS代码会交给Python，Python负责将代码传递给SAS执行；\n然后将执行的结果返回给Jupyter Notebook显示。\n\nSAS版本要求9.4，也可以是 SAS Viya。\n\n\n\n启动 cmd，输入命令：\npip install saspy\nspecific release：\npip install http://github.com/sassoftware/saspy/releases/saspy-X.X.X.tar.gz\n然后就会自动安装 saspy 及其相应的依赖项。\n最好的更新或重装方式：\npip uninstall -y saspy\npip install saspy\n\n\n\nuv init name-of-project\ncd name-of-project\nuv add saspy # adds saspy to your project from PyPI\n\n\n\nconda create --name name-of-my-environment\nconda install --channel conda-forge saspy # Installs latest version of saspy from conda-forge channel.\nconda install --channel conda-forge saspy==X.X.X # Where X.X.X is the version you'd like to install.\n安装完成后可以输入命令：\njupyter kernelspec list\n来检测 saspy 是否安装成功，如果成功，理论上会看到如下形式的输出：\nAvailable kernels:\n    python3    /home/sas/anaconda3/lib/python3.5/site-packages/ipykernel/resources\n    sas        /home/sas/.local/share/jupyter/kernels/sas\n\n\n\n安装好 saspy 后找到 Anaconda 或 Python 的安装目录，会有一个相应的文件夹出现，例如我的文件路径如下：\nC:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\saspy\n在这个文件路径下找到 sascfg.py 文件，该文件中需要配置连接SAS的信息。可以配置连接本地机器的SAS；也可以配置连接远程机器的SAS Server，无论是Linux Server还是Windows Server都可以。此处就以连接本地SAS为例进行说明。\n\n打开该文件，首先是一大段注释；\n在这段注释后定义的第一个变量 SAS_config_names 用于指定连接SAS的配置方式，提供了 10 种方式：default, ssh, iomlinux, iomwin, winlocal, winiomlinux, winiomwin, httpsviya, httpviya, iomcom。默认为 default 方式。\n因为我们需要连接Windows机器本地的SAS，所以需要将 SAS_config_names 的值修改为 winlocal 。\n\n\nsascfg.py 的内容（2025年版配置文件）\n#\n# Copyright SAS Institute\n#\n#  Licensed under the Apache License, Version 2.0 (the License);\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\n# THIS IS AN EXAMPLE CONFIG FILE. PLEASE CREATE YOUR OWN sascfg_personal.py FILE USING THE APPROPRIATE TEMPLATES FROM BELOW\n# SEE THE CONFIGURATION DOC AT https://sassoftware.github.io/saspy/install.html#configuration\n\n\n# Configuration Names for SAS - python List\n# This is the list of allowed configuration definitions that can be used. The definition are defined below.\n# if there is more than one name in the list, and cfgname= is not specified in SASsession(), then the user\n# will be prompted to choose which configuration to use.\n#\n# The various options for the different access methods can be specified on the SASsession() i.e.:\n# sas = SASsession(cfgname='default', options='-fullstimer', user='me')\n#\n# Based upon the lock_down configuration option below, you may or may not be able to override option\n# that are defined already. Any necessary option (like user, pw for IOM or HTTP) that are not defined will be\n# prompted for at run time. To dissallow overrides of as OPTION, when you don't have a value, simply\n# specify options=''. This way it's specified so it can't be overridden, even though you don't have any\n# specific value you want applied.\n#\n#SAS_config_names = ['default', 'ssh', 'iomlinux', 'iomwin', 'winlocal', 'winiomlinux', 'winiomwin', 'httpsviya', 'httpviya', 'iomcom']\n#\n\nSAS_config_names=['winlocal']\n\n\n\n# Configuration options for saspy - python Dict   # not required unless changing any of the defaults\n# valid key are:\n#\n# 'lock_down' - True | False. True = Prevent runtime overrides of SAS_Config values below\n#\n# 'verbose'   - True | False. True = Allow print statements for debug type messages\n#\n# 'prompt'    - True | False. True = Allow prompting as necessary\n#\nSAS_config_options = {'lock_down': False,\n                      'verbose'  : True,\n                      'prompt'   : True\n                     }\n\n\n\n# Configuration options for SAS output. By default output is HTML 5.0 (using \"ods html5\" statement) but certain templates might not work\n# properly with HTML 5.0 so it can also be set to HTML 4.0 instead (using \"ods html\" statement). This option will only work when using IOM\n# in local mode. Note that HTML 4.0 will generate images separately which clutters the workspace and if you download the notebook as HTML,\n# the HTML file will need to be put in the same folder as the images for them to appear.\n# valid keys are:\n#\n# 'output' = ['html5', 'html']\n# 'style'  = any valid style   # this will be the default for SASsession.HTML_Style, which you can also change dynamically in your code\n#\n#\nSAS_output_options = {'output' : 'html5',       # not required unless changing any of the default\n                      'style'  : 'HTMLBlue'}\n\n\n# Configuration Definitions\n#\n# For STDIO and STDIO over SSH access methods\n# These need path to SASHome and optional startup options - python Dict\n# The default path to the sas start up script is: /opt/sasinside/SASHome/SASFoundation/9.4/sas\n# A usual install path is: /opt/sasinside/SASHome\n#\n# The encoding is figured out by saspy. You don't need to specify it, unless you just want to get rid of the message about which encoding was determined.\n#\n# valid keys are:\n# 'saspath'  - [REQUIRED] path to SAS startup script i.e.: /opt/sasinside/SASHome/SASFoundation/9.4/sas\n# 'options'  - SAS options to include in the start up command line - Python List\n# 'encoding' - This is the python encoding value that matches the SAS session encoding your SAS session is using\n#\n# For passwordless ssh connection, the following are also reuqired:\n# 'ssh'     - [REQUIRED] the ssh command to run\n# 'host'    - [REQUIRED] the host to connect to\n#\n# Additional valid keys for ssh:\n# 'port'    - [integer] the remote ssh port\n# 'tunnel'  - [integer] local port to open via reverse tunnel, if remote host cannot otherwise reach this client\n#\ndefault  = {'saspath'  : 'C:/Program Files/SASHome/SASFoundation/9.4/sas.exe'\n            }\n\nwinlocal = {\n    'saspath': 'C:\\\\Program Files\\\\SASHome\\\\SASFoundation\\\\9.4\\\\sas.exe'\n}\n\n# If you installed SAS by default path,the above path maybe effect for your Windows.\n\nssh      = {'saspath' : '/opt/sasinside/SASHome/SASFoundation/9.4/bin/sas_en',\n            'ssh'     : '/usr/bin/ssh',\n            'host'    : 'remote.linux.host',\n            'encoding': 'latin1',\n            'options' : [\"-fullstimer\"]\n            }\n\n\n# For IOM (Grid Manager or any IOM) and Local Windows via IOM access method\n# These configuration definitions are for connecting over IOM. This is designed to be used to connect to any Workspace server, including SAS Grid, via Grid Manager\n# and also to connect to a local Windows SAS session. The client side (python and java) for this access method can be either Linux or Windows.\n# The STDIO access method above is only for Linux. PC SAS requires this IOM interface.\n#\n# The absence of the iomhost option triggers local Windows SAS mode. In this case none of 'iomhost', 'iomport', 'omruser', 'omrpw' are needed.\n# a local SAS session is started up and connected to.\n#\n# The encoding is figured out by saspy. You don't need to specify it, unless you just want to get rid of the message about which encoding was determined.\n\n# NONE OF THE PATHS IN THESE EAMPLES ARE RIGHT FOR YOUT INSTALL. YOU HAVE TO CHANGE THE PATHS TO BE CORRECT FOR YOUR INSTALLATION\n#\n# valid keys are:\n# 'java'      - [REQUIRED] the path to the java executable to use\n# 'iomhost'   - [REQUIRED for remote IOM case, Don't specify to use a local Windows Session] the resolvable host name, or ip to the IOM server to connect to\n# 'iomport'   - [REQUIRED for remote IOM case, Don't specify to use a local Windows Session] the port IOM is listening on\n# 'authkey'   - identifier for user/password credentials to read from .authinfo file. Eliminates prompting for credentials.\n# 'omruser'   - not suggested        [REQUIRED for remote IOM case but PROMPTED for at runtime] Don't specify to use a local Windows Session\n# 'omrpw'     - really not suggested [REQUIRED for remote IOM case but PROMPTED for at runtime] Don't specify to use a local Windows Session\n# 'encoding'  - This is the python encoding value that matches the SAS session encoding of the IOM server you are connecting to\n# 'appserver' - name of physical workspace server (when more than one app server defined in OMR) i.e.: 'SASApp - Workspace Server'\n# 'sspi'      - boolean. use IWA instead of user/pw to connect to the IOM workspace server\n\n\niomlinux = {'java'      : '/usr/bin/java',\n            'iomhost'   : 'linux.iom.host',\n            'iomport'   : 8591,\n            }\n\niomwin   = {'java'      : '/usr/bin/java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            }\n\nwinlocal = {'java'      : 'java',\n            'encoding'  : 'EUC-CN',\n            }\n\nwiniomlinux = {'java'   : 'java',\n            'iomhost'   : 'linux.iom.host',\n            'iomport'   : 8591,\n            }\n\nwiniomwin  = {'java'    : 'java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            }\n\nwiniomIWA  = {'java'    : 'java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            'sspi'      : True\n            }\n\n\n# For Remote and Local IOM access methods using COM interface\n# These configuration definitions are for connecting over IOM using COM. This\n# access method is for Windows clients connecting to remote hosts. Local\n# SAS instances may also be supported.\n#\n# This access method does not require a Java dependency.\n#\n# Valid Keys:\n#   iomhost     - Required for remote connections only. The Resolvable SAS\n#                 server dns name.\n#   iomport     - Required for remote connections only. The SAS workspace\n#                 server port. Generally 8591 on standard remote\n#                 installations. For local connections, 0 is the default.\n#   class_id    - Required for remote connections only. The IOM workspace\n#                 server class identifier. Use `PROC IOMOPERATE` to identify\n#                 the correct value. This option is ignored on local connections.\n#   provider    - [REQUIRED] IOM provider. \"sas.iomprovider\" is recommended.\n#   encoding    - This is the python encoding value that matches the SAS\n#                 session encoding of the IOM server.\n#   omruser     - SAS user. This option is ignored on local connections.\n#   omrpw       - SAS password. This option is ignored on local connections.\n#   authkey     - Identifier for credentials to read from .authinfo file.\n\niomcom = {\n    'iomhost' : 'mynode.mycompany.org',\n    'iomport' : 8591,\n    'provider': 'sas.iomprovider',\n    'encoding': 'windows-1252'}\n\n\n# HTTP access method to connect to the Compute Service\n# These need ip addr, other values will be prompted for - python Dict\n# valid keys are:\n# 'url'     - (Required if ip not specified) The URL to Viya, of the form \"http[s]://host.idenifier[:port]\".\n#             When this is specified, ip= will not be used, as the host's ip is retrieved from the url. Also, ssl= is\n#             set based upon http or https and port= is also parsed from the url, if provided, else defaulted based\n#             upon the derived ssl= value. So neither ip, port nor ssl are needed when url= is used.\n# 'ip'      - (Required if url not specified) The resolvable host name, or IP address to the Viya Compute Service\n# 'port'    - port; the code Defaults this to based upon the 'ssl' key; 443 default else 80\n# 'ssl'     - whether to use HTTPS or just HTTP protocal. Default is True, using ssl and poort 443\n# 'context' - context name defined on the compute service  [PROMTED for at runtime if more than one defined]\n# 'authkey' - identifier for user/password credentials to read from .authinfo file. Eliminates prompting for credentials.\n# 'options' - SAS options to include (no '-' (dashes), just option names and values)\n# 'user'    - not suggested [REQUIRED but PROMTED for at runtime]\n# 'pw'      - really not suggested [REQUIRED but PROMTED for at runtime]\n#\n#\n\nhttpsviya = {'url'     : 'https://viya.deployment.com',\n             'context' : 'SAS Studio compute context',\n             'authkey' : 'viya_user-pw',\n             'options' : [\"fullstimer\", \"memsize=1G\"]\n             }\n\nhttpviya = {'url'     : 'https://sastpw.rndk8s.openstack.sas.com:23456',\n           #'port'    :  23456,   # can put different port here or ^ if it's not using the default port\n            'context' : 'SAS Studio compute context',\n            'authkey' : 'viya_user-pw',\n            'options' : [\"fullstimer\", \"memsize=1G\"]\n            }\n\n\n\n\n这是一个示例配置文件。请使用下面的相应模板创建您自己的 sascfg_personal.py 文件。有关配置的详细信息，请参阅以下文档：https://sassoftware.github.io/saspy/install.html#configuration\n\n\n\n\n这是允许使用的配置定义列表。定义如下所示。如果列表中有多个名称，并且在 SASsession() 中未指定 cfgname=，则系统将提示用户选择要使用的配置。\nSASsession() 中可以指定不同访问方法的各种选项，例如： sas = SASsession(cfgname='default', options='-fullstimer', user='me')\n根据下面的 lock_down 配置选项，您可能能够或不能够覆盖已定义的选项。任何必要但未定义的选项（如 IOM 或 HTTP 的 user、pw）将在运行时提示输入。要禁止覆盖作为 OPTION 的选项（当您没有值时），只需指定 options=''。这样它就被指定了，即使您没有要应用的特定值，也无法被覆盖。\n#SAS_config_names = ['default', 'ssh', 'iomlinux', 'iomwin', 'winlocal', 'winiomlinux', 'winiomwin', 'httpsviya', 'httpviya', 'iomcom']\n\nSAS_config_names=['winlocal']\n\n\n\n\n这些选项不是必需的，除非您需要更改任何默认值。有效的键包括：\n\n‘lock_down’: True | False。设置为 True 可防止在运行时覆盖下面的 SAS_Config 值。\n‘verbose’: True | False。设置为 True 可允许打印调试类型的消息。\n‘prompt’: True | False。设置为 True 可在必要时允许提示。\n\n\nSAS_config_options = {'lock_down': False,\n                      'verbose'  : True,\n                      'prompt'   : True\n                     }\n\n\n\n\nSAS 输出默认为 HTML 5.0（使用 “ods html5” 语句），但某些模板可能无法与 HTML 5.0 正常工作，因此也可以将其设置为 HTML 4.0（使用 “ods html” 语句）。此选项仅在使用 IOM 本地模式时有效。请注意，HTML 4.0 会单独生成图像，这会使工作区变得杂乱；如果您将笔记本下载为 HTML，HTML 文件需要与图像放在同一文件夹中才能显示图像。\n有效的键包括：\n\n‘output’: ['html5', 'html']\n‘style’: 任何有效的样式。这将是 SASsession.HTML_Style 的默认值，您也可以在代码中动态更改它。\n\n\nSAS_output_options = {'output' : 'html5',       # not required unless changing any of the default\n                      'style'  : 'HTMLBlue'}\n\n\n\n\n\n\n\n这些需要 SASHome 的路径和可选的启动选项——Python 字典。 SAS 启动脚本的默认路径是：/opt/sasinside/SASHome/SASFoundation/9.4/sas 常见的安装路径是：/opt/sasinside/SASHome\n编码由 saspy 自动识别。您不需要指定它，除非您想消除有关确定编码的消息。\n有效的键包括：\n\n‘saspath’: [必需] SAS 启动脚本的路径，例如：/opt/sasinside/SASHome/SASFoundation/9.4/sas\n‘options’: 要包含在启动命令行中的 SAS 选项 - Python 列表\n‘encoding’: 与您的 SAS 会话使用的 SAS 会话编码匹配的 Python 编码值\n\n对于无密码 SSH 连接，还需要以下项：\n\n‘ssh’: [必需] 要运行的 ssh 命令\n‘host’: [必需] 要连接的主机\n\nSSH 的其他有效键包括：\n\n‘port’: [整数] 远程 SSH 端口\n‘tunnel’: [整数] 如果远程主机无法连接此客户端，则通过反向隧道打开的本地端口\n\n\ndefault  = {'saspath'  : 'C:/Program Files/SASHome/SASFoundation/9.4/sas.exe'\n            }\n\nwinlocal = {\n    'saspath': 'C:\\\\Program Files\\\\SASHome\\\\SASFoundation\\\\9.4\\\\sas.exe'\n}\n\n# If you installed SAS by default path,the above path maybe effect for your Windows.\n\nssh      = {'saspath' : '/opt/sasinside/SASHome/SASFoundation/9.4/bin/sas_en',\n            'ssh'     : '/usr/bin/ssh',\n            'host'    : 'remote.linux.host',\n            'encoding': 'latin1',\n            'options' : [\"-fullstimer\"]\n            }\n\n\n\n\n这些配置定义用于通过 IOM 进行连接。这旨在用于连接到任何工作区服务器，包括 SAS Grid（通过 Grid Manager），以及连接到本地 Windows SAS 会话。此访问方法的客户端（Python 和 Java）可以是 Linux 或 Windows。上面提到的 STDIO 访问方法仅适用于 Linux。PC SAS 需要此 IOM 接口。\n缺少 iomhost 选项会触发本地 Windows SAS 模式。在这种情况下，不需要 iomhost、iomport、omruser、omrpw 中的任何一个。系统会启动并连接到本地 SAS 会话。\n编码由 saspy 自动识别。您不需要指定它，除非您想消除有关确定编码的消息。\n这些示例中的所有路径都不适合您的安装。您必须更改路径以使其与您的安装相符。\n有效的键包括：\n\n‘java’: [必需] 要使用的 Java 可执行文件的路径\n‘iomhost’: [远程 IOM 情况必需，不指定则使用本地 Windows 会话] 要连接的 IOM 服务器的可解析主机名或 IP\n‘iomport’: [远程 IOM 情况必需，不指定则使用本地 Windows 会话] IOM 正在监听的端口\n‘authkey’: 用于从 .authinfo 文件读取用户/密码凭据的标识符。消除了凭据提示。\n‘omruser’: 不建议使用 [远程 IOM 情况必需，但在运行时提示输入] 不指定则使用本地 Windows 会话\n‘omrpw’: 强烈不建议使用 [远程 IOM 情况必需，但在运行时提示输入] 不指定则使用本地 Windows 会话\n‘encoding’: 与您要连接的 IOM 服务器的 SAS 会话编码匹配的 Python 编码值\n‘appserver’: 物理工作区服务器的名称（当 OMR 中定义了多个应用程序服务器时），例如：‘SASApp - Workspace Server’\n‘sspi’: 布尔值。使用 IWA 而不是用户/密码连接到 IOM 工作区服务器\n\n\niomlinux = {'java'      : '/usr/bin/java',\n            'iomhost'   : 'linux.iom.host',\n            'iomport'   : 8591,\n            }\n\niomwin   = {'java'      : '/usr/bin/java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            }\n\nwinlocal = {'java'      : 'java',\n            'encoding'  : 'EUC-CN',\n            }\n\nwiniomlinux = {'java'   : 'java',\n            'iomhost'   : 'linux.iom.host',\n            'iomport'   : 8591,\n            }\n\nwiniomwin  = {'java'    : 'java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            }\n\nwiniomIWA  = {'java'    : 'java',\n            'iomhost'   : 'windows.iom.host',\n            'iomport'   : 8591,\n            'sspi'      : True\n            }\n\n\n\n\n这些配置定义用于通过 COM 连接到 IOM。此访问方法适用于连接到远程主机的 Windows 客户端。本地 SAS 实例也可能受支持。\n此访问方法不需要 Java 依赖项。\n有效键：\n\niomhost: 仅适用于远程连接。可解析的 SAS 服务器 DNS 名称。\niomport: 仅适用于远程连接。SAS 工作区服务器端口。在标准远程安装中通常为 8591。对于本地连接，默认值为 0。\nclass_id: 仅适用于远程连接。IOM 工作区服务器类标识符。使用 PROC IOMOPERATE 来识别正确的值。此选项在本地连接上被忽略。\nprovider: [必需] IOM 提供程序。建议使用 “sas.iomprovider”。\nencoding: 与 IOM 服务器的 SAS 会话编码匹配的 Python 编码值。\nomruser: SAS 用户。此选项在本地连接上被忽略。\nomrpw: SAS 密码。此选项在本地连接上被忽略。\nauthkey: 用于从 .authinfo 文件读取凭据的标识符。\n\n\niomcom = {\n    'iomhost' : 'mynode.mycompany.org',\n    'iomport' : 8591,\n    'provider': 'sas.iomprovider',\n    'encoding': 'windows-1252'}\n\n\n\n\n这些需要 IP 地址，其他值将在运行时提示输入 - Python Dict。\n有效键包括：\n\n‘url’: （如果未指定 IP 则必需）Viya 的 URL，形式为 “http[s]://host.identifier[:port]”。当指定此项时，ip= 将不被使用，因为主机的 IP 是从 URL 中检索的。此外，ssl= 会根据 http 或 https 设置，并且 port= 也会从 URL 中解析（如果提供），否则会根据派生的 ssl= 值采用默认值。因此，当使用 url= 时，不需要 ip、port 或 ssl。\n‘ip’: （如果未指定 URL 则必需）Viya 计算服务的可解析主机名或 IP 地址\n‘port’: 端口；代码会根据 ‘ssl’ 键将其默认为 443（如果使用 SSL）或 80（否则）\n‘ssl’: 是否使用 HTTPS 或仅使用 HTTP 协议。默认为 True，使用 SSL 和端口 443。\n‘context’: 在计算服务上定义的上下文名称 [如果定义了多个，则在运行时提示选择]\n‘authkey’: 用于从 .authinfo 文件读取用户/密码凭据的标识符。消除了凭据提示。\n‘options’: 要包含的 SAS 选项（不带 ‘-’（破折号），只有选项名称和值）\n‘user’: 不建议使用 [必需，但在运行时提示输入]\n‘pw’: 强烈不建议使用 [必需，但在运行时提示输入]\n\n\nhttpsviya = {'url'     : 'https://viya.deployment.com',\n             'context' : 'SAS Studio compute context',\n             'authkey' : 'viya_user-pw',\n             'options' : [\"fullstimer\", \"memsize=1G\"]\n             }\n\nhttpviya = {'url'     : 'https://sastpw.rndk8s.openstack.sas.com:23456',\n           #'port'    :  23456,   # can put different port here or ^ if it's not using the default port\n            'context' : 'SAS Studio compute context',\n            'authkey' : 'viya_user-pw',\n            'options' : [\"fullstimer\", \"memsize=1G\"]\n            }\n后续有一些安装步骤，但是大多是在2016-2020年更新的教程，无法找到复现的路径，可能相关的配置已被优化。\n包括这个 [SAS岩论 | 在Jupyter Notebook中使用SAS ](https://www.sohu.com/a/218339423_278472) 中写到的需要使用 cpW 定义 SAS 路径。\n\n\n\n\n将sas相关文件 sspiauth.dll 添加到系统环境变量，该文件很可能在如下目录：\nC:\\Program Files\\SASHome\\SASFoundation\\9.4\\core\\sasext\n（注意添加变量时不要包含 sspiauth.dll 文件本身）\nWarning: 环境变量添加完成后，要重启电脑才会生效。\n\n\n\n新建文件，选择使用 SAS 内核，或者在 cell 中通过 magic command 指定内核。\n%%sas\n使用语法如下所示：\n%%sas\ndata iris;\n    set sashelp.iris;\nrun;\n\nproc print data=iris(obs=10);\nrun;\n在Notebook中写SAS代码了，跟Python一样，同样有代码提示、语法高亮的功能。但是你会注意到过程步的结果显示了，运行的日志去哪里了？\n如果代码运行错误或者没有输出（例如纯DATA步）的话，那么输出就是日志信息。\n能够正确运行且有输出结果的代码就不会显示日志了。\n\n\n\n\n如果想要像SAS Base一样，随时查看所有程序运行的日志结果也没问题。安装一个Notebook的SAS日志扩展组件就可以了。打开Anaconda Prompt，输入以下命令安装：\njupyter nbextension install --py sas_kernel.showSASLog\n运行完毕后，输入以下命令启用SAS日志组件：\njupyter nbextension enable sas_kernel.showSASLog –py\n\n\n\n如果需要配置连接远程的SAS Server，如连接远程Windows机器的SAS Server，需在sascfg.py中做以下修改：\n\n将SAS_config_names的值改为“wintowin”；\n在wintowin连接方式中将参数iomhost的值修改为远程Windows机器的IP地址；将参数encoding的值修改为euc-cn；\n将cpW中5个Jar包的路径修改为远程Windows机器中SAS对应的目录。\n\n修改完毕后，启动Notebook，首次运行SAS代码时，会提示输入访问SAS Server的有效SAS用户和密码。1\n\n\n\n\n\nSASPy 是一个 Python 库，允许你通过 Python 代码与 SAS 进行交互。SAS Kernel 依赖于 SASPy，因此在使用 SAS Kernel 之前，你需要配置 SASPy。\n\n\n\nSAS Kernel 支持 JupyterLab 扩展，这些扩展可以提高你在 JupyterLab 中的编程效率。你可以通过以下命令安装这些扩展：\npip install sas_kernel[jlab_ext]\n\n\n\nNBGrader 是一个用于分配和评分 Jupyter Notebook 的系统，它与 SAS Kernel 兼容。你可以使用 NBGrader 来创建和评分包含 SAS 代码的作业。\n通过这些生态项目，SAS Kernel 不仅扩展了 Jupyter Notebook 的功能，还增强了其在数据科学和分析领域的应用能力。2",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#sas-在-windows-系统中的安装",
    "href": "Guide/SAS/25-03-11-SAS-install.html#sas-在-windows-系统中的安装",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "",
    "text": "[SAS9.4下载安装教程](https://blog.csdn.net/wzk4869/article/details/128893187)",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#sas-在-linux-系统中的安装",
    "href": "Guide/SAS/25-03-11-SAS-install.html#sas-在-linux-系统中的安装",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "",
    "text": "[Linux 环境下 SAS 软件安装与配置详细教程](https://my.oschina.net/emacs_8839324/blog/17390712)\nSAS 在 Linux 系统中的安装极为复杂，笔者曾在某短时间研究多日，最后还是没有安装成功。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#sas-在-macos-中的安装",
    "href": "Guide/SAS/25-03-11-SAS-install.html#sas-在-macos-中的安装",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "",
    "text": "在 macOS 中安装SAS一般有两种途径：\n\n下载 SAS University Edition for mac（这个版本是免费版，目前还不了解哪些功能被阉割）\n在 macOS 上搭建 Windows 虚拟机，然后按照 Windows上装 SAS 的方法进行。\n\n如果你有可用的 SID 可以考虑安装虚拟机后运行 SAS，如果没有可用的 SID，则考虑使用免费的 免费使用 SAS 云软件，需要按照身份和需求进行注册，最后登录 SAS® OnDemand for Academics进行使用。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#典型生态项目",
    "href": "Guide/SAS/25-03-11-SAS-install.html#典型生态项目",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "",
    "text": "SASPy 是一个 Python 库，允许你通过 Python 代码与 SAS 进行交互。SAS Kernel 依赖于 SASPy，因此在使用 SAS Kernel 之前，你需要配置 SASPy。\n\n\n\nSAS Kernel 支持 JupyterLab 扩展，这些扩展可以提高你在 JupyterLab 中的编程效率。你可以通过以下命令安装这些扩展：\npip install sas_kernel[jlab_ext]\n\n\n\nNBGrader 是一个用于分配和评分 Jupyter Notebook 的系统，它与 SAS Kernel 兼容。你可以使用 NBGrader 来创建和评分包含 SAS 代码的作业。\n通过这些生态项目，SAS Kernel 不仅扩展了 Jupyter Notebook 的功能，还增强了其在数据科学和分析领域的应用能力。2",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Learn/Advance/00-advance.html#一主要研究内容",
    "href": "Learn/Advance/00-advance.html#一主要研究内容",
    "title": "00-医学统计学（伪）高级部分",
    "section": "",
    "text": "高级医学统计学涵盖以下几类主要内容：\n\n\n\n广义线性模型（GLM）：如 Logistic 回归、Poisson 回归；\n广义估计方程（GEE）：用于相关数据（如重复测量）；\n混合效应模型（Mixed Models）：线性/非线性，适用于多层级数据。\n\n\n\n\n\nCox 比例风险模型：分析事件发生时间；\n竞争风险模型；\n多状态模型；\n加速失效时间模型（AFT）。\n\n\n\n\n\n先验信息的引入；\nMCMC 模拟；\n贝叶斯参数估计与预测。\n\n\n\n\n\n倾向得分匹配（PSM）、加权（IPTW）；\n工具变量（IV）；\n双重稳健估计；\n结构方程模型（SEM）；\n鲁宾因果模型（RCM）与结构因果模型（SCM）。\n\n\n\n\n\nBonferroni、Holm 方法；\nFDR（False Discovery Rate）；\n多重检验在组学/影像/大数据中的应用。\n\n\n\n\n\n多重插补（MI）；\n最大似然估计（MLE）；\n蒙特卡洛模拟。\n\n\n\n\n\n纵向数据分析；\n时间序列分析；\n空间统计分析；\n网络医学统计（如脑连接网络）；\n高维数据统计学（如组学、大数据中的变量选择）。\n\n\n\n\n\n随机对照试验（RCT）；\n自适应设计；\n非劣效设计；\n样本量估计、功效分析。",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "00-医学统计学（伪）高级部分"
    ]
  },
  {
    "objectID": "Learn/Advance/00-advance.html#二常用工具和软件",
    "href": "Learn/Advance/00-advance.html#二常用工具和软件",
    "title": "00-医学统计学（伪）高级部分",
    "section": "",
    "text": "R / Python：用于建模、可视化、高维数据分析；\nSAS：广泛用于临床试验数据管理与分析；\nStata：广泛应用于经管类的数据建模与分析，适用于医疗政策评估等内容的应用；\nWinBUGS / JAGS / Stan：贝叶斯建模；\nSPSS：较适用于传统统计分析。",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "00-医学统计学（伪）高级部分"
    ]
  },
  {
    "objectID": "Learn/Advance/00-advance.html#三应用场景",
    "href": "Learn/Advance/00-advance.html#三应用场景",
    "title": "00-医学统计学（伪）高级部分",
    "section": "",
    "text": "高级医学统计学的应用包括但不限于：\n\n复杂临床试验设计与分析；\n公共卫生大数据分析；\n预测模型开发（如疾病风险模型）；\n基因组学、蛋白质组学等组学研究；\n医疗政策评估；\n医学人工智能建模（统计学习、机器学习的融合）。",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "00-医学统计学（伪）高级部分"
    ]
  },
  {
    "objectID": "Learn/Advance/00-advance.html#四学习资料",
    "href": "Learn/Advance/00-advance.html#四学习资料",
    "title": "00-医学统计学（伪）高级部分",
    "section": "",
    "text": "[因果推断：从概念到实践](https://github.com/xieliaing/CausalInferenceIntro)",
    "crumbs": [
      "Home",
      "Learn",
      "Advance",
      "00-医学统计学（伪）高级部分"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#格式冲突修复尝试",
    "href": "Guide/SAS/25-03-11-SAS-install.html#格式冲突修复尝试",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "2.4 格式冲突修复尝试",
    "text": "2.4 格式冲突修复尝试\n和 AI 多聊了一下这个问题，不是很好解决，涉及到不同的 html 格式版本和 html 格式的完整程度的不同，因为 Stata 输出的格式要干净许多，所以不会和 Quarto 的格式发生冲突，但是 SAS 不行。",
    "crumbs": [
      "Home",
      "Guide",
      "SAS",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/R/04-ggplot2-folded-line.html",
    "href": "Guide/R/04-ggplot2-folded-line.html",
    "title": "04-ggplot2 绘图——折线图",
    "section": "",
    "text": "运行 ggplot() 函数和 geom_line() 函数，并分别指定一个变量映射到 x 和 y 。\n\n# load package\nlibrary(ggplot2)\n# load dataset\nBOD\n\n  Time demand\n1    1    8.3\n2    2   10.3\n3    3   19.0\n4    4   16.0\n5    5   15.6\n6    7   19.8\n\nggplot(BOD,aes(x = Time,y = demand))+\ngeom_line()\n\n\n\n\n\n\n图 1: 基本折线图\n\n\n\n\n折线图的 X 轴既可以对应于离散（分类）变量，也可以对应于连续（数值型）变量。\n在上图中，Demand 变量为数值型变量，但我们可以借助于 factor() 函数将其转换为因子型变量，然后，将其当作分类变量来处理。\n当 X 对应于因子型变量时，必须使用命令 aex(group = 1) 以确保 ggplot 知道这些数据点属于同一个分组，应该用一条折现将他们连在一起。\n\n# load package\nlibrary(ggplot2)\n# load dataset and copy a dataset\nBOD1 &lt;- BOD\nBOD1$Time &lt;- factor(BOD1$Time)\n\nggplot(BOD1,aes(x = Time,y = demand,group = 1))+\ngeom_line()\n\n\n\n\n\n\n图 2: 因子型变量折线图\n\n\n\n\n\n在代码中加上 geom_point() 即可：\n\n# load package\nlibrary(ggplot2)\n# load dataset\nBOD\n\n  Time demand\n1    1    8.3\n2    2   10.3\n3    3   19.0\n4    4   16.0\n5    5   15.6\n6    7   19.8\n\nggplot(BOD,aes(x = Time,y = demand))+\ngeom_line()+\ngeom_point()\n\n\n\n\n\n\n图 3: 折线图添加标记\n\n\n\n\n有时候，在折线图上添加数据标记很有用。\n当数据点的密度较低或数据采集频率不规则时，尤其有用。\n例如，BOD 数据集中没有与 Time=6 相对应的输入，然而，这在一张单独的折线图中看起来并不明显。\n\nworldpop数据集对应的采集时间间隔不是常数，时间距今较久远的数据采集频率比新近不久的数据采集频率低。\n折线图中的数据标记表明了数据的采集时间：\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\nggplot(worldpop,aes(x = Year,y = Population))+\ngeom_line()+\ngeom_point()\n\n# when Y axis turn to log()\nggplot(worldpop,aes(x = Year,y = Population))+\ngeom_line()+\ngeom_point()+\nscale_y_log10()\n\n\n\n\n\n\n图 4: worldpop 数据集示例\n\n\n\n\n\n\n\n\n\n图 5: worldpop 数据集示例\n\n\n\n\n\n除了分别设定一个映射到 x 和 y 的变量，再将另一个（离散）变量映射到颜色（colour）或者线性（linetype）即可：\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n#colour\nggplot(tg,aes(x = dose,y = length, colour = supp))+\ngeom_line()\n\n# linetype\nggplot(tg,aes(x = dose,y = length, linetype = supp))+\ngeom_line()\n\n\n\n\n\n\n图 6: 绘制多重折线图\n\n\n\n\n\n\n\n\n\n图 7: 绘制多重折线图\n\n\n\n\ntg 数据集共有三列，其中一列正是我们映射到 colour 和 linetype 的 supp 因子（离散型变量），而将 dose 作为 x 轴，length 作为 y 轴（连续型变量）。\n折线图的 X 轴既可以对应于连续变量也可以对应于分类变量。\n有时候，映射到 X 的变量虽然被存储为数值型变量，但被看做分类变量来处理。\n在上述的 tg 数据集中，dose 有三个取值：0.5、1.0和2.0。\n或许用户更想将其当做分类变量而不是连续变量来处理，那么运行 factor() 函数将其转换为因子型变量。\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n#colour\nggplot(tg,aes(x = factor(dose),y = length, colour = supp,group = supp))+\ngeom_line()\n\n\n\n\n\n\n图 8: 转换变量绘制折线图\n\n\n\n\n\n如果折线图上有数据标记，你也可以将分组变量映射到数据标记的属性，如 shape 和 fill 等。\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n#shape\nggplot(tg,aes(x = dose,y = length, shape = supp))+\ngeom_line()+\ngeom_point(size = 4)\n\n# fill\nggplot(tg,aes(x = dose,y = length, fill = supp))+\ngeom_line()+\ngeom_point(size = 4,shape = 21)\n\n\n\n\n\n\n图 9: 折线图数据标记\n\n\n\n\n\n\n\n\n\n图 10: 折线图数据标记\n\n\n\n\n\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n#shape\nggplot(tg,aes(x = dose,y = length, shape = supp))+\ngeom_line(position = position_dodge(0.2))+\ngeom_point(position = position_dodge(0.2),size = 4)\n\n\n\n\n\n\n图 11: 折线图错开数据标记\n\n\n\n\n\n通过设置线型（linetype）、线宽（linewidth）和颜色（colour或color）参数可以分别修改折线的线性（实线、虚线、点线等），线宽（单位为毫米）和颜色。\n将上述参数的值传递给 geom_line() 函数可以设置折线图的对应属性。\n\n# load package\nlibrary(ggplot2)\n\n#shape\nggplot(BOD,aes(x = Time,y = demand,))+\ngeom_line(linetype = \"dashed\",linewidth = 1,colour = \"blue\")\n\n\n\n\n\n\n图 12: 修改折线图线条样式\n\n\n\n\n\n对于多重折线图而言，设定图形属性会对图上的所有折线产生影响。\n而将变量映射到图形属性则会使图上的每条折线具有不同的外观。\n折线图默认颜色并不是很吸引眼球，所以用户可以使用其他调色板1着色，可以调用 scale_colour_brewer() 函数和 scale_colour_manual 函数实线。\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n#shape\nggplot(tg,aes(x = dose,y = length, colour = supp))+\ngeom_line()+\nscale_colour_brewer(palette = \"Set1\")\n\n\n\n\n\n\n图 13: 使用调色板\n\n\n\n\n\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n# 如果两条折线的图形属性相同，需要指定一个分组变量\nggplot(tg,aes(x = dose,y = length, group = supp))+\ngeom_line(colour = \"darkgreen\",size = 1.5)\n\n# 因为变量 supp 被映射到了颜色（colour）属性，所以它自动作为分组变量\nggplot(tg,aes(x = dose,y = length, colour = supp))+\ngeom_line(linetype = \"dashed\")+\ngeom_point(shape = 22,size = 3,fill = \"white\")\n\n\n\n\n\n\n图 14: 折线图高级配色与线条\n\n\n\n\n\n\n\n\n\n图 15: 折线图高级配色与线条\n\n\n\n\n\n在函数 aes() 外部设定函数 geom_point() 的参数 size（大小）、shape（形状）、colour（颜色）和 fill（填充色）即可。\n\n# load package\nlibrary(ggplot2)\n\nggplot(BOD,aes(x = Time,y = demand))+\ngeom_line()+\ngeom_point(size = 4,shape = 22,colour = \"darkred\",fill = \"pink\")\n\n\n\n\n\n\n图 16: 折线图修改数据标记样式\n\n\n\n\nThe default shape for points is a solid circle, the default size is 2, and the default colour is black.The fill color is relevant only for some point shapes (numbered 21–25), which have separate outline and fill colors. The fill color is typically NA, or empty; you can fill it with white to get hollow-looking circles\n\n# load package\nlibrary(ggplot2)\n\nggplot(BOD, aes(x = Time, y = demand)) +\n  geom_line() +\n  geom_point(size = 4, shape = 21, fill = \"white\")\n\n\n\n\n\n\n图 17: 修改折线图默认样式\n\n\n\n\n如果要将数据标记和折线设定为不同的颜色，用户必须折线绘制完成后再设定数据标记的颜色，此时，数据标记被绘制在更上面的图层，以避免被折线遮盖。\n这里定义了一个函数 pd = position_dodge(0.2) 来将图形错开，原始用法可参照 图 11。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook)  # Load gcookbook for the tg data set\n\n# Save the position_dodge specification because we'll use it multiple times\npd &lt;- position_dodge(0.2)\n\nggplot(tg, aes(x = dose, y = length, fill = supp)) +\n  geom_line(position = pd) +\n  geom_point(shape = 21, size = 3, position = pd) +\n  scale_fill_manual(values = c(\"black\",\"white\"))\n\n\n\n\n\n\n图 18: 折线图复杂数据标记\n\n\n\n\n\n运行 geom_area() 函数即可绘制面积图。\n\n# load package\nlibrary(ggplot2)\n\n# Convert the sunspot.year data set into a data frame for this example\nsunspotyear &lt;- data.frame(\n    Year     = as.numeric(time(sunspot.year)),\n    Sunspots = as.numeric(sunspot.year)\n  )\n\nggplot(sunspotyear, aes(x = Year, y = Sunspots)) +\n  geom_area()\n\n\n\n\n\n\n图 19: 绘制面积图\n\n\n\n\n默认情况下，面积图的填充色都是黑灰色且没有边框线，通过设定 fill 可以修改面积图的填充色。\n下图 图 20 将把填充色设定为蓝色，并通过 alpha=0.2 将面积图的透明度设定为 80% ，另外，用户可以看到面积图的网格线，通过设置 colour 为面积图添加边框线。\n\n# load package\nlibrary(ggplot2)\n\nsunspotyear &lt;- data.frame(\n    Year     = as.numeric(time(sunspot.year)),\n    Sunspots = as.numeric(sunspot.year)\n  )\n\nggplot(sunspotyear, aes(x = Year, y = Sunspots)) +\n  geom_area(colour = \"black\", fill = \"blue\", alpha = .2)\n\n\n\n\n\n\n图 20: 带半透明阴影区域和边框线的面积图\n\n\n\n\n给整个面积图添加边框线后的效果可能并不十分令人满意，因为此时系统会在面积图的起点和终点位置分别绘制一条垂直线，且在底部绘制一条横线。\n为了避免上述情况，可以先绘制不带框线的面积图（不设定 colour），然后添加图层，并用 geom_line() 函数绘制轨迹线，如下图 图 21 。\n\n# load package\nlibrary(ggplot2)\n\nsunspotyear &lt;- data.frame(\n    Year     = as.numeric(time(sunspot.year)),\n    Sunspots = as.numeric(sunspot.year)\n  )\n\nggplot(sunspotyear, aes(x = Year, y = Sunspots)) +\n  geom_area(fill = \"blue\", alpha = .2)+\n  geom_line()\n\n\n\n\n\n\n图 21: 不带下横线的面积图\n\n\n\n\n\n依旧是运行 geom_area() 函数，但是需要映射一个因子型变量到 fill 即可，如下图 图 22 所示。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the uspopage data set\n\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area()\n\n\n\n\n\n\n图 22: 绘制堆积面积图\n\n\n\n\n堆积面积图对应的基础数据通常为宽格式（wide format），但 ggplot 要求数据必须是长格式（long format），两种格式之间需要进行转换。2\n将 图 22 的调色板设定为蓝色渐变色，并在各个区域之间添加细线（size = .2），同时我们将填充区域设定为半透明（alpha = .4 )，这样可以透过填充区域看见网格线。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the uspopage data set\n\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area(colour = \"black\", size = .2, alpha = .4) +\n  scale_fill_brewer(palette = \"Blues\")\n\n\n\n\n\n\n图 23: 绘制渐变堆积面积图\n\n\n\n\n因为堆积面积图中的各个部分是由多边形构成的，所以其具有左、右边框线，这样的绘图效果不尽人意且可能产生误导效果。\n下图将绘制一个不带边框线的堆积面积图：\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the uspopage data set\n\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup, order = dplyr::desc(AgeGroup))) +\n  geom_area(colour = NA, alpha = .4) +\n  scale_fill_brewer(palette = \"Blues\") +\n  geom_line(position = \"stack\", size = .2)\n\n\n\n\n\n\n图 24: 不带边框线的渐变堆积面积图\n\n\n\n\n\n使用 geom_area(position = \"fill\") 函数实现这一效果：\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the uspopage data set\n\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area(position = \"fill\",colour = \"black\", size = .2, alpha = .4) +\n  scale_fill_brewer(palette = \"Blues\")\n\n\n\n\n\n\n图 25: 百分比堆积面积图\n\n\n\n\n带百分比标签的百分比堆积面积图，通过运行 position = \"fill\" 可将 y 值的范围按比例调整为 0-1。\n再运行 scale_y_continuous(labels = scales::percent) 可输出百分比标签。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the uspopage data set\n\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area(position = \"fill\",colour = \"black\", size = .2, alpha = .4) +\n  scale_fill_brewer(palette = \"Blues\")+\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n图 26: 带百分比标签的百分比堆积面积图\n\n\n\n\n\n使用 geom_ribbon() 函数，然后分别映射一个变量到 ymin 和 ymax 即可。\n这里使用的数据集为 climate 数据集，其中 Anomaly10y 变量表示了各年温度相对于 1950~1980 年平均水平偏差（以摄氏度衡量）的 10 年移动平均。变量 Unc10y 表示其 95% 置信水平下的置信区间。\n这里设定 ymax 和 ymin 分别等于Anomaly10y+Unc10y 和 Anomaly10y-Unc10y\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the climate data set\nlibrary(dplyr)\n\n# Grab a subset of the climate data\nclimate_mod &lt;- climate %&gt;%\n  filter(Source == \"Berkeley\") %&gt;%\n  select(Year, Anomaly10y, Unc10y)\n\nhead(climate_mod)\n\n  Year Anomaly10y Unc10y\n1 1800     -0.435  0.505\n2 1801     -0.453  0.493\n3 1802     -0.460  0.486\n4 1803     -0.493  0.489\n5 1804     -0.536  0.483\n6 1805     -0.541  0.475\n\n#&gt;     Year Anomaly10y Unc10y\n#&gt; 1   1800     -0.435  0.505\n#&gt; 2   1801     -0.453  0.493\n#&gt; 3   1802     -0.460  0.486\n#&gt;  ...&lt;199 more rows&gt;...\n#&gt; 203 2002      0.856  0.028\n#&gt; 204 2003      0.869  0.028\n#&gt; 205 2004      0.884  0.029\n\n# Shaded region\nggplot(climate_mod, aes(x = Year, y = Anomaly10y)) +\n  geom_ribbon(aes(ymin = Anomaly10y - Unc10y, ymax = Anomaly10y + Unc10y), alpha = 0.2) +\n  geom_line()\n\n\n\n\n\n\n图 27: 折线图添加置信域示例\n\n\n\n\n阴影部分的颜色实际上是黑灰色，但是看起来几乎是半透明的，这是因为通过设定 alpha = 0.2 将阴影部分的透明度设置为 80%。\n需要注意的是，如果使用这种带阴影或颜色的置信域，应该将 geom_line() 函数放在 geom_ribbon() 之后，这样折线会被绘制在阴影区域上面的图层，如果反转顺序的话，阴影区域的颜色有可能使折线模糊不清。\n但是用户可以选择使用虚线来表示置信域的上下边界，这样就不用考虑色域的问题，如下图 图 28 所示。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the climate data set\nlibrary(dplyr)\n\n# With a dotted line for upper and lower bounds\nggplot(climate_mod, aes(x = Year, y = Anomaly10y)) +\n  geom_line(aes(y = Anomaly10y - Unc10y), colour = \"grey50\", linetype = \"dotted\") +\n  geom_line(aes(y = Anomaly10y + Unc10y), colour = \"grey50\", linetype = \"dotted\") +\n  geom_line()\n\n\n\n\n\n\n图 28: 折线图的虚线置信域示例\n\n\n\n\nend.",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "04-ggplot2 绘图——折线图"
    ]
  },
  {
    "objectID": "Guide/R/04-ggplot2-folded-line.html#绘制基本折线图",
    "href": "Guide/R/04-ggplot2-folded-line.html#绘制基本折线图",
    "title": "04-ggplot2 绘图——折线图",
    "section": "",
    "text": "运行 ggplot() 函数和 geom_line() 函数，并分别指定一个变量映射到 x 和 y 。\n\n# load package\nlibrary(ggplot2)\n# load dataset\nBOD\n\n  Time demand\n1    1    8.3\n2    2   10.3\n3    3   19.0\n4    4   16.0\n5    5   15.6\n6    7   19.8\n\nggplot(BOD,aes(x = Time,y = demand))+\ngeom_line()\n\n\n\n\n\n\n图 1: 基本折线图\n\n\n\n\n折线图的 X 轴既可以对应于离散（分类）变量，也可以对应于连续（数值型）变量。\n在上图中，Demand 变量为数值型变量，但我们可以借助于 factor() 函数将其转换为因子型变量，然后，将其当作分类变量来处理。\n当 X 对应于因子型变量时，必须使用命令 aex(group = 1) 以确保 ggplot 知道这些数据点属于同一个分组，应该用一条折现将他们连在一起。\n\n# load package\nlibrary(ggplot2)\n# load dataset and copy a dataset\nBOD1 &lt;- BOD\nBOD1$Time &lt;- factor(BOD1$Time)\n\nggplot(BOD1,aes(x = Time,y = demand,group = 1))+\ngeom_line()\n\n\n\n\n\n\n图 2: 因子型变量折线图",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "04-ggplot2 绘图——折线图"
    ]
  },
  {
    "objectID": "Guide/R/04-ggplot2-folded-line.html#向折线图添加数据标记",
    "href": "Guide/R/04-ggplot2-folded-line.html#向折线图添加数据标记",
    "title": "04-ggplot2 绘图——折线图",
    "section": "",
    "text": "在代码中加上 geom_point() 即可：\n\n# load package\nlibrary(ggplot2)\n# load dataset\nBOD\n\n  Time demand\n1    1    8.3\n2    2   10.3\n3    3   19.0\n4    4   16.0\n5    5   15.6\n6    7   19.8\n\nggplot(BOD,aes(x = Time,y = demand))+\ngeom_line()+\ngeom_point()\n\n\n\n\n\n\n图 3: 折线图添加标记\n\n\n\n\n有时候，在折线图上添加数据标记很有用。\n当数据点的密度较低或数据采集频率不规则时，尤其有用。\n例如，BOD 数据集中没有与 Time=6 相对应的输入，然而，这在一张单独的折线图中看起来并不明显。\n\nworldpop数据集对应的采集时间间隔不是常数，时间距今较久远的数据采集频率比新近不久的数据采集频率低。\n折线图中的数据标记表明了数据的采集时间：\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\nggplot(worldpop,aes(x = Year,y = Population))+\ngeom_line()+\ngeom_point()\n\n# when Y axis turn to log()\nggplot(worldpop,aes(x = Year,y = Population))+\ngeom_line()+\ngeom_point()+\nscale_y_log10()\n\n\n\n\n\n\n图 4: worldpop 数据集示例\n\n\n\n\n\n\n\n\n\n图 5: worldpop 数据集示例",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "04-ggplot2 绘图——折线图"
    ]
  },
  {
    "objectID": "Guide/R/04-ggplot2-folded-line.html#绘制多重折线图",
    "href": "Guide/R/04-ggplot2-folded-line.html#绘制多重折线图",
    "title": "04-ggplot2 绘图——折线图",
    "section": "",
    "text": "除了分别设定一个映射到 x 和 y 的变量，再将另一个（离散）变量映射到颜色（colour）或者线性（linetype）即可：\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n#colour\nggplot(tg,aes(x = dose,y = length, colour = supp))+\ngeom_line()\n\n# linetype\nggplot(tg,aes(x = dose,y = length, linetype = supp))+\ngeom_line()\n\n\n\n\n\n\n图 6: 绘制多重折线图\n\n\n\n\n\n\n\n\n\n图 7: 绘制多重折线图\n\n\n\n\ntg 数据集共有三列，其中一列正是我们映射到 colour 和 linetype 的 supp 因子（离散型变量），而将 dose 作为 x 轴，length 作为 y 轴（连续型变量）。\n折线图的 X 轴既可以对应于连续变量也可以对应于分类变量。\n有时候，映射到 X 的变量虽然被存储为数值型变量，但被看做分类变量来处理。\n在上述的 tg 数据集中，dose 有三个取值：0.5、1.0和2.0。\n或许用户更想将其当做分类变量而不是连续变量来处理，那么运行 factor() 函数将其转换为因子型变量。\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n#colour\nggplot(tg,aes(x = factor(dose),y = length, colour = supp,group = supp))+\ngeom_line()\n\n\n\n\n\n\n图 8: 转换变量绘制折线图\n\n\n\n\n\n如果折线图上有数据标记，你也可以将分组变量映射到数据标记的属性，如 shape 和 fill 等。\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n#shape\nggplot(tg,aes(x = dose,y = length, shape = supp))+\ngeom_line()+\ngeom_point(size = 4)\n\n# fill\nggplot(tg,aes(x = dose,y = length, fill = supp))+\ngeom_line()+\ngeom_point(size = 4,shape = 21)\n\n\n\n\n\n\n图 9: 折线图数据标记\n\n\n\n\n\n\n\n\n\n图 10: 折线图数据标记\n\n\n\n\n\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n#shape\nggplot(tg,aes(x = dose,y = length, shape = supp))+\ngeom_line(position = position_dodge(0.2))+\ngeom_point(position = position_dodge(0.2),size = 4)\n\n\n\n\n\n\n图 11: 折线图错开数据标记",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "04-ggplot2 绘图——折线图"
    ]
  },
  {
    "objectID": "Guide/R/04-ggplot2-folded-line.html#修改线条样式",
    "href": "Guide/R/04-ggplot2-folded-line.html#修改线条样式",
    "title": "04-ggplot2 绘图——折线图",
    "section": "",
    "text": "通过设置线型（linetype）、线宽（linewidth）和颜色（colour或color）参数可以分别修改折线的线性（实线、虚线、点线等），线宽（单位为毫米）和颜色。\n将上述参数的值传递给 geom_line() 函数可以设置折线图的对应属性。\n\n# load package\nlibrary(ggplot2)\n\n#shape\nggplot(BOD,aes(x = Time,y = demand,))+\ngeom_line(linetype = \"dashed\",linewidth = 1,colour = \"blue\")\n\n\n\n\n\n\n图 12: 修改折线图线条样式\n\n\n\n\n\n对于多重折线图而言，设定图形属性会对图上的所有折线产生影响。\n而将变量映射到图形属性则会使图上的每条折线具有不同的外观。\n折线图默认颜色并不是很吸引眼球，所以用户可以使用其他调色板1着色，可以调用 scale_colour_brewer() 函数和 scale_colour_manual 函数实线。\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n#shape\nggplot(tg,aes(x = dose,y = length, colour = supp))+\ngeom_line()+\nscale_colour_brewer(palette = \"Set1\")\n\n\n\n\n\n\n图 13: 使用调色板\n\n\n\n\n\n\n# load package\nlibrary(ggplot2)\n# load dataset\nlibrary(gcookbook)\n\n# 如果两条折线的图形属性相同，需要指定一个分组变量\nggplot(tg,aes(x = dose,y = length, group = supp))+\ngeom_line(colour = \"darkgreen\",size = 1.5)\n\n# 因为变量 supp 被映射到了颜色（colour）属性，所以它自动作为分组变量\nggplot(tg,aes(x = dose,y = length, colour = supp))+\ngeom_line(linetype = \"dashed\")+\ngeom_point(shape = 22,size = 3,fill = \"white\")\n\n\n\n\n\n\n图 14: 折线图高级配色与线条\n\n\n\n\n\n\n\n\n\n图 15: 折线图高级配色与线条",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "04-ggplot2 绘图——折线图"
    ]
  },
  {
    "objectID": "Guide/R/04-ggplot2-folded-line.html#修改数据标记样式",
    "href": "Guide/R/04-ggplot2-folded-line.html#修改数据标记样式",
    "title": "04-ggplot2 绘图——折线图",
    "section": "",
    "text": "在函数 aes() 外部设定函数 geom_point() 的参数 size（大小）、shape（形状）、colour（颜色）和 fill（填充色）即可。\n\n# load package\nlibrary(ggplot2)\n\nggplot(BOD,aes(x = Time,y = demand))+\ngeom_line()+\ngeom_point(size = 4,shape = 22,colour = \"darkred\",fill = \"pink\")\n\n\n\n\n\n\n图 16: 折线图修改数据标记样式\n\n\n\n\nThe default shape for points is a solid circle, the default size is 2, and the default colour is black.The fill color is relevant only for some point shapes (numbered 21–25), which have separate outline and fill colors. The fill color is typically NA, or empty; you can fill it with white to get hollow-looking circles\n\n# load package\nlibrary(ggplot2)\n\nggplot(BOD, aes(x = Time, y = demand)) +\n  geom_line() +\n  geom_point(size = 4, shape = 21, fill = \"white\")\n\n\n\n\n\n\n图 17: 修改折线图默认样式\n\n\n\n\n如果要将数据标记和折线设定为不同的颜色，用户必须折线绘制完成后再设定数据标记的颜色，此时，数据标记被绘制在更上面的图层，以避免被折线遮盖。\n这里定义了一个函数 pd = position_dodge(0.2) 来将图形错开，原始用法可参照 图 11。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook)  # Load gcookbook for the tg data set\n\n# Save the position_dodge specification because we'll use it multiple times\npd &lt;- position_dodge(0.2)\n\nggplot(tg, aes(x = dose, y = length, fill = supp)) +\n  geom_line(position = pd) +\n  geom_point(shape = 21, size = 3, position = pd) +\n  scale_fill_manual(values = c(\"black\",\"white\"))\n\n\n\n\n\n\n图 18: 折线图复杂数据标记",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "04-ggplot2 绘图——折线图"
    ]
  },
  {
    "objectID": "Guide/R/04-ggplot2-folded-line.html#footnotes",
    "href": "Guide/R/04-ggplot2-folded-line.html#footnotes",
    "title": "04-ggplot2 绘图——折线图",
    "section": "脚注",
    "text": "脚注\n\n\nggplot2 本身内置了一些默认的调色板，但它也高度集成了其他 R 包和颜色系统的调色板，以提供更丰富的选择：\n\nR 语言内置颜色: ggplot2 可以直接使用 R 语言中预定义的颜色名称（例如 “red”, “blue”）或十六进制颜色代码（例如 “#FF0000”）。\n\nColorBrewer: 这是一个非常流行的调色板集合，由地理学家 Cynthia Brewer 开发，主要用于地图制作，但非常适用于各种数据可视化。ColorBrewer 的调色板设计考虑了色彩感知和色盲友好性，分为：\n\n顺序型 (Sequential)：适用于表示有序数据，从低到高渐变。\n发散型 (Diverging)：适用于表示有中间值的数据，例如正负值、同意/不同意等，颜色从中间值向两端发散。\n定性型 (Qualitative)：适用于表示分类数据，颜色之间没有内在顺序。 ggplot2 通过 scale_color_brewer() 和 scale_fill_brewer() 函数来使用 ColorBrewer 调色板。\n\n\nviridis: 这是一组在 ggplot2 中非常受欢迎的调色板（包括 viridis, magma, inferno, plasma, cividis）。它们的主要特点是感知均匀性和色盲友好性。这意味着无论颜色如何变化，人眼对颜色变化的感知是均匀的，并且即使是色盲人士也能区分不同的颜色。ggplot2 通过 scale_color_viridis_d() / scale_fill_viridis_d() （离散型）和 scale_color_viridis_c() / scale_fill_viridis_c() （连续型）来使用这些调色板。\n自定义调色板: 用户可以根据需要创建自己的调色板，通过 scale_color_manual() 和 scale_fill_manual() 函数手动指定颜色。\n\n其他扩展包: 还有许多其他 R 包提供了针对 ggplot2 的调色板，例如 ggsci 包提供了受科学期刊、数据可视化库等启发的调色板。\n功能\n\n\nggplot2 调色板的主要功能是将数据映射到视觉美学属性（特别是颜色和填充色），从而增强数据图表的可读性和信息传递效率。\n\n区分数据类别: 当你的数据中有离散变量（分类数据）时，调色板会为每个类别分配不同的颜色，让观众能够轻松区分不同的组别。例如，在散点图中用不同颜色表示不同的物种。\n表示数据强度/趋势: 对于连续变量，调色板通常使用颜色渐变来表示数值的大小或趋势。例如，颜色从浅到深表示数值从小到大。\n突出数据特征: 通过选择合适的调色板，可以突出数据中的特定模式、异常值或重要区域。\n提高图表美观度: 美观的颜色选择能够提升图表的整体视觉吸引力，使其更具专业性和易读性。\n支持色盲友好: 许多调色板（如 viridis 和 ColorBrewer 的部分调色板）都经过设计，以确保色盲人士也能良好地区分颜色，从而提高图表的可访问性。\n映射离散和连续数据。\n\n↩︎\n\n\n在 ggplot2 中绘制堆积面积图时，确实需要将宽格式数据转换为长格式。这并不是 ggplot2 独有的要求，而是 “整洁数据”（Tidy Data） 原则在数据可视化中的体现。\n为什么需要转换？\n\n\nggplot2 的设计哲学 (Grammar of Graphics)\nggplot2 基于 Leland Wilkinson 的 图形语法（Grammar of Graphics）。其核心思想是将图表分解为独立的组件：数据、几何对象（geoms）、统计变换（stats）、标度（scales）等。这种语法要求数据以特定的方式组织，以便它能直接映射到图表的视觉属性上。\n\n映射（Mapping）: 在 ggplot2 中，你需要明确地将数据变量映射到美学属性（如 x 轴、y 轴、颜色、填充、大小等）。\n\n整洁数据: ggplot2 最适合处理整洁数据。整洁数据的定义是：\n\n每个变量一列：数据集中的每个变量都应该有自己的一列。\n每个观测值一行：每个观测值（或案例）都应该有自己的一行。\n每个单元格一个值：每个单元格只包含一个值。\n\n\n\n对于堆积面积图，如果你有宽格式数据，例如：\n\n\n年份\n产品A\n产品B\n产品C\n\n\n2020\n100\n50\n20\n\n\n2021\n120\n60\n30\n\n\n在这种格式下，“产品A”、“产品B”、“产品C” 都是独立的列。ggplot2 很难直接理解 “产品A”、“产品B”、“产品C” 应该被视为一个共同的“产品类型”变量，并且它们的数值应该被堆叠。\n\n\n堆积图的要求\n堆积图的本质是按照某个分类变量（在堆积面积图通常是类别或分组）来对数值变量进行累加。为了实现这一点，ggplot2 需要一个明确的列来表示“类别”（例如，“产品类型”），以及另一个明确的列来表示“值”（例如，“销售额”）。\n当数据是长格式时：\n\n\n年份\n产品类型\n销售额\n\n\n2020\n产品A\n100\n\n\n2020\n产品B\n50\n\n\n2020\n产品C\n20\n\n\n2021\n产品A\n120\n\n\n2021\n产品B\n60\n\n\n2021\n产品C\n30\n\n\n现在，ggplot2 可以清晰地识别：\n\nX 轴: 年份\nY 轴: 销售额\n填充颜色（或分组）: 产品类型\n\n它知道如何根据 产品类型 对 销售额 进行堆积。\n\n\n如何转换？\n在 R 语言中，最常用的数据转换包是 tidyr，它是 tidyverse 生态系统的一部分。tidyr 包提供了 pivot_longer()（以前是 gather()）函数，用于将宽格式数据转换为长格式。\n假设我们有以下宽格式数据框 df_wide：\n# 创建一个示例宽格式数据 \ndf_wide &lt;- data.frame(   \n    Year = c(2020, 2021, 2022),   \n    ProductA = c(100, 120, 150),   \n    ProductB = c(50, 60, 70),   \n    ProductC = c(20, 30, 40) )  \nprint(df_wide) \n输出：\n Year ProductA ProductB ProductC \n1 2020      100       50       20 \n2 2021      120       60       30 \n3 2022      150       70       40 \n现在，我们使用 pivot_longer() 将其转换为长格式：\nlibrary(tidyr) \nlibrary(ggplot2) # 用于后续绘图  \ndf_long &lt;- df_wide %&gt;%   \npivot_longer(     \n    cols = starts_with(\"Product\"), \n    # 选择以 \"Product\" 开头的列进行转换     \n    names_to = \"ProductType\",     \n    # 新的类别变量的名称     \n    values_to = \"Sales\"           \n    # 新的数值变量的名称   \n)  \nprint(df_long) \n输出：\n# A tibble: 9 × 3    \nYear ProductType Sales   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; \n1  2020 ProductA      100 \n2  2020 ProductB       50 \n3  2020 ProductC       20 \n4  2021 ProductA      120 \n5  2021 ProductB       60 \n6  2021 ProductC       30 \n7  2022 ProductA      150 \n8  2022 ProductB       70 \n9  2022 ProductC       40 \n使用长格式数据绘制堆积面积图\n现在我们有了长格式数据 df_long，就可以轻松绘制堆积面积图了：\nggplot(df_long, aes(x = Year, y = Sales, fill = ProductType)) +   \ngeom_area(alpha = 0.8) + \n# alpha 用于调整透明度   \nlabs(title = \"不同产品销售额堆积面积图\",        \nx = \"年份\",        \ny = \"销售额\",        \nfill = \"产品类型\") +   \ntheme_minimal() \n总结来说，将宽格式数据转换为长格式是 ggplot2 绘制堆积图的关键步骤。这不仅符合 ggplot2 的图形语法和整洁数据原则，也使得数据能够清晰地映射到图表的视觉属性上，从而正确地进行堆积和可视化。↩︎",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "04-ggplot2 绘图——折线图"
    ]
  },
  {
    "objectID": "Guide/R/03-ggplot2-barplot.html#pastel1-调色板",
    "href": "Guide/R/03-ggplot2-barplot.html#pastel1-调色板",
    "title": "03-ggplot2 绘图——条形图",
    "section": "\n2.1 pastel1 调色板",
    "text": "2.1 pastel1 调色板\n\n代码library(gcookbook)  # Load gcookbook for the cabbage_exp data set\n# check dataset\ncabbage_exp\n\n  Cultivar Date Weight        sd  n         se\n1      c39  d16   3.18 0.9566144 10 0.30250803\n2      c39  d20   2.80 0.2788867 10 0.08819171\n3      c39  d21   2.74 0.9834181 10 0.31098410\n4      c52  d16   2.26 0.4452215 10 0.14079141\n5      c52  d20   3.11 0.7908505 10 0.25008887\n6      c52  d21   1.47 0.2110819 10 0.06674995\n\n代码# load ggplot2\nlibrary(ggplot2)\nggplot(cabbage_exp, aes(x = Date, y = Weight, fill = Cultivar)) +\n  geom_col(position = \"dodge\", colour = \"black\") +\n  scale_fill_brewer(palette = \"Pastel1\")",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "03-ggplot2 绘图——条形图"
    ]
  },
  {
    "objectID": "Guide/R/04-ggplot2-folded-line.html#绘制面积图",
    "href": "Guide/R/04-ggplot2-folded-line.html#绘制面积图",
    "title": "04-ggplot2 绘图——折线图",
    "section": "",
    "text": "运行 geom_area() 函数即可绘制面积图。\n\n# load package\nlibrary(ggplot2)\n\n# Convert the sunspot.year data set into a data frame for this example\nsunspotyear &lt;- data.frame(\n    Year     = as.numeric(time(sunspot.year)),\n    Sunspots = as.numeric(sunspot.year)\n  )\n\nggplot(sunspotyear, aes(x = Year, y = Sunspots)) +\n  geom_area()\n\n\n\n\n\n\n图 19: 绘制面积图\n\n\n\n\n默认情况下，面积图的填充色都是黑灰色且没有边框线，通过设定 fill 可以修改面积图的填充色。\n下图 图 20 将把填充色设定为蓝色，并通过 alpha=0.2 将面积图的透明度设定为 80% ，另外，用户可以看到面积图的网格线，通过设置 colour 为面积图添加边框线。\n\n# load package\nlibrary(ggplot2)\n\nsunspotyear &lt;- data.frame(\n    Year     = as.numeric(time(sunspot.year)),\n    Sunspots = as.numeric(sunspot.year)\n  )\n\nggplot(sunspotyear, aes(x = Year, y = Sunspots)) +\n  geom_area(colour = \"black\", fill = \"blue\", alpha = .2)\n\n\n\n\n\n\n图 20: 带半透明阴影区域和边框线的面积图\n\n\n\n\n给整个面积图添加边框线后的效果可能并不十分令人满意，因为此时系统会在面积图的起点和终点位置分别绘制一条垂直线，且在底部绘制一条横线。\n为了避免上述情况，可以先绘制不带框线的面积图（不设定 colour），然后添加图层，并用 geom_line() 函数绘制轨迹线，如下图 图 21 。\n\n# load package\nlibrary(ggplot2)\n\nsunspotyear &lt;- data.frame(\n    Year     = as.numeric(time(sunspot.year)),\n    Sunspots = as.numeric(sunspot.year)\n  )\n\nggplot(sunspotyear, aes(x = Year, y = Sunspots)) +\n  geom_area(fill = \"blue\", alpha = .2)+\n  geom_line()\n\n\n\n\n\n\n图 21: 不带下横线的面积图",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "04-ggplot2 绘图——折线图"
    ]
  },
  {
    "objectID": "Guide/R/04-ggplot2-folded-line.html#绘制堆积面积图",
    "href": "Guide/R/04-ggplot2-folded-line.html#绘制堆积面积图",
    "title": "04-ggplot2 绘图——折线图",
    "section": "",
    "text": "依旧是运行 geom_area() 函数，但是需要映射一个因子型变量到 fill 即可，如下图 图 22 所示。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the uspopage data set\n\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area()\n\n\n\n\n\n\n图 22: 绘制堆积面积图\n\n\n\n\n堆积面积图对应的基础数据通常为宽格式（wide format），但 ggplot 要求数据必须是长格式（long format），两种格式之间需要进行转换。2\n将 图 22 的调色板设定为蓝色渐变色，并在各个区域之间添加细线（size = .2），同时我们将填充区域设定为半透明（alpha = .4 )，这样可以透过填充区域看见网格线。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the uspopage data set\n\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area(colour = \"black\", size = .2, alpha = .4) +\n  scale_fill_brewer(palette = \"Blues\")\n\n\n\n\n\n\n图 23: 绘制渐变堆积面积图\n\n\n\n\n因为堆积面积图中的各个部分是由多边形构成的，所以其具有左、右边框线，这样的绘图效果不尽人意且可能产生误导效果。\n下图将绘制一个不带边框线的堆积面积图：\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the uspopage data set\n\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup, order = dplyr::desc(AgeGroup))) +\n  geom_area(colour = NA, alpha = .4) +\n  scale_fill_brewer(palette = \"Blues\") +\n  geom_line(position = \"stack\", size = .2)\n\n\n\n\n\n\n图 24: 不带边框线的渐变堆积面积图",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "04-ggplot2 绘图——折线图"
    ]
  },
  {
    "objectID": "Guide/R/04-ggplot2-folded-line.html#绘制百分比堆积面积图",
    "href": "Guide/R/04-ggplot2-folded-line.html#绘制百分比堆积面积图",
    "title": "04-ggplot2 绘图——折线图",
    "section": "",
    "text": "使用 geom_area(position = \"fill\") 函数实现这一效果：\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the uspopage data set\n\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area(position = \"fill\",colour = \"black\", size = .2, alpha = .4) +\n  scale_fill_brewer(palette = \"Blues\")\n\n\n\n\n\n\n图 25: 百分比堆积面积图\n\n\n\n\n带百分比标签的百分比堆积面积图，通过运行 position = \"fill\" 可将 y 值的范围按比例调整为 0-1。\n再运行 scale_y_continuous(labels = scales::percent) 可输出百分比标签。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the uspopage data set\n\nggplot(uspopage, aes(x = Year, y = Thousands, fill = AgeGroup)) +\n  geom_area(position = \"fill\",colour = \"black\", size = .2, alpha = .4) +\n  scale_fill_brewer(palette = \"Blues\")+\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n图 26: 带百分比标签的百分比堆积面积图",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "04-ggplot2 绘图——折线图"
    ]
  },
  {
    "objectID": "Guide/R/04-ggplot2-folded-line.html#为折线图添加置信域",
    "href": "Guide/R/04-ggplot2-folded-line.html#为折线图添加置信域",
    "title": "04-ggplot2 绘图——折线图",
    "section": "",
    "text": "使用 geom_ribbon() 函数，然后分别映射一个变量到 ymin 和 ymax 即可。\n这里使用的数据集为 climate 数据集，其中 Anomaly10y 变量表示了各年温度相对于 1950~1980 年平均水平偏差（以摄氏度衡量）的 10 年移动平均。变量 Unc10y 表示其 95% 置信水平下的置信区间。\n这里设定 ymax 和 ymin 分别等于Anomaly10y+Unc10y 和 Anomaly10y-Unc10y\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the climate data set\nlibrary(dplyr)\n\n# Grab a subset of the climate data\nclimate_mod &lt;- climate %&gt;%\n  filter(Source == \"Berkeley\") %&gt;%\n  select(Year, Anomaly10y, Unc10y)\n\nhead(climate_mod)\n\n  Year Anomaly10y Unc10y\n1 1800     -0.435  0.505\n2 1801     -0.453  0.493\n3 1802     -0.460  0.486\n4 1803     -0.493  0.489\n5 1804     -0.536  0.483\n6 1805     -0.541  0.475\n\n#&gt;     Year Anomaly10y Unc10y\n#&gt; 1   1800     -0.435  0.505\n#&gt; 2   1801     -0.453  0.493\n#&gt; 3   1802     -0.460  0.486\n#&gt;  ...&lt;199 more rows&gt;...\n#&gt; 203 2002      0.856  0.028\n#&gt; 204 2003      0.869  0.028\n#&gt; 205 2004      0.884  0.029\n\n# Shaded region\nggplot(climate_mod, aes(x = Year, y = Anomaly10y)) +\n  geom_ribbon(aes(ymin = Anomaly10y - Unc10y, ymax = Anomaly10y + Unc10y), alpha = 0.2) +\n  geom_line()\n\n\n\n\n\n\n图 27: 折线图添加置信域示例\n\n\n\n\n阴影部分的颜色实际上是黑灰色，但是看起来几乎是半透明的，这是因为通过设定 alpha = 0.2 将阴影部分的透明度设置为 80%。\n需要注意的是，如果使用这种带阴影或颜色的置信域，应该将 geom_line() 函数放在 geom_ribbon() 之后，这样折线会被绘制在阴影区域上面的图层，如果反转顺序的话，阴影区域的颜色有可能使折线模糊不清。\n但是用户可以选择使用虚线来表示置信域的上下边界，这样就不用考虑色域的问题，如下图 图 28 所示。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # Load gcookbook for the climate data set\nlibrary(dplyr)\n\n# With a dotted line for upper and lower bounds\nggplot(climate_mod, aes(x = Year, y = Anomaly10y)) +\n  geom_line(aes(y = Anomaly10y - Unc10y), colour = \"grey50\", linetype = \"dotted\") +\n  geom_line(aes(y = Anomaly10y + Unc10y), colour = \"grey50\", linetype = \"dotted\") +\n  geom_line()\n\n\n\n\n\n\n图 28: 折线图的虚线置信域示例\n\n\n\n\nend.",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "04-ggplot2 绘图——折线图"
    ]
  },
  {
    "objectID": "Guide/R/05-ggplot2-scatter-plot.html",
    "href": "Guide/R/05-ggplot2-scatter-plot.html",
    "title": "05-ggplot2 绘图——散点图",
    "section": "",
    "text": "散点图常用来刻画两个连续变量之间的关系。\n绘制散点图时，数据集中的每一个观测值都由散点图中的一个点来表示。通常，人们还会向散点图中添加一条直线，以用来表示基于某些统计模型的预测值。当散点图中的数据趋势难以用肉眼识别时，这些直线对我们理解数据的特征很有帮助。\n当数据集很大时，散点图上的数据点会相互重叠，此时，很难在图上清楚地显示出所有的数据点。为了解决重叠的问题，我们可以先对数据进行汇总，再绘制散点图。\n\n使用 geom_point() 函数，分别映射一个变量到 x 和一个变量到 y。\n这里使用 heightweight 数据集，这是一个多列数据集，接下来的例子我们只用到其中两列。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # 加载gcookbook是为了使用heightweight数据集\nlibrary(dplyr)\n\n# 列出我们绘制散点图要用到的两列的标题\nheightweight %&gt;% \n  select(ageYear, heightIn)\n\n    ageYear heightIn\n1     11.92     56.3\n2     12.92     62.3\n3     12.75     63.3\n4     13.42     59.0\n5     15.92     62.5\n6     14.25     62.5\n7     15.42     59.0\n8     11.83     56.5\n9     13.33     62.0\n10    11.67     53.8\n11    11.58     61.5\n12    14.83     61.5\n13    13.08     64.5\n14    12.42     58.3\n15    11.92     51.3\n16    12.08     58.8\n17    15.92     65.3\n18    12.50     59.5\n19    12.25     61.3\n20    15.00     63.3\n21    11.75     61.8\n22    11.67     53.5\n23    13.67     58.0\n24    14.67     61.3\n25    15.42     63.3\n26    13.83     61.5\n27    14.58     60.8\n28    15.00     59.0\n29    17.50     65.5\n30    12.17     56.3\n31    14.17     64.3\n32    13.50     58.0\n33    12.42     64.3\n34    11.58     57.5\n35    15.50     57.8\n36    16.42     61.5\n37    14.08     62.3\n38    14.75     61.8\n39    15.42     65.3\n40    15.17     58.3\n41    14.42     62.8\n42    13.83     59.3\n43    14.00     61.5\n44    14.08     62.0\n45    12.50     61.3\n46    15.33     62.3\n47    11.58     52.8\n48    12.25     59.8\n49    12.00     59.5\n50    14.75     61.3\n51    14.83     63.5\n52    16.42     64.8\n53    12.17     60.0\n54    12.08     59.0\n55    12.25     55.8\n56    12.08     57.8\n57    12.92     61.3\n58    13.92     62.3\n59    15.25     64.3\n60    11.92     55.5\n61    15.25     64.5\n62    15.42     60.0\n63    12.33     56.3\n64    12.25     58.3\n65    12.83     60.0\n66    13.00     54.5\n67    12.00     55.8\n68    12.83     62.8\n69    12.67     60.5\n70    15.92     63.3\n71    15.83     66.8\n72    11.67     60.0\n73    12.33     60.5\n74    15.75     64.3\n75    11.92     58.3\n76    14.83     66.5\n77    13.67     65.3\n78    13.08     60.5\n79    12.25     59.5\n80    12.33     59.0\n81    14.75     61.3\n82    14.25     61.5\n83    14.33     64.8\n84    15.83     56.8\n85    15.25     66.5\n86    11.92     61.5\n87    14.92     63.0\n88    15.50     57.0\n89    15.17     65.5\n90    15.17     62.0\n91    11.83     56.0\n92    13.75     61.3\n93    13.75     55.5\n94    12.83     61.0\n95    12.50     54.5\n96    12.92     66.0\n97    13.58     56.5\n98    11.75     56.0\n99    12.25     51.5\n100   17.50     62.0\n101   14.25     63.0\n102   13.92     61.0\n103   15.17     64.0\n104   12.00     61.0\n105   16.08     59.8\n106   11.75     61.3\n107   13.67     63.3\n108   15.50     63.5\n109   14.08     61.5\n110   14.58     60.3\n111   15.00     61.3\n112   13.75     64.8\n113   13.08     60.5\n114   12.00     57.3\n115   12.50     59.5\n116   12.50     60.8\n117   11.58     60.5\n118   15.75     67.0\n119   15.25     64.8\n120   12.25     50.5\n121   12.17     57.5\n122   13.33     60.5\n123   13.00     61.8\n124   14.42     61.3\n125   12.58     66.3\n126   11.75     53.3\n127   12.50     59.0\n128   13.67     57.8\n129   12.75     60.0\n130   17.17     68.3\n132   14.67     63.8\n133   14.67     65.0\n134   11.67     59.5\n135   15.42     66.0\n136   15.00     61.8\n137   12.17     57.3\n138   15.25     66.0\n139   11.67     56.5\n140   12.58     58.3\n141   12.58     61.0\n142   12.00     62.8\n143   13.33     59.3\n144   14.83     67.3\n145   16.08     66.3\n146   13.50     64.5\n147   13.67     60.5\n148   15.50     66.0\n149   11.92     57.5\n150   14.58     64.0\n151   14.58     68.0\n152   14.58     63.5\n153   14.42     69.0\n154   14.17     63.8\n155   14.50     66.0\n156   13.67     63.5\n157   12.00     59.5\n158   13.00     66.3\n159   12.42     57.0\n160   12.00     60.0\n161   12.25     57.0\n162   15.67     67.3\n163   14.08     62.0\n164   14.33     65.0\n165   12.50     59.5\n166   16.08     67.8\n167   13.08     58.0\n168   14.00     60.0\n169   11.67     58.5\n170   13.00     58.3\n171   13.00     61.5\n172   13.17     65.0\n173   15.33     66.5\n174   13.00     68.5\n175   12.00     57.0\n176   14.67     61.5\n177   14.00     66.5\n178   12.42     52.5\n179   11.83     55.0\n180   15.67     71.0\n181   16.92     66.5\n182   11.83     58.8\n183   15.75     66.3\n184   15.67     65.8\n185   16.67     71.0\n186   12.67     59.5\n187   14.50     69.8\n188   13.83     62.5\n189   12.08     56.5\n190   11.92     57.5\n191   13.58     65.3\n192   13.83     67.3\n193   15.17     67.0\n194   14.42     66.0\n195   12.92     61.8\n196   13.50     60.0\n197   14.75     63.0\n198   14.75     60.5\n199   14.58     65.5\n200   13.83     62.0\n201   12.50     59.0\n202   12.50     61.8\n203   15.67     63.3\n204   13.58     66.0\n205   14.25     61.8\n206   13.50     63.0\n207   11.75     57.5\n208   14.50     63.0\n209   11.83     56.0\n210   12.33     60.5\n211   11.67     56.8\n212   13.33     64.0\n213   12.00     60.0\n214   17.17     69.5\n215   13.25     63.3\n216   12.42     56.3\n217   16.08     72.0\n218   16.17     65.3\n219   12.67     60.8\n220   12.17     55.0\n221   11.58     55.0\n222   15.50     66.5\n223   13.42     56.8\n224   12.75     64.8\n225   16.33     64.5\n226   13.67     58.0\n227   13.25     62.8\n228   14.83     63.8\n229   12.75     57.8\n230   12.92     57.3\n231   14.83     63.5\n232   11.83     55.0\n233   13.67     66.5\n234   15.75     65.0\n235   13.67     61.5\n236   13.92     62.0\n237   12.58     59.3\n\n#&gt;     ageYear heightIn\n#&gt; 1     11.92     56.3\n#&gt; 2     12.92     62.3\n#&gt; 3     12.75     63.3\n#&gt;  ...&lt;230 more rows&gt;...\n#&gt; 235   13.67     61.5\n#&gt; 236   13.92     62.0\n#&gt; 237   12.58     59.3\n\nggplot(heightweight, aes(x = ageYear, y = heightIn)) + \n  geom_point()\n\n\n\n基本散点图\n\n\n\n通过设定形状（shape）参数可以在散点图中绘制点以外的形状。例如，我们常用空心圈 shape=21（图 (fig5-3?) ）代替默认的实心圆 shape=19（图 (fig5-1?) ）​。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # 加载gcookbook是为了使用heightweight数据集\nlibrary(dplyr)\n\n# 列出我们绘制散点图要用到的两列的标题\nheightweight %&gt;% \n  select(ageYear, heightIn)\n\n    ageYear heightIn\n1     11.92     56.3\n2     12.92     62.3\n3     12.75     63.3\n4     13.42     59.0\n5     15.92     62.5\n6     14.25     62.5\n7     15.42     59.0\n8     11.83     56.5\n9     13.33     62.0\n10    11.67     53.8\n11    11.58     61.5\n12    14.83     61.5\n13    13.08     64.5\n14    12.42     58.3\n15    11.92     51.3\n16    12.08     58.8\n17    15.92     65.3\n18    12.50     59.5\n19    12.25     61.3\n20    15.00     63.3\n21    11.75     61.8\n22    11.67     53.5\n23    13.67     58.0\n24    14.67     61.3\n25    15.42     63.3\n26    13.83     61.5\n27    14.58     60.8\n28    15.00     59.0\n29    17.50     65.5\n30    12.17     56.3\n31    14.17     64.3\n32    13.50     58.0\n33    12.42     64.3\n34    11.58     57.5\n35    15.50     57.8\n36    16.42     61.5\n37    14.08     62.3\n38    14.75     61.8\n39    15.42     65.3\n40    15.17     58.3\n41    14.42     62.8\n42    13.83     59.3\n43    14.00     61.5\n44    14.08     62.0\n45    12.50     61.3\n46    15.33     62.3\n47    11.58     52.8\n48    12.25     59.8\n49    12.00     59.5\n50    14.75     61.3\n51    14.83     63.5\n52    16.42     64.8\n53    12.17     60.0\n54    12.08     59.0\n55    12.25     55.8\n56    12.08     57.8\n57    12.92     61.3\n58    13.92     62.3\n59    15.25     64.3\n60    11.92     55.5\n61    15.25     64.5\n62    15.42     60.0\n63    12.33     56.3\n64    12.25     58.3\n65    12.83     60.0\n66    13.00     54.5\n67    12.00     55.8\n68    12.83     62.8\n69    12.67     60.5\n70    15.92     63.3\n71    15.83     66.8\n72    11.67     60.0\n73    12.33     60.5\n74    15.75     64.3\n75    11.92     58.3\n76    14.83     66.5\n77    13.67     65.3\n78    13.08     60.5\n79    12.25     59.5\n80    12.33     59.0\n81    14.75     61.3\n82    14.25     61.5\n83    14.33     64.8\n84    15.83     56.8\n85    15.25     66.5\n86    11.92     61.5\n87    14.92     63.0\n88    15.50     57.0\n89    15.17     65.5\n90    15.17     62.0\n91    11.83     56.0\n92    13.75     61.3\n93    13.75     55.5\n94    12.83     61.0\n95    12.50     54.5\n96    12.92     66.0\n97    13.58     56.5\n98    11.75     56.0\n99    12.25     51.5\n100   17.50     62.0\n101   14.25     63.0\n102   13.92     61.0\n103   15.17     64.0\n104   12.00     61.0\n105   16.08     59.8\n106   11.75     61.3\n107   13.67     63.3\n108   15.50     63.5\n109   14.08     61.5\n110   14.58     60.3\n111   15.00     61.3\n112   13.75     64.8\n113   13.08     60.5\n114   12.00     57.3\n115   12.50     59.5\n116   12.50     60.8\n117   11.58     60.5\n118   15.75     67.0\n119   15.25     64.8\n120   12.25     50.5\n121   12.17     57.5\n122   13.33     60.5\n123   13.00     61.8\n124   14.42     61.3\n125   12.58     66.3\n126   11.75     53.3\n127   12.50     59.0\n128   13.67     57.8\n129   12.75     60.0\n130   17.17     68.3\n132   14.67     63.8\n133   14.67     65.0\n134   11.67     59.5\n135   15.42     66.0\n136   15.00     61.8\n137   12.17     57.3\n138   15.25     66.0\n139   11.67     56.5\n140   12.58     58.3\n141   12.58     61.0\n142   12.00     62.8\n143   13.33     59.3\n144   14.83     67.3\n145   16.08     66.3\n146   13.50     64.5\n147   13.67     60.5\n148   15.50     66.0\n149   11.92     57.5\n150   14.58     64.0\n151   14.58     68.0\n152   14.58     63.5\n153   14.42     69.0\n154   14.17     63.8\n155   14.50     66.0\n156   13.67     63.5\n157   12.00     59.5\n158   13.00     66.3\n159   12.42     57.0\n160   12.00     60.0\n161   12.25     57.0\n162   15.67     67.3\n163   14.08     62.0\n164   14.33     65.0\n165   12.50     59.5\n166   16.08     67.8\n167   13.08     58.0\n168   14.00     60.0\n169   11.67     58.5\n170   13.00     58.3\n171   13.00     61.5\n172   13.17     65.0\n173   15.33     66.5\n174   13.00     68.5\n175   12.00     57.0\n176   14.67     61.5\n177   14.00     66.5\n178   12.42     52.5\n179   11.83     55.0\n180   15.67     71.0\n181   16.92     66.5\n182   11.83     58.8\n183   15.75     66.3\n184   15.67     65.8\n185   16.67     71.0\n186   12.67     59.5\n187   14.50     69.8\n188   13.83     62.5\n189   12.08     56.5\n190   11.92     57.5\n191   13.58     65.3\n192   13.83     67.3\n193   15.17     67.0\n194   14.42     66.0\n195   12.92     61.8\n196   13.50     60.0\n197   14.75     63.0\n198   14.75     60.5\n199   14.58     65.5\n200   13.83     62.0\n201   12.50     59.0\n202   12.50     61.8\n203   15.67     63.3\n204   13.58     66.0\n205   14.25     61.8\n206   13.50     63.0\n207   11.75     57.5\n208   14.50     63.0\n209   11.83     56.0\n210   12.33     60.5\n211   11.67     56.8\n212   13.33     64.0\n213   12.00     60.0\n214   17.17     69.5\n215   13.25     63.3\n216   12.42     56.3\n217   16.08     72.0\n218   16.17     65.3\n219   12.67     60.8\n220   12.17     55.0\n221   11.58     55.0\n222   15.50     66.5\n223   13.42     56.8\n224   12.75     64.8\n225   16.33     64.5\n226   13.67     58.0\n227   13.25     62.8\n228   14.83     63.8\n229   12.75     57.8\n230   12.92     57.3\n231   14.83     63.5\n232   11.83     55.0\n233   13.67     66.5\n234   15.75     65.0\n235   13.67     61.5\n236   13.92     62.0\n237   12.58     59.3\n\nggplot(heightweight, aes(x = ageYear, y = heightIn)) + \n  geom_point(shape = 21)\n\n\n\n基本散点图-修改形状\n\n\n\n修改点的大小（在默认实心圆点的情况下），默认点的大小为：size=2。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # 加载gcookbook是为了使用heightweight数据集\nlibrary(dplyr)\n\n# 列出我们绘制散点图要用到的两列的标题\nheightweight %&gt;% \n  select(ageYear, heightIn)\n\n    ageYear heightIn\n1     11.92     56.3\n2     12.92     62.3\n3     12.75     63.3\n4     13.42     59.0\n5     15.92     62.5\n6     14.25     62.5\n7     15.42     59.0\n8     11.83     56.5\n9     13.33     62.0\n10    11.67     53.8\n11    11.58     61.5\n12    14.83     61.5\n13    13.08     64.5\n14    12.42     58.3\n15    11.92     51.3\n16    12.08     58.8\n17    15.92     65.3\n18    12.50     59.5\n19    12.25     61.3\n20    15.00     63.3\n21    11.75     61.8\n22    11.67     53.5\n23    13.67     58.0\n24    14.67     61.3\n25    15.42     63.3\n26    13.83     61.5\n27    14.58     60.8\n28    15.00     59.0\n29    17.50     65.5\n30    12.17     56.3\n31    14.17     64.3\n32    13.50     58.0\n33    12.42     64.3\n34    11.58     57.5\n35    15.50     57.8\n36    16.42     61.5\n37    14.08     62.3\n38    14.75     61.8\n39    15.42     65.3\n40    15.17     58.3\n41    14.42     62.8\n42    13.83     59.3\n43    14.00     61.5\n44    14.08     62.0\n45    12.50     61.3\n46    15.33     62.3\n47    11.58     52.8\n48    12.25     59.8\n49    12.00     59.5\n50    14.75     61.3\n51    14.83     63.5\n52    16.42     64.8\n53    12.17     60.0\n54    12.08     59.0\n55    12.25     55.8\n56    12.08     57.8\n57    12.92     61.3\n58    13.92     62.3\n59    15.25     64.3\n60    11.92     55.5\n61    15.25     64.5\n62    15.42     60.0\n63    12.33     56.3\n64    12.25     58.3\n65    12.83     60.0\n66    13.00     54.5\n67    12.00     55.8\n68    12.83     62.8\n69    12.67     60.5\n70    15.92     63.3\n71    15.83     66.8\n72    11.67     60.0\n73    12.33     60.5\n74    15.75     64.3\n75    11.92     58.3\n76    14.83     66.5\n77    13.67     65.3\n78    13.08     60.5\n79    12.25     59.5\n80    12.33     59.0\n81    14.75     61.3\n82    14.25     61.5\n83    14.33     64.8\n84    15.83     56.8\n85    15.25     66.5\n86    11.92     61.5\n87    14.92     63.0\n88    15.50     57.0\n89    15.17     65.5\n90    15.17     62.0\n91    11.83     56.0\n92    13.75     61.3\n93    13.75     55.5\n94    12.83     61.0\n95    12.50     54.5\n96    12.92     66.0\n97    13.58     56.5\n98    11.75     56.0\n99    12.25     51.5\n100   17.50     62.0\n101   14.25     63.0\n102   13.92     61.0\n103   15.17     64.0\n104   12.00     61.0\n105   16.08     59.8\n106   11.75     61.3\n107   13.67     63.3\n108   15.50     63.5\n109   14.08     61.5\n110   14.58     60.3\n111   15.00     61.3\n112   13.75     64.8\n113   13.08     60.5\n114   12.00     57.3\n115   12.50     59.5\n116   12.50     60.8\n117   11.58     60.5\n118   15.75     67.0\n119   15.25     64.8\n120   12.25     50.5\n121   12.17     57.5\n122   13.33     60.5\n123   13.00     61.8\n124   14.42     61.3\n125   12.58     66.3\n126   11.75     53.3\n127   12.50     59.0\n128   13.67     57.8\n129   12.75     60.0\n130   17.17     68.3\n132   14.67     63.8\n133   14.67     65.0\n134   11.67     59.5\n135   15.42     66.0\n136   15.00     61.8\n137   12.17     57.3\n138   15.25     66.0\n139   11.67     56.5\n140   12.58     58.3\n141   12.58     61.0\n142   12.00     62.8\n143   13.33     59.3\n144   14.83     67.3\n145   16.08     66.3\n146   13.50     64.5\n147   13.67     60.5\n148   15.50     66.0\n149   11.92     57.5\n150   14.58     64.0\n151   14.58     68.0\n152   14.58     63.5\n153   14.42     69.0\n154   14.17     63.8\n155   14.50     66.0\n156   13.67     63.5\n157   12.00     59.5\n158   13.00     66.3\n159   12.42     57.0\n160   12.00     60.0\n161   12.25     57.0\n162   15.67     67.3\n163   14.08     62.0\n164   14.33     65.0\n165   12.50     59.5\n166   16.08     67.8\n167   13.08     58.0\n168   14.00     60.0\n169   11.67     58.5\n170   13.00     58.3\n171   13.00     61.5\n172   13.17     65.0\n173   15.33     66.5\n174   13.00     68.5\n175   12.00     57.0\n176   14.67     61.5\n177   14.00     66.5\n178   12.42     52.5\n179   11.83     55.0\n180   15.67     71.0\n181   16.92     66.5\n182   11.83     58.8\n183   15.75     66.3\n184   15.67     65.8\n185   16.67     71.0\n186   12.67     59.5\n187   14.50     69.8\n188   13.83     62.5\n189   12.08     56.5\n190   11.92     57.5\n191   13.58     65.3\n192   13.83     67.3\n193   15.17     67.0\n194   14.42     66.0\n195   12.92     61.8\n196   13.50     60.0\n197   14.75     63.0\n198   14.75     60.5\n199   14.58     65.5\n200   13.83     62.0\n201   12.50     59.0\n202   12.50     61.8\n203   15.67     63.3\n204   13.58     66.0\n205   14.25     61.8\n206   13.50     63.0\n207   11.75     57.5\n208   14.50     63.0\n209   11.83     56.0\n210   12.33     60.5\n211   11.67     56.8\n212   13.33     64.0\n213   12.00     60.0\n214   17.17     69.5\n215   13.25     63.3\n216   12.42     56.3\n217   16.08     72.0\n218   16.17     65.3\n219   12.67     60.8\n220   12.17     55.0\n221   11.58     55.0\n222   15.50     66.5\n223   13.42     56.8\n224   12.75     64.8\n225   16.33     64.5\n226   13.67     58.0\n227   13.25     62.8\n228   14.83     63.8\n229   12.75     57.8\n230   12.92     57.3\n231   14.83     63.5\n232   11.83     55.0\n233   13.67     66.5\n234   15.75     65.0\n235   13.67     61.5\n236   13.92     62.0\n237   12.58     59.3\n\nggplot(heightweight, aes(x = ageYear, y = heightIn)) + \n  geom_point(size = 1.5)\n\n\n\n基本散点图-修改散点大小\n\n\n\n\n将分组变量映射到点形（shape）或颜色（colour）属性。\n接下来的例子中，我们将用到 heightweight 数据集中的3列。\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # 加载gcookbook是为了使用heightweight数据集\nlibrary(dplyr)\n\n# 列出要用到的3列的标题\nheightweight %&gt;%\n  select(sex, ageYear, heightIn)\n#&gt;     sex ageYear heightIn\n#&gt; 1     f   11.92     56.3\n#&gt; 2     f   12.92     62.3\n#&gt; 3     f   12.75     63.3\n#&gt;  ...&lt;230 more rows&gt;...\n#&gt; 235   m   13.67     61.5\n#&gt; 236   m   13.92     62.0\n#&gt; 237   m   12.58     59.3\n\nggplot(heightweight, aes(x = ageYear, y = heightIn,color = sex)) + \n  geom_point()\n\nggplot(heightweight, aes(x = ageYear, y = heightIn,shape = sex)) + \n  geom_point()\n\n\n\n    sex ageYear heightIn\n1     f   11.92     56.3\n2     f   12.92     62.3\n3     f   12.75     63.3\n4     f   13.42     59.0\n5     f   15.92     62.5\n6     f   14.25     62.5\n7     f   15.42     59.0\n8     f   11.83     56.5\n9     f   13.33     62.0\n10    f   11.67     53.8\n11    f   11.58     61.5\n12    f   14.83     61.5\n13    f   13.08     64.5\n14    f   12.42     58.3\n15    f   11.92     51.3\n16    f   12.08     58.8\n17    f   15.92     65.3\n18    f   12.50     59.5\n19    f   12.25     61.3\n20    f   15.00     63.3\n21    f   11.75     61.8\n22    f   11.67     53.5\n23    f   13.67     58.0\n24    f   14.67     61.3\n25    f   15.42     63.3\n26    f   13.83     61.5\n27    f   14.58     60.8\n28    f   15.00     59.0\n29    f   17.50     65.5\n30    f   12.17     56.3\n31    f   14.17     64.3\n32    f   13.50     58.0\n33    f   12.42     64.3\n34    f   11.58     57.5\n35    f   15.50     57.8\n36    f   16.42     61.5\n37    f   14.08     62.3\n38    f   14.75     61.8\n39    f   15.42     65.3\n40    f   15.17     58.3\n41    f   14.42     62.8\n42    f   13.83     59.3\n43    f   14.00     61.5\n44    f   14.08     62.0\n45    f   12.50     61.3\n46    f   15.33     62.3\n47    f   11.58     52.8\n48    f   12.25     59.8\n49    f   12.00     59.5\n50    f   14.75     61.3\n51    f   14.83     63.5\n52    f   16.42     64.8\n53    f   12.17     60.0\n54    f   12.08     59.0\n55    f   12.25     55.8\n56    f   12.08     57.8\n57    f   12.92     61.3\n58    f   13.92     62.3\n59    f   15.25     64.3\n60    f   11.92     55.5\n61    f   15.25     64.5\n62    f   15.42     60.0\n63    f   12.33     56.3\n64    f   12.25     58.3\n65    f   12.83     60.0\n66    f   13.00     54.5\n67    f   12.00     55.8\n68    f   12.83     62.8\n69    f   12.67     60.5\n70    f   15.92     63.3\n71    f   15.83     66.8\n72    f   11.67     60.0\n73    f   12.33     60.5\n74    f   15.75     64.3\n75    f   11.92     58.3\n76    f   14.83     66.5\n77    f   13.67     65.3\n78    f   13.08     60.5\n79    f   12.25     59.5\n80    f   12.33     59.0\n81    f   14.75     61.3\n82    f   14.25     61.5\n83    f   14.33     64.8\n84    f   15.83     56.8\n85    f   15.25     66.5\n86    f   11.92     61.5\n87    f   14.92     63.0\n88    f   15.50     57.0\n89    f   15.17     65.5\n90    f   15.17     62.0\n91    f   11.83     56.0\n92    f   13.75     61.3\n93    f   13.75     55.5\n94    f   12.83     61.0\n95    f   12.50     54.5\n96    f   12.92     66.0\n97    f   13.58     56.5\n98    f   11.75     56.0\n99    f   12.25     51.5\n100   f   17.50     62.0\n101   f   14.25     63.0\n102   f   13.92     61.0\n103   f   15.17     64.0\n104   f   12.00     61.0\n105   f   16.08     59.8\n106   f   11.75     61.3\n107   f   13.67     63.3\n108   f   15.50     63.5\n109   f   14.08     61.5\n110   f   14.58     60.3\n111   f   15.00     61.3\n112   m   13.75     64.8\n113   m   13.08     60.5\n114   m   12.00     57.3\n115   m   12.50     59.5\n116   m   12.50     60.8\n117   m   11.58     60.5\n118   m   15.75     67.0\n119   m   15.25     64.8\n120   m   12.25     50.5\n121   m   12.17     57.5\n122   m   13.33     60.5\n123   m   13.00     61.8\n124   m   14.42     61.3\n125   m   12.58     66.3\n126   m   11.75     53.3\n127   m   12.50     59.0\n128   m   13.67     57.8\n129   m   12.75     60.0\n130   m   17.17     68.3\n132   m   14.67     63.8\n133   m   14.67     65.0\n134   m   11.67     59.5\n135   m   15.42     66.0\n136   m   15.00     61.8\n137   m   12.17     57.3\n138   m   15.25     66.0\n139   m   11.67     56.5\n140   m   12.58     58.3\n141   m   12.58     61.0\n142   m   12.00     62.8\n143   m   13.33     59.3\n144   m   14.83     67.3\n145   m   16.08     66.3\n146   m   13.50     64.5\n147   m   13.67     60.5\n148   m   15.50     66.0\n149   m   11.92     57.5\n150   m   14.58     64.0\n151   m   14.58     68.0\n152   m   14.58     63.5\n153   m   14.42     69.0\n154   m   14.17     63.8\n155   m   14.50     66.0\n156   m   13.67     63.5\n157   m   12.00     59.5\n158   m   13.00     66.3\n159   m   12.42     57.0\n160   m   12.00     60.0\n161   m   12.25     57.0\n162   m   15.67     67.3\n163   m   14.08     62.0\n164   m   14.33     65.0\n165   m   12.50     59.5\n166   m   16.08     67.8\n167   m   13.08     58.0\n168   m   14.00     60.0\n169   m   11.67     58.5\n170   m   13.00     58.3\n171   m   13.00     61.5\n172   m   13.17     65.0\n173   m   15.33     66.5\n174   m   13.00     68.5\n175   m   12.00     57.0\n176   m   14.67     61.5\n177   m   14.00     66.5\n178   m   12.42     52.5\n179   m   11.83     55.0\n180   m   15.67     71.0\n181   m   16.92     66.5\n182   m   11.83     58.8\n183   m   15.75     66.3\n184   m   15.67     65.8\n185   m   16.67     71.0\n186   m   12.67     59.5\n187   m   14.50     69.8\n188   m   13.83     62.5\n189   m   12.08     56.5\n190   m   11.92     57.5\n191   m   13.58     65.3\n192   m   13.83     67.3\n193   m   15.17     67.0\n194   m   14.42     66.0\n195   m   12.92     61.8\n196   m   13.50     60.0\n197   m   14.75     63.0\n198   m   14.75     60.5\n199   m   14.58     65.5\n200   m   13.83     62.0\n201   m   12.50     59.0\n202   m   12.50     61.8\n203   m   15.67     63.3\n204   m   13.58     66.0\n205   m   14.25     61.8\n206   m   13.50     63.0\n207   m   11.75     57.5\n208   m   14.50     63.0\n209   m   11.83     56.0\n210   m   12.33     60.5\n211   m   11.67     56.8\n212   m   13.33     64.0\n213   m   12.00     60.0\n214   m   17.17     69.5\n215   m   13.25     63.3\n216   m   12.42     56.3\n217   m   16.08     72.0\n218   m   16.17     65.3\n219   m   12.67     60.8\n220   m   12.17     55.0\n221   m   11.58     55.0\n222   m   15.50     66.5\n223   m   13.42     56.8\n224   m   12.75     64.8\n225   m   16.33     64.5\n226   m   13.67     58.0\n227   m   13.25     62.8\n228   m   14.83     63.8\n229   m   12.75     57.8\n230   m   12.92     57.3\n231   m   14.83     63.5\n232   m   11.83     55.0\n233   m   13.67     66.5\n234   m   15.75     65.0\n235   m   13.67     61.5\n236   m   13.92     62.0\n237   m   12.58     59.3\n\n\n\n\n按颜色分组\n\n\n\n\n\n\n\n按形状分组\n\n\n\n基本散点图-按形/色分组\n\n\n\n选用的分组变量必须是分类变量，换言之，它必须是因子型或者字符型的向量。如果分组变量是数值型向量，则需要将它转化为因子型变量之后，才能以其作为分组变量。\n可以将一个变量同时映射到 shape 和 colour 属性。当有多个分组变量时，可以将它们分别映射到不同的图形属性。下面，我们把变量 sex 同时映射到 shape 和 colour 属性，见图 (fig5-5?) 。\n\nggplot(heightweight, aes(x = ageYear, y = heightIn,color = sex,shape = sex)) + \n  geom_point()\n\n\n\n基本散点图-按形和色分组\n\n\n\n有时需要使用不同于默认设置的点形和颜色。通过调用scale_shape_manual() 函数可以使用其他点形；\n调用 scale_colour_brewer() 函数或者 scale_colour_manual() 函数可以使用其他调色板，见图 (fig5-6?) 。\n\nggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex, colour = sex)) + \n  geom_point() +\n  scale_shape_manual(values = c(1,2)) + \n  scale_colour_brewer(palette = \"Set1\")\n\n\n\n基本散点图-修改调色板\n\n\n\n\n通过指定 geom_point() 函数中的点形（shape）参数，可以同时设定散点图中所有数据点的点形，如图 (fig5-7?) 所示。\n\nggplot(heightweight, aes(x = ageYear, y = heightIn)) + \n  geom_point(shape = 3)\n\n\n\n指定散点形状\n\n\n\nR系统绘图可以调用的图形较多，一些点形只有边框线（1～14）​；一些只有实心填充区域（15～20）​；还有一些则由可分离的边框线和具有填充色的实心区域共同组成（21～25）​，也可以用字符作点形。\n如果已将分组变量映射到 shape，则可以调用 scale_shape_manual() 函数来手动修改该分组变量不同水平的点形\n下图将使用略大且自定义点形的数据点：\n\nggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex)) + \n  geom_point(size = 3)+\n  scale_shape_manual(values = c(1,4))\n\n\n\n自定义散点形状\n\n\n\n\n点形1～20的点的颜色，包括实心区域的颜色都可由 colour 参数来控制。\n对于点形21～25而言，边框线和实心区域的颜色则分别由 colour 和 fill 参数来控制。\n\n我们可以将两个不同的变量分别由点形和填充色（空心或有填充）属性来表示。\n为了实现这一功能，首先需要选择一个同时具有 colour 和 fill 属性的点形，在 scale_shape_manual 中设定。之后需要在scale_fill_manual() 中选择一个包括 NA 和其他颜色的调色板（NA会生成一个空心的形状）​。",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "05-ggplot2 绘图——散点图"
    ]
  },
  {
    "objectID": "Guide/R/05-ggplot2-scatter-plot.html#绘制基本散点图",
    "href": "Guide/R/05-ggplot2-scatter-plot.html#绘制基本散点图",
    "title": "05-ggplot2 绘图——散点图",
    "section": "",
    "text": "使用 geom_point() 函数，分别映射一个变量到 x 和一个变量到 y。\n这里使用 heightweight 数据集，这是一个多列数据集，接下来的例子我们只用到其中两列。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # 加载gcookbook是为了使用heightweight数据集\nlibrary(dplyr)\n\n# 列出我们绘制散点图要用到的两列的标题\nheightweight %&gt;% \n  select(ageYear, heightIn)\n\n    ageYear heightIn\n1     11.92     56.3\n2     12.92     62.3\n3     12.75     63.3\n4     13.42     59.0\n5     15.92     62.5\n6     14.25     62.5\n7     15.42     59.0\n8     11.83     56.5\n9     13.33     62.0\n10    11.67     53.8\n11    11.58     61.5\n12    14.83     61.5\n13    13.08     64.5\n14    12.42     58.3\n15    11.92     51.3\n16    12.08     58.8\n17    15.92     65.3\n18    12.50     59.5\n19    12.25     61.3\n20    15.00     63.3\n21    11.75     61.8\n22    11.67     53.5\n23    13.67     58.0\n24    14.67     61.3\n25    15.42     63.3\n26    13.83     61.5\n27    14.58     60.8\n28    15.00     59.0\n29    17.50     65.5\n30    12.17     56.3\n31    14.17     64.3\n32    13.50     58.0\n33    12.42     64.3\n34    11.58     57.5\n35    15.50     57.8\n36    16.42     61.5\n37    14.08     62.3\n38    14.75     61.8\n39    15.42     65.3\n40    15.17     58.3\n41    14.42     62.8\n42    13.83     59.3\n43    14.00     61.5\n44    14.08     62.0\n45    12.50     61.3\n46    15.33     62.3\n47    11.58     52.8\n48    12.25     59.8\n49    12.00     59.5\n50    14.75     61.3\n51    14.83     63.5\n52    16.42     64.8\n53    12.17     60.0\n54    12.08     59.0\n55    12.25     55.8\n56    12.08     57.8\n57    12.92     61.3\n58    13.92     62.3\n59    15.25     64.3\n60    11.92     55.5\n61    15.25     64.5\n62    15.42     60.0\n63    12.33     56.3\n64    12.25     58.3\n65    12.83     60.0\n66    13.00     54.5\n67    12.00     55.8\n68    12.83     62.8\n69    12.67     60.5\n70    15.92     63.3\n71    15.83     66.8\n72    11.67     60.0\n73    12.33     60.5\n74    15.75     64.3\n75    11.92     58.3\n76    14.83     66.5\n77    13.67     65.3\n78    13.08     60.5\n79    12.25     59.5\n80    12.33     59.0\n81    14.75     61.3\n82    14.25     61.5\n83    14.33     64.8\n84    15.83     56.8\n85    15.25     66.5\n86    11.92     61.5\n87    14.92     63.0\n88    15.50     57.0\n89    15.17     65.5\n90    15.17     62.0\n91    11.83     56.0\n92    13.75     61.3\n93    13.75     55.5\n94    12.83     61.0\n95    12.50     54.5\n96    12.92     66.0\n97    13.58     56.5\n98    11.75     56.0\n99    12.25     51.5\n100   17.50     62.0\n101   14.25     63.0\n102   13.92     61.0\n103   15.17     64.0\n104   12.00     61.0\n105   16.08     59.8\n106   11.75     61.3\n107   13.67     63.3\n108   15.50     63.5\n109   14.08     61.5\n110   14.58     60.3\n111   15.00     61.3\n112   13.75     64.8\n113   13.08     60.5\n114   12.00     57.3\n115   12.50     59.5\n116   12.50     60.8\n117   11.58     60.5\n118   15.75     67.0\n119   15.25     64.8\n120   12.25     50.5\n121   12.17     57.5\n122   13.33     60.5\n123   13.00     61.8\n124   14.42     61.3\n125   12.58     66.3\n126   11.75     53.3\n127   12.50     59.0\n128   13.67     57.8\n129   12.75     60.0\n130   17.17     68.3\n132   14.67     63.8\n133   14.67     65.0\n134   11.67     59.5\n135   15.42     66.0\n136   15.00     61.8\n137   12.17     57.3\n138   15.25     66.0\n139   11.67     56.5\n140   12.58     58.3\n141   12.58     61.0\n142   12.00     62.8\n143   13.33     59.3\n144   14.83     67.3\n145   16.08     66.3\n146   13.50     64.5\n147   13.67     60.5\n148   15.50     66.0\n149   11.92     57.5\n150   14.58     64.0\n151   14.58     68.0\n152   14.58     63.5\n153   14.42     69.0\n154   14.17     63.8\n155   14.50     66.0\n156   13.67     63.5\n157   12.00     59.5\n158   13.00     66.3\n159   12.42     57.0\n160   12.00     60.0\n161   12.25     57.0\n162   15.67     67.3\n163   14.08     62.0\n164   14.33     65.0\n165   12.50     59.5\n166   16.08     67.8\n167   13.08     58.0\n168   14.00     60.0\n169   11.67     58.5\n170   13.00     58.3\n171   13.00     61.5\n172   13.17     65.0\n173   15.33     66.5\n174   13.00     68.5\n175   12.00     57.0\n176   14.67     61.5\n177   14.00     66.5\n178   12.42     52.5\n179   11.83     55.0\n180   15.67     71.0\n181   16.92     66.5\n182   11.83     58.8\n183   15.75     66.3\n184   15.67     65.8\n185   16.67     71.0\n186   12.67     59.5\n187   14.50     69.8\n188   13.83     62.5\n189   12.08     56.5\n190   11.92     57.5\n191   13.58     65.3\n192   13.83     67.3\n193   15.17     67.0\n194   14.42     66.0\n195   12.92     61.8\n196   13.50     60.0\n197   14.75     63.0\n198   14.75     60.5\n199   14.58     65.5\n200   13.83     62.0\n201   12.50     59.0\n202   12.50     61.8\n203   15.67     63.3\n204   13.58     66.0\n205   14.25     61.8\n206   13.50     63.0\n207   11.75     57.5\n208   14.50     63.0\n209   11.83     56.0\n210   12.33     60.5\n211   11.67     56.8\n212   13.33     64.0\n213   12.00     60.0\n214   17.17     69.5\n215   13.25     63.3\n216   12.42     56.3\n217   16.08     72.0\n218   16.17     65.3\n219   12.67     60.8\n220   12.17     55.0\n221   11.58     55.0\n222   15.50     66.5\n223   13.42     56.8\n224   12.75     64.8\n225   16.33     64.5\n226   13.67     58.0\n227   13.25     62.8\n228   14.83     63.8\n229   12.75     57.8\n230   12.92     57.3\n231   14.83     63.5\n232   11.83     55.0\n233   13.67     66.5\n234   15.75     65.0\n235   13.67     61.5\n236   13.92     62.0\n237   12.58     59.3\n\n#&gt;     ageYear heightIn\n#&gt; 1     11.92     56.3\n#&gt; 2     12.92     62.3\n#&gt; 3     12.75     63.3\n#&gt;  ...&lt;230 more rows&gt;...\n#&gt; 235   13.67     61.5\n#&gt; 236   13.92     62.0\n#&gt; 237   12.58     59.3\n\nggplot(heightweight, aes(x = ageYear, y = heightIn)) + \n  geom_point()\n\n\n\n基本散点图\n\n\n\n通过设定形状（shape）参数可以在散点图中绘制点以外的形状。例如，我们常用空心圈 shape=21（图 (fig5-3?) ）代替默认的实心圆 shape=19（图 (fig5-1?) ）​。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # 加载gcookbook是为了使用heightweight数据集\nlibrary(dplyr)\n\n# 列出我们绘制散点图要用到的两列的标题\nheightweight %&gt;% \n  select(ageYear, heightIn)\n\n    ageYear heightIn\n1     11.92     56.3\n2     12.92     62.3\n3     12.75     63.3\n4     13.42     59.0\n5     15.92     62.5\n6     14.25     62.5\n7     15.42     59.0\n8     11.83     56.5\n9     13.33     62.0\n10    11.67     53.8\n11    11.58     61.5\n12    14.83     61.5\n13    13.08     64.5\n14    12.42     58.3\n15    11.92     51.3\n16    12.08     58.8\n17    15.92     65.3\n18    12.50     59.5\n19    12.25     61.3\n20    15.00     63.3\n21    11.75     61.8\n22    11.67     53.5\n23    13.67     58.0\n24    14.67     61.3\n25    15.42     63.3\n26    13.83     61.5\n27    14.58     60.8\n28    15.00     59.0\n29    17.50     65.5\n30    12.17     56.3\n31    14.17     64.3\n32    13.50     58.0\n33    12.42     64.3\n34    11.58     57.5\n35    15.50     57.8\n36    16.42     61.5\n37    14.08     62.3\n38    14.75     61.8\n39    15.42     65.3\n40    15.17     58.3\n41    14.42     62.8\n42    13.83     59.3\n43    14.00     61.5\n44    14.08     62.0\n45    12.50     61.3\n46    15.33     62.3\n47    11.58     52.8\n48    12.25     59.8\n49    12.00     59.5\n50    14.75     61.3\n51    14.83     63.5\n52    16.42     64.8\n53    12.17     60.0\n54    12.08     59.0\n55    12.25     55.8\n56    12.08     57.8\n57    12.92     61.3\n58    13.92     62.3\n59    15.25     64.3\n60    11.92     55.5\n61    15.25     64.5\n62    15.42     60.0\n63    12.33     56.3\n64    12.25     58.3\n65    12.83     60.0\n66    13.00     54.5\n67    12.00     55.8\n68    12.83     62.8\n69    12.67     60.5\n70    15.92     63.3\n71    15.83     66.8\n72    11.67     60.0\n73    12.33     60.5\n74    15.75     64.3\n75    11.92     58.3\n76    14.83     66.5\n77    13.67     65.3\n78    13.08     60.5\n79    12.25     59.5\n80    12.33     59.0\n81    14.75     61.3\n82    14.25     61.5\n83    14.33     64.8\n84    15.83     56.8\n85    15.25     66.5\n86    11.92     61.5\n87    14.92     63.0\n88    15.50     57.0\n89    15.17     65.5\n90    15.17     62.0\n91    11.83     56.0\n92    13.75     61.3\n93    13.75     55.5\n94    12.83     61.0\n95    12.50     54.5\n96    12.92     66.0\n97    13.58     56.5\n98    11.75     56.0\n99    12.25     51.5\n100   17.50     62.0\n101   14.25     63.0\n102   13.92     61.0\n103   15.17     64.0\n104   12.00     61.0\n105   16.08     59.8\n106   11.75     61.3\n107   13.67     63.3\n108   15.50     63.5\n109   14.08     61.5\n110   14.58     60.3\n111   15.00     61.3\n112   13.75     64.8\n113   13.08     60.5\n114   12.00     57.3\n115   12.50     59.5\n116   12.50     60.8\n117   11.58     60.5\n118   15.75     67.0\n119   15.25     64.8\n120   12.25     50.5\n121   12.17     57.5\n122   13.33     60.5\n123   13.00     61.8\n124   14.42     61.3\n125   12.58     66.3\n126   11.75     53.3\n127   12.50     59.0\n128   13.67     57.8\n129   12.75     60.0\n130   17.17     68.3\n132   14.67     63.8\n133   14.67     65.0\n134   11.67     59.5\n135   15.42     66.0\n136   15.00     61.8\n137   12.17     57.3\n138   15.25     66.0\n139   11.67     56.5\n140   12.58     58.3\n141   12.58     61.0\n142   12.00     62.8\n143   13.33     59.3\n144   14.83     67.3\n145   16.08     66.3\n146   13.50     64.5\n147   13.67     60.5\n148   15.50     66.0\n149   11.92     57.5\n150   14.58     64.0\n151   14.58     68.0\n152   14.58     63.5\n153   14.42     69.0\n154   14.17     63.8\n155   14.50     66.0\n156   13.67     63.5\n157   12.00     59.5\n158   13.00     66.3\n159   12.42     57.0\n160   12.00     60.0\n161   12.25     57.0\n162   15.67     67.3\n163   14.08     62.0\n164   14.33     65.0\n165   12.50     59.5\n166   16.08     67.8\n167   13.08     58.0\n168   14.00     60.0\n169   11.67     58.5\n170   13.00     58.3\n171   13.00     61.5\n172   13.17     65.0\n173   15.33     66.5\n174   13.00     68.5\n175   12.00     57.0\n176   14.67     61.5\n177   14.00     66.5\n178   12.42     52.5\n179   11.83     55.0\n180   15.67     71.0\n181   16.92     66.5\n182   11.83     58.8\n183   15.75     66.3\n184   15.67     65.8\n185   16.67     71.0\n186   12.67     59.5\n187   14.50     69.8\n188   13.83     62.5\n189   12.08     56.5\n190   11.92     57.5\n191   13.58     65.3\n192   13.83     67.3\n193   15.17     67.0\n194   14.42     66.0\n195   12.92     61.8\n196   13.50     60.0\n197   14.75     63.0\n198   14.75     60.5\n199   14.58     65.5\n200   13.83     62.0\n201   12.50     59.0\n202   12.50     61.8\n203   15.67     63.3\n204   13.58     66.0\n205   14.25     61.8\n206   13.50     63.0\n207   11.75     57.5\n208   14.50     63.0\n209   11.83     56.0\n210   12.33     60.5\n211   11.67     56.8\n212   13.33     64.0\n213   12.00     60.0\n214   17.17     69.5\n215   13.25     63.3\n216   12.42     56.3\n217   16.08     72.0\n218   16.17     65.3\n219   12.67     60.8\n220   12.17     55.0\n221   11.58     55.0\n222   15.50     66.5\n223   13.42     56.8\n224   12.75     64.8\n225   16.33     64.5\n226   13.67     58.0\n227   13.25     62.8\n228   14.83     63.8\n229   12.75     57.8\n230   12.92     57.3\n231   14.83     63.5\n232   11.83     55.0\n233   13.67     66.5\n234   15.75     65.0\n235   13.67     61.5\n236   13.92     62.0\n237   12.58     59.3\n\nggplot(heightweight, aes(x = ageYear, y = heightIn)) + \n  geom_point(shape = 21)\n\n\n\n基本散点图-修改形状\n\n\n\n修改点的大小（在默认实心圆点的情况下），默认点的大小为：size=2。\n\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # 加载gcookbook是为了使用heightweight数据集\nlibrary(dplyr)\n\n# 列出我们绘制散点图要用到的两列的标题\nheightweight %&gt;% \n  select(ageYear, heightIn)\n\n    ageYear heightIn\n1     11.92     56.3\n2     12.92     62.3\n3     12.75     63.3\n4     13.42     59.0\n5     15.92     62.5\n6     14.25     62.5\n7     15.42     59.0\n8     11.83     56.5\n9     13.33     62.0\n10    11.67     53.8\n11    11.58     61.5\n12    14.83     61.5\n13    13.08     64.5\n14    12.42     58.3\n15    11.92     51.3\n16    12.08     58.8\n17    15.92     65.3\n18    12.50     59.5\n19    12.25     61.3\n20    15.00     63.3\n21    11.75     61.8\n22    11.67     53.5\n23    13.67     58.0\n24    14.67     61.3\n25    15.42     63.3\n26    13.83     61.5\n27    14.58     60.8\n28    15.00     59.0\n29    17.50     65.5\n30    12.17     56.3\n31    14.17     64.3\n32    13.50     58.0\n33    12.42     64.3\n34    11.58     57.5\n35    15.50     57.8\n36    16.42     61.5\n37    14.08     62.3\n38    14.75     61.8\n39    15.42     65.3\n40    15.17     58.3\n41    14.42     62.8\n42    13.83     59.3\n43    14.00     61.5\n44    14.08     62.0\n45    12.50     61.3\n46    15.33     62.3\n47    11.58     52.8\n48    12.25     59.8\n49    12.00     59.5\n50    14.75     61.3\n51    14.83     63.5\n52    16.42     64.8\n53    12.17     60.0\n54    12.08     59.0\n55    12.25     55.8\n56    12.08     57.8\n57    12.92     61.3\n58    13.92     62.3\n59    15.25     64.3\n60    11.92     55.5\n61    15.25     64.5\n62    15.42     60.0\n63    12.33     56.3\n64    12.25     58.3\n65    12.83     60.0\n66    13.00     54.5\n67    12.00     55.8\n68    12.83     62.8\n69    12.67     60.5\n70    15.92     63.3\n71    15.83     66.8\n72    11.67     60.0\n73    12.33     60.5\n74    15.75     64.3\n75    11.92     58.3\n76    14.83     66.5\n77    13.67     65.3\n78    13.08     60.5\n79    12.25     59.5\n80    12.33     59.0\n81    14.75     61.3\n82    14.25     61.5\n83    14.33     64.8\n84    15.83     56.8\n85    15.25     66.5\n86    11.92     61.5\n87    14.92     63.0\n88    15.50     57.0\n89    15.17     65.5\n90    15.17     62.0\n91    11.83     56.0\n92    13.75     61.3\n93    13.75     55.5\n94    12.83     61.0\n95    12.50     54.5\n96    12.92     66.0\n97    13.58     56.5\n98    11.75     56.0\n99    12.25     51.5\n100   17.50     62.0\n101   14.25     63.0\n102   13.92     61.0\n103   15.17     64.0\n104   12.00     61.0\n105   16.08     59.8\n106   11.75     61.3\n107   13.67     63.3\n108   15.50     63.5\n109   14.08     61.5\n110   14.58     60.3\n111   15.00     61.3\n112   13.75     64.8\n113   13.08     60.5\n114   12.00     57.3\n115   12.50     59.5\n116   12.50     60.8\n117   11.58     60.5\n118   15.75     67.0\n119   15.25     64.8\n120   12.25     50.5\n121   12.17     57.5\n122   13.33     60.5\n123   13.00     61.8\n124   14.42     61.3\n125   12.58     66.3\n126   11.75     53.3\n127   12.50     59.0\n128   13.67     57.8\n129   12.75     60.0\n130   17.17     68.3\n132   14.67     63.8\n133   14.67     65.0\n134   11.67     59.5\n135   15.42     66.0\n136   15.00     61.8\n137   12.17     57.3\n138   15.25     66.0\n139   11.67     56.5\n140   12.58     58.3\n141   12.58     61.0\n142   12.00     62.8\n143   13.33     59.3\n144   14.83     67.3\n145   16.08     66.3\n146   13.50     64.5\n147   13.67     60.5\n148   15.50     66.0\n149   11.92     57.5\n150   14.58     64.0\n151   14.58     68.0\n152   14.58     63.5\n153   14.42     69.0\n154   14.17     63.8\n155   14.50     66.0\n156   13.67     63.5\n157   12.00     59.5\n158   13.00     66.3\n159   12.42     57.0\n160   12.00     60.0\n161   12.25     57.0\n162   15.67     67.3\n163   14.08     62.0\n164   14.33     65.0\n165   12.50     59.5\n166   16.08     67.8\n167   13.08     58.0\n168   14.00     60.0\n169   11.67     58.5\n170   13.00     58.3\n171   13.00     61.5\n172   13.17     65.0\n173   15.33     66.5\n174   13.00     68.5\n175   12.00     57.0\n176   14.67     61.5\n177   14.00     66.5\n178   12.42     52.5\n179   11.83     55.0\n180   15.67     71.0\n181   16.92     66.5\n182   11.83     58.8\n183   15.75     66.3\n184   15.67     65.8\n185   16.67     71.0\n186   12.67     59.5\n187   14.50     69.8\n188   13.83     62.5\n189   12.08     56.5\n190   11.92     57.5\n191   13.58     65.3\n192   13.83     67.3\n193   15.17     67.0\n194   14.42     66.0\n195   12.92     61.8\n196   13.50     60.0\n197   14.75     63.0\n198   14.75     60.5\n199   14.58     65.5\n200   13.83     62.0\n201   12.50     59.0\n202   12.50     61.8\n203   15.67     63.3\n204   13.58     66.0\n205   14.25     61.8\n206   13.50     63.0\n207   11.75     57.5\n208   14.50     63.0\n209   11.83     56.0\n210   12.33     60.5\n211   11.67     56.8\n212   13.33     64.0\n213   12.00     60.0\n214   17.17     69.5\n215   13.25     63.3\n216   12.42     56.3\n217   16.08     72.0\n218   16.17     65.3\n219   12.67     60.8\n220   12.17     55.0\n221   11.58     55.0\n222   15.50     66.5\n223   13.42     56.8\n224   12.75     64.8\n225   16.33     64.5\n226   13.67     58.0\n227   13.25     62.8\n228   14.83     63.8\n229   12.75     57.8\n230   12.92     57.3\n231   14.83     63.5\n232   11.83     55.0\n233   13.67     66.5\n234   15.75     65.0\n235   13.67     61.5\n236   13.92     62.0\n237   12.58     59.3\n\nggplot(heightweight, aes(x = ageYear, y = heightIn)) + \n  geom_point(size = 1.5)\n\n\n\n基本散点图-修改散点大小",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "05-ggplot2 绘图——散点图"
    ]
  },
  {
    "objectID": "Guide/R/05-ggplot2-scatter-plot.html#使用点形或颜色属性对数据点进行分组",
    "href": "Guide/R/05-ggplot2-scatter-plot.html#使用点形或颜色属性对数据点进行分组",
    "title": "05-ggplot2 绘图——散点图",
    "section": "",
    "text": "将分组变量映射到点形（shape）或颜色（colour）属性。\n接下来的例子中，我们将用到 heightweight 数据集中的3列。\n# load package\nlibrary(ggplot2)\nlibrary(gcookbook) # 加载gcookbook是为了使用heightweight数据集\nlibrary(dplyr)\n\n# 列出要用到的3列的标题\nheightweight %&gt;%\n  select(sex, ageYear, heightIn)\n#&gt;     sex ageYear heightIn\n#&gt; 1     f   11.92     56.3\n#&gt; 2     f   12.92     62.3\n#&gt; 3     f   12.75     63.3\n#&gt;  ...&lt;230 more rows&gt;...\n#&gt; 235   m   13.67     61.5\n#&gt; 236   m   13.92     62.0\n#&gt; 237   m   12.58     59.3\n\nggplot(heightweight, aes(x = ageYear, y = heightIn,color = sex)) + \n  geom_point()\n\nggplot(heightweight, aes(x = ageYear, y = heightIn,shape = sex)) + \n  geom_point()\n\n\n\n    sex ageYear heightIn\n1     f   11.92     56.3\n2     f   12.92     62.3\n3     f   12.75     63.3\n4     f   13.42     59.0\n5     f   15.92     62.5\n6     f   14.25     62.5\n7     f   15.42     59.0\n8     f   11.83     56.5\n9     f   13.33     62.0\n10    f   11.67     53.8\n11    f   11.58     61.5\n12    f   14.83     61.5\n13    f   13.08     64.5\n14    f   12.42     58.3\n15    f   11.92     51.3\n16    f   12.08     58.8\n17    f   15.92     65.3\n18    f   12.50     59.5\n19    f   12.25     61.3\n20    f   15.00     63.3\n21    f   11.75     61.8\n22    f   11.67     53.5\n23    f   13.67     58.0\n24    f   14.67     61.3\n25    f   15.42     63.3\n26    f   13.83     61.5\n27    f   14.58     60.8\n28    f   15.00     59.0\n29    f   17.50     65.5\n30    f   12.17     56.3\n31    f   14.17     64.3\n32    f   13.50     58.0\n33    f   12.42     64.3\n34    f   11.58     57.5\n35    f   15.50     57.8\n36    f   16.42     61.5\n37    f   14.08     62.3\n38    f   14.75     61.8\n39    f   15.42     65.3\n40    f   15.17     58.3\n41    f   14.42     62.8\n42    f   13.83     59.3\n43    f   14.00     61.5\n44    f   14.08     62.0\n45    f   12.50     61.3\n46    f   15.33     62.3\n47    f   11.58     52.8\n48    f   12.25     59.8\n49    f   12.00     59.5\n50    f   14.75     61.3\n51    f   14.83     63.5\n52    f   16.42     64.8\n53    f   12.17     60.0\n54    f   12.08     59.0\n55    f   12.25     55.8\n56    f   12.08     57.8\n57    f   12.92     61.3\n58    f   13.92     62.3\n59    f   15.25     64.3\n60    f   11.92     55.5\n61    f   15.25     64.5\n62    f   15.42     60.0\n63    f   12.33     56.3\n64    f   12.25     58.3\n65    f   12.83     60.0\n66    f   13.00     54.5\n67    f   12.00     55.8\n68    f   12.83     62.8\n69    f   12.67     60.5\n70    f   15.92     63.3\n71    f   15.83     66.8\n72    f   11.67     60.0\n73    f   12.33     60.5\n74    f   15.75     64.3\n75    f   11.92     58.3\n76    f   14.83     66.5\n77    f   13.67     65.3\n78    f   13.08     60.5\n79    f   12.25     59.5\n80    f   12.33     59.0\n81    f   14.75     61.3\n82    f   14.25     61.5\n83    f   14.33     64.8\n84    f   15.83     56.8\n85    f   15.25     66.5\n86    f   11.92     61.5\n87    f   14.92     63.0\n88    f   15.50     57.0\n89    f   15.17     65.5\n90    f   15.17     62.0\n91    f   11.83     56.0\n92    f   13.75     61.3\n93    f   13.75     55.5\n94    f   12.83     61.0\n95    f   12.50     54.5\n96    f   12.92     66.0\n97    f   13.58     56.5\n98    f   11.75     56.0\n99    f   12.25     51.5\n100   f   17.50     62.0\n101   f   14.25     63.0\n102   f   13.92     61.0\n103   f   15.17     64.0\n104   f   12.00     61.0\n105   f   16.08     59.8\n106   f   11.75     61.3\n107   f   13.67     63.3\n108   f   15.50     63.5\n109   f   14.08     61.5\n110   f   14.58     60.3\n111   f   15.00     61.3\n112   m   13.75     64.8\n113   m   13.08     60.5\n114   m   12.00     57.3\n115   m   12.50     59.5\n116   m   12.50     60.8\n117   m   11.58     60.5\n118   m   15.75     67.0\n119   m   15.25     64.8\n120   m   12.25     50.5\n121   m   12.17     57.5\n122   m   13.33     60.5\n123   m   13.00     61.8\n124   m   14.42     61.3\n125   m   12.58     66.3\n126   m   11.75     53.3\n127   m   12.50     59.0\n128   m   13.67     57.8\n129   m   12.75     60.0\n130   m   17.17     68.3\n132   m   14.67     63.8\n133   m   14.67     65.0\n134   m   11.67     59.5\n135   m   15.42     66.0\n136   m   15.00     61.8\n137   m   12.17     57.3\n138   m   15.25     66.0\n139   m   11.67     56.5\n140   m   12.58     58.3\n141   m   12.58     61.0\n142   m   12.00     62.8\n143   m   13.33     59.3\n144   m   14.83     67.3\n145   m   16.08     66.3\n146   m   13.50     64.5\n147   m   13.67     60.5\n148   m   15.50     66.0\n149   m   11.92     57.5\n150   m   14.58     64.0\n151   m   14.58     68.0\n152   m   14.58     63.5\n153   m   14.42     69.0\n154   m   14.17     63.8\n155   m   14.50     66.0\n156   m   13.67     63.5\n157   m   12.00     59.5\n158   m   13.00     66.3\n159   m   12.42     57.0\n160   m   12.00     60.0\n161   m   12.25     57.0\n162   m   15.67     67.3\n163   m   14.08     62.0\n164   m   14.33     65.0\n165   m   12.50     59.5\n166   m   16.08     67.8\n167   m   13.08     58.0\n168   m   14.00     60.0\n169   m   11.67     58.5\n170   m   13.00     58.3\n171   m   13.00     61.5\n172   m   13.17     65.0\n173   m   15.33     66.5\n174   m   13.00     68.5\n175   m   12.00     57.0\n176   m   14.67     61.5\n177   m   14.00     66.5\n178   m   12.42     52.5\n179   m   11.83     55.0\n180   m   15.67     71.0\n181   m   16.92     66.5\n182   m   11.83     58.8\n183   m   15.75     66.3\n184   m   15.67     65.8\n185   m   16.67     71.0\n186   m   12.67     59.5\n187   m   14.50     69.8\n188   m   13.83     62.5\n189   m   12.08     56.5\n190   m   11.92     57.5\n191   m   13.58     65.3\n192   m   13.83     67.3\n193   m   15.17     67.0\n194   m   14.42     66.0\n195   m   12.92     61.8\n196   m   13.50     60.0\n197   m   14.75     63.0\n198   m   14.75     60.5\n199   m   14.58     65.5\n200   m   13.83     62.0\n201   m   12.50     59.0\n202   m   12.50     61.8\n203   m   15.67     63.3\n204   m   13.58     66.0\n205   m   14.25     61.8\n206   m   13.50     63.0\n207   m   11.75     57.5\n208   m   14.50     63.0\n209   m   11.83     56.0\n210   m   12.33     60.5\n211   m   11.67     56.8\n212   m   13.33     64.0\n213   m   12.00     60.0\n214   m   17.17     69.5\n215   m   13.25     63.3\n216   m   12.42     56.3\n217   m   16.08     72.0\n218   m   16.17     65.3\n219   m   12.67     60.8\n220   m   12.17     55.0\n221   m   11.58     55.0\n222   m   15.50     66.5\n223   m   13.42     56.8\n224   m   12.75     64.8\n225   m   16.33     64.5\n226   m   13.67     58.0\n227   m   13.25     62.8\n228   m   14.83     63.8\n229   m   12.75     57.8\n230   m   12.92     57.3\n231   m   14.83     63.5\n232   m   11.83     55.0\n233   m   13.67     66.5\n234   m   15.75     65.0\n235   m   13.67     61.5\n236   m   13.92     62.0\n237   m   12.58     59.3\n\n\n\n\n按颜色分组\n\n\n\n\n\n\n\n按形状分组\n\n\n\n基本散点图-按形/色分组\n\n\n\n选用的分组变量必须是分类变量，换言之，它必须是因子型或者字符型的向量。如果分组变量是数值型向量，则需要将它转化为因子型变量之后，才能以其作为分组变量。\n可以将一个变量同时映射到 shape 和 colour 属性。当有多个分组变量时，可以将它们分别映射到不同的图形属性。下面，我们把变量 sex 同时映射到 shape 和 colour 属性，见图 (fig5-5?) 。\n\nggplot(heightweight, aes(x = ageYear, y = heightIn,color = sex,shape = sex)) + \n  geom_point()\n\n\n\n基本散点图-按形和色分组\n\n\n\n有时需要使用不同于默认设置的点形和颜色。通过调用scale_shape_manual() 函数可以使用其他点形；\n调用 scale_colour_brewer() 函数或者 scale_colour_manual() 函数可以使用其他调色板，见图 (fig5-6?) 。\n\nggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex, colour = sex)) + \n  geom_point() +\n  scale_shape_manual(values = c(1,2)) + \n  scale_colour_brewer(palette = \"Set1\")\n\n\n\n基本散点图-修改调色板",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "05-ggplot2 绘图——散点图"
    ]
  },
  {
    "objectID": "Guide/R/05-ggplot2-scatter-plot.html#使用不同于默认设置的点形",
    "href": "Guide/R/05-ggplot2-scatter-plot.html#使用不同于默认设置的点形",
    "title": "05-ggplot2 绘图——散点图",
    "section": "",
    "text": "通过指定 geom_point() 函数中的点形（shape）参数，可以同时设定散点图中所有数据点的点形，如图 (fig5-7?) 所示。\n\nggplot(heightweight, aes(x = ageYear, y = heightIn)) + \n  geom_point(shape = 3)\n\n\n\n指定散点形状\n\n\n\nR系统绘图可以调用的图形较多，一些点形只有边框线（1～14）​；一些只有实心填充区域（15～20）​；还有一些则由可分离的边框线和具有填充色的实心区域共同组成（21～25）​，也可以用字符作点形。\n如果已将分组变量映射到 shape，则可以调用 scale_shape_manual() 函数来手动修改该分组变量不同水平的点形\n下图将使用略大且自定义点形的数据点：\n\nggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex)) + \n  geom_point(size = 3)+\n  scale_shape_manual(values = c(1,4))\n\n\n\n自定义散点形状\n\n\n\n\n点形1～20的点的颜色，包括实心区域的颜色都可由 colour 参数来控制。\n对于点形21～25而言，边框线和实心区域的颜色则分别由 colour 和 fill 参数来控制。\n\n我们可以将两个不同的变量分别由点形和填充色（空心或有填充）属性来表示。\n为了实现这一功能，首先需要选择一个同时具有 colour 和 fill 属性的点形，在 scale_shape_manual 中设定。之后需要在scale_fill_manual() 中选择一个包括 NA 和其他颜色的调色板（NA会生成一个空心的形状）​。",
    "crumbs": [
      "Home",
      "Guide",
      "R",
      "05-ggplot2 绘图——散点图"
    ]
  }
]