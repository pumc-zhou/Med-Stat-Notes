[
  {
    "objectID": "Quarto/quarto-LaTeX.html",
    "href": "Quarto/quarto-LaTeX.html",
    "title": "Quarto与LaTeX的记录",
    "section": "",
    "text": "在2025-02-23迁移博客到本网站时遇到一些问题，主要是因为在相关与回归部分使用plotly包制作了一个三维图形，在以前的Rmarkdown博客中，因为未知原因导致从2024年中的一次更新开始就无法在生成PDF文件，所以这个问题没有被发现。\n简而言之就是使用plotly绘制如图所示的双元正态分布图形后，如果Quarto被指定要生成PDF文件，需要载入更多的R包且更新TeXLive的宏包来提供支持。\n## 安装和加载所需的包\n# install.packages(\"plotly\")\n# install.packages(\"mvtnorm\")\nlibrary(plotly)\nlibrary(mvtnorm)\nlibrary(webshot2)\n\n# 创建网格数据\nx &lt;- seq(150, 190, length.out = 100)\n#身高150-190，等距的100个值\ny &lt;- seq(50, 80, length.out = 100)\n#体重50-80，等距的100个值\ngrid &lt;- expand.grid(X = x, Y = y)\n#生成 x 和 y 的所有组合，用于构建一个网格数据框，以便计算多元正态分布的概率密度。\n\n# 设置均值和协方差矩阵\nmu &lt;- c(170, 65)\n#设置双元正态分布的均值向量，表示均值分别为身高 170 cm 和体重 65 kg\n\nsigma &lt;- matrix(c(100, 20, 20, 25), nrow = 2)\n#设置协方差矩阵，表示身高的方差为 100，体重的方差为 25，身高和体重之间的协方差为 20\n\n# 计算概率密度\nz &lt;- dmvnorm(as.matrix(grid), mean = mu, sigma = sigma)\n#计算每个网格点上双元正态分布的概率密度。\n\n# 将概率密度矩阵转换为适合绘图的形状\nz_matrix &lt;- matrix(z, nrow = 100, ncol = 100)\n\n# 绘制三维表面图\nplot_ly(x = x, y = y, z = z_matrix, type = \"surface\") %&gt;%\n  layout(title = list(text = \"双元正态分布的三维概率密度图\", y=0.95),\n         scene = list(xaxis = list(title = \"身高 (cm)\"),\n                      yaxis = list(title = \"体重 (kg)\"),\n                      zaxis = list(title = \"概率密度\")))\n\n\n双元正态分布示例\n上述图像在被转换为PDF文件时，会发生报错：Quarto 文档中包含了一些生成 HTML 输出的函数（比如交互式图表或其他 HTML 小部件），但你当前的目标输出格式是 PDF。由于 PDF 是静态格式，无法直接渲染 HTML 内容，Quarto 会报错并停止执行。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/quarto-LaTeX.html#解决方案1",
    "href": "Quarto/quarto-LaTeX.html#解决方案1",
    "title": "Quarto与LaTeX的记录",
    "section": "\n1 解决方案1",
    "text": "1 解决方案1\n此章节不转换为PDF格式。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/quarto-LaTeX.html#解决方案2",
    "href": "Quarto/quarto-LaTeX.html#解决方案2",
    "title": "Quarto与LaTeX的记录",
    "section": "\n2 解决方案2",
    "text": "2 解决方案2\n增加支持：如果你仍想输出 PDF，但希望将 HTML 小部件作为静态截图嵌入，可以安装 R 的 webshot 或 webshot2 包。Quarto 会利用它们将 HTML 内容转换为图片。\n需要安装：\n\ninstall.packages(\"webshot2\")\n\n然后在这段程序的前部导入该包：library(webshot2)。\n也可以使用\n\ninstall.packages(\"webshot\")\n\n但是使用 webshot 还需要安装 PhantomJS\n\nwebshot::install_phantomjs()",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/quarto-LaTeX.html#解决方案3启用-prefer-html-true-选项",
    "href": "Quarto/quarto-LaTeX.html#解决方案3启用-prefer-html-true-选项",
    "title": "Quarto与LaTeX的记录",
    "section": "\n3 解决方案3：启用 prefer-html: true 选项\n",
    "text": "3 解决方案3：启用 prefer-html: true 选项\n\n如果你不在乎 HTML 内容在 PDF 中不可见，可以通过添加 prefer-html: true 来跳过这个错误。这种方法会忽略 HTML 输出，PDF 中不会显示相关内容。\n\n\n在 .qmd 文件的 YAML 前置元数据中添加：\n\nformat: pdf: toc: true # 可选{yaml}",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/quarto-LaTeX.html#解决方法-4检查并移除-html-输出代码",
    "href": "Quarto/quarto-LaTeX.html#解决方法-4检查并移除-html-输出代码",
    "title": "Quarto与LaTeX的记录",
    "section": "\n4 解决方法 4：检查并移除 HTML 输出代码\n",
    "text": "4 解决方法 4：检查并移除 HTML 输出代码\n\n如果你的目标是纯 PDF 输出，且不需要 HTML 小部件，可以检查文档中的代码块，移除或调整生成 HTML 的部分。例如：\n\n如果使用了 R 的 plotly 或 htmlwidgets，将其替换为静态图形库（如 ggplot2）。\n检查是否有 {r, results='asis'} 或其他生成 HTML 的设置，改为适合 PDF 的输出。\n\n示例：\n将交互式图表改为静态图表：\n\n# 原代码（生成 HTML）\nlibrary(plotly)\nplot_ly(data, x = ~x, y = ~y, type = \"scatter\")\n\n# 修改后（适合 PDF）\nlibrary(ggplot2)\nggplot(data, aes(x = x, y = y)) + geom_point()",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/quarto-LaTeX.html#quarto调用texlive报错",
    "href": "Quarto/quarto-LaTeX.html#quarto调用texlive报错",
    "title": "Quarto与LaTeX的记录",
    "section": "\n5 Quarto调用TeXLive报错",
    "text": "5 Quarto调用TeXLive报错\n在使用方法一进行完善的过程中，terminal输出了一些关于TeX Live的信息：\n\nTeX Live infrastructure update in progress ... \nTeX Live infrastructure update in progress ... \nDetailed command logging to \"C:\\texlive\\2024\\temp\\update-self.log\" \nself update: texlive.infra (70084 -&gt; 73495) \ntexlive.infra.windows (69813 -&gt; 71447) \ntlperl.windows (69939 -&gt; 71515) \nInfrastructure update finished successfully. \nYou may now close this window.\n\n这是因为Quarto的预览过程中触发了TeX Live的更新，这里的输出显示已完成更新，但是preview .qmd文档仍然不顺利，怀疑没有正确更新，且因为太晚关闭了电脑，导致更新中断。\n试着从terminal对TeX Live进行更新，在cmd中使用如下命令\n\ntlmgr update --self --all\n\n你的Windows电脑可能会和我一样报错：\n\nC:\\Users\\asus&gt;tlmgr update --self --all\nLocale 'English_United States.936' is unsupported, and may crash the interpreter.\n\n这是因为：\n当你运行 tlmgr update --self --all 来更新 TeX Live 时，出现了 Locale 'English_United States.936' is unsupported, and may crash the interpreter 的警告。这表明 TeX Live 的包管理器（tlmgr）在你的系统区域设置（locale）下遇到了兼容性问题，可能是因为你的 Windows 系统使用了中文（代码页 936，简体中文 GBK）作为默认区域设置，而 tlmgr 默认期望一个支持的区域设置（如 UTF-8 或英语）。\n以下是解决这个问题的步骤：\n\n\n5.1 方案 1：检查系统区域设置\n\n\n查看当前区域设置：\n\n按 Win + R，输入 intl.cpl，回车，打开“区域”设置。\n在“格式”选项卡中，查看当前格式（可能是“中文（简体，中国）”）。\n点击“管理”选项卡 -&gt; “更改系统区域设置”，查看“当前系统区域设置”（可能是“中文（简体，中国）”）。\n\n\n\n问题原因：\n\n代码页 936（中文 GBK）不是 TeX Live 的 tlmgr 完全支持的区域设置，可能导致编码或解释器错误。\n\n\n\n\n5.2 方案 2：临时更改区域设置运行 tlmgr\n\n为了让 tlmgr 正常工作，可以临时将区域设置为英语（美国），运行更新后再改回。\n操作步骤：\n\n\n更改系统区域设置：\n\n按 Win + R，输入 intl.cpl。\n点击“管理”选项卡 -&gt; “更改系统区域设置”。\n在下拉菜单中选择“英语（美国）”（English (United States)）。\n勾选“Beta: 使用 UTF-8 提供全球语言支持”（推荐），然后点击“确定”。\n系统会提示重启，点击“立即重启”。\n\n\n\n运行更新：\n\n重启后，打开命令行（cmd）。\n\n输入：\n\ntlmgr update --self --all\n\n\n观察是否还有错误，等待更新完成。\n\n\n\n恢复区域设置（可选）：\n\n更新完成后，重复上述步骤，将区域设置改回“中文（简体，中国）”，再次重启。\n\n\n\n\n5.3 方案 3：使用环境变量绕过区域问题\n如果你不想更改系统区域设置，可以通过设置环境变量临时调整 tlmgr 的运行环境。\n操作步骤：\n\n打开命令行（cmd）。\n\n输入以下命令设置临时环境变量：\n\nset LC_ALL=en_US.UTF-8\n\n\n\n紧接着运行更新：\n\ntlmgr update --self --all\n\n\n\n检查是否成功执行。如果仍然报错，尝试：\n\nset LANG=en_US.UTF-8\ntlmgr update --self --all\n\n\n说明：\n\n\nLC_ALL 或 LANG 是控制区域设置的环境变量，设置为 en_US.UTF-8 可以让 tlmgr 在英语环境下运行，避免中文编码问题。\n这种方法无需重启，适合临时解决。\n\n\n5.4 方案 4：验证更新结果\n更新完成后，检查 TeX Live 是否正常工作： 1. 运行： tlmgr info --list installed 查看已安装的包列表，确保更新生效。 2. 返回 Quarto 项目，运行： quarto preview 确认是否还有 TeX 相关的问题。\n\n5.5 额外建议\n\n\n检查 TeX Live 安装： 如果问题持续存在，可能是 TeX Live 安装不完整。可以尝试重新安装：\n\n下载最新版 TeX Live（tug.org/texlive）。\n安装时选择“完整安装”以确保所有组件齐全。\n\n\n\n使用 PowerShell 或 WSL： 如果 cmd 仍然有问题，可以尝试在 PowerShell 或 Windows Subsystem for Linux (WSL) 中运行 tlmgr，这些环境可能更好地处理区域设置。\n\n\n5.6 推荐方案\n\n\n快速解决：用 Step 3 设置 LC_ALL=en_US.UTF-8，无需重启，最简单。\n\n彻底解决：用 Step 2 临时改为英语区域设置，更新后再改回。\n\n我采用了方案 1，在cmd中输入命令后，出现如下：\n\nC:\\Users\\asus&gt;tlmgr update --self --all\ntlmgr.pl: package repository https://mirrors.hust.edu.cn/CTAN/systems/texlive/tlnet (not verified: gpg unavailable)\ntlmgr.pl: saving backups to C:/texlive/2024/tlpkg/backups\ntlmgr.pl: no self-updates for tlmgr available\ntlmgr.pl: skipping forcibly removed package: extractbb\ntlmgr.pl: skipping forcibly removed package: extractbb.windows\n[  1/764, ??:??/??:??] update: ebgaramond [8780k] (66604 -&gt; 71069) ... done\n[  2/764, 00:16/42:39] update: ebgaramond-maths [567k] (52168 -&gt; 74174) ... done\n\n表示在更新中，这个过程大概需要45分钟左右。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Quarto与LaTeX的记录"
    ]
  },
  {
    "objectID": "Quarto/config-giscus-github.html",
    "href": "Quarto/config-giscus-github.html",
    "title": "Qmd config giscus",
    "section": "",
    "text": "[giscus](https://giscus.app/zh-CN)\n\n\n\nGiscus是一个基于Github Discussion的评论插件，可以为无服务器端的博客运营者提供简易的部署和拓展。根据官网，其存在以下特性：\n开源！\n无跟踪，无广告，永久免费；\n无需数据库。全部数据均储存在 GitHub Discussions 中；\n支持自定义主题、多种语言、高度可配置；\n自动从 GitHub 拉取新评论与编辑；\n可自建服务！\n工作原理:\n  Giscus在加载时，会使用 GitHub Discussions 搜索 API 根据选定的映射方式（如 URL、pathname等）来查找与当前页面关联的discussion。如果找不到匹配的discussion，giscus bot就会在第一次有人留下评论或回应时自动创建一个discussion。\n\n  在评论时，访客必须按 GitHub OAuth 流程授权 giscus app 代表他发帖。或者访客也可以直接在 GitHub Discussion 里评论。你可以在 GitHub 上管理评论。\n\n\n\n\n首先要在GitHub建一个开放、可以使用Discussion的仓库，命名可以根据自己需要来，不能同网站仓库一致，因为独立存放评论内容。\n\n创建仓库完成后，请点击仓库最后一个选项setting，往下滑，找到Features功能区中的Discussion，点击勾选，开启仓库的Discussion功能，并安装giscus app ，点击连接并且install到对应新建的仓库就可以了。\n\n\n\n\n\n\nhttps://github.com/apps/giscus\n\nGiscus 应用库可以帮助你更方便地管理设置。通过上述链接进入网页进行安装：\n\n安装完成后转到如下页面，选择已创建好的comments仓库作为评论仓库。\n\n\n\n\n进入&lt;https://giscus.app/zh-CN&gt;配置页面，在该页面中根据自己的需求，选择配置。\n\n仓库选项是必须的，你需要把你创建好的comments仓库输入其中。\n选择合适的分类：\n\n嵌入到网站，在上述配置完成后，会生成一段script，将其放置在你想要展示评论的位置即可。\n\nend.",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Qmd config giscus"
    ]
  },
  {
    "objectID": "Quarto/config-giscus-github.html#什么是giscus",
    "href": "Quarto/config-giscus-github.html#什么是giscus",
    "title": "Qmd config giscus",
    "section": "",
    "text": "Giscus是一个基于Github Discussion的评论插件，可以为无服务器端的博客运营者提供简易的部署和拓展。根据官网，其存在以下特性：\n开源！\n无跟踪，无广告，永久免费；\n无需数据库。全部数据均储存在 GitHub Discussions 中；\n支持自定义主题、多种语言、高度可配置；\n自动从 GitHub 拉取新评论与编辑；\n可自建服务！\n工作原理:\n  Giscus在加载时，会使用 GitHub Discussions 搜索 API 根据选定的映射方式（如 URL、pathname等）来查找与当前页面关联的discussion。如果找不到匹配的discussion，giscus bot就会在第一次有人留下评论或回应时自动创建一个discussion。\n\n  在评论时，访客必须按 GitHub OAuth 流程授权 giscus app 代表他发帖。或者访客也可以直接在 GitHub Discussion 里评论。你可以在 GitHub 上管理评论。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Qmd config giscus"
    ]
  },
  {
    "objectID": "Quarto/config-giscus-github.html#如何使用giscus",
    "href": "Quarto/config-giscus-github.html#如何使用giscus",
    "title": "Qmd config giscus",
    "section": "",
    "text": "首先要在GitHub建一个开放、可以使用Discussion的仓库，命名可以根据自己需要来，不能同网站仓库一致，因为独立存放评论内容。\n\n创建仓库完成后，请点击仓库最后一个选项setting，往下滑，找到Features功能区中的Discussion，点击勾选，开启仓库的Discussion功能，并安装giscus app ，点击连接并且install到对应新建的仓库就可以了。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Qmd config giscus"
    ]
  },
  {
    "objectID": "Quarto/config-giscus-github.html#安装giscus-app",
    "href": "Quarto/config-giscus-github.html#安装giscus-app",
    "title": "Qmd config giscus",
    "section": "",
    "text": "https://github.com/apps/giscus\n\nGiscus 应用库可以帮助你更方便地管理设置。通过上述链接进入网页进行安装：\n\n安装完成后转到如下页面，选择已创建好的comments仓库作为评论仓库。",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Qmd config giscus"
    ]
  },
  {
    "objectID": "Quarto/config-giscus-github.html#配置giscus",
    "href": "Quarto/config-giscus-github.html#配置giscus",
    "title": "Qmd config giscus",
    "section": "",
    "text": "进入&lt;https://giscus.app/zh-CN&gt;配置页面，在该页面中根据自己的需求，选择配置。\n\n仓库选项是必须的，你需要把你创建好的comments仓库输入其中。\n选择合适的分类：\n\n嵌入到网站，在上述配置完成后，会生成一段script，将其放置在你想要展示评论的位置即可。\n\nend.",
    "crumbs": [
      "Home",
      "Quarto introduction",
      "Qmd config giscus"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html",
    "href": "Learn/Bayes/01-Bayes-PGM.html",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "",
    "text": "概率图模型是一种结合概率论与图论的理论框架，用于表示和分析复杂系统中的不确定性及概率关系。\n\n\n贝叶斯网络\n马尔科夫随机场（MRF)\n隐马尔科夫模型（HMM)\n\n\n安装：\n\ninstall.packages(\"gRain\")\n\n加载：\n\nlibrary(\"gRbase\")\n\n首先定义一个带有变量的A、B、C、D、E的简单无向图\n\ngraph &lt;- ug(\"A:B:E + C:E:D\")\nclass(graph)\n\n[1] \"igraph\"\n\n\n安装可视化程序包，使用流行的Rgraphviz\nRgraphviz是 Bioconductor 生态系统的一部分，所以可以通过 Bioconductor 来安装它。首先要确保已经安装了BiocManager，若未安装，可使用以下代码进行安装：\n\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\n安装好BiocManager之后，使用它来安装Rgraphviz：\n\nBiocManager::install(\"Rgraphviz\")\n\n载入并使用：\n\nlibrary(\"Rgraphviz\")\nplot(graph)\n\n\n\n\n\n\n\n定义有向图：\n\ndag &lt;- dag(\"A + B:A + C:B + D:B + E:C:D\")\ndag\n\nIGRAPH 28898ca DN-- 1 0 -- \n+ attr: name (v/c)\n+ edges from 28898ca (vertex names):\n\nplot(dag)\n\n\n\n\n\n\n\n\n报错：无法正确显示图形\n原因：Rgraphviz 没有被正确的安装。\n需要从 bioconductor 中安装：\n\n&gt; install.packages(\"Rgraphviz\")\nInstalling package into 'C:/Users/asus/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\nWarning message:\npackage 'Rgraphviz' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n当使用 bidconductor 源来安装 Rgraphviz ，首先安装 BiocManager ：\n\n&gt; if (!require(\"BiocManager\", quietly = TRUE))\n+     install.packages(\"BiocManager\")\nInstalling package into 'C:/Users/asus/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/BiocManager_1.30.25.zip'\nContent type 'application/zip' length 506482 bytes (494 KB)\ndownloaded 494 KB\n\npackage 'BiocManager' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\n\n然后使用 BiocManager 安装 Rgraphviz ，需要安装的配套程序包很多，需要几分钟，但是这里出现了报错。\n\n&gt; BiocManager::install(\"Rgraphviz\")\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.sustech.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nInstalling package(s) 'BiocVersion', 'Rgraphviz'\nalso installing the dependencies 'BiocGenerics', 'graph'\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/BiocGenerics_0.52.0.zip'\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nContent type 'application/zip' length 639558 bytes (624 KB)\ndownloaded 624 KB\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/graph_1.84.1.zip'\nContent type 'application/zip' length 2173761 bytes (2.1 MB)\ndownloaded 2.1 MB\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/BiocVersion_3.20.0.zip'\nContent type 'application/zip' length 8386 bytes\ndownloaded 8386 bytes\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/Rgraphviz_2.50.0.zip'\nContent type 'application/zip' length 1457153 bytes (1.4 MB)\ndownloaded 1.4 MB\n\npackage 'BiocGenerics' successfully unpacked and MD5 sums checked\npackage 'graph' successfully unpacked and MD5 sums checked\npackage 'BiocVersion' successfully unpacked and MD5 sums checked\npackage 'Rgraphviz' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\nInstallation paths not writeable, unable to update packages\n  path: C:/Program Files/R/R-4.4.2/library\n  packages:\n    class, cli, cluster, curl, data.table, foreign, glue, jsonlite, KernSmooth,\n    lintr, MASS, Matrix, nlme, nnet, processx, ps, purrr, R.utils, R6, renv,\n    reticulate, rlang, rpart, sessioninfo, spatial, survival, tinytex, xfun,\n    xml2, zoo\nOld packages: 'httpgd', 'readxl', 'unigd'\napdate all/some/none? [a/s/n]:\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/httpgd_2.0.3.zip'\nContent type 'application/zip' length 874316 bytes (853 KB)\ndownloaded 853 KB\n\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/readxl_1.4.4.zip' \nContent type 'application/zip' length 750422 bytes (732 KB)\ndownloaded 732 KB\n\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/unigd_0.1.3.zip'\nContent type 'application/zip' length 5591347 bytes (5.3 MB)\ndownloaded 5.3 MB\n\npackage 'httpgd' successfully unpacked and MD5 sums checked\npackage 'readxl' successfully unpacked and MD5 sums checked\nWarning: cannot remove prior installation of package 'readxl'\nWarning: restored 'readxl'\npackage 'unigd' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\nWarning message:\nIn file.copy(savedcopy, lib, recursive = TRUE) :\n  problem copying C:\\Users\\asus\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\readxl\\libs\\x64\\readxl.dll to C:\\Users\\asus\\AppData\\Local\\R\\win-library\\4.4\\readxl\\libs\\x64\\readxl.dll: Permission denied\n\n主要问题是没有写入权限，这可能和在 vscode 中安装有关，因此，换到 R 的终端中进行安装（注意，可能需要使用管理员权限），但是仍然免不了一系列的包冲突问题，需要根据提示选择操作：\n\n使用管理员身份运行 R 。\n选择合适的源来下载。\n\n处理有冲突的包（卸载），或者重新创建一个 env 。\n\n # 删除 readxl 包\n readxl_path &lt;- system.file(package = \"readxl\")\n if (!is.na(readxl_path)) {\n unlink(readxl_path, recursive = TRUE)\n }\n # 删除 cli 包\n cli_path &lt;- system.file(package = \"cli\")\n if (!is.na(cli_path)) {\n unlink(cli_path, recursive = TRUE)\n }\n # 再次尝试安装 Rgraphviz\n BiocManager::install(\"Rgraphviz\")\n\n\n强制更新，忽略冲突\n\n有些包的版本已经是最新的，但安装过程仍然提示需要更新，你可以使用 force = TRUE 参数强制重新安装。\n\nBiocManager::install(\"Rgraphviz\", force = TRUE)\n\n\n\n具体信息如下：\n\n&gt; BiocManager::install(\"Rgraphviz\")\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.nju.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nWarning message:\npackage(s) not installed when version(s) same as or greater than current; use\n  `force = TRUE` to re-install: 'Rgraphviz' \n&gt; BiocManager::install(\"Rgraphviz\", force = TRUE)\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.nju.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nInstalling package(s) 'Rgraphviz'\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/Rgraphviz_2.50.0.zip'\nContent type 'application/zip' length 1457153 bytes (1.4 MB)\ndownloaded 1.4 MB\n\npackage ‘Rgraphviz’ successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\Rtmp2j7SlA\\downloaded_packages\n\n\n\n首先，为每一个节点定义取值：\n\nmachine_val &lt;- c(\"working\", \"broken\")\nlight_bulb_val &lt;- c(\"good\", \"bad\")\n\n为两个随机变量定义百分比数值：\n\nmachine_val &lt;- c(99,1)\nlight_bulb_val &lt;- c(99,1,60,40)\n\n使用 gRain 定义随机变量：\n\nlibrary(gRain)\nM &lt;- cptable(~machine, values = machine_prob,\n            levels = machine_val)\nL &lt;- cptable(~light_bulb | machine,\n            values = light_bulb_prob,\n            levels = light_bulb_val)\n\n这里的 cptable 表示条件概率表1：它是离散型随机变量概率分布的内存表示2。\n\n\nplist &lt;- compileCPT(list(M,L))\nplist\n\n输出结果如上，这里可以清楚地看到之前定义的概率分布\n2025.04.05 再次尝试复现程序，失败，暂时停止概率图R程序的复现工作。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html#常见类型",
    "href": "Learn/Bayes/01-Bayes-PGM.html#常见类型",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "",
    "text": "贝叶斯网络\n马尔科夫随机场（MRF)\n隐马尔科夫模型（HMM)",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html#用r来展示概率图模型",
    "href": "Learn/Bayes/01-Bayes-PGM.html#用r来展示概率图模型",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "",
    "text": "安装：\n\ninstall.packages(\"gRain\")\n\n加载：\n\nlibrary(\"gRbase\")\n\n首先定义一个带有变量的A、B、C、D、E的简单无向图\n\ngraph &lt;- ug(\"A:B:E + C:E:D\")\nclass(graph)\n\n[1] \"igraph\"\n\n\n安装可视化程序包，使用流行的Rgraphviz\nRgraphviz是 Bioconductor 生态系统的一部分，所以可以通过 Bioconductor 来安装它。首先要确保已经安装了BiocManager，若未安装，可使用以下代码进行安装：\n\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\n安装好BiocManager之后，使用它来安装Rgraphviz：\n\nBiocManager::install(\"Rgraphviz\")\n\n载入并使用：\n\nlibrary(\"Rgraphviz\")\nplot(graph)\n\n\n\n\n\n\n\n定义有向图：\n\ndag &lt;- dag(\"A + B:A + C:B + D:B + E:C:D\")\ndag\n\nIGRAPH 28898ca DN-- 1 0 -- \n+ attr: name (v/c)\n+ edges from 28898ca (vertex names):\n\nplot(dag)",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html#报错",
    "href": "Learn/Bayes/01-Bayes-PGM.html#报错",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "",
    "text": "报错：无法正确显示图形\n原因：Rgraphviz 没有被正确的安装。\n需要从 bioconductor 中安装：\n\n&gt; install.packages(\"Rgraphviz\")\nInstalling package into 'C:/Users/asus/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\nWarning message:\npackage 'Rgraphviz' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n当使用 bidconductor 源来安装 Rgraphviz ，首先安装 BiocManager ：\n\n&gt; if (!require(\"BiocManager\", quietly = TRUE))\n+     install.packages(\"BiocManager\")\nInstalling package into 'C:/Users/asus/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/BiocManager_1.30.25.zip'\nContent type 'application/zip' length 506482 bytes (494 KB)\ndownloaded 494 KB\n\npackage 'BiocManager' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\n\n然后使用 BiocManager 安装 Rgraphviz ，需要安装的配套程序包很多，需要几分钟，但是这里出现了报错。\n\n&gt; BiocManager::install(\"Rgraphviz\")\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.sustech.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nInstalling package(s) 'BiocVersion', 'Rgraphviz'\nalso installing the dependencies 'BiocGenerics', 'graph'\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/BiocGenerics_0.52.0.zip'\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nContent type 'application/zip' length 639558 bytes (624 KB)\ndownloaded 624 KB\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/graph_1.84.1.zip'\nContent type 'application/zip' length 2173761 bytes (2.1 MB)\ndownloaded 2.1 MB\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/BiocVersion_3.20.0.zip'\nContent type 'application/zip' length 8386 bytes\ndownloaded 8386 bytes\n\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/Rgraphviz_2.50.0.zip'\nContent type 'application/zip' length 1457153 bytes (1.4 MB)\ndownloaded 1.4 MB\n\npackage 'BiocGenerics' successfully unpacked and MD5 sums checked\npackage 'graph' successfully unpacked and MD5 sums checked\npackage 'BiocVersion' successfully unpacked and MD5 sums checked\npackage 'Rgraphviz' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\nInstallation paths not writeable, unable to update packages\n  path: C:/Program Files/R/R-4.4.2/library\n  packages:\n    class, cli, cluster, curl, data.table, foreign, glue, jsonlite, KernSmooth,\n    lintr, MASS, Matrix, nlme, nnet, processx, ps, purrr, R.utils, R6, renv,\n    reticulate, rlang, rpart, sessioninfo, spatial, survival, tinytex, xfun,\n    xml2, zoo\nOld packages: 'httpgd', 'readxl', 'unigd'\napdate all/some/none? [a/s/n]:\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/httpgd_2.0.3.zip'\nContent type 'application/zip' length 874316 bytes (853 KB)\ndownloaded 853 KB\n\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/readxl_1.4.4.zip' \nContent type 'application/zip' length 750422 bytes (732 KB)\ndownloaded 732 KB\n\ntrying URL 'https://mirrors.sustech.edu.cn/CRAN/bin/windows/contrib/4.4/unigd_0.1.3.zip'\nContent type 'application/zip' length 5591347 bytes (5.3 MB)\ndownloaded 5.3 MB\n\npackage 'httpgd' successfully unpacked and MD5 sums checked\npackage 'readxl' successfully unpacked and MD5 sums checked\nWarning: cannot remove prior installation of package 'readxl'\nWarning: restored 'readxl'\npackage 'unigd' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\RtmpSCcZNG\\downloaded_packages\nWarning message:\nIn file.copy(savedcopy, lib, recursive = TRUE) :\n  problem copying C:\\Users\\asus\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\readxl\\libs\\x64\\readxl.dll to C:\\Users\\asus\\AppData\\Local\\R\\win-library\\4.4\\readxl\\libs\\x64\\readxl.dll: Permission denied\n\n主要问题是没有写入权限，这可能和在 vscode 中安装有关，因此，换到 R 的终端中进行安装（注意，可能需要使用管理员权限），但是仍然免不了一系列的包冲突问题，需要根据提示选择操作：\n\n使用管理员身份运行 R 。\n选择合适的源来下载。\n\n处理有冲突的包（卸载），或者重新创建一个 env 。\n\n # 删除 readxl 包\n readxl_path &lt;- system.file(package = \"readxl\")\n if (!is.na(readxl_path)) {\n unlink(readxl_path, recursive = TRUE)\n }\n # 删除 cli 包\n cli_path &lt;- system.file(package = \"cli\")\n if (!is.na(cli_path)) {\n unlink(cli_path, recursive = TRUE)\n }\n # 再次尝试安装 Rgraphviz\n BiocManager::install(\"Rgraphviz\")\n\n\n强制更新，忽略冲突\n\n有些包的版本已经是最新的，但安装过程仍然提示需要更新，你可以使用 force = TRUE 参数强制重新安装。\n\nBiocManager::install(\"Rgraphviz\", force = TRUE)\n\n\n\n具体信息如下：\n\n&gt; BiocManager::install(\"Rgraphviz\")\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.nju.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nWarning message:\npackage(s) not installed when version(s) same as or greater than current; use\n  `force = TRUE` to re-install: 'Rgraphviz' \n&gt; BiocManager::install(\"Rgraphviz\", force = TRUE)\n'getOption(\"repos\")' replaces Bioconductor standard repositories, see\n'help(\"repositories\", package = \"BiocManager\")' for details.\nReplacement repositories:\n    CRAN: https://mirrors.nju.edu.cn/CRAN\nBioconductor version 3.20 (BiocManager 1.30.25), R 4.4.2 (2024-10-31 ucrt)\nInstalling package(s) 'Rgraphviz'\ntrying URL 'https://bioconductor.org/packages/3.20/bioc/bin/windows/contrib/4.4/Rgraphviz_2.50.0.zip'\nContent type 'application/zip' length 1457153 bytes (1.4 MB)\ndownloaded 1.4 MB\n\npackage ‘Rgraphviz’ successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n        C:\\Users\\asus\\AppData\\Local\\Temp\\Rtmp2j7SlA\\downloaded_packages",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html",
    "href": "Learn/Basic/10-regression-correlation.html",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "two variables relationship\n\n\n\n\n\nThe basic process of straight-line regression analysis\n\n\n\n\n\n\n\n\n名称\n适用条件\n\n\n\nPearson直线相关系数\n双变量正态分布的资料\\(\\rightarrow\\)定量\\(\\rightarrow\\)类比t检验、方差分析\n\n\n列联系数\n非等级资料\\(\\rightarrow\\)分类\\(\\rightarrow\\)类比卡方检验\n\n\nSpearman秩相关系数\n不满足双变量正态分布、分布未知、等级资料\\(\\rightarrow\\)定量+分类\\(\\rightarrow\\)类比秩和检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#两变量关系分析",
    "href": "Learn/Basic/10-regression-correlation.html#两变量关系分析",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "two variables relationship",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#常见相关系数",
    "href": "Learn/Basic/10-regression-correlation.html#常见相关系数",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "The basic process of straight-line regression analysis\n\n\n\n\n\n\n\n\n名称\n适用条件\n\n\n\nPearson直线相关系数\n双变量正态分布的资料\\(\\rightarrow\\)定量\\(\\rightarrow\\)类比t检验、方差分析\n\n\n列联系数\n非等级资料\\(\\rightarrow\\)分类\\(\\rightarrow\\)类比卡方检验\n\n\nSpearman秩相关系数\n不满足双变量正态分布、分布未知、等级资料\\(\\rightarrow\\)定量+分类\\(\\rightarrow\\)类比秩和检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#简单直线回归",
    "href": "Learn/Basic/10-regression-correlation.html#简单直线回归",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "Draw a scatterplot\n\n\n选择一组数据集的“最佳拟合直线”，需要设法通过观测数据确定参数\\(\\alpha\\)与\\(\\beta\\)的估计值a和b，使得直线\n\\[\\hat y=a+bx\\]\n能最佳地反映\\((x_i,y_i)\\)之间的变化关系，该直线称为一元回归直线。\n常用最小二乘估计法（least squares estimation）来最佳直线，其基本原理是通过最小化残差平方和，使得各观测点到回归直线的纵向距离的平方和最小。\n\\[\n\\begin{cases}\na=\\bar y-b \\bar x\\\\\nb=\\frac{\\sum\\limits_{i=1}^n x_i y_i - \\frac{1}{n}(\\sum\\limits_{i=1}^n x_i)(\\sum\\limits_{i=1}^n y_i)}{\\sum\\limits_{i=1}^n x_i^2 - \\frac{1}{n}(\\sum\\limits_{i=1}^n x_i)^2}\n\\end{cases}\n\\] 为方便，引入以下记号： \\[\nSS_{xx}=\\sum_{i}(x_i-\\bar x)^2=\\sum_{i}x_i^2-\\frac{1}{n}(\\sum_{i}x_i)^2\\\\\nSS_{yy}=\\sum_{i}(y_i-\\bar y)^2=\\sum_{i}y_i^2-\\frac{1}{n}(\\sum_{i}y_i)^2\\\\\nSS_{xy}=\\sum_{i}(x_i-\\bar x)(y_i-\\bar y)=\\sum_{i}x_i y_i-\\frac{1}{n}(\\sum_{i}x_i)(\\sum_{i}y_i)\n\\] 其中，\\(SS_{xx}\\)和\\(SS_{yy}\\)是离均差平方和，\\(SS_{xy}\\)称为离均差积和。\n这样可以简化为： \\[\n\\begin{cases}\na=\\bar y-b \\bar x\\\\\nb=\\frac{SS_{xy}}{SS_{xx}}\n\\end{cases}\n\\]\n\n\nF检验\n\n\\(y_i\\)的总离均差平方和为：\n\\[SS_{yy}=\\sum_{i}(y_i-\\bar y)^2\\] 对其做分解，得到等式：\n\\[SS_{yy}=\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\] \\(\\sum_{i}^{n}(\\hat y_i-\\bar y)^2\\)为回归平方和（regression sum of squares），记为\\(SS_R\\)，表示回归估计值\\(\\hat y_i\\)与均数\\(\\bar y\\)的离差平方和，其公式为：\n\\[\n\\begin{align}\nSS_{yy} &= \\sum_{i=1}^{n}(\\hat y_i - \\bar y)^2 \\\\\n        &= \\sum_{i=1}^{n}[a + bx_i - (a + b\\bar x)]^2 \\\\\n        &= SS_{xx}b^2 \\\\\n        &= SS_{xy}b\n\\end{align}\n\\] 显然，回归平方和\\(SS_{R}\\)反映的是在y的总变异中由x与y的直线回归关系解释的那部分变异。\\(SS_R\\)值越大，说明回归直线的拟合效果就越好。\n\\(\\sum_{i}^{n}(y_i-\\hat y_i)^2\\)为残差平方和（residual sum of squares），记为\\(SS_E\\)，表示观测值\\(y_i\\)与回归估计值\\(\\hat y_i\\)的离差平方和，其公式为： \\[SS_E=\\sum_{i=1}^{n}(y_i-\\hat y_i)^2\\] \\(SS_E\\)反映了在总变异中扣除自变量x对因变量y的线性影响以后的其他因素（包括x对y的非线性影响和随机误差等）对y变异的影响，也就是在总平方和中无法用y和x线性回归关系解释的部分。\\(SS_E\\)值越小，说明回归直线的拟合效果就越好。\n对公式进行简化： \\[\\begin{align}\nSS_{yy}=&\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\\\\n=&SS_R+SS_E\n\\end{align}\\] 上述三个平方和，各有其相应的自由度\\(v\\)，并有如下关系： \\[v_{yy}=v_R+v_E\\\\\nv_{yy}=n-1,v_R=1,v_E=n-2\\]\n在\\(H_0\\)成立的条件下，有： \\[\\frac{SS_R}{\\sigma^2}\\sim \\chi^2(v_R),\\frac{SS_E}{\\sigma^2}\\sim \\chi^2(v_E)\\] 且\\(SS_R\\)和\\(SS_E\\)相互独立。\n检验统计量：\n\\[F=\\frac{SS_R/v_R}{SS_E/v_E}\\] 服从自由度\\(v_R=1,v_E=n-2\\)的F分布。如果y和x确实存在直线回归关系，那么回归所解释的变异\\(SS_R\\)应大于其他因素所解释的变异\\(SS_E\\)。由此可见，F检验正是建立在这个基础上的。\n对于给定的检验水准\\(\\alpha\\)， 如果\\(F&gt;F_{(v_R,v_E),1-\\alpha}\\)，则拒绝\\(H_0\\)，认为直线回归方程有统计学显著性；\n如果\\(F\\leq F_{(v_R,v_E),1-\\alpha}\\)，则不拒绝\\(H_0\\)，尚不能认为直线回归方程有统计学显著性。\n\nt检验法",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#直线相关与直线回归的比较",
    "href": "Learn/Basic/10-regression-correlation.html#直线相关与直线回归的比较",
    "title": "简单线性相关和回归",
    "section": "\n2.3 直线相关与直线回归的比较",
    "text": "2.3 直线相关与直线回归的比较\n\n\n\n\n\n\n\n区别与联系\n类目\n内容\n\n\n\n区别\n资料要求\n1. 线性相关要求X,Y服从双变量正态分布，对这种资料进行回归分析称为\\(\\textrm{II}\\)型回归，即可以把X当自变量，也可以当因变量，反之亦然。2. 线性回归要求Y在给定X值时服从正态分布，X可以是精确测量和严格控制的变量，这时的回归称为型回归，即不可以把X当因变量，Y当自变量进行回归分析。\n\n\n\n\n应用\n1. 线性相关用来表达两个变量间的互依关系，两个变量的研究地位是相等的，谁做X，谁做Y都可以；2. 线性回归用来表达两个变量间的依存变化的数量关系，即一个变量（为因变量Y）如何依存于另一个变量（为自变量X）而变化，两个变量的研究地位是不相等的。\n\n\n\n意义\n1. 相关系数r说明具有线性关系的两个变量之间的密切程度和相关方向；2. 回归系数b表示X每变化一个单位所导致的Y的平均变化量。\n\n\n\nr和b的取值范围\nr没有单位，而b有单位（其单位是：Y的单位/X的单位），所以导致两者的取值范围不同；\\(-1 \\le r \\le 1\\),\\(-\\infty&lt;b&lt;+\\infty\\)\n\n\n\n\nr和b的计算公式不同\n\n\\(r=\\frac{l_{xy}}{\\sqrt{l_{xx}l_{yy}}}\\),\\(b=\\frac{SS_{xy}}{SS_{xx}}\\)\n\n\n\n联系\n符号\n对于既可以做相关又可作回归的同一组资料，计算出r与b的正负号相同\n\n\n\n假设检验\n对于同一组资料，相关系数和回归系数的假设检验等价。即有：\\(t_b=t_r\\)\n\n\n\n\n相互换算\n对于同一组资料，相关系数和回归系数可通过下式换算：\\(b=r\\frac{S_Y}{S_X}\\)，式中的\\(S_X,S_Y\\)分别是\\(X,Y\\)的标准差\n\n\n\n用回归解释相关\n又决定系数\\(R^2=\\frac{SS_{回}}{SS_{总}}\\in [0,1]\\)当总平方和的大小决定了相关的密切程度，回归平方和越接近总平方和，则\\(R^2\\)越接近1，相关的效果越好，说明回归效果越好，相关的密切程度也越高。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/08-anova-test.html",
    "href": "Learn/Basic/08-anova-test.html",
    "title": "方差分析",
    "section": "",
    "text": "类目\n完全随机设计的方差分析\n\n\n\n数据要求\n独立性、正态性、方差齐性\n\n\n检验目的\n推断多个样本所代表的总体均数是否不等\n\n\n\n\\(H_0\\)与\\(H_1\\)\n\n\n\\(H_0\\):\\(\\mu_1=\\mu_2=\\dots =\\mu_a\\)，各组所代表的总体均数相等。\\(H_1\\):\\(\\mu_1,\\mu_2,\\dots,\\mu_a\\)各组总体均数不全相等（至少有一个不等式成立）\n\n\n检验统计量\n\\(F=\\frac{MS_{组间}}{MS_{组内}}\\sim(v_{组间}=k-1，v_{组内}=n-k）\\)\n\n\n关键要点\n总变异分解为组间变异和组内变异\n\n\n\n\n\n\n\n\n\n类目\n随机区组设计的方差分析\n\n\n\n数据要求\n处理组间、区组间数据满足独立、正态性和方差齐性\n\n\n处理组假设\n\n\\(H_0\\)：不同处理组水平的均数相同；\\(H_1\\)：不同处理组水平的均属不全相同\n\n\n区组假设\n\n\\(H_0\\)：不同区组对观测指标的影响很大；\\(H_1\\)：不同区组对观测指标的影响不全相同\n\n\n检验统计量\n\n\\(F=\\frac{MS_{处理}}{MS_{误差}}\\sim(v_{处理}=k-1,v_{误差}=(b-1)×(k-1))\\)\\(F=\\frac{MS_{区组}}{MS_{误差}}\\sim(v_{区组}=k-1,v_{误差}=(b-1)×(k-1))\\)\n\n\n\n关键要点\n总变异分解为处理组变异、区组变异和随机误差变异\n\n\n\n\n\n\n\n\n\n\n类目\n析因设计的方差分析\n\n\n\n数据要求\n因素之间的数据独立，样本数据满足正态性和方差齐性假设\n\n\n检验目的\n推断多个因素及其交互作用是否对观测指标存在显著影响\n\n\n主效应假设\n\n\\(H_0\\): 各因素的水平对观测指标的均数无显著影响；\\(H_1\\): 各因素的水平对观测指标的均数存在显著影响\n\n\n交互作用假设\n\n\\(H_0\\): 不同因素水平的交互作用对观测指标的均数无显著影响；\\(H_1\\): 不同因素水平的交互作用对观测指标的均数存在显著影响\n\n\n检验统计量\n\\(F=\\frac{MS_{主效应或交互作用}}{MS_{误差}}\\sim F(v_{效应},v_{误差})\\)\n\n\n关键要点\n- 总变异分解为主效应变异、交互作用变异和随机误差变异- 各因素主效应和交互作用的显著性需要单独检验- 每个因素包含多个水平，可能是固定效应或随机效应",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "方差分析"
    ]
  },
  {
    "objectID": "Learn/Basic/08-anova-test.html#完全随机设计的方差分析",
    "href": "Learn/Basic/08-anova-test.html#完全随机设计的方差分析",
    "title": "方差分析",
    "section": "",
    "text": "类目\n完全随机设计的方差分析\n\n\n\n数据要求\n独立性、正态性、方差齐性\n\n\n检验目的\n推断多个样本所代表的总体均数是否不等\n\n\n\n\\(H_0\\)与\\(H_1\\)\n\n\n\\(H_0\\):\\(\\mu_1=\\mu_2=\\dots =\\mu_a\\)，各组所代表的总体均数相等。\\(H_1\\):\\(\\mu_1,\\mu_2,\\dots,\\mu_a\\)各组总体均数不全相等（至少有一个不等式成立）\n\n\n检验统计量\n\\(F=\\frac{MS_{组间}}{MS_{组内}}\\sim(v_{组间}=k-1，v_{组内}=n-k）\\)\n\n\n关键要点\n总变异分解为组间变异和组内变异",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "方差分析"
    ]
  },
  {
    "objectID": "Learn/Basic/08-anova-test.html#随机区组设计的方差分析",
    "href": "Learn/Basic/08-anova-test.html#随机区组设计的方差分析",
    "title": "方差分析",
    "section": "",
    "text": "类目\n随机区组设计的方差分析\n\n\n\n数据要求\n处理组间、区组间数据满足独立、正态性和方差齐性\n\n\n处理组假设\n\n\\(H_0\\)：不同处理组水平的均数相同；\\(H_1\\)：不同处理组水平的均属不全相同\n\n\n区组假设\n\n\\(H_0\\)：不同区组对观测指标的影响很大；\\(H_1\\)：不同区组对观测指标的影响不全相同\n\n\n检验统计量\n\n\\(F=\\frac{MS_{处理}}{MS_{误差}}\\sim(v_{处理}=k-1,v_{误差}=(b-1)×(k-1))\\)\\(F=\\frac{MS_{区组}}{MS_{误差}}\\sim(v_{区组}=k-1,v_{误差}=(b-1)×(k-1))\\)\n\n\n\n关键要点\n总变异分解为处理组变异、区组变异和随机误差变异",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "方差分析"
    ]
  },
  {
    "objectID": "Learn/Basic/08-anova-test.html#析因设计的总结",
    "href": "Learn/Basic/08-anova-test.html#析因设计的总结",
    "title": "方差分析",
    "section": "",
    "text": "类目\n析因设计的方差分析\n\n\n\n数据要求\n因素之间的数据独立，样本数据满足正态性和方差齐性假设\n\n\n检验目的\n推断多个因素及其交互作用是否对观测指标存在显著影响\n\n\n主效应假设\n\n\\(H_0\\): 各因素的水平对观测指标的均数无显著影响；\\(H_1\\): 各因素的水平对观测指标的均数存在显著影响\n\n\n交互作用假设\n\n\\(H_0\\): 不同因素水平的交互作用对观测指标的均数无显著影响；\\(H_1\\): 不同因素水平的交互作用对观测指标的均数存在显著影响\n\n\n检验统计量\n\\(F=\\frac{MS_{主效应或交互作用}}{MS_{误差}}\\sim F(v_{效应},v_{误差})\\)\n\n\n关键要点\n- 总变异分解为主效应变异、交互作用变异和随机误差变异- 各因素主效应和交互作用的显著性需要单独检验- 每个因素包含多个水平，可能是固定效应或随机效应",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "方差分析"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html",
    "href": "Learn/Basic/06-parameter-estimation.html",
    "title": "参数估计",
    "section": "",
    "text": "统计量实际上是一种对样本数据信息的压缩。一个好的统计量，应该能把样本中包含总体的信息全部提炼出来，而不损失任何信息，这样的统计量称为充分统计量（sufficient statistic）。\n\n\n抽样误差是抽样研究固有的属性，不可避免，它是由个体变异和抽样共同引起的。\n\n总体方差已知，或总体方差未知但样本量足够大时\n\n\\[\\bar X \\sim N(\\mu,\\sigma_{\\bar X}^2)\\] 将\\(\\bar X\\)标准化，有： \\[U=\\frac{\\bar X-\\mu}{\\sigma_{\\bar X}}=\\frac{\\bar X-\\mu}{\\sigma_ X/\\sqrt{n}}\\] U为标准化随机变量，\\(U\\sim N(0,1)\\)。\n若从一个非正态总体中随机抽样，且样本量足够大\\((n\\geq30)\\)，样本均数\\(\\bar X\\)的抽样分布又该如何？\n中心极限定理(Central limit theorems):中心极限定理指的是给定一个任意分布的总体\\(X\\)，只要存在有限的方差\\(\\sigma^2(\\sigma^2\\neq0)\\)，则当样本量n足够大时，样本均数\\(\\bar X\\)的抽样分布将近似的服从均数为\\(\\mu\\)和方差为\\(\\sigma_{\\bar X}^2\\)的正态分布。 \\[\\bar X\\simeq N(\\mu,\\frac{\\sigma^2}{n})\\] 在大样本量条件下，由于样本方差\\(S^2\\)对总体方差\\(\\sigma^2\\)的估计误差非常小，实践中我们可以直接用\\(S^2\\)替代\\(\\sigma^2\\)进行计算。\n每次从这些总体中随机抽取\\(n\\)个抽样，一共抽\\(m\\)次。然后把这\\(m\\)组抽样分别求出平均值。这些平均值的分布接近正态分布。\n\n\\[\\frac{(n-1)S^2}{\\sigma^2}\\sim \\chi^2(v)\\] \\(\\chi^2\\)分布式赫尔默特（F.R. Helmert）于1875年研究来自正态总体的样本方差的抽样分布时得出的，其密度函数为： \\[f_v(x)=\\begin{cases} \\frac{1}{2^{\\frac{v}{2}}\\Gamma\\left(\\frac{v}{2}\\right)}y^{\\frac{v}{2}-1}\\mathrm{e}^{-\\frac{\\chi^2}{2}},&\\chi^2&gt;0\\\\ 0,&\\chi^2\\leq0\\end{cases}\\] \\(\\chi^2\\)分布和\\(t\\)分布一样，是依赖于参数（自由度）的一簇分布。随着自由度的增加，其分布曲线由正偏态分布趋近于正态分布。\n\n\n\n\n\n\n\n率的统计指标\n计算公式\n\n\n\n样本率\\(p\\)的总体均数\n\\(\\mu_{p}=\\pi\\)\n\n\n样本量\\(p\\)的方差\n\n\\(\\sigma_p^2=\\frac{\\pi(1-\\pi)}{n}\\)(理论值);\\(S_p^2=\\frac{p(1-p)}{n}\\)(估计值)\n\n\n样本率\\(p\\)的标准差\n\n\\(\\sigma_p=\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\)(理论值);\\(S_p=\\sqrt{\\frac{p(1-p)}{n}}\\)(估计值)\n\n\n率的标准误\n\n\\(\\sigma_p=\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\)(理论值);\\(S_p=\\sqrt{\\frac{p(1-p)}{n}}\\)(估计值)\n\n\n\n\n\n\n\n\n\n均数的统计指标\n计算公式\n\n\n\n样本均数\n\\(\\bar X=\\frac{\\sum_\\limits{i=1}^{n}X_i}{n}\\)\n\n\n样本方差\n\n\\(\\sigma^2=\\frac{\\sum_\\limits{i=1}^{n}(\\mu-\\bar \\mu)^2}{n}\\)(理论值);\\(S^2=\\frac{\\sum_\\limits{i=1}^{n}(X_i-\\bar X)^2}{n-1}\\)(估计值) 1\n\n\n\n样本均数标准误(SE)\n\n\\(\\sigma_{\\bar X}=\\frac{\\sigma}{\\sqrt{n}}\\)(理论值);\\(S_{\\bar X}=\\frac{S}{\\sqrt{n}}\\)(估计值)\n\n\n\n大数定律(Law of large Numbers):当随机事件发生的次数足够多时，随机事件发生的频率趋近于预期的概率。可以简单理解为样本数量越多，其平概率越接近于期望值。大数定律的条件：\n\n独立重复事件；\n重复次数足够多。\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n标准差\n均数的标准误\n\n\n\n定义\n描述一组变量的离散程度，并可以作为总体标准差的点估计\n描述多个样本均数的离散程度，并且是样本均数的标准差估计值\n\n\n应用\n1. 标准差越小，个体资料的离散程度就越小，说明变量值围绕均数分布越紧密，均数的代表性越好  2.估计医学参考值范围，计算变异系数和标准误\n1. 标准误越小，统计量的平均抽样误差就越小，说明样本均数和总体均数的平均差异越小，用样本均数估计总体均数的可靠性越大；2. 计算置信区间、进行假设检验\n\n\n与n的关系\nn越大，样本标准差随机波动的幅度越来越小，并且稳定在总体标准差附近\nn越大，样本均数的标准误越小，并且趋向于0\n\n\n控制方法\n个体差异，不能通过统计方法控制\n增加n，可以减小标准误\n\n\n二者联系\n1. 两者都是变异指标  2. 在n相同的情况下，标准差越大，标准误相对越大；标准差越小，标准误也相对越小。正比关系  3. \\(\\sigma_{\\bar x}=\\frac{\\sigma}{\\sqrt{n}}\\),\\(\\sigma_{\\bar x}\\)与\\(\\sigma\\)成正比，与\\(\\sqrt{n}\\)成反比。\n\n\n\n\n\n\n\n\n\n\n\n类目\n总体均数的置信区间\n医学参考值范围\n\n\n\n含义\n按照预先给定的概率，确定的包含未知总体参数\\(\\mu\\)（总体均数）的可能范围\n指特定的“正常”人群（排除了对所研究指标有影响的疾病和有关因素的人群）的生理生化指标中大多数个体的取值所在的范围\n\n\n举例\n若某一样本的均值为10，其95%可信区间为（9.5,10.5），这就表示总体均数介于（9.5,10.5）之间的可信度为95%\n假设空腹血糖95%正常值范围为（3.6,6.1），这就是指95%正常人的空腹血糖值介于（3.6,6.1）之间\n\n\n计算\n1. 总体标准差位置，且样本量n不大，根据t分布计算；2. 总体标准差未知，n足够大，正态近似法；3. 总体标准差已知，根据Z分布计算\n1. 正态分布法； 2. 偏态分布法\n\n\n用途\n总体均数的区间估计（估计未知的总体均数所在范围\n1. 个体值的波动范围；  2. 绝大多数观察对象某指标分布范围； 3. 医学判断个体某指标是否正常\n\n\n95%理解的常见误区\n\n\n\n\n区别\n\n\n\n\n\n\n在95%置信区间内有95%的总体参数在该区间？×\n在95%置信区间内，该区间包含了95%的总体参数？×\n以\\(1-\\alpha=95\\%\\)算得的100个可信区间中，平均有95个可信区间包含了总体均数，而另外5个未包含总体均数。√\n在95%置信区间，该区间有95%的可能包含总体参数？×\n该区间包含总体参数，可信度在95%。√\n总体参数有95%的可能落在该区间。×\n\n\n\n置信水平（Confidence Level）\n置信水平是指在多次重复抽样时，置信区间覆盖总体参数的比例。常见的置信水平有95%、99%等。置信水平越高，表示对总体参数的估计越保守，但置信区间也会变得更宽。\n置信区间的宽度（Width of Confidence Interval）\n置信区间的宽度是指上下限之间的距离，反映了估计的精确程度。置信区间越窄，说明估计的精度越高，越宽则精度越低。\n\n\n样本大小（Sample Size）\n样本大小是置信区间宽度的一个关键决定因素。样本量越大，标准误差越小，置信区间越窄，从而提高估计的精确度。\n样本标准差（Sample Standard Deviation）\n样本标准差反映数据的离散程度。样本的波动越大，置信区间越宽；样本的波动越小，置信区间越窄。\n置信水平\n提高置信水平（例如从95%提高到99%）会导致置信区间变宽，因为要包含更多可能的参数值范围。\n总体分布的形状\n如果数据服从正态分布且样本量较大，置信区间估计会更精确；对于非正态分布，特别是在样本量较小时，置信区间的估计可能不够准确。\n估计方法（Point Estimate and Statistical Technique）\n使用不同的统计方法（如t分布、z分布）会对置信区间的范围造成影响。通常情况下，样本量较小时采用t分布，样本量较大时可以近似采用z分布。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#统计量",
    "href": "Learn/Basic/06-parameter-estimation.html#统计量",
    "title": "参数估计",
    "section": "",
    "text": "统计量实际上是一种对样本数据信息的压缩。一个好的统计量，应该能把样本中包含总体的信息全部提炼出来，而不损失任何信息，这样的统计量称为充分统计量（sufficient statistic）。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#抽样分布",
    "href": "Learn/Basic/06-parameter-estimation.html#抽样分布",
    "title": "参数估计",
    "section": "",
    "text": "抽样误差是抽样研究固有的属性，不可避免，它是由个体变异和抽样共同引起的。\n\n总体方差已知，或总体方差未知但样本量足够大时\n\n\\[\\bar X \\sim N(\\mu,\\sigma_{\\bar X}^2)\\] 将\\(\\bar X\\)标准化，有： \\[U=\\frac{\\bar X-\\mu}{\\sigma_{\\bar X}}=\\frac{\\bar X-\\mu}{\\sigma_ X/\\sqrt{n}}\\] U为标准化随机变量，\\(U\\sim N(0,1)\\)。\n若从一个非正态总体中随机抽样，且样本量足够大\\((n\\geq30)\\)，样本均数\\(\\bar X\\)的抽样分布又该如何？\n中心极限定理(Central limit theorems):中心极限定理指的是给定一个任意分布的总体\\(X\\)，只要存在有限的方差\\(\\sigma^2(\\sigma^2\\neq0)\\)，则当样本量n足够大时，样本均数\\(\\bar X\\)的抽样分布将近似的服从均数为\\(\\mu\\)和方差为\\(\\sigma_{\\bar X}^2\\)的正态分布。 \\[\\bar X\\simeq N(\\mu,\\frac{\\sigma^2}{n})\\] 在大样本量条件下，由于样本方差\\(S^2\\)对总体方差\\(\\sigma^2\\)的估计误差非常小，实践中我们可以直接用\\(S^2\\)替代\\(\\sigma^2\\)进行计算。\n每次从这些总体中随机抽取\\(n\\)个抽样，一共抽\\(m\\)次。然后把这\\(m\\)组抽样分别求出平均值。这些平均值的分布接近正态分布。\n\n\\[\\frac{(n-1)S^2}{\\sigma^2}\\sim \\chi^2(v)\\] \\(\\chi^2\\)分布式赫尔默特（F.R. Helmert）于1875年研究来自正态总体的样本方差的抽样分布时得出的，其密度函数为： \\[f_v(x)=\\begin{cases} \\frac{1}{2^{\\frac{v}{2}}\\Gamma\\left(\\frac{v}{2}\\right)}y^{\\frac{v}{2}-1}\\mathrm{e}^{-\\frac{\\chi^2}{2}},&\\chi^2&gt;0\\\\ 0,&\\chi^2\\leq0\\end{cases}\\] \\(\\chi^2\\)分布和\\(t\\)分布一样，是依赖于参数（自由度）的一簇分布。随着自由度的增加，其分布曲线由正偏态分布趋近于正态分布。\n\n\n\n\n\n\n\n率的统计指标\n计算公式\n\n\n\n样本率\\(p\\)的总体均数\n\\(\\mu_{p}=\\pi\\)\n\n\n样本量\\(p\\)的方差\n\n\\(\\sigma_p^2=\\frac{\\pi(1-\\pi)}{n}\\)(理论值);\\(S_p^2=\\frac{p(1-p)}{n}\\)(估计值)\n\n\n样本率\\(p\\)的标准差\n\n\\(\\sigma_p=\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\)(理论值);\\(S_p=\\sqrt{\\frac{p(1-p)}{n}}\\)(估计值)\n\n\n率的标准误\n\n\\(\\sigma_p=\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\)(理论值);\\(S_p=\\sqrt{\\frac{p(1-p)}{n}}\\)(估计值)\n\n\n\n\n\n\n\n\n\n均数的统计指标\n计算公式\n\n\n\n样本均数\n\\(\\bar X=\\frac{\\sum_\\limits{i=1}^{n}X_i}{n}\\)\n\n\n样本方差\n\n\\(\\sigma^2=\\frac{\\sum_\\limits{i=1}^{n}(\\mu-\\bar \\mu)^2}{n}\\)(理论值);\\(S^2=\\frac{\\sum_\\limits{i=1}^{n}(X_i-\\bar X)^2}{n-1}\\)(估计值) 1\n\n\n\n样本均数标准误(SE)\n\n\\(\\sigma_{\\bar X}=\\frac{\\sigma}{\\sqrt{n}}\\)(理论值);\\(S_{\\bar X}=\\frac{S}{\\sqrt{n}}\\)(估计值)\n\n\n\n大数定律(Law of large Numbers):当随机事件发生的次数足够多时，随机事件发生的频率趋近于预期的概率。可以简单理解为样本数量越多，其平概率越接近于期望值。大数定律的条件：\n\n独立重复事件；\n重复次数足够多。\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n标准差\n均数的标准误\n\n\n\n定义\n描述一组变量的离散程度，并可以作为总体标准差的点估计\n描述多个样本均数的离散程度，并且是样本均数的标准差估计值\n\n\n应用\n1. 标准差越小，个体资料的离散程度就越小，说明变量值围绕均数分布越紧密，均数的代表性越好  2.估计医学参考值范围，计算变异系数和标准误\n1. 标准误越小，统计量的平均抽样误差就越小，说明样本均数和总体均数的平均差异越小，用样本均数估计总体均数的可靠性越大；2. 计算置信区间、进行假设检验\n\n\n与n的关系\nn越大，样本标准差随机波动的幅度越来越小，并且稳定在总体标准差附近\nn越大，样本均数的标准误越小，并且趋向于0\n\n\n控制方法\n个体差异，不能通过统计方法控制\n增加n，可以减小标准误\n\n\n二者联系\n1. 两者都是变异指标  2. 在n相同的情况下，标准差越大，标准误相对越大；标准差越小，标准误也相对越小。正比关系  3. \\(\\sigma_{\\bar x}=\\frac{\\sigma}{\\sqrt{n}}\\),\\(\\sigma_{\\bar x}\\)与\\(\\sigma\\)成正比，与\\(\\sqrt{n}\\)成反比。\n\n\n\n\n\n\n\n\n\n\n\n类目\n总体均数的置信区间\n医学参考值范围\n\n\n\n含义\n按照预先给定的概率，确定的包含未知总体参数\\(\\mu\\)（总体均数）的可能范围\n指特定的“正常”人群（排除了对所研究指标有影响的疾病和有关因素的人群）的生理生化指标中大多数个体的取值所在的范围\n\n\n举例\n若某一样本的均值为10，其95%可信区间为（9.5,10.5），这就表示总体均数介于（9.5,10.5）之间的可信度为95%\n假设空腹血糖95%正常值范围为（3.6,6.1），这就是指95%正常人的空腹血糖值介于（3.6,6.1）之间\n\n\n计算\n1. 总体标准差位置，且样本量n不大，根据t分布计算；2. 总体标准差未知，n足够大，正态近似法；3. 总体标准差已知，根据Z分布计算\n1. 正态分布法； 2. 偏态分布法\n\n\n用途\n总体均数的区间估计（估计未知的总体均数所在范围\n1. 个体值的波动范围；  2. 绝大多数观察对象某指标分布范围； 3. 医学判断个体某指标是否正常\n\n\n95%理解的常见误区\n\n\n\n\n区别",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#置信区间的含义与常见说法辨析",
    "href": "Learn/Basic/06-parameter-estimation.html#置信区间的含义与常见说法辨析",
    "title": "参数估计",
    "section": "",
    "text": "在95%置信区间内有95%的总体参数在该区间？×\n在95%置信区间内，该区间包含了95%的总体参数？×\n以\\(1-\\alpha=95\\%\\)算得的100个可信区间中，平均有95个可信区间包含了总体均数，而另外5个未包含总体均数。√\n在95%置信区间，该区间有95%的可能包含总体参数？×\n该区间包含总体参数，可信度在95%。√\n总体参数有95%的可能落在该区间。×",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#置信区间的两要素及影响因素",
    "href": "Learn/Basic/06-parameter-estimation.html#置信区间的两要素及影响因素",
    "title": "参数估计",
    "section": "",
    "text": "置信水平（Confidence Level）\n置信水平是指在多次重复抽样时，置信区间覆盖总体参数的比例。常见的置信水平有95%、99%等。置信水平越高，表示对总体参数的估计越保守，但置信区间也会变得更宽。\n置信区间的宽度（Width of Confidence Interval）\n置信区间的宽度是指上下限之间的距离，反映了估计的精确程度。置信区间越窄，说明估计的精度越高，越宽则精度越低。\n\n\n样本大小（Sample Size）\n样本大小是置信区间宽度的一个关键决定因素。样本量越大，标准误差越小，置信区间越窄，从而提高估计的精确度。\n样本标准差（Sample Standard Deviation）\n样本标准差反映数据的离散程度。样本的波动越大，置信区间越宽；样本的波动越小，置信区间越窄。\n置信水平\n提高置信水平（例如从95%提高到99%）会导致置信区间变宽，因为要包含更多可能的参数值范围。\n总体分布的形状\n如果数据服从正态分布且样本量较大，置信区间估计会更精确；对于非正态分布，特别是在样本量较小时，置信区间的估计可能不够准确。\n估计方法（Point Estimate and Statistical Technique）\n使用不同的统计方法（如t分布、z分布）会对置信区间的范围造成影响。通常情况下，样本量较小时采用t分布，样本量较大时可以近似采用z分布。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#假设检验的基本步骤",
    "href": "Learn/Basic/06-parameter-estimation.html#假设检验的基本步骤",
    "title": "参数估计",
    "section": "\n3.1 假设检验的基本步骤",
    "text": "3.1 假设检验的基本步骤\n\n\n\n\n\n\n步骤\n内容\n\n\n\n建立假设检验，确定检验水准\n1. 双侧检验：\\(H_{0}:\\mu_{d}=0;H_{1}:\\mu_{d}\\neq 0,\\alpha=0.05\\)  2. 单侧检验：\\(H_{0}:\\mu_{d}=0;H_{1}:\\mu_{d}&lt;0或\\mu_{d}&gt;0,\\alpha=0.05\\)\n\n\n\n\n1. 假设检验是针对总体的，而非样本；2. 单双侧检验主要根据专业知识预先确定，并且还需要考虑差异的方向； 3. 单侧检验的检验效能更高。\n\n\n计算并选择检验统计量\n1. 根据研究设计方案、资料类型、样本含量大小及分析目的选用适当的检验方法，并根据样本资料计算相应的检验统计量；2. 不同的检验方法要用不同的公式计算现有样本的检验统计量（\\(t\\)检验、\\(\\chi^2\\)检验、\\(F\\)检验）；3. 检验统计量是在\\(H_{0}\\)成立的前提下计算的。\n\n\n确定P值，做出推断\n假设检验的统计学结论：1. 若\\(P\\le \\alpha\\)，按所取\\(\\alpha\\)检验水准，拒绝\\(H_{0}\\)，接受\\(H_{1}\\)，可以认为…有差异；2. 若\\(P&gt;\\alpha\\)时，现有样本信息还不足以拒绝H0，尚不能认为…有差异\n\n\n\n假设检验所做出的的结论是具有概率性质的，不是绝对的肯定或否定。不论拒绝或不拒绝\\(H_{0}\\)都可能发生错误。下结论时，只能两种：1. 两总体有无差异；2. 两样本差异有无统计学意义。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#假设检验的两型错误检验效能",
    "href": "Learn/Basic/06-parameter-estimation.html#假设检验的两型错误检验效能",
    "title": "参数估计",
    "section": "\n3.2 假设检验的两型错误、检验效能",
    "text": "3.2 假设检验的两型错误、检验效能\n\n\n\n\n\n\n\n客观实际\n拒绝\\(H_{0}\\)，接受\\(H_{1}\\)\n\n不拒绝\\(H_{0}\\)\n\n\n\n\n\n\\(H_{0}\\)成立\n\n\\(\\textrm{I}\\)型错误(\\(\\alpha\\))(假阳性) 错误拒绝实际成立的\\(H_{0}\\)\n\n正确推断(\\(1-\\alpha\\))\n\n\n\n\\(H_{0}\\)不成立\n正确推断(\\(1-\\beta\\))\\(H_{1}\\)为真，能够拒绝\\(H_{0}\\)的概率称为发现该\\(H_{1}\\)的检验效能，用\\(1-\\beta\\)表示\n\n\\(\\textrm{II}\\)型错误(\\(\\beta\\))(假阴性)不拒绝实际不成立的\\(H_{0}\\)\n\n\n\n\n\n3.2.1 \\(1-\\beta\\)的影响因素：\n\n检验水准\\(\\alpha\\)（正向）——检验水准\\(\\alpha\\)越大，检验效能越大\n\n\\(H_{0}\\)与\\(H_{1}\\)的差异大小（正向）——差异越大，检验效能越大\n样本量（正向）——样本量越大，检验效能越大\n标准差越大（反向）——个体差异（标准差）越小，检验效能越大\n单双侧检验：单侧检验效能高于双侧检验效能\n\n3.2.2 \\(\\alpha 、\\beta 、1-\\beta\\)关系：\n\n当样本量确定时，\\(\\alpha\\)与\\(\\beta\\)呈反向变化关系，与\\(1-\\beta\\)呈正向变化关系。如果把\\(\\alpha\\)设置得很小，势必增加犯\\(\\textrm{II}\\)型错误的概率，从而降低检验效能；反之，如果把重点放在减少\\(\\beta\\)上，势必增加犯\\(\\textrm{I}\\)型错误的概率，从而降低了置信度。\n要同时减小\\(\\alpha\\)和\\(\\beta\\)，只有通过增加样本含量来计算。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#假设检验与置信区间的关系",
    "href": "Learn/Basic/06-parameter-estimation.html#假设检验与置信区间的关系",
    "title": "参数估计",
    "section": "\n3.3 假设检验与置信区间的关系",
    "text": "3.3 假设检验与置信区间的关系\n\n\n\n\n\n\n\n基本思想\n假设检验\n置信区间\n\n\n\n基本思想\n假设检验的假设是指我们对总体特征（如参数、分布）的某种推测，从而用概率来判断样本数据所提供的的信息和我们对总体特征猜想的一致性，进而结合专业知识判断这一猜想的正确性\n置信区间是指有样本统计量所构造的总体参数的估计区间，区间估计是按照一定的概率和可信度\\((1-\\alpha)\\)用一个区间估计总体参数所在的范围，这个范围称作可信度为\\((1-\\alpha)\\)的可信区间\n\n\n区别\n1. 假设检验用于推断总体参数之间是否不同\n1. 置信区间用于推断总体参数所在范围； 2.置信区间比假设检验提供更多的信息，置信区间能够回答假设检验的问题； 3. 置信区间在回答差别有无统计学意义时，还可以提示差别是否具有实际意义。\n\n\n联系\n1. 假设检验与置信区间都属于统计推断方法；2. 置信区间估计总体参数所采用的的统计量与假设检验的检验统计量相同；\n3.置信区间能够回答假设检验的问题。根据置信度\\(1-\\alpha\\)构造置信区间，如果统计量在置信区间内，那么不拒绝原假设；如果不在置信区间中，那么拒绝原假设；4. 双侧检验时，置信区间确定的\\(z'\\)与检验水准\\(\\alpha\\)确定的检验统计量的分布界值相同，因此，在双侧检验时\\(C=1-\\alpha\\)。根据显著水平\\(\\alpha\\)，可以构造置信度为\\(1-\\alpha\\)的置信区间",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/06-parameter-estimation.html#footnotes",
    "href": "Learn/Basic/06-parameter-estimation.html#footnotes",
    "title": "参数估计",
    "section": "脚注",
    "text": "脚注\n\n无偏方差:\\(S^2\\)作为样本方差，称之为无偏方差。样本方差是度量样本离散程度的统计量，其中n为样本量， \\(\\sum_{i=1}^{n}(x_i-\\bar x)^2\\)为偏差平方和，\\(n-1\\)称为偏差平方和的自由度，因为在\\(\\bar x\\)确定后，\\(x_i(i=1,2,\\dots,n)\\)中只有\\(n-1\\)个可以自由变动。↩︎",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数估计"
    ]
  },
  {
    "objectID": "Learn/Basic/04-discrete-type-random-variable.html",
    "href": "Learn/Basic/04-discrete-type-random-variable.html",
    "title": "离散型随机变量的概率分布",
    "section": "",
    "text": "type of data\n\n\n定义：\\(n\\)次伯努利试验，成功的次数为\\(X\\)的离散概率分布，其中每次试验的成功概率为\\(\\pi\\)，失败的概率为\\(1-\\pi\\)。\n\n\n\\(X\\)的总体均数\\(\\mu_{x}=n\\pi\\)\n\n总体方差\\(\\sigma_{x}=n\\pi(1-\\pi)\\)\n\n\nnotice：\n\n实际上，当\\(n=1\\)时，二项分布就是伯努利试验。\n伯努利试验要求：互斥、独立、重复\n\n\n\n\n\n\nBinomial Distribution with Different n/π\n\n\n\n\n定义：描述在单位面积、单位时间或单位空间中罕见事件发生次数的概率分布为泊松分布，记作\\(P(\\mu)\\)。泊松分布是二项分布的极限形式，当一个二项分布的\\(n\\)很大，\\(\\pi\\)很小时，此时，这个二项分布近似于泊松分布。\n\n其总体均数与总体方差相等，记为\\(\\mu\\)\n\n可加性：\\(X\\sim P(\\mu_{1})\\)，\\(Y\\sim P(\\mu_{2})\\)，若\\(X\\)与\\(Y\\) 独立，则\\(X+Y \\sim P(\\mu_{1}+\\mu_{1})\\)\n\n泊松分布只有一个参数\\(\\lambda(\\mu)\\)\n\n服从泊松分布的随机变量，其取值为\\(0\\)到\\(+\\infty\\)的概率之和为1\n一般来说，当\\(\\mu \\ge20\\)时，可以认为近似正态分布\n\n\nlibrary(ggplot2)\n# Define the range for x\nx &lt;- 0:40\n\n# Define the lambda values\nlambdas &lt;- c(1, 4, 10, 20)\n\n# Set up the plot area\nplot(x, dpois(x, lambdas[1]), type=\"n\", ylim=c(0, max(dpois(x, lambdas))), \n     xlab=\"x\", ylab=\"Probability\", main=\"Poisson Distribution with Different λ Values\")\n\n# Plot the Poisson distributions for each lambda\ncolors &lt;- c(\"blue\", \"green\", \"red\", \"purple\")\nfor (i in 1:length(lambdas)) {\n  lines(x, dpois(x, lambdas[i]), type=\"b\", pch=19, col=colors[i])\n}\n\n# Add a legend\nlegend(\"topright\", legend=paste(\"λ =\", lambdas), col=colors, pch=19)\n\n\n\n\n\n\n\n\n\n\n\nPoisson Distribution with Different λ=nπ\n\n\n\n\n\n统计描述角度：直接法计算概率 [ Pr(X=K)={k}(1-){n-k},k=0,1,2,3,,n ]\n统计推断角度：区间估计、假设检验\n\n\n统计描述角度：直接法计算概率 [ Pr(X=K)=,k=0,1,2,]\n统计推断角度：区间估计、假设检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "离散型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/04-discrete-type-random-variable.html#二项分布binomial-distribution",
    "href": "Learn/Basic/04-discrete-type-random-variable.html#二项分布binomial-distribution",
    "title": "离散型随机变量的概率分布",
    "section": "",
    "text": "定义：\\(n\\)次伯努利试验，成功的次数为\\(X\\)的离散概率分布，其中每次试验的成功概率为\\(\\pi\\)，失败的概率为\\(1-\\pi\\)。\n\n\n\\(X\\)的总体均数\\(\\mu_{x}=n\\pi\\)\n\n总体方差\\(\\sigma_{x}=n\\pi(1-\\pi)\\)\n\n\nnotice：\n\n实际上，当\\(n=1\\)时，二项分布就是伯努利试验。\n伯努利试验要求：互斥、独立、重复\n\n\n\n\n\n\nBinomial Distribution with Different n/π",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "离散型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/04-discrete-type-random-variable.html#泊松分布poission-distribution",
    "href": "Learn/Basic/04-discrete-type-random-variable.html#泊松分布poission-distribution",
    "title": "离散型随机变量的概率分布",
    "section": "",
    "text": "定义：描述在单位面积、单位时间或单位空间中罕见事件发生次数的概率分布为泊松分布，记作\\(P(\\mu)\\)。泊松分布是二项分布的极限形式，当一个二项分布的\\(n\\)很大，\\(\\pi\\)很小时，此时，这个二项分布近似于泊松分布。\n\n其总体均数与总体方差相等，记为\\(\\mu\\)\n\n可加性：\\(X\\sim P(\\mu_{1})\\)，\\(Y\\sim P(\\mu_{2})\\)，若\\(X\\)与\\(Y\\) 独立，则\\(X+Y \\sim P(\\mu_{1}+\\mu_{1})\\)\n\n泊松分布只有一个参数\\(\\lambda(\\mu)\\)\n\n服从泊松分布的随机变量，其取值为\\(0\\)到\\(+\\infty\\)的概率之和为1\n一般来说，当\\(\\mu \\ge20\\)时，可以认为近似正态分布\n\n\nlibrary(ggplot2)\n# Define the range for x\nx &lt;- 0:40\n\n# Define the lambda values\nlambdas &lt;- c(1, 4, 10, 20)\n\n# Set up the plot area\nplot(x, dpois(x, lambdas[1]), type=\"n\", ylim=c(0, max(dpois(x, lambdas))), \n     xlab=\"x\", ylab=\"Probability\", main=\"Poisson Distribution with Different λ Values\")\n\n# Plot the Poisson distributions for each lambda\ncolors &lt;- c(\"blue\", \"green\", \"red\", \"purple\")\nfor (i in 1:length(lambdas)) {\n  lines(x, dpois(x, lambdas[i]), type=\"b\", pch=19, col=colors[i])\n}\n\n# Add a legend\nlegend(\"topright\", legend=paste(\"λ =\", lambdas), col=colors, pch=19)\n\n\n\n\n\n\n\n\n\n\n\nPoisson Distribution with Different λ=nπ",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "离散型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/04-discrete-type-random-variable.html#二项分布的应用",
    "href": "Learn/Basic/04-discrete-type-random-variable.html#二项分布的应用",
    "title": "离散型随机变量的概率分布",
    "section": "",
    "text": "统计描述角度：直接法计算概率 [ Pr(X=K)={k}(1-){n-k},k=0,1,2,3,,n ]\n统计推断角度：区间估计、假设检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "离散型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/04-discrete-type-random-variable.html#泊松分布的应用",
    "href": "Learn/Basic/04-discrete-type-random-variable.html#泊松分布的应用",
    "title": "离散型随机变量的概率分布",
    "section": "",
    "text": "统计描述角度：直接法计算概率 [ Pr(X=K)=,k=0,1,2,]\n统计推断角度：区间估计、假设检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "离散型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html",
    "href": "Learn/Basic/02-descriptive-statistics.html",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "指标\n定义→本质\n表示方法\n计算方法\n应用条件\n\n\n\n\n算术均数\n先求和再平均\n(1)样本均数:\\(\\bar x\\)  (2)总体均数:\\(\\mu\\)\n(1)直接法：\\(\\bar X=\\frac{\\sum{X_{i}}}{n}\\)  (2)加权法：\\(\\bar X = \\frac{\\sum{f_{i}X_{i}}}{\\sum{f}}\\)，（\\(X\\)为组中值，\\(f\\)为频数）\n(1)对称分布，尤其是正态分布  (2)不含极端值\n\n\n几何均数\n先乘积再开方\n\\(G\\)\n(1)直接法：\\(G=\\sqrt[n]{x_{1}·x_{2}·x_{3}\\cdots}x_{n}\\)  (2)加权法：\\(G=\\ln^{-1}(\\frac{\\sum{f_{i}\\ln X_{i}}}{\\sum{f_{i}}})\\)\n(1)数据呈倍数变化或对数正态分布→正偏态分布  (2)观察值中不能有零且不能同时有正数和负数→对数性质\n\n\n中位数\n从小到大找中间  还要注意奇偶性\n\\(M\\)\n(1)直接法：n为奇数，\\(M=X_{\\frac{n+1}{x}}\\)  n为偶数，\\(M=\\frac{1}{2} (X_{\\frac{n}{2}}+X_{\\frac{n}{2}+1})\\)  (2)加权法：百分位数法\\(P_{X}=L_{X}+\\frac{i}{f_{x}}(nX\\%-\\sum{f_{i}})\\)\n任何资料\n\n\n众数\n出现次数最多\n\\\n(1)直接法：一组数据中出现次数最多的数值  加权法：f最多的组段的组中值\\(X\\)\n任何数据\n\n\n\n\n\n\n\n对称分布：算术均数\\(\\approx\\)中位数\n右偏态：算术均数\\(&gt;\\)中位数\n左偏态：算术均数\\(&lt;\\)中位数\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n指标\n本质→定义\n表示方式\n计算公式\n适用条件\n\n\n\n\n极差\n\\(X_{Max}-X_{Min}\\)\n\\(R\\)\n\n\n\n\n四分位间距/范围\n位置指标\n\\(IQR\\)\n\\(IQR=P_{75}-P_{25}\\)\n任何资料\n\n\n方差\n离均差平方和求平均\n样本：\\(s^{2}\\)  总体：\\(\\sigma^{2}\\)\n\\(s^{2}=\\frac{\\sum(x_{i}-x)^2}{(n-1)}\\)\n对称分布，尤其是正态分布;不含极端值\n\n\n标准差\n方差开根号\n样本：\\(s\\)  总体：\\(\\sigma\\)\n\\(s=\\sqrt{\\frac{\\sum(x_{i}-x)^2}{(n-1)}}\\)\n同上\n\n\n变异系数\n测量数据变异程度的相对统计量\n\\(CV\\)\n\\(CV=\\frac{s}{\\bar x}×100%\\)\n(1)单位相同：但均数相差悬殊；(2)单位不同\n\n\n\n\n\n\n\n直接法 \\[s=\\sqrt{\\frac{\\sum\\limits_{i=1}^nx_i^2-\\frac{\\left(\\sum\\limits_{i=1}^nx_i\\right)^2}{n}}{n-1}}\\]\n加权法：与讨算均数的方法类似，对频数表资料采用加权法，讨算公式为\n\n\\[s=\\sqrt{\\frac{\\sum\\limits_{k=1}^gf_kx_{mk}^2-\\left(\\sum\\limits_{k=1}^gf_kx_{mk}\\right)^2 \\left(\\sum\\limits_{k=1}^gf_k\\right)}{\\sum\\limits_{k=1}^gf_k-1}}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n指标\n相对比\n构成比\n频率型指标\n强度性指标\n\n\n\n\n定义\n两个有关联的指标之比\n某一部分与总体之比\n某时期内累计出现的频率\n单位时间内某件事发生的频率\n\n\n计算公式\n\\(\\frac{A指标}{B指标}\\)\n\\(\\frac{某一事物总体中某一部分}{某一事物所有组成部分的总体}×100\\%\\)\n\\(\\frac{同时期实际发生某现象的观察单位数}{某时期可能发生某现象的观察单位总数}×K\\)\n\\(\\frac{发生某件事的观察单位数}{\\sum(观察单位×观察时间)}×K\\)\n\n\n量纲\n可有可无\n一般无量纲\n无\n有\n\n\n取值\n没有限制\n[0,1]\n[0,1]\n可大于1\n\n\n举例\nRR,变异系数CV\n死因构成比\n病死率，累计发病率\n发病率，发病密度\n\n\n\n\n\n\n\n绝对量指标\n\n累计增长量\n逐年增长量\n\n定基类指标\n\n定基发展速度\n定基增长速度\n\n环比类指标\n\n环比发展速度\n环比增长速度\n\n平均类指标\n\n平均发展速度\n平均增长速度\n\n\n\n\n\n\n频率型指标的解释要紧扣总体和属性\n计算相对数分母应该有足够的观察单位数 -如果观察例数太少，则相对数波动较大\n\n若因实际因素，观察例数确实过少，建议直接采用绝对数\n\n正确计算合计率：分子分母分别相加，再求合计率\n不能用结构相对数代替强度相对数，不能混淆频率型指标和强度型指标，不能以比代率\n注意资料的可比性\n不能仅用样本率比较，因为样本和总体之间存在抽样误差，需要进行假设检验推断总体的情况\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n指标\n参照人数\n目标人群\n过程\n\n\n\n\n直接标化法\n人口构成\n率\n各年龄组期望死亡数→期望死亡合计数→直接标化率\n\n\n简介标化法\n率\n人口构成\n各年龄组期望死亡数→期望死亡合计数→变化死亡比→间接标化率\n\n\n\nnotice：\n\n变化标化比\\(SMR=\\frac{实际死亡人数}{期望死亡人数}\\)\n直接标准化选择的标准是：各年龄组标准人口构成比或各年龄组标准人口数\n\n\n\n\n\n\n\n连续型变量\n\n计量资料\n定量资料\n\n离散型变量\n\n不具有分类性质的资料\n离散型定量资料\n\n分类资料\n\n有序分类资料\n等级资料\n半定量资料\n无序分类资料\n名义变量\n\n\nnotice：一般来说，统计图的选择，是综合考量了变量取值特点+研究目的。\n\n\n\n\n\n\n\n\n\n\n\n统计图\n资料类型\n分析目的\n\n\n\n\n圆图和百分条图\n构成比资料\n用圆的扇形面积或直条各段的长度表示事物各组成部分的构成情况\n\n\n直条图\n相互独立资料\n用直条长短表示相互独立的各指标的数值大小，一般用于比较不同组别的指标大小\n\n\n直方图\n连续型变量的频数分布\n用矩阵面积表示各组段的频数（频率）\n\n\n箱式图\n连续型资料\n描述数据的分布特征（包括中位数、四分位范围、最大值和最小值）\n\n\n普通线图\n连续型资料\n用线段的升降表示某事物在时间上的变化趋势、或某一现象随着另一现象变化的情况\n\n\n半对数线图\n连续型资料\n用线段的升降表示事物的相对变化速度\n\n\n散点图\n双变量连续型资料\n表示两种事物变化的相关性和趋势\n\n\n\n\n\n\n\n\n\nChoice of Statistical Charts",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#定量资料的描述指标",
    "href": "Learn/Basic/02-descriptive-statistics.html#定量资料的描述指标",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "指标\n定义→本质\n表示方法\n计算方法\n应用条件\n\n\n\n\n算术均数\n先求和再平均\n(1)样本均数:\\(\\bar x\\)  (2)总体均数:\\(\\mu\\)\n(1)直接法：\\(\\bar X=\\frac{\\sum{X_{i}}}{n}\\)  (2)加权法：\\(\\bar X = \\frac{\\sum{f_{i}X_{i}}}{\\sum{f}}\\)，（\\(X\\)为组中值，\\(f\\)为频数）\n(1)对称分布，尤其是正态分布  (2)不含极端值\n\n\n几何均数\n先乘积再开方\n\\(G\\)\n(1)直接法：\\(G=\\sqrt[n]{x_{1}·x_{2}·x_{3}\\cdots}x_{n}\\)  (2)加权法：\\(G=\\ln^{-1}(\\frac{\\sum{f_{i}\\ln X_{i}}}{\\sum{f_{i}}})\\)\n(1)数据呈倍数变化或对数正态分布→正偏态分布  (2)观察值中不能有零且不能同时有正数和负数→对数性质\n\n\n中位数\n从小到大找中间  还要注意奇偶性\n\\(M\\)\n(1)直接法：n为奇数，\\(M=X_{\\frac{n+1}{x}}\\)  n为偶数，\\(M=\\frac{1}{2} (X_{\\frac{n}{2}}+X_{\\frac{n}{2}+1})\\)  (2)加权法：百分位数法\\(P_{X}=L_{X}+\\frac{i}{f_{x}}(nX\\%-\\sum{f_{i}})\\)\n任何资料\n\n\n众数\n出现次数最多\n\\\n(1)直接法：一组数据中出现次数最多的数值  加权法：f最多的组段的组中值\\(X\\)\n任何数据\n\n\n\n\n\n\n\n对称分布：算术均数\\(\\approx\\)中位数\n右偏态：算术均数\\(&gt;\\)中位数\n左偏态：算术均数\\(&lt;\\)中位数",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#离散数据的描述指标",
    "href": "Learn/Basic/02-descriptive-statistics.html#离散数据的描述指标",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "指标\n本质→定义\n表示方式\n计算公式\n适用条件\n\n\n\n\n极差\n\\(X_{Max}-X_{Min}\\)\n\\(R\\)\n\n\n\n\n四分位间距/范围\n位置指标\n\\(IQR\\)\n\\(IQR=P_{75}-P_{25}\\)\n任何资料\n\n\n方差\n离均差平方和求平均\n样本：\\(s^{2}\\)  总体：\\(\\sigma^{2}\\)\n\\(s^{2}=\\frac{\\sum(x_{i}-x)^2}{(n-1)}\\)\n对称分布，尤其是正态分布;不含极端值\n\n\n标准差\n方差开根号\n样本：\\(s\\)  总体：\\(\\sigma\\)\n\\(s=\\sqrt{\\frac{\\sum(x_{i}-x)^2}{(n-1)}}\\)\n同上\n\n\n变异系数\n测量数据变异程度的相对统计量\n\\(CV\\)\n\\(CV=\\frac{s}{\\bar x}×100%\\)\n(1)单位相同：但均数相差悬殊；(2)单位不同\n\n\n\n\n\n\n\n直接法 \\[s=\\sqrt{\\frac{\\sum\\limits_{i=1}^nx_i^2-\\frac{\\left(\\sum\\limits_{i=1}^nx_i\\right)^2}{n}}{n-1}}\\]\n加权法：与讨算均数的方法类似，对频数表资料采用加权法，讨算公式为\n\n\\[s=\\sqrt{\\frac{\\sum\\limits_{k=1}^gf_kx_{mk}^2-\\left(\\sum\\limits_{k=1}^gf_kx_{mk}\\right)^2 \\left(\\sum\\limits_{k=1}^gf_k\\right)}{\\sum\\limits_{k=1}^gf_k-1}}\\]",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#分类资料的描述指标",
    "href": "Learn/Basic/02-descriptive-statistics.html#分类资料的描述指标",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "指标\n相对比\n构成比\n频率型指标\n强度性指标\n\n\n\n\n定义\n两个有关联的指标之比\n某一部分与总体之比\n某时期内累计出现的频率\n单位时间内某件事发生的频率\n\n\n计算公式\n\\(\\frac{A指标}{B指标}\\)\n\\(\\frac{某一事物总体中某一部分}{某一事物所有组成部分的总体}×100\\%\\)\n\\(\\frac{同时期实际发生某现象的观察单位数}{某时期可能发生某现象的观察单位总数}×K\\)\n\\(\\frac{发生某件事的观察单位数}{\\sum(观察单位×观察时间)}×K\\)\n\n\n量纲\n可有可无\n一般无量纲\n无\n有\n\n\n取值\n没有限制\n[0,1]\n[0,1]\n可大于1\n\n\n举例\nRR,变异系数CV\n死因构成比\n病死率，累计发病率\n发病率，发病密度\n\n\n\n\n\n\n\n绝对量指标\n\n累计增长量\n逐年增长量\n\n定基类指标\n\n定基发展速度\n定基增长速度\n\n环比类指标\n\n环比发展速度\n环比增长速度\n\n平均类指标\n\n平均发展速度\n平均增长速度\n\n\n\n\n\n\n频率型指标的解释要紧扣总体和属性\n计算相对数分母应该有足够的观察单位数 -如果观察例数太少，则相对数波动较大\n\n若因实际因素，观察例数确实过少，建议直接采用绝对数\n\n正确计算合计率：分子分母分别相加，再求合计率\n不能用结构相对数代替强度相对数，不能混淆频率型指标和强度型指标，不能以比代率\n注意资料的可比性\n不能仅用样本率比较，因为样本和总体之间存在抽样误差，需要进行假设检验推断总体的情况",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#率的标准化",
    "href": "Learn/Basic/02-descriptive-statistics.html#率的标准化",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "指标\n参照人数\n目标人群\n过程\n\n\n\n\n直接标化法\n人口构成\n率\n各年龄组期望死亡数→期望死亡合计数→直接标化率\n\n\n简介标化法\n率\n人口构成\n各年龄组期望死亡数→期望死亡合计数→变化死亡比→间接标化率\n\n\n\nnotice：\n\n变化标化比\\(SMR=\\frac{实际死亡人数}{期望死亡人数}\\)\n直接标准化选择的标准是：各年龄组标准人口构成比或各年龄组标准人口数",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#常见统计图",
    "href": "Learn/Basic/02-descriptive-statistics.html#常见统计图",
    "title": "不同资料的统计描述",
    "section": "",
    "text": "连续型变量\n\n计量资料\n定量资料\n\n离散型变量\n\n不具有分类性质的资料\n离散型定量资料\n\n分类资料\n\n有序分类资料\n等级资料\n半定量资料\n无序分类资料\n名义变量\n\n\nnotice：一般来说，统计图的选择，是综合考量了变量取值特点+研究目的。\n\n\n\n\n\n\n\n\n\n\n\n统计图\n资料类型\n分析目的\n\n\n\n\n圆图和百分条图\n构成比资料\n用圆的扇形面积或直条各段的长度表示事物各组成部分的构成情况\n\n\n直条图\n相互独立资料\n用直条长短表示相互独立的各指标的数值大小，一般用于比较不同组别的指标大小\n\n\n直方图\n连续型变量的频数分布\n用矩阵面积表示各组段的频数（频率）\n\n\n箱式图\n连续型资料\n描述数据的分布特征（包括中位数、四分位范围、最大值和最小值）\n\n\n普通线图\n连续型资料\n用线段的升降表示某事物在时间上的变化趋势、或某一现象随着另一现象变化的情况\n\n\n半对数线图\n连续型资料\n用线段的升降表示事物的相对变化速度\n\n\n散点图\n双变量连续型资料\n表示两种事物变化的相关性和趋势\n\n\n\n\n\n\n\n\n\nChoice of Statistical Charts",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#常见的概率抽样",
    "href": "Learn/Basic/02-descriptive-statistics.html#常见的概率抽样",
    "title": "不同资料的统计描述",
    "section": "2.1 常见的概率抽样",
    "text": "2.1 常见的概率抽样\n\n\n\n\n\n\n\n\n\n\n\n类别\n简单随机抽样\n系统抽样\n整群抽样\n分层抽样\n多阶段抽样\n\n\n\n\n概念\n将全部的观察单位编号，形成抽样框，在抽样框中随机抽取部分观察单位组成样本\n先将总体的观察单位按照某一顺序分成n个部分，再从第一部分随机抽取第k号观察单位，依次用相等间隔，从每一部分各抽取一个观察单位组成样本\n是以“群”为基本单位的抽样方法，先将总体分成若干群，从中随机抽取一些群，被抽中群内的全部个体组成调查的样本\n先将总体中全部个体按某种特征分成若干“层”，再从每一层内随机抽取一定数量的个体组成样本\n将整个抽样过程分成若干阶段进行，在初级抽样单位中抽取二级抽样单位，又在二级抽样单位中抽取三级抽样单位\n\n\n优点\n简单直观；均数（率）及其标准误计算简便\n易于理解、简便易行；可得到按比例分配的样本；样本在总体中的分布均匀\n便于组织调查；节约成本；容易控制调查质量\n抽样误差相对较小；可对不同层采用不同的抽样方法；可对不同层进行独立分析\n充分利用各种抽样方法的优势，克服各自的不足，并能节省人力、物力\n\n\n缺点\n观察单位较多，编号在实际工作中难以实现；当总体变异大时，抽样误差较分层抽样误差大\n观察单位按顺序有周期趋势或递增（减）时易产生偏差\n样本例数一定时，抽样误差大于简单随机抽样（因样本为广泛散布于总体中\n若分层变量选择不当，层内变异较大，层间变异较小，则分层抽样失去意义\n在抽样之前要掌握各级调查单位的人口资料及特点\n\n\n适用范围\n是其他抽样方法的基础，主要用于总体不太大的情形\n主要用于按抽样顺序个体随机分布的情形\n主要用于群间差异较小的情形\n主要用于层间差异较大的情形\n大型流行病学调查\n\n\n\n误差大小： 整群抽样&gt;简单随机抽样&gt;系统抽样&gt;分层抽样\n样本量大小：整群抽样&gt;简单随机抽样&gt;系统抽样&gt;分层抽样\n概率抽样：是指每个个体被抽样抽中的概率是非零的、已知的或可计算的。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/02-descriptive-statistics.html#常见的非概率抽样",
    "href": "Learn/Basic/02-descriptive-statistics.html#常见的非概率抽样",
    "title": "不同资料的统计描述",
    "section": "2.2 常见的非概率抽样",
    "text": "2.2 常见的非概率抽样\n\n特点\n\n不需要考虑等概率原则\n依赖研究人员的经验和专业知识\n简便易行、节约资源\n结果的稳定性容易受主观影响\n\n\n\n\n\n\n\n\n\n类别\n概念\n\n\n\n\n偶遇抽样\n又称便利抽样，指研究者根据实际情况而采用最便利的方法来选取样本，可以抽取偶然遇到的人，或选择那些距离最近的、最容易找到的人作为调查对象\n\n\n目的抽样\n又称判断抽样，指研究者根据研究目标和对情况的主观判断来选择和确定调查对象的方法，是“有目的”地去选择对总体具有代表性的样本\n\n\n滚雪球抽样\n又称链式抽样或网络抽样，指当无法了解总体情形时，可以从能找到的少数个体入手，对他们进行调查，并请他们介绍其他符合条件的人，扩大调查面，如此重复下去直到达到所需的样本量\n\n\n定额抽样\n又称配额抽样，是按照总体的某种特征（年龄、性别、社会阶层等）进行分层（组），然后在每一层（组）中按照事先规定的比例或数量（即定额）用便利抽样或目的抽样的方法选取样本\n\n\n空间抽样\n指对具有空间关联性的各种调查对象及资源进行抽样的一种方法\n\n\n\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "不同资料的统计描述"
    ]
  },
  {
    "objectID": "Learn/Basic/00-basic.html",
    "href": "Learn/Basic/00-basic.html",
    "title": "医学统计学基础",
    "section": "",
    "text": "本章主要介绍基本的医学统计学内容，主要参照内容为：人卫第八版《卫生统计学》、科社第二版《医学统计学（基础版）》。\n\n章节主要分布如下：\n\n绪论\n\n医学统计学\n统计学的基本概念\n统计学在医学科研中的基本步骤\n统计学与相关学科的关系\n\n描述性统计\n随机事件与概率\n离散型随机变量\n连续型随机变量\n参数估计\n一个正态总体参数的假设检验\n两个正态总体参数的假设检验\n单因素方差分析\n多因素方差分析\n\\(\\chi^2\\)检验\n基于秩次的非参数检验\n一元线性回归\n相关分析\n\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-intro.html",
    "href": "Guide/SAS/SAS-intro.html",
    "title": "SAS",
    "section": "",
    "text": "SAS introduction\nSAS的历史很长，很强大，但是现代化做的一般（交互界面）。\n使用场景也是有限的，至少一般情况下用不上这么高级的工具。\n但是在某些领域又是极其重要的，像银行和药企，他们要追求足够的稳定和严谨，那么多年不曾有重大改变且一向以稳定著称的 SAS 自然可以很好的满足这一需求。\n\nSAS 对学术研究的支持是比较不友好的。正版太贵，除非学校有提供，个人基本不可能使用正版，这里下载破解版，搞SID(SAS的授权证书)需要时间成本，还容易有安装问题，没错你可以选择使用SAS的教育版，不过谁用谁知道。\n安装比较麻烦，尤其是在Linux上，我曾用两周的时间折腾在 Linux 上安装一个 SAS ，最后以失败告终，且在互联网上找不到解决的方案，AI也束手无策。\nSAS不开源，意味着你看到某些论文，里面使用一些比较新的统计分析方法，SAS不大可能有现成代码可以使用，而 Python 和 R 则大几率有现成的包可以调用，这里也会节省不少时间。\nSAS的强大一方面是性能稳定，可以处理几十上百GB的数据而不容易崩溃，但是医学数据一般容量比较小，并不是非得SAS才能跑的动。\nSAS相比其他编程语言来说是独树一帜(奇葩)的存在（proc和data步独步天下），从语法上面来说并没有什么和它相接近的语言，相反 R 和 Python 则会和一般的编程语言例如 Java, C 等有一些类似的地方，对以后万一还需要学习其他语言或者学习以后新诞生的编程软件诞有一定帮助。\nSAS 的支持有限，互联网上关于 SAS 的使用信息较少，一般都在出版的书中有可复现的内容，也没有像 Python 和 R 等活跃的社区可以提供较多的互动和支持，编程遇上问题不容易找到答案。\n\n用肯定能用，但是在使用中占多大的比重，就需要权衡一下，在 AI 时代，不一定要全部掌握，看得懂，知道怎么做，应该也可以了，当然如果要深入，那就另说。\n每个人的资源和时间都是有限的，用最少的资源和时间做最多的事才是最重要的。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "SAS"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html",
    "href": "Guide/SAS/2025-02-16-CLHLS.html",
    "title": "使用SAS处理CLHLS数据",
    "section": "",
    "text": "“中国老年健康影响因素跟踪调查”（简称“中国老年健康调查”，英文缩写CLHLS），以下简称CLHLS数据，是由北京大学健康老龄与发展研究中心/国家发展研究院组织的老年人追踪调查，调查范围覆盖全国23个省市自治区，调查对象为65岁及以上老年人和35-64岁成年子女，调查问卷分为存活被访者问卷和死亡老人家属问卷两种。存活被访者问卷的调查内容包括老人及家庭基本状况、社会经济背景及家庭结构、经济来源和经济状况、健康和生活质量自评、认知功能、性格心理特征、日常活动能力、生活方式、生活照料、疾病治疗和医疗费承担；死亡老人家属问卷的调查内容包括老人死亡时间、死因等内容。该调查项目在1998年进行基线调查后分别于2000 年、2002年、2005年、2008-2009年、2011-2012年、2014年和2017-2018年进行了跟踪调查，最近的一次跟踪调查(2017-2018年)共访问15,874 名65+岁老年人,收集了2014-2018年期间死亡的2,226位老年人的信息。“中国老年健康调查”累计入户访问11.3万人次，其中最需照料的80岁及以上高龄老人占总样本67.4%，其余为较低龄老人和中年对照组；同时访问2.89万位65+岁已死亡被访老人的直接家庭成员，收集了老人死亡前健康状况、生活质量与医疗和照料需求成本等详细数据。CLHLS数据共有15874例样本，761个变量，由于本文主要研究老年人的自评和客观健康水平以及代际支持模式对健康水平的影响，因此需要对样本和变量进行筛选，保留符合要求的样本和变量进行后续的分析。DVN/WBO7LK_2020",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html#因变量的选取",
    "href": "Guide/SAS/2025-02-16-CLHLS.html#因变量的选取",
    "title": "使用SAS处理CLHLS数据",
    "section": "2.1 因变量的选取",
    "text": "2.1 因变量的选取\n老年人自评健康状况（SHEALTH）：对应变量 b12 （self-reported health），为有序变量（Ordinal），可用于衡量老年人对自身健康的主观评价，其赋值为：很好=1，好=2，一般=3，不好=4，很不好=5。\n生活自理能力（ADL）：对应变量 e1 （bathing，洗澡）、e2 （dressing，穿衣）、e3 （toileting，如厕）、e4 （indoor transferring，室内移动）、e5 （continence，大小便控制）、e6 （feeding，进食），均为有序变量（Ordinal）。通过这些变量的得分计算可体现老年人的生活自理能力。\n变量 e1b 、e2b 、e3b 、e4b 、e5b 、e6b 分别记录了在需要他人帮助情况下，这些活动的帮助持续天数，可作为辅助信息进一步分析生活自理能力情况。其变量赋值为：不需要帮扶=1，需要一个帮扶=2，需要多个帮扶=3，将其合并为ADL变量，相加值越低则表示生活自理能力越好，反之越差。\n工具型生活自理能力（IADL）：对应变量 e7 （able to go outside to visit neighbors，能外出拜访邻居）、e8 （able to go shopping by yourself，能自己去购物）、e9 （able to make food by yourself，能自己做饭）、e10 （able to wash clothes，能洗衣服）、e11 （able to walk one kilometer，能步行一公里）、e12 （able to carry 5kg weight，能搬运 5 公斤重物）、e13 （able to crouch and stand three times，能蹲下并站立三次）、e14 （able to take public transportation，能乘坐公共交通工具），均为有序变量（Ordinal）。这些变量可综合反映老年人在更复杂日常生活任务上的能力。其变量赋值为：可以=1，些微困难=2，不能=3，将其合并为IADL变量，相加值越低则表示工具型生活自理能力越好，反之越差。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html#自变量的选取",
    "href": "Guide/SAS/2025-02-16-CLHLS.html#自变量的选取",
    "title": "使用SAS处理CLHLS数据",
    "section": "2.2 自变量的选取",
    "text": "2.2 自变量的选取\n经济支持：新设变量为 economic-support，对应的原始变量 f12a （how much did you receive from your son (s) or daughter (s)-in-law last year，去年从儿子或儿媳处收到多少钱）、f12b （how much did you receive from your daughter (s) or son (s)-in-law last year，去年从女儿或女婿处收到多少钱）、f12c （how much did you receive from your grandchild (ren) last year，去年从孙辈处收到多少钱），均为尺度变量（Scale），其中给与物质支持=99998，不知道=88888，缺失=99999，通过统计99998的个数来估计经济支持的程度，如果没有99998则赋值为0,。通过这些变量可衡量子女及孙辈在物质上给予老年人的经济支持。\n照料支持：新设变量为：residence，对应原始变量a51（co-residence of interviewee，受访者居住情况），有人同住=1，独居=2，住在机构=3；新设变量为living，对应变量 a52（how many people are living with you，有多少人居住在一起），数值型变量；新设变量 visit-fren，对应细分变量为f103a5 （frequent visits of the 1st child，第一个孩子的探访频率）、f103b5 （frequent visits of the 2nd child，第二个孩子的探访频率）、f103c5 （frequent visits of the 3rd child，第三个孩子的探访频率）等（一直到 f103k5 等关于各个孩子的探访频率变量），均为有序变量（Ordinal），有探访=1，没有=2，任一一个孩子=1即可认为有探访，同时给新变量赋值为1，如果没有则给新变量赋值为0。\n设置care-support变量，如果residence=1或visit-fren=1则给care-support赋值为1，否则赋值为0。\n用这些变量以老年人与子女相处和见面的频率来间接反映子女对老人的照料支持情况。\n情感支持（emotion support）：对应细分变量 f103a6 （contact with the 1st child，与第一个孩子的联系情况）、f103b6 （contact with the 2nd child，与第二个孩子的联系情况）、f103c6 （contact with the 3rd child，与第三个孩子的联系情况）等（一直到 f103k6 等关于各个孩子的联系情况变量），均为名义变量（Nominal）。根据这些变量所反映的与子女联系的情况来体现子女对老人的情感关怀。有联系=1，没联系=2，任一一个为1即可说明有孩子联系。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html#控制变量的选取",
    "href": "Guide/SAS/2025-02-16-CLHLS.html#控制变量的选取",
    "title": "使用SAS处理CLHLS数据",
    "section": "2.3 控制变量的选取",
    "text": "2.3 控制变量的选取\n年龄：问卷中的 trueage “真实年龄”，用于获取老年人的实际年龄。\n性别：问卷中 a1 “性别”，男性赋值为 1，女性赋值为 2。\n受教育程度：问卷中的 f1 “受教育年限”，数值型变量，但是其中如果赋值为88或99则表示“不知道”或“缺失”。\n退休前的工作类型：问卷中 f2 “60 岁之前的主要职业”，名义变量，专业技术人员=0，政府、机构或管理人员=1，商业、服务或工业工人=2，自雇人士=3，农、林、牧或渔业工人=4，家政工人=5，军事人员=6，从未工作过=7，其他=8。\n婚姻状况：问卷中 f41 “您目前的婚姻状态是？”，将 “已婚或与伴侣同居” 赋值为 1，其他赋值为 0。\n户口类型：问卷中的 hukou “hukou type of the elderly being visited，被访老人的户口类型”， “城镇” 赋值为 1，“农村” 赋值为 2。\n社保和养老保险：问卷中的 nf64a f64b f64c（do you have any social security and social insurance now? 你是否有任何社保和社会保险？），有任何一种社保或养老保险则赋值为 1，没有赋值为 0；。\n医疗保险：问卷中f64d f64e f64f f64h(是否有医疗保险)，有任何一种医疗保险则赋值为 1，没有赋值为 0。\n子女年龄：通过对问卷中的 f103a4 f103b4 到 f103m4 来检查该老人的子女的年龄，如果最后一个还活着的孩子年龄&gt;60，赋值为1，则表示该老人的子女也是老年状态，否则为0。\n慢性病：问卷中 g15a1 g15b1 一直到 g15x1 表示是否患有某种慢性病，有任何一种慢性病赋值为 1，否则为 0。\n抽烟：问卷中 g151 “你24小时内抽烟吗？”，是赋值为 1，否赋值为 2。\n喝酒：问卷中 g161 “你24小时内饮酒吗？”，是赋值为 1，否赋值为 2。\n体育锻炼：问卷中d91 d92 分别表示“目前是否锻炼？”和“过去是否锻炼过？”，是赋值为 1，否赋值为 2。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html#纳入标准",
    "href": "Guide/SAS/2025-02-16-CLHLS.html#纳入标准",
    "title": "使用SAS处理CLHLS数据",
    "section": "3.1 纳入标准",
    "text": "3.1 纳入标准\n年龄要求：年龄大于 60 岁，通过实际年龄 &gt; 60进行筛选。60 岁以上人群处于老年阶段，其健康、社会角色和代际关系有老年群体特征，符合本研究对老年人的研究范围。\n生育情况：生育过子女，通过反映生育子女数量的变量（如 f10 生育子女数），要求f10 &gt; 0。代际支持模式研究需存在代际关系，生育子女是形成代际关系的前提。\n子女存活情况：所生育子女中至少有一个存活。可通过检查如 f103a3 - f103m3 等表示孩子存活情况的变量，只要有一个值为表示存活的标识即可纳入（原始数据的变量赋值为存活=1，去世=2）。有在世子女才能开展代际支持研究。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/SAS/2025-02-16-CLHLS.html#排除标准",
    "href": "Guide/SAS/2025-02-16-CLHLS.html#排除标准",
    "title": "使用SAS处理CLHLS数据",
    "section": "3.2 排除标准",
    "text": "3.2 排除标准\n关键变量缺失值：排除关键变量存在缺失值和无效值的样本。关键变量涵盖上述控制变量及后续分析的因变量、自变量等。缺失值或无效值会影响数据质量和分析结果准确性。\n不合理生育数量：生育子女数量超过合理上限（结合研究背景和数据确定，上限为 7）或者为缺失值，则需要排除（f10&gt;7 or f10 = null）。不合理生育数量可能是数据录入错误或特殊情况，干扰研究结果。\n通过多次尝试，以下是用于筛选的SAS代码：\n#| eval: false\n/* 导入必要的库 */\nproc import \n    datafile = 'C:\\Users\\asus\\Desktop\\test\\CLHLS\\Analysis-0214\\clhls_2018_15874.sav'  /* 请将这里替换为你的SAV文件的实际路径 */\n    out = raw_data \n    dbms = sav \n    replace;\nrun;\n\n/* 筛选合适的变量并排序 */\ndata selected_data;\n    set raw_data;\n    \n    /* 因变量 */\n    /* 老年人自评健康状况（SHEALTH） */\n    SHEALTH = b12;\n    \n    /* 生活自理能力（ADL） */\n    array adl_vars(*) e1 e2 e3 e4 e5 e6;\n    array adl_help_days(*) e1b e2b e3b e4b e5b e6b;\n    ADL = 0;\n    do i = 1 to dim(adl_vars);\n        if adl_help_days[i] = 1 then ADL = ADL + adl_vars[i];\n        else if adl_help_days[i] = 2 then ADL = ADL + adl_vars[i] * 2;\n        else if adl_help_days[i] = 3 then ADL = ADL + adl_vars[i] * 3;\n    end;\n    \n    /* 新增：生活自理障碍二分类变量 ADL2 */\n    ADL2 = 0;\n    do i = 1 to dim(adl_vars);\n        if adl_vars[i] &gt; 1 then do;\n            ADL2 = 1;\n            leave;\n        end;\n    end;\n    \n    /* 工具型生活自理能力（IADL） */\n    array iadl_vars(*) e7 e8 e9 e10 e11 e12 e13 e14;\n    IADL = 0;\n    do i = 1 to dim(iadl_vars);\n        IADL = IADL + iadl_vars[i];\n    end;\n    \n    /* 新增：工具型生活自理障碍二分类变量 IADL2 */\n    IADL2 = 0;\n    do i = 1 to dim(iadl_vars);\n        if iadl_vars[i] &gt; 1 then do;\n            IADL2 = 1;\n            leave;\n        end;\n    end;\n    \n    /* 自变量 */\n    /* 经济支持 */\n    array economic_vars(*) f12a f12b f12c;\n    economic_support = 0;\n    do i = 1 to dim(economic_vars);\n        if economic_vars[i] = 99998 then\n            economic_support = economic_support + 10000;\n        else if economic_vars[i] in (88888, 99999) then\n            continue; /* 跳过无效值或缺失值 */\n        else\n            economic_support = economic_support + economic_vars[i];\n    end;\n    \n    /* 照料支持 */\n    residence = a51;\n    living = a52;\n    array visit_freq(*) f103a5 f103b5 f103c5 f103d5 f103e5 f103f5 f103g5 f103h5 f103i5 f103j5 f103k5;\n    visit_fren = 0;\n    do i = 1 to dim(visit_freq);\n        if visit_freq[i] = 1 then do;\n            visit_fren = 1;\n            leave;\n        end;\n    end;\n    \n    /* 情感支持 */\n    array contact_vars(*) f103a6 f103b6 f103c6 f103d6 f103e6 f103f6 f103g6 f103h6 f103i6 f103j6 f103k6;\n    emotion_support = 2; /* 先假设没联系 */\n    do i = 1 to dim(contact_vars);\n        if contact_vars[i] = 1 then do;\n            emotion_support = 1;\n            leave;\n        end;\n    end;\n    \n    /* 控制变量 */\n    /* 年龄 */\n    age = trueage;\n    \n    /* 性别 */\n    gender = a1;\n    \n    /* 受教育程度 */\n    education = f1;\n    \n    /* 退休前的工作类型 */\n    job_type = f2;\n    \n    /* 婚姻状况 */\n    if f41 = 1 then marriage_status = 1; /* 假设 1 代表已婚或与伴侣同居 */\n    else marriage_status = 0;\n    \n    /* 户口类型 */\n    hukou_type = hukou;\n    \n    /* 社保和养老保险 */\n    if nf64a = 0 or f64b = 1 or f64c = 1 or f64i = 1 then social_insurance = 1;\n    else social_insurance = 0;\n    \n    /* 医疗保险 */\n    if f64d = 1 or f64e = 1 or f64g = 1 or f64h = 1 then medical_insurance = 1;\n    else medical_insurance = 0;\n    \n    /* 慢性病 */\n    array chronic_vars(*) g15a1 g15b1 g15c1 g15d1 g15e1 g15f1 g15g1 g15h1 g15i1 g15j1 g15k1 g15l1 g15m1 g15n1 g15o1 g15p1 g15q1 g15r1 g15s1 g15t1 g15u1 g15v1 g15w1 g15x1;\n    chronic_disease = 0;\n    do i = 1 to dim(chronic_vars);\n        if chronic_vars[i] = 1 then chronic_disease = 1;\n        if chronic_disease = 1 then leave;\n    end;\n    \n    /* 抽烟 */\n    smoking = g151;\n    \n    /* 喝酒 */\n    drinking = g161;\n    \n    /* 体育锻炼 */\n    if d91 = 1 or d92 = 1 then exercise = 1;\n    else exercise = 2;\n\n    /* 子女年龄状态 */\n    array child_ages(*) f103a4 f103b4 f103c4 f103d4 f103e4 f103f4 f103g4 f103h4 f103i4 f103j4 f103k4 f103l4 f103m4;\n    array child_alive(*) f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\n    last_alive_child_age = .;\n    do i = dim(child_ages) to 1 by -1;\n        if child_alive[i] = 1 then do;\n            last_alive_child_age = child_ages[i];\n            leave;\n        end;\n    end;\n    if last_alive_child_age &gt; 60 then child_elderly_status = 1;\n    else child_elderly_status = 0;\n\n    /* 生成 care - support 变量 */\n    if residence = 1 or visit_fren = 1 then care_support = 1;\n    else care_support = 0;\n    \n    /* 选择需要的变量 */\n    keep SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support \n         f10 age gender education job_type marriage_status hukou_type \n         social_insurance medical_insurance chronic_disease smoking drinking exercise\n         child_elderly_status care_support f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\nrun;\n\n/* 将筛选后的保存为 XLSX 格式文件 */\n/*\nproc export data=selected_data\n    outfile='C:\\Users\\asus\\Desktop\\test\\CLHLS\\CLHLS数据\\CLHLS数据\\clhls_2018_sort0220.xlsx'\n    dbms=xlsx\n    replace;\nrun;\n*/\n\n/*检查f10 生育子女数的分布情况*/\nproc freq data=selected_data;\n    tables f10;\nrun;\n\n/*检查子女存活状态*/\nproc freq data=selected_data;\n    tables f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\nrun;\n\n/* 样本筛选 */\ndata temp_data;\n    set selected_data;\n\n    /* 纳入标准 */\n    /* 年龄要求 */\n    /*age_include = (age &gt; 60);*/\n    /* 生育情况 */\n    fertility_include = (f10 &gt; 0);\n\n    /* 子女存活情况 */\n    array child_alive(*) f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\n    child_alive_include = 0;\n    do i = 1 to dim(child_alive);\n        if child_alive[i] = 1 then do;\n            child_alive_include = 1;\n            leave;\n        end;\n    end;\n    meet_include = fertility_include and child_alive_include;\n\n    /* 排除标准 */ \n    /* 关键变量缺失值检查 */\n    /*array key_vars(*) SHEALTH ADL IADL;*/\n    array key_vars(*) SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support \n                      age gender education job_type marriage_status hukou_type \n                      social_insurance medical_insurance chronic_disease smoking drinking exercise\n                      child_elderly_status care_support;\n    has_missing = 0;\n    do i = 1 to dim(key_vars);\n        if missing(key_vars[i]) then do;\n            has_missing = 1;\n            leave;\n        end;\n    end;\n    /* 不合理生育数量检查 */\n    unreasonable_fertility = (f10 &gt; 7 or missing(f10));\n    meet_exclude = has_missing or unreasonable_fertility;\n\n    /* 筛选符合条件的样本 */\n    if meet_include and not meet_exclude;\n\n\n    /* 移除临时判断变量 */\n    drop fertility_include child_alive_include has_missing unreasonable_fertility meet_include meet_exclude;\n    /*移除部分原始变量*/\n    drop f103a3 f103b3 f103c3 f103d3 f103e3 f103f3 f103g3 f103h3 f103i3 f103j3 f103k3 f103l3 f103m3;\nrun;\n\n/*打印所有变量的频数分布表，检查是否含有异常值*/\n/*\nproc freq data=final_data;\n    tables _all_;\nrun;\n*/\n\n/*还需要删除含有的样本，即某些变量中赋值为9（not applicable）和88（don't know）的样本*/\n/*具体需要剔除变量满足这些条件的样本：SHEALTH&gt;8,ADL&gt;18,IADL&gt;24,residence&gt;3,eudcation&gt;22,smoking&gt;2,drinking&gt;2*/\n/* 删除满足特定条件的样本 */\ndata final_data;\n    set temp_data;\n    if (SHEALTH &lt;= 8) and (ADL &lt;= 18) and (IADL &lt;= 24) and (residence &lt;= 3) and (age&gt;=60) and (education &lt;= 22) and (smoking &lt;= 2) and (drinking &lt;= 2);\nrun;\n\nproc freq data=final_data;\n    tables _all_;\nrun;\n\n/* 保存筛选后的数据为 XLSX 格式 */\nproc export\n    data = final_data\n    outfile = 'C:\\Users\\asus\\Desktop\\test\\CLHLS\\Analysis-0214\\final_data.xlsx' /* 请替换为实际保存路径 */\n    dbms = xlsx\n    replace;\nrun;\n\n/* 对 age 变量进行分组 */\ndata final_data_grouped;\n    set final_data;\n    if age &lt; 70 then age_group = '60 - 69';\n    else if age &lt; 80 then age_group = '70 - 79';\n    else if age &lt; 90 then age_group = '80 - 89';\n    else age_group = '90+';\nrun;\n\n/* 探查每个变量的基本统计信息，查看是否有异常值 */\nproc means data=final_data n nmiss min max mean std;\n    var SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support\n        age gender education job_type marriage_status hukou_type\n        social_insurance medical_insurance chronic_disease smoking drinking exercise\n        child_elderly_status care_support;\nrun;\n\n/* 查看字符型变量的唯一值，看是否有异常字符 */\nproc freq data=final_data_grouped;\n    tables age_group gender education job_type marriage_status hukou_type;\nrun;\n\n/* 打印因变量、自变量和控制变量的频数分布表并汇总 */\nproc freq data=final_data_grouped noprint;\n    tables SHEALTH ADL ADL2 IADL IADL2 economic_support residence living visit_fren emotion_support\n           age_group gender education job_type marriage_status hukou_type\n           social_insurance medical_insurance chronic_disease smoking drinking exercise\n           child_elderly_status care_support / out=freq_summary;\nrun;\n\n/* 导出频数汇总表到 Excel */\nproc export\n    data = freq_summary\n    outfile = 'C:\\Users\\asus\\Desktop\\test\\CLHLS\\Analysis-0214\\frequency_summary.xlsx' /* 请替换为实际保存路径 */\n    dbms = xlsx\n    replace;\nrun;\nNotes:在Qmd中输入SAS代码时，不可以直接使用\n```{sas eval=false} your SAS code here ```\n进行标注，应使用如下形式：\n```{sas} #| eval: false your code here ```\n否则会在输出报错，使用了错误的\n``````````````````` :::\n原因是因为，在 Quarto 中，fenced div 的正确语法是：\n::: {.class-name}  Content here  :::",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "SAS",
      "使用SAS处理CLHLS数据"
    ]
  },
  {
    "objectID": "Guide/R/R-install.html",
    "href": "Guide/R/R-install.html",
    "title": "MacOS安装 R 和 Rstudio",
    "section": "",
    "text": "进入官网：https://www.r-project.org/\n找到适合操作系统的安装包与合适的版本\n\n点击download R\n找个离你近的学校或者随便哪个都行\n根据电脑选择一下安装包\nmacOS需要看一下是Intel还是arm的CPU，arm目前在M1/M2上搭载\n下载到download中\n在download中查看，选中R-4.3.2-x86_64.pkg这个文件，右键后选择第一个option“open”\n\n进入安装界面\n\n然后一直continue\n出现上面页面后，点击install后需要输入电脑密码，解锁后安装\n安装完成后可以选择保留安装包或者将其移入trash（废纸篓）\n安装完成后在Launchpad中找到R，双击进入操作界面\n进入后发现系统有点缺陷，因为是新电脑，有很多不完善的地方，找了一些办法解决\nDuring startup - Warning messages:  \n1: Setting LC_CTYPE failed, using \"C\"   \n2: Setting LC_COLLATE failed, using \"C\"   \n3: Setting LC_TIME failed, using \"C\"   \n4: Setting LC_MESSAGES failed, using \"C\"   \n5: Setting LC_MONETARY failed, using \"C\"   \n[R.app GUI 1.80 (8281) x86_64-apple-darwin20]    \nWARNING: You're using a non-UTF8 locale, therefore only ASCII characters will work. Please read R for Mac OS X FAQ (see Help) section 9 and adjust your system preferences accordingly.\n以上是warning，解决办法是\n⌘+space(空格键）调出Spotlight Search，然后输入terminal再点击open，打开后输入locale查看本地的设置，得到如下\n{bash,eval = FALSE} LANG=\"\" LC_COLLATE=\"C\" LC_CTYPE=\"UTF-8\" LC_MESSAGES=\"C\" LC_MONETARY=\"C\" LC_NUMERIC=\"C\" LC_TIME=\"C\" LC_ALL=\n这些还没有配置好，进行一些配置：\n{bash，eval = FALSE}        export LC_CTYPE=en_US.UTF-8 export LC_COLLATE=en_US.UTF-8 export LC_TIME=en_US.UTF-8 export LC_MESSAGES=en_US.UTF-8 export LC_MONETARY=en_US.UTF-8\n这样就可以了。\n然后再安装Rstudio，RStudio是为R语言设计的一种跨平台集成开发环境。其特色包括可客制化的软件套件视觉化界面与同团队开发的一系列数据可视化与出版工具。\n在Rstudio中下载合适的版本，按照上述操作再来亿遍即可。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "R introduction",
      "MacOS安装 R 和 Rstudio"
    ]
  },
  {
    "objectID": "Guide/Python/Python-intro.html",
    "href": "Guide/Python/Python-intro.html",
    "title": "Python",
    "section": "",
    "text": "Python（英式发音：/ˈpaɪθən/；美式发音：/ˈpaɪθɑːn/），是一种广泛使用的解释型、高级和通用的编程语言。Python支持多种编程范型，包括结构化、过程式、反射式、面向对象和函数式编程。它拥有动态类型系统和垃圾回收功能，能够自动管理内存使用，并且其本身拥有一个巨大而广泛的标准库。它的语言结构以及面向对象的方法，旨在帮助程序员为小型的和大型的项目编写逻辑清晰的代码。\n和Python结缘在2020年末，当时朋友正在学，我也跟着一起，跑了几个代码，做了几个图，但是没有啥应用计划，后来就没咋关注了。\n等毕了业，有感觉应该学一学，要是工作了也算有一个技能，于是又捡了起来，还行，随着理解力的提升，逐渐能掌握这一门语言了，当然，AI 的辅助也是功不可没。\nPython 的故事很多，关于它的内容在互联网可能看一年都看不完，全世界的人都在使用它，因此我这里也不做过多的赘述，显得很多余。\n使用Python的第一步，就是来一个电脑，当然没有电脑也行，因为可以用Google的 Colab ，挺好用，如果做机器学习，是一个很好的选择（当然是要开Pro）。\n然后安装，新手就安装普通的 Python 吧，先试试，来个编辑器，用 VS code、vim、pycharm 都可以，个人推荐用 VS code（这个也要学，没那么简单）。\n再然后学会 pip 安装包/库，创建一个文件 .py 开始运行。\nMOOC和B站上有很多教程，市面上的书也很多，选一本自己看得过去/感兴趣的。\n关于书的话，这些可以看看：\n\nPython Cookbook 3rd\nPython编程:从入门到实践\nPython基础教程 \n利用Python进行数据分析（做数据分析的可以着重看这一本）\n\n等逐渐熟悉Python了，可以用下 anaconda ，对环境的管理要好不少，且可以使用 spyder（神器，就是还不支持 AI）。\nend.",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python"
    ]
  },
  {
    "objectID": "Guide/Guide-intro.html",
    "href": "Guide/Guide-intro.html",
    "title": "统计软件",
    "section": "",
    "text": "一些统计软件的使用记录。\n最早是学的Python，应该是20年的时候，看的MOOC上的北理嵩天老师的课程，过了一遍，没有深入研究。\n后来学的SPSS，21年，因为开始上统计学课了，课上老师用SPSS做分析演示，我还负责给全专业分发SPSS安装包和密钥，并帮他们安装。\n21年还试着用过MATLAB，因为20年的时候数学老师说可以准备数学建模的比赛，20年疫请然后没人管事，21年计算机学院的一位老师担下了这个活，随机我们组队在21年秋天比赛，但是数理教研组所承诺的培训和指导都是泡影，我和队友在数理教研室办公室待了两个晚上，不断地从互联网上查找相关的资料和代码，然后在MATLAB中进行复现，其实这个时候的认识和技能是很糟糕的，数学上了微积分和线代，但是和建模关系最大的运筹学要在大二的下学期才上。就这样，紧张又无聊的三天就这么过去了，现在来看（2025年）真的是既心酸又好笑，但是也不失为一个有趣的记忆。\n21年写完大创的论文后，就逐渐没有再关注这些软件的使用了，因为用不上，直到23年毕业的时候，又拿着SPSS做了一下毕业论文的分析。\n23年毕业后，选择了二战，暑假又回了学校和同学租房备考，在逛丁香园的时候，发现本校的一位老师在上面更新了Rmarkdown的课程，我好奇，随机花了一周的时间学了一下，是一个很好用的“thing”，结合了R、Markdown和LaTeX。正好当时需要整理统计学的笔记，随开始尝试并使用。\n其实23年的时候Posit已经在开发Quarto了，但是新事物到达普通人的视野中总是需要多耗费一些时间，到了24年我才知道有这么个“thing”，但是后面的学习节奏，不允许我抽出大量的时间来“改换门庭”。同时我的Rstudio一直无法正确的创建和使用Quarto，和社区交流后无果，只能使用VS Code来作为编辑器。\n直到24年考完空闲下来，我才开始系统地转换这个笔记并重新部署在GitHub上。\n24年上半年在长沙的时候，二战失败，五月找工作也失败。安慰自己说，没事，我可以干点别的，就重新开始学Python，并尝试学了SAS。\n25年开年朋友请我帮他做一些Stata的分析，我便拿出了22年朋友送的《Stata统计分析·社会科学应用指南》开始速成，得益于AI的发展，专供某一个方向也是不难的。在这种理解下，我开始系统地学习统计学和数据分析的内容，这个网站被用来记录和整理相关的内容与想法。\n我也不知道最后会学成什么样，但是希望能留下一些有意义的东西，以供后来者。\n2025-04-28 Stata作为一个商业软件，确有其独到之处，稳定，简便，支持很好是很好的有点，相较于R的开源与不稳定，这在多次使用与复现中是很重要的一点。最近在用R做概率图模型的时候就复现不出来，功能强大是优点，但是无法复现也就意味着断层，对于学习者来说是一件很麻烦的事情。\n2025-04-29 昨晚弄好 Stata 18 以后，晚上想起 Stata 官网说可以和 Python 联用，开发了包在 Ipython 中调用 Stata ，这很有意思，我在想，既然 Quarto 可以编译 .ipynb 文件为 PDF、html 等文件，那么是否可以在 Quarto 中使用 .ipynb 文件做一个容器，然后将 Satat 程序放在里面进行编译后，再由 Quarto 生成 html 文件，最后再组成网站，可以无损/流畅地展示 Stata 程序和输出结果。网上有 Python + Stata 结合使用的相关信息，但是没有和 Quarto 配合的，于是自己进行尝试，成功，能够实现在 .ipynb 中编辑 Stata 代码，并通过 Quarto 编译成网页，在网站进行展示。\n2025-05-05 看着桌边的 《SAS统计软件应用》，想了一下 SAS ，软件自带的编辑器很糟糕，又想，vscode 是万能编辑器，是不是也有 SAS 的扩展，搜了一下，嘿，还真有，那是不是也可以像 Stata 一样通过 .ipynb 文件进行编辑和运行，然后通过 Quarto 进行编译转为 .html 文件然后网页输出，看了下也是可以的，有趣，感觉 Quarto 真的可以借助 .ipynb 和 .qmd/rmd 实现统计软件的展现大一统。",
    "crumbs": [
      "Home",
      "统计软件"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "本篇博客用来记录2025年1月到2月帮助学弟处理一份某二级医院2018-2023年的医疗费用，最开始用的Stata，但是越往后，越感觉到Stata的难用，以及AI对这种程序的支持程度极其有限，随改用 Python + Stata 来继续完成相关的分析。\n\n\n运行此文档需要电脑上以安装Python，并且下列包已被安装并且能被调用：\nnumpy jupyter-cache pandas openpyxl\n你可以使用 pip 或 conda 进行安装： pip install jupyter-cache\n\n\n\n我们可以使用pandas包来查看部分原始数据，数据的基本样式如下：\n\n# 安装并加载必要的包\nimport pandas as pd\nimport numpy as np\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\ndata = pd.read_excel(file_path, sheet_name=0, engine='openpyxl')     \n\n# 数据脱敏，删除地方\ncolumns_to_drop = [\"籍贯\", \"出生地\"]\ndata = data.drop(columns=columns_to_drop, errors='ignore')  # errors='ignore' 防止列不存在时报错\n\n# 随机抽取10个样本数据\nsample_data = data.sample(n=10, random_state=42)\n\n# 打印样本数据\nprint(sample_data)\n\n      次数        出生日期 性别   年龄      医疗付费方式  国籍 新生儿出生体重 新生儿入院体重  民族     职业  ...  \\\n1138   1  1956-09-06  男  61岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n2024   1  1959-08-15  女  59岁  城镇居民基本医疗保险  中国       －       －  汉族     居民  ...   \n1605   1  1973-02-05  女  45岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1975  11  2012-09-20  男   6岁    新型农村合作医疗  中国       －       －  汉族  学龄前儿童  ...   \n1701   1  1963-01-23  男  55岁         全自费  中国       －       －  汉族      无  ...   \n218    1  1969-06-08  女  48岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1344   1  1960-09-11  女  57岁    新型农村合作医疗  中国       －       －  汉族     务农  ...   \n252    1  1944-05-01  女  73岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1921   5  2013-10-17  女   5岁    新型农村合作医疗  中国       －       －  汉族  学龄前儿童  ...   \n643    1  1997-09-02  男  20岁         全自费  中国       －       －  汉族     战士  ...   \n\n     麻醉开始时间3 麻醉结束时间3 麻醉方式3 麻醉分级3  切口部位3 切口等级3 NNIS分级3  手术部位感染3  术前住院天数3  \\\n1138     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n2024     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1605     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1975     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1701     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n218      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1344     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n252      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1921     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n643      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n\n     手术持续时间3  \n1138     NaN  \n2024     NaN  \n1605     NaN  \n1975     NaN  \n1701     NaN  \n218      NaN  \n1344     NaN  \n252      NaN  \n1921     NaN  \n643      NaN  \n\n[10 rows x 200 columns]\n\n\n我们可以看到，该数据的列很多，第一张表中有200列，我们需要对其进行一些筛选。\n\n\n\n从哪里开始是一个需要思考的问题，对于数据的认识决定了你处理问题的方向和效率。首先，理解数据的来源至关重要，这包括了解数据是如何收集的、收集过程中可能出现的偏差或错误。其次，明确数据的类型与结构也是关键步骤之一，不同类型的数据（如定量数据、定性数据）需要采用不同的分析方法。再者，对数据进行初步探索，比如通过可视化手段观察数据分布特征，或是计算一些基本统计量来了解数据的基本情况，能够帮助你更好地制定数据处理策略。\n在真正开始处理数据之前，还需要考虑你的目标是什么。是为了回答一个具体的问题，还是为了探索潜在的模式？明确了目标之后，才能有针对性地选择合适的工具和技术。此外，考虑到数据质量的问题，数据清洗是不可跳过的一步，它包括去除异常值、填补缺失值等操作，这对于提高分析结果的准确性非常关键。\n最后，保持对数据伦理的关注同样重要，在整个数据分析的过程中，确保遵循相关的隐私保护法规和道德标准，这样才能确保你的工作不仅有效，而且负责任。通过对数据全面而深刻的理解，你可以更加自信地从数据中提取有价值的信息，并为决策提供有力支持。\n\n\n这份Excel文件有6张sheet，分别是2018-2023年，首先需要检查这六张sheet中的变量是否一致：\n\nimport pandas as pd\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\nsheet_names = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n\n# 读取所有 sheet 的数据\nsheets_data = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}\n\n# 获取每个 sheet 的列名\nsheets_columns = {sheet: set(data.columns) for sheet, data in sheets_data.items()}\n\n# 找出所有 sheet 的共同变量和不一致的变量\ncommon_columns = set.intersection(*sheets_columns.values())\nall_columns = set.union(*sheets_columns.values())\ninconsistent_columns = all_columns - common_columns\n\n# 打印结果\nprint(\"一致的变量名:\")\nprint(common_columns)\n\nprint(\"\\n不一致的变量名:\")\nprint(inconsistent_columns)\n\n# 打印每个 sheet 的变量\nfor sheet, columns in sheets_columns.items():\n    print(f\"\\n{sheet} 的变量: {columns}\")\n\n一致的变量名:\n{'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '1愈合', '死亡患者尸检', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '3级别', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '级别1'}\n\n不一致的变量名:\n{'愈合2', '手术开始时间3', '7手术编码', '7级别', '7切口', '手术部位感染3', '手术次数3', '8麻醉方式', '7手术时间', '手术持续时间3', '是否微创手术2', '8手术时间', '8愈合', '4麻醉方式', '麻醉开始时间3', '术前住院天数3', '切口部位3', 'NNIS分级3', '8切口', '麻醉结束时间3', '麻醉分级3', '8手术编码', '切口等级3', '手术操作名称3', '麻醉方式3', '手术操作编码3', '8级别', '4麻醉医师', '8手术名称', '7愈合', '手术结束时间3', '择期手术3', '7手术名称', '7麻醉方式'}\n\n2018 的变量: {'目的', '愈合2', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '手术开始时间3', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉结束时间3', '麻醉开始时间1', '切口等级3', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '麻醉方式3', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '2切口', '手术部位感染3', '3手术时间', '是否有使用抗菌药物1', '入院日期', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '麻醉开始时间3', '6愈合', '切口部位3', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '手术操作名称3', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '手术操作编码3', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '手术结束时间3', '1麻醉方式', '3.1非手术治疗项目费', '择期手术3', '4愈合', '1手术名称', '病案质量', '5级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '3级别', '手术次数3', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '手术持续时间3', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '是否微创手术2', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '4麻醉方式', '3.3手术治疗费', '术前住院天数3', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '麻醉分级3', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', 'NNIS分级3', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '级别1'}\n\n2019 的变量: {'目的', '愈合2', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '手术开始时间3', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '手术操作名称3', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '手术操作编码3', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '手术结束时间3', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '手术次数3', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '是否微创手术2', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '4麻醉方式', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2020 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '8愈合', '6手术编码', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2021 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2022 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2023 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '4麻醉医师', '血管介入治疗抗菌药物使用天数', '是否非计划重返手术室病例1', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '输液反应次数', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n\n然后剔除不一致的变量数据，同时创建一个新变量year，用sheet的年份对其进行填充，再按变量名对应合并6张表格的数据称为一张总表，命名为merge-sheet.xlsx输出到你需要存放数据的文件夹中。\n变量还是太多了，那接下来对变量进行筛选，首先我们可以对所有键值为空的变量进行剔除，或者根据实际的研究需要，剔除一部分键值全部为null的变量。\n这里我选择对键值全部为null或0的变量进行剔除。\n第一次尝试的时候，打开表后进行查看，发现变量顺序很乱，没有按照原始顺序进行排列，处理办法则是在前面的变量筛选部分使用DataFrame的loc方法选择列，同时保持列的顺序。\n同时为了节省时间，因为在Quarto中运行Python代码很慢，暂时还不知道原因，待以后调试一下。所以最后用一个程序解决上述这些问题，节省时间。\n\nimport pandas as pd\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\noutput_path = \"C:/Users/asus/Desktop/test/stata/data/merge-data.xlsx\"\nfinal_output_path = \"C:/Users/asus/Desktop/test/stata/data/cleaned-merge-data.xlsx\"\nsheet_names = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n\n# 读取所有 sheet 的数据\nsheets_data = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}\n\n# 获取每个 sheet 的列名\nsheets_columns = {sheet: set(data.columns) for sheet, data in sheets_data.items()}\n\n# 找出所有 sheet 的共同变量\ncommon_columns = set.intersection(*sheets_columns.values())\n# 保持原始顺序\ncommon_columns = list(common_columns)  \n\n# 剔除不一致的变量数据，并添加 year 变量\nfor sheet, data in sheets_data.items():\n    sheets_data[sheet] = data[list(common_columns)]\n    sheets_data[sheet]['year'] = sheet\n\n# 合并所有 sheet 的数据\nmerged_data = pd.concat(sheets_data.values(), ignore_index=True)\n\n# 输出合并后的数据到指定路径\nmerged_data.to_excel(output_path, index=False)\n\n# 重新导入合并后的数据\nmerged_data = pd.read_excel(output_path)\n\n# 剔除键值全部为 null 或 0 的变量，同时保持原始变量的顺序\nnon_null_columns = merged_data.dropna(axis=1, how='all').columns\nnon_zero_columns = merged_data.loc[:, (merged_data != 0).any(axis=0)].columns\nvalid_columns = [col for col in merged_data.columns if col in non_null_columns and col in non_zero_columns]\n\ncleaned_data = merged_data.loc[:, valid_columns]\n\n# 输出清理后的数据到指定路径\ncleaned_data.to_excel(final_output_path, index=False)\n\nprint(f\"清理后的数据已输出到 {final_output_path}\")\n\n# 展示部分数据\n\n# 随机抽取10个样本数据\nsample_data = cleaned_data.sample(n=10)\n\n# 打印样本数据\nprint(sample_data)\n\n清理后的数据已输出到 C:/Users/asus/Desktop/test/stata/data/cleaned-merge-data.xlsx\n      目的 断脐后预防性使用抗菌药物给药时间1 6麻醉方式  1愈合 死亡患者尸检 入院途径  2.4临床诊断项目费  \\\n14170  -               NaN   NaN  NaN    NaN   急诊      1553.0   \n20116  -               NaN   NaN  NaN    NaN   门诊      1272.5   \n37053  -               NaN   NaN  NaN    NaN   门诊         0.0   \n43824  -               NaN   NaN    甲      否   门诊      1106.0   \n32348  -               NaN   NaN  NaN    NaN   门诊       398.5   \n37403  -               NaN   NaN  NaN    NaN   门诊        29.0   \n31026  -               NaN   NaN    甲    NaN   门诊      1814.0   \n12348  -               NaN   NaN    乙    NaN   门诊       592.0   \n28312  -               NaN   NaN   其他    NaN   门诊      2022.5   \n35851  -               NaN   NaN   其他    NaN   门诊      1417.5   \n\n       清洁手术预防使用抗菌药物总天数              麻醉结束时间1 麻醉结束时间2  ...  2.3影像学诊断费 麻醉方式2  \\\n14170              NaN                  NaN     NaN  ...          0   NaN   \n20116              NaN                  NaN     NaN  ...          0   NaN   \n37053              NaN                  NaN     NaN  ...          0   NaN   \n43824              1.0  2023-07-07 12:05:33     NaN  ...        256   NaN   \n32348              NaN                  NaN     NaN  ...          0   NaN   \n37403              NaN                  NaN     NaN  ...          0   NaN   \n31026              NaN                  NaN     NaN  ...          0   NaN   \n12348              NaN                  NaN     NaN  ...          0   NaN   \n28312              NaN                  NaN     NaN  ...        640   NaN   \n35851              NaN  2022-07-07 16:25:49     NaN  ...          0   NaN   \n\n       4手术时间 入院后颅脑损伤昏迷时间  级别2  细菌名称1 是否药物过敏 细菌名称3  级别1  year  \n14170    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n20116    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n37053    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n43824    NaN      -天-时-分  NaN      -      无     -  4.0  2023  \n32348    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n37403    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n31026    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2021  \n12348    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n28312    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2021  \n35851    NaN      -天-时-分  NaN      -      无     -  2.0  2022  \n\n[10 rows x 160 columns]\n\n\n\n\n\n\n经过上述筛选后的变量依然还有很多，其中不乏无用信息变量或无效信息变量，对变量做进一步筛选。\n对样本进行筛选，需要满足在当年收治且出院，满足常规医保使用条件，关键变量含有缺失值的样本。\n使用 pandas 进行处理（根据Excel视图挑选的缺失数据的变量或含有较多缺失值的变量）：\n\nimport pandas as pd\n\n# 文件路径\ninput_file = r\"C:\\Users\\asus\\Desktop\\test\\stata\\data\\cleaned-merge-data.xlsx\"\noutput_file = r\"C:\\Users\\asus\\Desktop\\test\\stata\\data\\allclean.xlsx\"\n\n# 读取 Excel 文件\ndf = pd.read_excel(input_file)\n\n# 要删除的列列表\ncolumns_to_drop = [\n    \"新生儿出生体重\", \"新生儿入院体重\", \"国籍\", \"籍贯\", \"病室\", \"病室.1\", \"是否有出院31天内再住院计划\",\n    \"病理诊断\", \"院内感染\", \"药物过敏\", \"死亡患者尸检\", \"血型\", \"RH\",\n    \"1手术编码\", \"1手术时间\", \"1级别\", \"1切口\", \"1愈合\", \"1麻醉方式\",\n    \"2手术名称\", \"2手术编码\", \"2手术时间\", \"2级别\", \"2切口\", \"2愈合\", \"2麻醉方式\",\n    \"3手术名称\", \"3手术编码\", \"3手术时间\", \"3级别\", \"3切口\", \"3愈合\", \"3麻醉方式\",\n    \"4手术名称\", \"4手术编码\", \"4手术时间\", \"4级别\", \"4切口\", \"4愈合\", \"4麻醉方式\",\n    \"5手术名称\", \"5手术编码\", \"5手术时间\", \"5级别\", \"5切口\", \"5愈合\", \"5麻醉方式\",\n    \"6手术名称\", \"6手术编码\", \"6手术时间\", \"6级别\", \"6切口\", \"6愈合\", \"6麻醉方式\",\n    \"7手术名称\", \"7手术编码\", \"7手术时间\", \"7级别\", \"7切口\", \"7愈合\", \"7麻醉方式\",\n    \"8手术名称\", \"8手术编码\", \"8手术时间\", \"8级别\", \"8切口\", \"8愈合\", \"8麻醉方式\",\n    \"目的\", \"入院前颅脑损伤昏迷时间\", \"入院后颅脑损伤昏迷时间\",\n    \"抗菌药物使用天数\", \"清洁手术预防使用抗菌药物品种数\", \"是否临床路径\", \"清洁手术预防使用抗菌药物总天数\",\n    \"患者入住重症监护室（ICU）情况\", \"感染情况\", \"输血反应\", \"输血反应次数\", \"输液反应\", \"输液反应次数\",\n    \"细菌名称1\", \"细菌名称2\", \"细菌名称3\", \"细菌名称4\", \"血管介入治疗\", \"血管介入治疗抗菌药物使用天数\",\n    \"手术次数1\", \"手术操作名称1\", \"手术操作编码1\", \"手术开始时间1\", \"手术结束时间1\", \"择期手术1\",\n    \"麻醉开始时间1\", \"麻醉结束时间1\", \"麻醉方式1\", \"麻醉分级1\", \"切口部位1\", \"切口等级1\", \"NNIS分级1\",\n    \"手术部位感染1\", \"术前住院天数1\", \"手术持续时间1\", \"是否非计划重返手术室病例1\", \"术前使用预防性抗菌药物1\",\n    \"术前预防性抗菌药物给药时间1\", \"是否在术后使用预防性抗菌药物1\", \"术后预防性抗菌药物结束时间1\",\n    \"手术预防性使用抗菌药物天数1\", \"是否有使用抗菌药物1\", \"预防性抗菌药物使用时机1\",\n    \"断脐后预防性使用抗菌药物给药时间1\", \"本次住院期间有无重返手术室的计划1\", \"腔镜手术名称1\", \"级别1\", \"愈合1\",\n    \"是否微创手术1\", \"手术次数2\", \"手术操作名称2\", \"手术操作编码2\", \"手术开始时间2\", \"手术结束时间2\",\n    \"择期手术2\", \"麻醉开始时间2\", \"麻醉结束时间2\", \"麻醉方式2\", \"麻醉分级2\", \"切口部位2\", \"切口等级2\",\n    \"NNIS分级2\", \"手术部位感染2\", \"术前住院天数2\", \"手术持续时间2\", \"是否非计划重返手术室病例2\",\n    \"术前使用预防性抗菌药物2\", \"术前预防性抗菌药物给药时间2\", \"是否在术后使用预防性抗菌药物2\",\n    \"术后预防性抗菌药物结束时间2\", \"手术预防性使用抗菌药物天数2\", \"是否有使用抗菌药物2\",\n    \"预防性抗菌药物使用时机2\", \"断脐后预防性使用抗菌药物给药时间2\", \"本次住院期间有无重返手术室的计划2\",\n    \"腔镜手术名称2\", \"级别2\", \"愈合2\", \"是否微创手术2\", \"手术次数3\", \"手术操作名称3\", \"手术操作编码3\", \"手术开始时间3\",\n    \"手术结束时间3\", \"择期手术3\", \"麻醉开始时间3\", \"麻醉结束时间3\", \"麻醉方式3\", \"麻醉分级3\", \"切口部位3\", \"切口等级3\",\n    \"NNIS分级3\", \"手术部位感染3\", \"术前住院天数3\", \"手术持续时间3\", \"4麻醉医师\", \"出生地\", \"籍贯\"\n]\n\n# 删除指定的列\ndf = df.drop(columns=columns_to_drop, errors='ignore')\n\n# 过滤掉 '公安病区'\nif '入院科别' in df.columns and '出院科别' in df.columns:\n    df = df[~df['入院科别'].isin(['公安病区'])]\n    df = df[~df['出院科别'].isin(['公安病区'])]\n\n# 打印随机 10 个样本\nprint(\"随机 10 个样本：\")\nprint(df.sample(10))\n\n# 将处理后的 DataFrame 写入新的 Excel 文件\n# df.to_excel(output_file, index=False)\n\n# print(f\"数据清洗完成，已保存到 {output_file}\")\n\n随机 10 个样本：\n      入院途径  2.4临床诊断项目费   10.其他费  7.2中草药费 性别    总药品费  \\\n45540   门诊       225.0    28.98      0.0  女   22.17   \n27023   门诊      1105.0  9923.46      0.0  男   77.87   \n10102   门诊       178.3  3442.00      0.0  女  168.30   \n29291   门诊       977.5   511.57      0.0  女  626.99   \n18117   门诊       624.0   139.90      0.0  女  526.41   \n48037   门诊       264.9  3286.00      0.0  男   88.97   \n35602   门诊       201.0   336.92      0.0  男  723.32   \n17413   门诊      1278.5   357.29      0.0  女  702.13   \n966     门诊         0.0    24.75      0.0  女    0.00   \n14589   门诊       147.3  4724.00      0.0  女  196.45   \n\n                                                    入院诊断     职业  1.3护理费  \\\n45540                       老年核性白内障|H25.100,翼状胬肉|H11.000     农民    25.0   \n27023              结肠恶性肿瘤个人史|Z85.006,手术后恶性肿瘤化学治疗|Z51.102     农民    75.0   \n10102                       老年性白内障|H25.900,玻璃体混浊|H43.300     农民    50.0   \n29291                           结肠息肉|K63.500,胃息肉|K31.703  自由职业者   196.0   \n18117  大疱性类天疱疮|L12.000,冠状动脉粥样硬化性心脏病|I25.103,心功能Ⅲ级|I50...     居民    85.0   \n48037                                    老年核性白内障|H25.100     农民    50.0   \n35602                       节肢动物咬伤|T63.402,过敏性皮炎|L23.901     农民   200.6   \n17413                                         腹痛|R10.400     居民   125.0   \n966                                      脑外伤后综合征|F07.201      -   156.0   \n14589  老年性白内障|H25.900,翼状胬肉|H11.000,玻璃体混浊|H43.300,特指手术...     农民    50.0   \n\n       1.1一般医疗服务费  ...  婚姻  3.5手术费   年龄  民族 2.2实验室诊断费  住院天数       总费用  \\\n45540        32.0  ...  已婚     0.0  61岁  汉族     372.0     1    716.25   \n27023       105.0  ...  已婚     0.0  29岁  汉族     499.0     3  11888.53   \n10102        52.0  ...  已婚  1976.0  76岁  汉族     387.0     2   6314.60   \n29291       245.0  ...  已婚     0.0  67岁  汉族     589.0     7  11239.16   \n18117       294.0  ...  已婚     0.0  92岁  汉族    1095.0     3   3133.31   \n48037        32.0  ...  已婚  1976.0  77岁  汉族     381.0     1   6164.87   \n35602       140.0  ...  已婚     0.0  56岁  汉族     731.0     4   2475.84   \n17413       160.0  ...  已婚     0.0  60岁  汉族     608.0     5   4195.02   \n966         414.0  ...  未婚     0.0   8岁  汉族       0.0    13   7004.75   \n14589        52.0  ...  已婚  1976.0  78岁  汉族       0.0     2   7166.75   \n\n      2.3影像学诊断费 是否药物过敏  year  \n45540         0      无  2023  \n27023         0      无  2021  \n10102        36      无  2019  \n29291         0      无  2021  \n18117         0      无  2020  \n48037        36      无  2023  \n35602         0      无  2022  \n17413         0      无  2020  \n966           0      无  2018  \n14589         0      无  2020  \n\n[10 rows x 39 columns]\n\n\n\n\n\n\n\n因为需要对疾病进行分类与根据诊断信息确定来生成共病信息，根据出院诊断来对疾病进行分类与赋值，这里使用 Stata 来完成。\n**************\n* 1. 清理环境并导入数据\n**************\nclear all\n\n* 读取 Excel 文件，假设第一行为列名\nimport excel \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\allclean.xlsx\", ///\n    firstrow case(lower) clear\n\n* 注意：\n*  - firstrow 表示将第一行作为变量名\n*  - case(lower) 将变量名转换为小写，避免中文或大小写冲突\n*  - 如果您的表格存在中文列名，可能需要手动 rename\n\n************************\n* 2. 处理、提取与分类: 以“出院诊断”列为例\n************************\n\n*------------------\ngen disease = 出院诊断\n*------------------\n\n* 假设您已经将\"出院诊断\"重命名为了 \"disease\"\n* 现在要从 disease 里提取 ICD 编码到 icd10 列。\n* 如果原数据已包含 icd10 这列，可跳过此步。\n* 这里只是示例，具体提取逻辑需根据实际字符串格式做 parsing:\n* 例如： 出院诊断 字符串为 \"急性化脓性阑尾炎|K35.902|有,高血压病|I10.x00|有\"\n\n*（示例）如果 disease 形如 \"XXX|K35.902|有,YYY|I10.x00|有\"\n* 可以先把逗号换成某种分隔，然后再拆分，这里仅给示例逻辑\n* 注意：以下只是思路示例，可能需正则表达式、substr、split 等更复杂处理\n\n// 对disease进行拆分\n* 1. 按 | 分隔 disease 列，生成多个新变量\nsplit disease, parse(\"|\") generate(disease_part)\n \n// 提取第一个 ICD 编码\n* 2. 提取第二部分（part2）作为 icd10，使用正则表达式剔除多余编码\n* 保留 disease_part2 的前7个字符作为 icd10，形如 C15.900\ngen icd10 = substr(disease_part2, 1, 7)\ngen icd_com = substr(disease_part4, 1, 7)\n* 去除前后的空格\nreplace icd10 = trim(icd10)\n\n* 3. 删除所有拆分部分\ndrop disease_part1-disease_part55\n\n* 4. 检查结果\nlist disease icd10 in 1/10\n\n***************************************\n* 按 ICD 数量判断是否共病\n***************************************\ngen comorbidity = 0  // 初始值为 0\nreplace comorbidity = 1 if !missing(icd10) & !missing(icd_com) \n// 如果ICD10和ICD_com都不为空，则赋值为1\n\n// 查看前10行的数据\nlist icd10 icd_com comorbidity in 1/10\n\n***************************************\n* 筛除部分变量\n***************************************\n\ndrop 入院日期 入院科别 出院日期 出院科别 出院诊断 disease 离院方式 病案质量\n\n***************************************\n* 按 ICD 编码生成截取变量\n***************************************\n\n* 如果 icd10 是数值型，转换为字符串型\ntostring icd10, replace  \n\n* 检查并创建 icd_3c 变量\ngen icd_3c = \"\"   // 如果 icd_3c 不存在，创建一个空的字符串变量\n\n* 截取 icd10 的前三位并赋值给 icd_3c\nreplace icd_3c = substr(icd10, 1, 3)  \n\n* 如果 icd_3c 是数值型，转换为字符串型\ntostring icd_3c, replace \n\n* 创建 icd_str1 变量\ngen icd_str1 = \"\"\n\n* icd_str1: ICD 编码首位\nreplace icd_str1 = substr(icd10,1,1)\n\n* 如果 icd_str1 是数值型，转换为字符串型\ntostring icd_str1, replace \n\n* 使用 trim() 来去除空格\nreplace icd_str1 = trim(icd_str1)\n\n* 查看 icd_str1 的数据类型\ndescribe icd_str1\n\n* 查看是否有空值或特殊字符\n* list icd_str1 if missing(icd_str1)\n\n************************\n* 按照ICD编码归为22类\n************************\n\n* 创建icd分类变量：icd_chapter\ngen icd_chapter = \"\" \n\n* 字符转换为数值\ndestring icd_chapter,replace\n\n// 为icd_chapter赋值\n\nreplace icd_chapter=1 if icd_str1==\"A\"|icd_str1==\"B\"\nreplace icd_chapter=2 if icd_str1==\"C\"|(icd_3c&gt;=\"D00\"&icd_3c&lt;=\"D48\")\nreplace icd_chapter=3 if icd_3c&gt;=\"D50\"&icd_3c&lt;=\"D89\"\nreplace icd_chapter=4 if icd_3c&gt;=\"E00\"&icd_3c&lt;=\"E90\"\nreplace icd_chapter=5 if icd_3c&gt;=\"F00\"&icd_3c&lt;=\"F99\"\nreplace icd_chapter=6 if icd_3c&gt;=\"G00\"&icd_3c&lt;=\"G99\"\nreplace icd_chapter=7 if icd_3c&gt;=\"H00\"&icd_3c&lt;=\"H59\"\nreplace icd_chapter=8 if icd_3c&gt;=\"H60\"&icd_3c&lt;=\"H99\"\nreplace icd_chapter=9 if icd_str1==\"I\"\nreplace icd_chapter=10 if icd_str1==\"J\"\nreplace icd_chapter=11 if icd_str1==\"K\"\nreplace icd_chapter=12 if icd_str1==\"L\"\nreplace icd_chapter=13 if icd_str1==\"M\"\nreplace icd_chapter=14 if icd_str1==\"N\"\nreplace icd_chapter=15 if icd_str1==\"O\"\nreplace icd_chapter=16 if icd_str1==\"P\"\nreplace icd_chapter=17 if icd_str1==\"Q\"\nreplace icd_chapter=18 if icd_str1==\"R\"\nreplace icd_chapter=19 if icd_str1==\"S\"| icd_str1==\"T\"\nreplace icd_chapter=20 if icd_str1==\"V\"| icd_str1==\"Y\"\nreplace icd_chapter=21 if icd_str1==\"Z\"\nreplace icd_chapter=22 if icd_str1==\"U\"\nreplace icd_chapter=20 if icd_str1==\"V\"| icd_str1==\"Y\"|icd_str1==\"X\"|icd_str1==\"W\"\nreplace icd_chapter=21 if icd_str1==\"Z\"| substr(trim(icd10),1,2)==\"WW\"\nreplace icd_chapter=22 if icd_str1==\"U\"\n\n***************************************\n* 检查分类缺失\n***************************************\ntab icd_3c if icd_chapter==.\n\n****************************\n* 3. 导出处理后的数据\n****************************\n\n* 导出为 Stata 格式\nsave \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\ICD-result.dta\", replace\n\n\n\n// 数据处理\nclear all\nuse \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\ICD-result.dta\",clear\n\ncapture drop Cost  // 捕获可能发生的错误，如果变量不存在则继续执行\n\n* 创建 id 变量并赋值\ngen id = _n\n\nsort id year\nxtset id year\n\n*- 次均费用，需要查看总费用是否和各项目费用加总一致，此处不一致\n* gen Cost = 总费用 / 次数 \n\n*- 住院天数\ngen Day = 住院天数\n\n*- DIP政策\ngen DIP = .\nreplace DIP = 0 if year == 2018\nreplace DIP = 0 if year == 2019\nreplace DIP = 0 if year == 2020\nreplace DIP = 0 if year == 2021\nreplace DIP = 1 if year == 2022\nreplace DIP = 1 if year == 2023\n\n*- 控制变量序列\n*- 年龄\ngen Age = 年龄\n\n*- 性别（虚拟变量；当受访者性别为女性时，赋值为\"0\"，否则为\"1\"）\ngen Gender = 0 if 性别 == \"女\"\nrecode Gender .= 1\n\n*- 婚姻（虚拟变量；当受访者已婚时，赋值为\"1\"，否则为\"0\"）\ngen Marriage = 1 if 婚姻 == \"已婚\"\nrecode Marriage .= 0\n\n*- 药物过敏\ngen Sensitive = 1 if 是否药物过敏 == \"有\"\nrecode Sensitive .= 0\n\n*- 是否手术\ngen Opera = 1 if 手术费 &gt; 0\nreplace Cmedicine = 0 if 手术费 == 0\n\n*- 职业\ngen Career = 1 if strmatch(职业, \"*农*\")\nreplace Career = 2 if strmatch(职业, \"*职*\")\nreplace Career = 3 if strmatch(职业, \"*无业*\")\nreplace Career = 4 if Career == .\n\n*- 是否共病\ngen Comorbidity = 1 if comorbidity == 1\nreplace Comorbidity = 0 if comorbidity == 0\n\n* 疾病类型按照 ICD-10 划分\ngen Disease = icd_chapter\n\ngen Insurance = 1 if strmatch(医疗付费方式, \"*自费*\")\nreplace Insurance = 2 if strmatch(医疗付费方式, \"*商业*\")\nreplace Insurance = 3 if strmatch(医疗付费方式, \"*城乡居民*\")\nreplace Insurance = 3 if strmatch(医疗付费方式, \"*城镇居民*\")\nreplace Insurance = 4 if strmatch(医疗付费方式, \"*城镇职工*\")\nreplace Insurance = 5 if strmatch(医疗付费方式, \"*贫困救助*\")\nreplace Insurance = 6 if strmatch(医疗付费方式, \"*新型农村合作*\")\nreplace Insurance = 7 if strmatch(医疗付费方式, \"*全公费*\")\nreplace Insurance = 2 if Insurance == .\n\n*- 是否使用中药\ngen Cmedicine = 1 if 中成药费 &gt; 0\nreplace Cmedicine = 1 if 中草药费 &gt; 0\nreplace Cmedicine = 0 if Cmedicine == .\n\n*- 入院途径\ngen category = 1 if 入院途径 == \"门诊\"\nreplace category = 0 if 入院途径 == \"急诊\"\n\n*- 自付金额\ngen SelfCost = 自付金额 / 次数\n\n*- 除去空值变量\ndrop 其他费用 病理诊断费 临床物理治疗费 手术治疗费 康复费 中医治疗费 抗菌药物费用 白蛋白类制品费 球蛋白类制品费 凝血因子类制品费 细胞因子类制品费 检查用一次性医用材料费 治疗用一次性医用材料费 手术用一次性医用材料费\n\n*- 费用变量数据\ngen GService = 一般医疗服务费 / 次数\n* gen GOperate = 一般治疗操作费 / 次数\ngen GSurgery = 手术费 / 次数\ngen GNurse = 护理费 / 次数\ngen GNonoperate = 非手术治疗项目费 / 次数\ngen GNarcosis = 麻醉费 / 次数\ngen GDrug = 西药费 / 次数\ngen GBlood = 血费 / 次数\ngen Others = 其他费 / 次数\n// 计算行总和\negen temp_total_diagnose = rowtotal(实验室诊断费 影像学诊断费 临床诊断项目费)\n\n// 进行除法运算\ngen GDiagnose = temp_total_diagnose / 次数\n\n// 删除临时变量\ndrop temp_total_diagnose\n\n// 计算行总和\negen temp_total_cdrug = rowtotal(中成药费 中草药费)\n\n// 进行除法运算\ngen GCDrug = temp_total_cdrug / 次数\n\n// 删除临时变量\ndrop temp_total_cdrug\n\n*- 次均费用\negen Cost = rowtotal(GService GSurgery GNurse GNonoperate GNarcosis GDrug GBlood GDiagnose GCDrug Others) \n\n*- 对变量进行排序\norder id year Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others\n\n\n\n\n\n\n// 描述性统计 保留小数点后两位\nestpost summarize Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose, detail\nesttab, cells(\"count mean(fmt(2)) sd(fmt(2)) min(fmt(2)) p50(fmt(2)) max(fmt(2))\") noobs compress replace title(Descriptive statistics)\nesttab using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result\\描述性统计0218.rtf\", cells(\"count mean(fmt(2)) sd(fmt(2)) min(fmt(2)) p50(fmt(2)) max(fmt(2))\") noobs compress replace title(Descriptive statistics)\n\n\n// 全样本费用指标\ntabstat Cost SelfCost Day GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others, s(mean) by(year)\n\n// 变量指标\ntabstat Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category, s(mean) by(year)\n\n* 计算所有年份的均值\nsummarize Cost SelfCost Day GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others\n\n\n\n为了稳健性，对因变量进行缩尾处理。\n*- 缩尾处理（目的：剔除异常值；剔除的比例根据研究而定）\nwinsor2 Cost Day Age, replace cuts(1, 99)\n\n// 全局暂元\nglobal Control Age Gender Career Marriage category Disease Opera Comorbidity Cmedicine Insurance \n\n// 基准模型\nreg Cost DIP $Control, r\nest store m1\nreg SelfCost DIP $Control, r\nest store m2\nreg Day DIP $Control Sensitive, r // Sensitive 只在Day的模型中出现\nest store m3\nreg Cost DIP $Control Day, r\nest store m4\nreg SelfCost DIP $Control Day, r\nest store m5\n\n* 输出基准模型结果\nesttab m1 m2 m3 m4 m5 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\基准模型结果.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\n// 调节效应分析\nglobal Control Age Gender Career Marriage Disease Opera Comorbidity Cmedicine Insurance\n\nreg Cost DIP $Control Day if category == 1, r\nest store m1\nreg Cost DIP $Control Day if category == 0, r\nest store m2\n\n\n*- esttab m1 m2 m3 using \nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-cost&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\nreg SelfCost DIP $Control Day if category==1, r\nest store m1\nreg SelfCost DIP $Control Day if category==0, r\nest store m2\n\n\nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-self&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\nreg Day DIP $Control Sensitive if category==1, r\nest store m1\nreg Day DIP $Control Sensitive if category==0, r\nest store m2\n\n\nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-day&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#必须配置",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#必须配置",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "运行此文档需要电脑上以安装Python，并且下列包已被安装并且能被调用：\nnumpy jupyter-cache pandas openpyxl\n你可以使用 pip 或 conda 进行安装： pip install jupyter-cache",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#初步认识数据",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#初步认识数据",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "我们可以使用pandas包来查看部分原始数据，数据的基本样式如下：\n\n# 安装并加载必要的包\nimport pandas as pd\nimport numpy as np\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\ndata = pd.read_excel(file_path, sheet_name=0, engine='openpyxl')     \n\n# 数据脱敏，删除地方\ncolumns_to_drop = [\"籍贯\", \"出生地\"]\ndata = data.drop(columns=columns_to_drop, errors='ignore')  # errors='ignore' 防止列不存在时报错\n\n# 随机抽取10个样本数据\nsample_data = data.sample(n=10, random_state=42)\n\n# 打印样本数据\nprint(sample_data)\n\n      次数        出生日期 性别   年龄      医疗付费方式  国籍 新生儿出生体重 新生儿入院体重  民族     职业  ...  \\\n1138   1  1956-09-06  男  61岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n2024   1  1959-08-15  女  59岁  城镇居民基本医疗保险  中国       －       －  汉族     居民  ...   \n1605   1  1973-02-05  女  45岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1975  11  2012-09-20  男   6岁    新型农村合作医疗  中国       －       －  汉族  学龄前儿童  ...   \n1701   1  1963-01-23  男  55岁         全自费  中国       －       －  汉族      无  ...   \n218    1  1969-06-08  女  48岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1344   1  1960-09-11  女  57岁    新型农村合作医疗  中国       －       －  汉族     务农  ...   \n252    1  1944-05-01  女  73岁    新型农村合作医疗  中国       －       －  汉族     农民  ...   \n1921   5  2013-10-17  女   5岁    新型农村合作医疗  中国       －       －  汉族  学龄前儿童  ...   \n643    1  1997-09-02  男  20岁         全自费  中国       －       －  汉族     战士  ...   \n\n     麻醉开始时间3 麻醉结束时间3 麻醉方式3 麻醉分级3  切口部位3 切口等级3 NNIS分级3  手术部位感染3  术前住院天数3  \\\n1138     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n2024     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1605     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1975     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1701     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n218      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1344     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n252      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n1921     NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n643      NaN     NaN   NaN   NaN    NaN   NaN     NaN      NaN      NaN   \n\n     手术持续时间3  \n1138     NaN  \n2024     NaN  \n1605     NaN  \n1975     NaN  \n1701     NaN  \n218      NaN  \n1344     NaN  \n252      NaN  \n1921     NaN  \n643      NaN  \n\n[10 rows x 200 columns]\n\n\n我们可以看到，该数据的列很多，第一张表中有200列，我们需要对其进行一些筛选。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#对数据的思考",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#对数据的思考",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "从哪里开始是一个需要思考的问题，对于数据的认识决定了你处理问题的方向和效率。首先，理解数据的来源至关重要，这包括了解数据是如何收集的、收集过程中可能出现的偏差或错误。其次，明确数据的类型与结构也是关键步骤之一，不同类型的数据（如定量数据、定性数据）需要采用不同的分析方法。再者，对数据进行初步探索，比如通过可视化手段观察数据分布特征，或是计算一些基本统计量来了解数据的基本情况，能够帮助你更好地制定数据处理策略。\n在真正开始处理数据之前，还需要考虑你的目标是什么。是为了回答一个具体的问题，还是为了探索潜在的模式？明确了目标之后，才能有针对性地选择合适的工具和技术。此外，考虑到数据质量的问题，数据清洗是不可跳过的一步，它包括去除异常值、填补缺失值等操作，这对于提高分析结果的准确性非常关键。\n最后，保持对数据伦理的关注同样重要，在整个数据分析的过程中，确保遵循相关的隐私保护法规和道德标准，这样才能确保你的工作不仅有效，而且负责任。通过对数据全面而深刻的理解，你可以更加自信地从数据中提取有价值的信息，并为决策提供有力支持。\n\n\n这份Excel文件有6张sheet，分别是2018-2023年，首先需要检查这六张sheet中的变量是否一致：\n\nimport pandas as pd\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\nsheet_names = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n\n# 读取所有 sheet 的数据\nsheets_data = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}\n\n# 获取每个 sheet 的列名\nsheets_columns = {sheet: set(data.columns) for sheet, data in sheets_data.items()}\n\n# 找出所有 sheet 的共同变量和不一致的变量\ncommon_columns = set.intersection(*sheets_columns.values())\nall_columns = set.union(*sheets_columns.values())\ninconsistent_columns = all_columns - common_columns\n\n# 打印结果\nprint(\"一致的变量名:\")\nprint(common_columns)\n\nprint(\"\\n不一致的变量名:\")\nprint(inconsistent_columns)\n\n# 打印每个 sheet 的变量\nfor sheet, columns in sheets_columns.items():\n    print(f\"\\n{sheet} 的变量: {columns}\")\n\n一致的变量名:\n{'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '1愈合', '死亡患者尸检', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '3级别', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '级别1'}\n\n不一致的变量名:\n{'愈合2', '手术开始时间3', '7手术编码', '7级别', '7切口', '手术部位感染3', '手术次数3', '8麻醉方式', '7手术时间', '手术持续时间3', '是否微创手术2', '8手术时间', '8愈合', '4麻醉方式', '麻醉开始时间3', '术前住院天数3', '切口部位3', 'NNIS分级3', '8切口', '麻醉结束时间3', '麻醉分级3', '8手术编码', '切口等级3', '手术操作名称3', '麻醉方式3', '手术操作编码3', '8级别', '4麻醉医师', '8手术名称', '7愈合', '手术结束时间3', '择期手术3', '7手术名称', '7麻醉方式'}\n\n2018 的变量: {'目的', '愈合2', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '手术开始时间3', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉结束时间3', '麻醉开始时间1', '切口等级3', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '麻醉方式3', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '2切口', '手术部位感染3', '3手术时间', '是否有使用抗菌药物1', '入院日期', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '麻醉开始时间3', '6愈合', '切口部位3', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '手术操作名称3', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '手术操作编码3', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '手术结束时间3', '1麻醉方式', '3.1非手术治疗项目费', '择期手术3', '4愈合', '1手术名称', '病案质量', '5级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '3级别', '手术次数3', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '手术持续时间3', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '是否微创手术2', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '4麻醉方式', '3.3手术治疗费', '术前住院天数3', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '麻醉分级3', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', 'NNIS分级3', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '级别1'}\n\n2019 的变量: {'目的', '愈合2', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '手术开始时间3', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '手术操作名称3', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '手术操作编码3', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '手术结束时间3', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '手术次数3', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '是否微创手术2', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '输液反应次数', '4麻醉方式', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2020 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '8愈合', '6手术编码', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2021 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2022 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '是否非计划重返手术室病例1', '血管介入治疗抗菌药物使用天数', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '4麻醉方式', '3.3手术治疗费', '输液反应次数', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n2023 的变量: {'目的', '断脐后预防性使用抗菌药物给药时间1', '6麻醉方式', '死亡患者尸检', '1愈合', '入院途径', '2.4临床诊断项目费', '清洁手术预防使用抗菌药物总天数', '麻醉结束时间1', '麻醉结束时间2', '术前使用预防性抗菌药物1', '10.其他费', '新生儿出生体重', '6.2抗菌药物费用', '7.2中草药费', '出生地', '抗菌药物使用天数', '1级别', '新生儿入院体重', '5.中医治疗费', '手术持续时间1', '术前住院天数2', '切口部位1', '病室', '性别', '病室.1', '是否有出院31天内再住院计划', '3愈合', '总药品费', '9.3手术用一次性医用材料费', '手术开始时间2', '术前使用预防性抗菌药物2', '入院诊断', '手术次数2', '断脐后预防性使用抗菌药物给药时间2', '麻醉开始时间1', '是否有使用抗菌药物2', '麻醉分级2', '3麻醉方式', '职业', '2愈合', '手术部位感染2', '5愈合', '术前预防性抗菌药物给药时间1', '1.3护理费', '输血反应', '1.1一般医疗服务费', 'RH', '2手术编码', '8.4凝血因子类制品费', '9.1检查用一次性医用材料费', '术前预防性抗菌药物给药时间2', '1手术时间', '手术操作名称1', '是否非计划重返手术室病例2', '7手术编码', '2切口', '3手术时间', '是否有使用抗菌药物1', '入院日期', '7手术时间', '出院诊断', '手术次数1', '血管介入治疗', '4手术编码', '6切口', '输液反应', '3.4麻醉费', '麻醉分级1', '6愈合', '患者入住重症监护室（ICU）情况', '3手术编码', '预防性抗菌药物使用时机2', '入院科别', '是否微创手术1', '药占比%', '5切口', '腔镜手术名称2', '6.1西药费', '切口等级1', '出院科别', '4切口', '出生日期', '2手术名称', '输血反应次数', '感染情况', '3手术名称', '出院日期', '细菌名称2', '4麻醉医师', '血管介入治疗抗菌药物使用天数', '是否非计划重返手术室病例1', '手术预防性使用抗菌药物天数2', '1麻醉方式', '3.1非手术治疗项目费', '4愈合', '1手术名称', '5级别', '病案质量', '7级别', '7切口', '3级别', '1.2一般治疗操作费', '术后预防性抗菌药物结束时间2', '4手术名称', '手术操作名称2', '国籍', '是否在术后使用预防性抗菌药物2', '7.1中成药费', '8.1血费', '医疗付费方式', '9.2治疗用一次性医用材料费', '本次住院期间有无重返手术室的计划1', '6手术时间', '5手术名称', '自付金额', '离院方式', '6手术名称', '择期手术1', '手术操作编码2', '病理诊断', '细菌名称4', '6手术编码', '8愈合', '输液反应次数', '3.3手术治疗费', '手术部位感染1', '次数', '2麻醉方式', '3切口', '是否临床路径', '婚姻', '2.1病理诊断费', '入院前颅脑损伤昏迷时间', '本次住院期间有无重返手术室的计划2', '8手术编码', '手术操作编码1', '麻醉开始时间2', '3.5手术费', '预防性抗菌药物使用时机1', 'NNIS分级2', '3.2临床物理治疗费', '6级别', 'NNIS分级1', '8手术名称', '7愈合', '5手术时间', '2手术时间', '术后预防性抗菌药物结束时间1', '7麻醉方式', '手术持续时间2', '1.4其他费用', '是否在术后使用预防性抗菌药物1', '手术结束时间1', '择期手术2', '麻醉方式1', '手术开始时间1', '8麻醉方式', '切口等级2', '切口部位2', '年龄', '4.康复费', '民族', '血型', '术前住院天数1', '5手术编码', '2.2实验室诊断费', '院内感染', '1切口', '籍贯', '8手术时间', '愈合1', '住院天数', '4级别', '总费用', '清洁手术预防使用抗菌药物品种数', '1手术编码', '手术预防性使用抗菌药物天数1', '8.3球蛋白类制品费', '腔镜手术名称1', '手术结束时间2', '2级别', '8切口', '药物过敏', '5麻醉方式', '2.3影像学诊断费', '麻醉方式2', '4手术时间', '入院后颅脑损伤昏迷时间', '8级别', '级别2', '8.2白蛋白类制品费', '细菌名称1', '是否药物过敏', '细菌名称3', '8.5细胞因子类制品费', '7手术名称', '级别1'}\n\n\n然后剔除不一致的变量数据，同时创建一个新变量year，用sheet的年份对其进行填充，再按变量名对应合并6张表格的数据称为一张总表，命名为merge-sheet.xlsx输出到你需要存放数据的文件夹中。\n变量还是太多了，那接下来对变量进行筛选，首先我们可以对所有键值为空的变量进行剔除，或者根据实际的研究需要，剔除一部分键值全部为null的变量。\n这里我选择对键值全部为null或0的变量进行剔除。\n第一次尝试的时候，打开表后进行查看，发现变量顺序很乱，没有按照原始顺序进行排列，处理办法则是在前面的变量筛选部分使用DataFrame的loc方法选择列，同时保持列的顺序。\n同时为了节省时间，因为在Quarto中运行Python代码很慢，暂时还不知道原因，待以后调试一下。所以最后用一个程序解决上述这些问题，节省时间。\n\nimport pandas as pd\n\n# 导入 Excel 文件\nfile_path = \"C:/Users/asus/Desktop/test/stata/prepare.xlsx\"\noutput_path = \"C:/Users/asus/Desktop/test/stata/data/merge-data.xlsx\"\nfinal_output_path = \"C:/Users/asus/Desktop/test/stata/data/cleaned-merge-data.xlsx\"\nsheet_names = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n\n# 读取所有 sheet 的数据\nsheets_data = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}\n\n# 获取每个 sheet 的列名\nsheets_columns = {sheet: set(data.columns) for sheet, data in sheets_data.items()}\n\n# 找出所有 sheet 的共同变量\ncommon_columns = set.intersection(*sheets_columns.values())\n# 保持原始顺序\ncommon_columns = list(common_columns)  \n\n# 剔除不一致的变量数据，并添加 year 变量\nfor sheet, data in sheets_data.items():\n    sheets_data[sheet] = data[list(common_columns)]\n    sheets_data[sheet]['year'] = sheet\n\n# 合并所有 sheet 的数据\nmerged_data = pd.concat(sheets_data.values(), ignore_index=True)\n\n# 输出合并后的数据到指定路径\nmerged_data.to_excel(output_path, index=False)\n\n# 重新导入合并后的数据\nmerged_data = pd.read_excel(output_path)\n\n# 剔除键值全部为 null 或 0 的变量，同时保持原始变量的顺序\nnon_null_columns = merged_data.dropna(axis=1, how='all').columns\nnon_zero_columns = merged_data.loc[:, (merged_data != 0).any(axis=0)].columns\nvalid_columns = [col for col in merged_data.columns if col in non_null_columns and col in non_zero_columns]\n\ncleaned_data = merged_data.loc[:, valid_columns]\n\n# 输出清理后的数据到指定路径\ncleaned_data.to_excel(final_output_path, index=False)\n\nprint(f\"清理后的数据已输出到 {final_output_path}\")\n\n# 展示部分数据\n\n# 随机抽取10个样本数据\nsample_data = cleaned_data.sample(n=10)\n\n# 打印样本数据\nprint(sample_data)\n\n清理后的数据已输出到 C:/Users/asus/Desktop/test/stata/data/cleaned-merge-data.xlsx\n      目的 断脐后预防性使用抗菌药物给药时间1 6麻醉方式  1愈合 死亡患者尸检 入院途径  2.4临床诊断项目费  \\\n14170  -               NaN   NaN  NaN    NaN   急诊      1553.0   \n20116  -               NaN   NaN  NaN    NaN   门诊      1272.5   \n37053  -               NaN   NaN  NaN    NaN   门诊         0.0   \n43824  -               NaN   NaN    甲      否   门诊      1106.0   \n32348  -               NaN   NaN  NaN    NaN   门诊       398.5   \n37403  -               NaN   NaN  NaN    NaN   门诊        29.0   \n31026  -               NaN   NaN    甲    NaN   门诊      1814.0   \n12348  -               NaN   NaN    乙    NaN   门诊       592.0   \n28312  -               NaN   NaN   其他    NaN   门诊      2022.5   \n35851  -               NaN   NaN   其他    NaN   门诊      1417.5   \n\n       清洁手术预防使用抗菌药物总天数              麻醉结束时间1 麻醉结束时间2  ...  2.3影像学诊断费 麻醉方式2  \\\n14170              NaN                  NaN     NaN  ...          0   NaN   \n20116              NaN                  NaN     NaN  ...          0   NaN   \n37053              NaN                  NaN     NaN  ...          0   NaN   \n43824              1.0  2023-07-07 12:05:33     NaN  ...        256   NaN   \n32348              NaN                  NaN     NaN  ...          0   NaN   \n37403              NaN                  NaN     NaN  ...          0   NaN   \n31026              NaN                  NaN     NaN  ...          0   NaN   \n12348              NaN                  NaN     NaN  ...          0   NaN   \n28312              NaN                  NaN     NaN  ...        640   NaN   \n35851              NaN  2022-07-07 16:25:49     NaN  ...          0   NaN   \n\n       4手术时间 入院后颅脑损伤昏迷时间  级别2  细菌名称1 是否药物过敏 细菌名称3  级别1  year  \n14170    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n20116    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n37053    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n43824    NaN      -天-时-分  NaN      -      无     -  4.0  2023  \n32348    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n37403    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2022  \n31026    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2021  \n12348    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2020  \n28312    NaN      -天-时-分  NaN    NaN      无   NaN  NaN  2021  \n35851    NaN      -天-时-分  NaN      -      无     -  2.0  2022  \n\n[10 rows x 160 columns]",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#筛选变量",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#筛选变量",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "经过上述筛选后的变量依然还有很多，其中不乏无用信息变量或无效信息变量，对变量做进一步筛选。\n对样本进行筛选，需要满足在当年收治且出院，满足常规医保使用条件，关键变量含有缺失值的样本。\n使用 pandas 进行处理（根据Excel视图挑选的缺失数据的变量或含有较多缺失值的变量）：\n\nimport pandas as pd\n\n# 文件路径\ninput_file = r\"C:\\Users\\asus\\Desktop\\test\\stata\\data\\cleaned-merge-data.xlsx\"\noutput_file = r\"C:\\Users\\asus\\Desktop\\test\\stata\\data\\allclean.xlsx\"\n\n# 读取 Excel 文件\ndf = pd.read_excel(input_file)\n\n# 要删除的列列表\ncolumns_to_drop = [\n    \"新生儿出生体重\", \"新生儿入院体重\", \"国籍\", \"籍贯\", \"病室\", \"病室.1\", \"是否有出院31天内再住院计划\",\n    \"病理诊断\", \"院内感染\", \"药物过敏\", \"死亡患者尸检\", \"血型\", \"RH\",\n    \"1手术编码\", \"1手术时间\", \"1级别\", \"1切口\", \"1愈合\", \"1麻醉方式\",\n    \"2手术名称\", \"2手术编码\", \"2手术时间\", \"2级别\", \"2切口\", \"2愈合\", \"2麻醉方式\",\n    \"3手术名称\", \"3手术编码\", \"3手术时间\", \"3级别\", \"3切口\", \"3愈合\", \"3麻醉方式\",\n    \"4手术名称\", \"4手术编码\", \"4手术时间\", \"4级别\", \"4切口\", \"4愈合\", \"4麻醉方式\",\n    \"5手术名称\", \"5手术编码\", \"5手术时间\", \"5级别\", \"5切口\", \"5愈合\", \"5麻醉方式\",\n    \"6手术名称\", \"6手术编码\", \"6手术时间\", \"6级别\", \"6切口\", \"6愈合\", \"6麻醉方式\",\n    \"7手术名称\", \"7手术编码\", \"7手术时间\", \"7级别\", \"7切口\", \"7愈合\", \"7麻醉方式\",\n    \"8手术名称\", \"8手术编码\", \"8手术时间\", \"8级别\", \"8切口\", \"8愈合\", \"8麻醉方式\",\n    \"目的\", \"入院前颅脑损伤昏迷时间\", \"入院后颅脑损伤昏迷时间\",\n    \"抗菌药物使用天数\", \"清洁手术预防使用抗菌药物品种数\", \"是否临床路径\", \"清洁手术预防使用抗菌药物总天数\",\n    \"患者入住重症监护室（ICU）情况\", \"感染情况\", \"输血反应\", \"输血反应次数\", \"输液反应\", \"输液反应次数\",\n    \"细菌名称1\", \"细菌名称2\", \"细菌名称3\", \"细菌名称4\", \"血管介入治疗\", \"血管介入治疗抗菌药物使用天数\",\n    \"手术次数1\", \"手术操作名称1\", \"手术操作编码1\", \"手术开始时间1\", \"手术结束时间1\", \"择期手术1\",\n    \"麻醉开始时间1\", \"麻醉结束时间1\", \"麻醉方式1\", \"麻醉分级1\", \"切口部位1\", \"切口等级1\", \"NNIS分级1\",\n    \"手术部位感染1\", \"术前住院天数1\", \"手术持续时间1\", \"是否非计划重返手术室病例1\", \"术前使用预防性抗菌药物1\",\n    \"术前预防性抗菌药物给药时间1\", \"是否在术后使用预防性抗菌药物1\", \"术后预防性抗菌药物结束时间1\",\n    \"手术预防性使用抗菌药物天数1\", \"是否有使用抗菌药物1\", \"预防性抗菌药物使用时机1\",\n    \"断脐后预防性使用抗菌药物给药时间1\", \"本次住院期间有无重返手术室的计划1\", \"腔镜手术名称1\", \"级别1\", \"愈合1\",\n    \"是否微创手术1\", \"手术次数2\", \"手术操作名称2\", \"手术操作编码2\", \"手术开始时间2\", \"手术结束时间2\",\n    \"择期手术2\", \"麻醉开始时间2\", \"麻醉结束时间2\", \"麻醉方式2\", \"麻醉分级2\", \"切口部位2\", \"切口等级2\",\n    \"NNIS分级2\", \"手术部位感染2\", \"术前住院天数2\", \"手术持续时间2\", \"是否非计划重返手术室病例2\",\n    \"术前使用预防性抗菌药物2\", \"术前预防性抗菌药物给药时间2\", \"是否在术后使用预防性抗菌药物2\",\n    \"术后预防性抗菌药物结束时间2\", \"手术预防性使用抗菌药物天数2\", \"是否有使用抗菌药物2\",\n    \"预防性抗菌药物使用时机2\", \"断脐后预防性使用抗菌药物给药时间2\", \"本次住院期间有无重返手术室的计划2\",\n    \"腔镜手术名称2\", \"级别2\", \"愈合2\", \"是否微创手术2\", \"手术次数3\", \"手术操作名称3\", \"手术操作编码3\", \"手术开始时间3\",\n    \"手术结束时间3\", \"择期手术3\", \"麻醉开始时间3\", \"麻醉结束时间3\", \"麻醉方式3\", \"麻醉分级3\", \"切口部位3\", \"切口等级3\",\n    \"NNIS分级3\", \"手术部位感染3\", \"术前住院天数3\", \"手术持续时间3\", \"4麻醉医师\", \"出生地\", \"籍贯\"\n]\n\n# 删除指定的列\ndf = df.drop(columns=columns_to_drop, errors='ignore')\n\n# 过滤掉 '公安病区'\nif '入院科别' in df.columns and '出院科别' in df.columns:\n    df = df[~df['入院科别'].isin(['公安病区'])]\n    df = df[~df['出院科别'].isin(['公安病区'])]\n\n# 打印随机 10 个样本\nprint(\"随机 10 个样本：\")\nprint(df.sample(10))\n\n# 将处理后的 DataFrame 写入新的 Excel 文件\n# df.to_excel(output_file, index=False)\n\n# print(f\"数据清洗完成，已保存到 {output_file}\")\n\n随机 10 个样本：\n      入院途径  2.4临床诊断项目费   10.其他费  7.2中草药费 性别    总药品费  \\\n45540   门诊       225.0    28.98      0.0  女   22.17   \n27023   门诊      1105.0  9923.46      0.0  男   77.87   \n10102   门诊       178.3  3442.00      0.0  女  168.30   \n29291   门诊       977.5   511.57      0.0  女  626.99   \n18117   门诊       624.0   139.90      0.0  女  526.41   \n48037   门诊       264.9  3286.00      0.0  男   88.97   \n35602   门诊       201.0   336.92      0.0  男  723.32   \n17413   门诊      1278.5   357.29      0.0  女  702.13   \n966     门诊         0.0    24.75      0.0  女    0.00   \n14589   门诊       147.3  4724.00      0.0  女  196.45   \n\n                                                    入院诊断     职业  1.3护理费  \\\n45540                       老年核性白内障|H25.100,翼状胬肉|H11.000     农民    25.0   \n27023              结肠恶性肿瘤个人史|Z85.006,手术后恶性肿瘤化学治疗|Z51.102     农民    75.0   \n10102                       老年性白内障|H25.900,玻璃体混浊|H43.300     农民    50.0   \n29291                           结肠息肉|K63.500,胃息肉|K31.703  自由职业者   196.0   \n18117  大疱性类天疱疮|L12.000,冠状动脉粥样硬化性心脏病|I25.103,心功能Ⅲ级|I50...     居民    85.0   \n48037                                    老年核性白内障|H25.100     农民    50.0   \n35602                       节肢动物咬伤|T63.402,过敏性皮炎|L23.901     农民   200.6   \n17413                                         腹痛|R10.400     居民   125.0   \n966                                      脑外伤后综合征|F07.201      -   156.0   \n14589  老年性白内障|H25.900,翼状胬肉|H11.000,玻璃体混浊|H43.300,特指手术...     农民    50.0   \n\n       1.1一般医疗服务费  ...  婚姻  3.5手术费   年龄  民族 2.2实验室诊断费  住院天数       总费用  \\\n45540        32.0  ...  已婚     0.0  61岁  汉族     372.0     1    716.25   \n27023       105.0  ...  已婚     0.0  29岁  汉族     499.0     3  11888.53   \n10102        52.0  ...  已婚  1976.0  76岁  汉族     387.0     2   6314.60   \n29291       245.0  ...  已婚     0.0  67岁  汉族     589.0     7  11239.16   \n18117       294.0  ...  已婚     0.0  92岁  汉族    1095.0     3   3133.31   \n48037        32.0  ...  已婚  1976.0  77岁  汉族     381.0     1   6164.87   \n35602       140.0  ...  已婚     0.0  56岁  汉族     731.0     4   2475.84   \n17413       160.0  ...  已婚     0.0  60岁  汉族     608.0     5   4195.02   \n966         414.0  ...  未婚     0.0   8岁  汉族       0.0    13   7004.75   \n14589        52.0  ...  已婚  1976.0  78岁  汉族       0.0     2   7166.75   \n\n      2.3影像学诊断费 是否药物过敏  year  \n45540         0      无  2023  \n27023         0      无  2021  \n10102        36      无  2019  \n29291         0      无  2021  \n18117         0      无  2020  \n48037        36      无  2023  \n35602         0      无  2022  \n17413         0      无  2020  \n966           0      无  2018  \n14589         0      无  2020  \n\n[10 rows x 39 columns]",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#使用-stata-赋值和分析",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#使用-stata-赋值和分析",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "因为需要对疾病进行分类与根据诊断信息确定来生成共病信息，根据出院诊断来对疾病进行分类与赋值，这里使用 Stata 来完成。\n**************\n* 1. 清理环境并导入数据\n**************\nclear all\n\n* 读取 Excel 文件，假设第一行为列名\nimport excel \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\allclean.xlsx\", ///\n    firstrow case(lower) clear\n\n* 注意：\n*  - firstrow 表示将第一行作为变量名\n*  - case(lower) 将变量名转换为小写，避免中文或大小写冲突\n*  - 如果您的表格存在中文列名，可能需要手动 rename\n\n************************\n* 2. 处理、提取与分类: 以“出院诊断”列为例\n************************\n\n*------------------\ngen disease = 出院诊断\n*------------------\n\n* 假设您已经将\"出院诊断\"重命名为了 \"disease\"\n* 现在要从 disease 里提取 ICD 编码到 icd10 列。\n* 如果原数据已包含 icd10 这列，可跳过此步。\n* 这里只是示例，具体提取逻辑需根据实际字符串格式做 parsing:\n* 例如： 出院诊断 字符串为 \"急性化脓性阑尾炎|K35.902|有,高血压病|I10.x00|有\"\n\n*（示例）如果 disease 形如 \"XXX|K35.902|有,YYY|I10.x00|有\"\n* 可以先把逗号换成某种分隔，然后再拆分，这里仅给示例逻辑\n* 注意：以下只是思路示例，可能需正则表达式、substr、split 等更复杂处理\n\n// 对disease进行拆分\n* 1. 按 | 分隔 disease 列，生成多个新变量\nsplit disease, parse(\"|\") generate(disease_part)\n \n// 提取第一个 ICD 编码\n* 2. 提取第二部分（part2）作为 icd10，使用正则表达式剔除多余编码\n* 保留 disease_part2 的前7个字符作为 icd10，形如 C15.900\ngen icd10 = substr(disease_part2, 1, 7)\ngen icd_com = substr(disease_part4, 1, 7)\n* 去除前后的空格\nreplace icd10 = trim(icd10)\n\n* 3. 删除所有拆分部分\ndrop disease_part1-disease_part55\n\n* 4. 检查结果\nlist disease icd10 in 1/10\n\n***************************************\n* 按 ICD 数量判断是否共病\n***************************************\ngen comorbidity = 0  // 初始值为 0\nreplace comorbidity = 1 if !missing(icd10) & !missing(icd_com) \n// 如果ICD10和ICD_com都不为空，则赋值为1\n\n// 查看前10行的数据\nlist icd10 icd_com comorbidity in 1/10\n\n***************************************\n* 筛除部分变量\n***************************************\n\ndrop 入院日期 入院科别 出院日期 出院科别 出院诊断 disease 离院方式 病案质量\n\n***************************************\n* 按 ICD 编码生成截取变量\n***************************************\n\n* 如果 icd10 是数值型，转换为字符串型\ntostring icd10, replace  \n\n* 检查并创建 icd_3c 变量\ngen icd_3c = \"\"   // 如果 icd_3c 不存在，创建一个空的字符串变量\n\n* 截取 icd10 的前三位并赋值给 icd_3c\nreplace icd_3c = substr(icd10, 1, 3)  \n\n* 如果 icd_3c 是数值型，转换为字符串型\ntostring icd_3c, replace \n\n* 创建 icd_str1 变量\ngen icd_str1 = \"\"\n\n* icd_str1: ICD 编码首位\nreplace icd_str1 = substr(icd10,1,1)\n\n* 如果 icd_str1 是数值型，转换为字符串型\ntostring icd_str1, replace \n\n* 使用 trim() 来去除空格\nreplace icd_str1 = trim(icd_str1)\n\n* 查看 icd_str1 的数据类型\ndescribe icd_str1\n\n* 查看是否有空值或特殊字符\n* list icd_str1 if missing(icd_str1)\n\n************************\n* 按照ICD编码归为22类\n************************\n\n* 创建icd分类变量：icd_chapter\ngen icd_chapter = \"\" \n\n* 字符转换为数值\ndestring icd_chapter,replace\n\n// 为icd_chapter赋值\n\nreplace icd_chapter=1 if icd_str1==\"A\"|icd_str1==\"B\"\nreplace icd_chapter=2 if icd_str1==\"C\"|(icd_3c&gt;=\"D00\"&icd_3c&lt;=\"D48\")\nreplace icd_chapter=3 if icd_3c&gt;=\"D50\"&icd_3c&lt;=\"D89\"\nreplace icd_chapter=4 if icd_3c&gt;=\"E00\"&icd_3c&lt;=\"E90\"\nreplace icd_chapter=5 if icd_3c&gt;=\"F00\"&icd_3c&lt;=\"F99\"\nreplace icd_chapter=6 if icd_3c&gt;=\"G00\"&icd_3c&lt;=\"G99\"\nreplace icd_chapter=7 if icd_3c&gt;=\"H00\"&icd_3c&lt;=\"H59\"\nreplace icd_chapter=8 if icd_3c&gt;=\"H60\"&icd_3c&lt;=\"H99\"\nreplace icd_chapter=9 if icd_str1==\"I\"\nreplace icd_chapter=10 if icd_str1==\"J\"\nreplace icd_chapter=11 if icd_str1==\"K\"\nreplace icd_chapter=12 if icd_str1==\"L\"\nreplace icd_chapter=13 if icd_str1==\"M\"\nreplace icd_chapter=14 if icd_str1==\"N\"\nreplace icd_chapter=15 if icd_str1==\"O\"\nreplace icd_chapter=16 if icd_str1==\"P\"\nreplace icd_chapter=17 if icd_str1==\"Q\"\nreplace icd_chapter=18 if icd_str1==\"R\"\nreplace icd_chapter=19 if icd_str1==\"S\"| icd_str1==\"T\"\nreplace icd_chapter=20 if icd_str1==\"V\"| icd_str1==\"Y\"\nreplace icd_chapter=21 if icd_str1==\"Z\"\nreplace icd_chapter=22 if icd_str1==\"U\"\nreplace icd_chapter=20 if icd_str1==\"V\"| icd_str1==\"Y\"|icd_str1==\"X\"|icd_str1==\"W\"\nreplace icd_chapter=21 if icd_str1==\"Z\"| substr(trim(icd10),1,2)==\"WW\"\nreplace icd_chapter=22 if icd_str1==\"U\"\n\n***************************************\n* 检查分类缺失\n***************************************\ntab icd_3c if icd_chapter==.\n\n****************************\n* 3. 导出处理后的数据\n****************************\n\n* 导出为 Stata 格式\nsave \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\ICD-result.dta\", replace\n\n\n\n// 数据处理\nclear all\nuse \"C:\\Users\\asus\\Desktop\\test\\stata\\data\\ICD-result.dta\",clear\n\ncapture drop Cost  // 捕获可能发生的错误，如果变量不存在则继续执行\n\n* 创建 id 变量并赋值\ngen id = _n\n\nsort id year\nxtset id year\n\n*- 次均费用，需要查看总费用是否和各项目费用加总一致，此处不一致\n* gen Cost = 总费用 / 次数 \n\n*- 住院天数\ngen Day = 住院天数\n\n*- DIP政策\ngen DIP = .\nreplace DIP = 0 if year == 2018\nreplace DIP = 0 if year == 2019\nreplace DIP = 0 if year == 2020\nreplace DIP = 0 if year == 2021\nreplace DIP = 1 if year == 2022\nreplace DIP = 1 if year == 2023\n\n*- 控制变量序列\n*- 年龄\ngen Age = 年龄\n\n*- 性别（虚拟变量；当受访者性别为女性时，赋值为\"0\"，否则为\"1\"）\ngen Gender = 0 if 性别 == \"女\"\nrecode Gender .= 1\n\n*- 婚姻（虚拟变量；当受访者已婚时，赋值为\"1\"，否则为\"0\"）\ngen Marriage = 1 if 婚姻 == \"已婚\"\nrecode Marriage .= 0\n\n*- 药物过敏\ngen Sensitive = 1 if 是否药物过敏 == \"有\"\nrecode Sensitive .= 0\n\n*- 是否手术\ngen Opera = 1 if 手术费 &gt; 0\nreplace Cmedicine = 0 if 手术费 == 0\n\n*- 职业\ngen Career = 1 if strmatch(职业, \"*农*\")\nreplace Career = 2 if strmatch(职业, \"*职*\")\nreplace Career = 3 if strmatch(职业, \"*无业*\")\nreplace Career = 4 if Career == .\n\n*- 是否共病\ngen Comorbidity = 1 if comorbidity == 1\nreplace Comorbidity = 0 if comorbidity == 0\n\n* 疾病类型按照 ICD-10 划分\ngen Disease = icd_chapter\n\ngen Insurance = 1 if strmatch(医疗付费方式, \"*自费*\")\nreplace Insurance = 2 if strmatch(医疗付费方式, \"*商业*\")\nreplace Insurance = 3 if strmatch(医疗付费方式, \"*城乡居民*\")\nreplace Insurance = 3 if strmatch(医疗付费方式, \"*城镇居民*\")\nreplace Insurance = 4 if strmatch(医疗付费方式, \"*城镇职工*\")\nreplace Insurance = 5 if strmatch(医疗付费方式, \"*贫困救助*\")\nreplace Insurance = 6 if strmatch(医疗付费方式, \"*新型农村合作*\")\nreplace Insurance = 7 if strmatch(医疗付费方式, \"*全公费*\")\nreplace Insurance = 2 if Insurance == .\n\n*- 是否使用中药\ngen Cmedicine = 1 if 中成药费 &gt; 0\nreplace Cmedicine = 1 if 中草药费 &gt; 0\nreplace Cmedicine = 0 if Cmedicine == .\n\n*- 入院途径\ngen category = 1 if 入院途径 == \"门诊\"\nreplace category = 0 if 入院途径 == \"急诊\"\n\n*- 自付金额\ngen SelfCost = 自付金额 / 次数\n\n*- 除去空值变量\ndrop 其他费用 病理诊断费 临床物理治疗费 手术治疗费 康复费 中医治疗费 抗菌药物费用 白蛋白类制品费 球蛋白类制品费 凝血因子类制品费 细胞因子类制品费 检查用一次性医用材料费 治疗用一次性医用材料费 手术用一次性医用材料费\n\n*- 费用变量数据\ngen GService = 一般医疗服务费 / 次数\n* gen GOperate = 一般治疗操作费 / 次数\ngen GSurgery = 手术费 / 次数\ngen GNurse = 护理费 / 次数\ngen GNonoperate = 非手术治疗项目费 / 次数\ngen GNarcosis = 麻醉费 / 次数\ngen GDrug = 西药费 / 次数\ngen GBlood = 血费 / 次数\ngen Others = 其他费 / 次数\n// 计算行总和\negen temp_total_diagnose = rowtotal(实验室诊断费 影像学诊断费 临床诊断项目费)\n\n// 进行除法运算\ngen GDiagnose = temp_total_diagnose / 次数\n\n// 删除临时变量\ndrop temp_total_diagnose\n\n// 计算行总和\negen temp_total_cdrug = rowtotal(中成药费 中草药费)\n\n// 进行除法运算\ngen GCDrug = temp_total_cdrug / 次数\n\n// 删除临时变量\ndrop temp_total_cdrug\n\n*- 次均费用\negen Cost = rowtotal(GService GSurgery GNurse GNonoperate GNarcosis GDrug GBlood GDiagnose GCDrug Others) \n\n*- 对变量进行排序\norder id year Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/Python/2025-02-20-Medical-expenses.html#数据分析",
    "href": "Guide/Python/2025-02-20-Medical-expenses.html#数据分析",
    "title": "用Python和Stata处理一份卫生费用数据",
    "section": "",
    "text": "// 描述性统计 保留小数点后两位\nestpost summarize Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose, detail\nesttab, cells(\"count mean(fmt(2)) sd(fmt(2)) min(fmt(2)) p50(fmt(2)) max(fmt(2))\") noobs compress replace title(Descriptive statistics)\nesttab using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result\\描述性统计0218.rtf\", cells(\"count mean(fmt(2)) sd(fmt(2)) min(fmt(2)) p50(fmt(2)) max(fmt(2))\") noobs compress replace title(Descriptive statistics)\n\n\n// 全样本费用指标\ntabstat Cost SelfCost Day GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others, s(mean) by(year)\n\n// 变量指标\ntabstat Cost SelfCost Day DIP Age Gender Marriage Sensitive Opera Career Disease Comorbidity Insurance Cmedicine category, s(mean) by(year)\n\n* 计算所有年份的均值\nsummarize Cost SelfCost Day GService GSurgery GNurse GNonoperate GNarcosis GDrug GCDrug GBlood GDiagnose Others\n\n\n\n为了稳健性，对因变量进行缩尾处理。\n*- 缩尾处理（目的：剔除异常值；剔除的比例根据研究而定）\nwinsor2 Cost Day Age, replace cuts(1, 99)\n\n// 全局暂元\nglobal Control Age Gender Career Marriage category Disease Opera Comorbidity Cmedicine Insurance \n\n// 基准模型\nreg Cost DIP $Control, r\nest store m1\nreg SelfCost DIP $Control, r\nest store m2\nreg Day DIP $Control Sensitive, r // Sensitive 只在Day的模型中出现\nest store m3\nreg Cost DIP $Control Day, r\nest store m4\nreg SelfCost DIP $Control Day, r\nest store m5\n\n* 输出基准模型结果\nesttab m1 m2 m3 m4 m5 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\基准模型结果.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\n// 调节效应分析\nglobal Control Age Gender Career Marriage Disease Opera Comorbidity Cmedicine Insurance\n\nreg Cost DIP $Control Day if category == 1, r\nest store m1\nreg Cost DIP $Control Day if category == 0, r\nest store m2\n\n\n*- esttab m1 m2 m3 using \nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-cost&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\nreg SelfCost DIP $Control Day if category==1, r\nest store m1\nreg SelfCost DIP $Control Day if category==0, r\nest store m2\n\n\nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-self&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap\n\nreg Day DIP $Control Sensitive if category==1, r\nest store m1\nreg Day DIP $Control Sensitive if category==0, r\nest store m2\n\n\nesttab m1 m2 using \"C:\\Users\\asus\\Desktop\\test\\stata\\ICD-10\\25.02.09\\analysis-result0218\\调节效应结果-day&dip.rtf\", replace b(2) t(2) ar2 star(* 0.1 ** 0.05 *** 0.01) nogap",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python和Stata处理一份卫生费用数据"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html",
    "href": "Guide/R/2025-02-22-CLHLS.html",
    "title": "CLHLS Data Analysis by R",
    "section": "",
    "text": "这是一个使用 R 语言对 CLHLS 数据进行清洗和分析的工作文档。\n\n本文数据源来自北大开放研究数据平台。DVN/WBO7LK_2020\n使用 SAS 逐渐让我失去的耐心，极其臃肿和笨重，Vintage Car，交互页面也很糟糕，用起来很令人烦躁，遂改用 R 对数据进行分析。\n2025-03-06 R 也没那么好用，反而觉得 Stata 的简便也是一种优势所在。",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html#加载必要的包",
    "href": "Guide/R/2025-02-22-CLHLS.html#加载必要的包",
    "title": "CLHLS Data Analysis by R",
    "section": "",
    "text": "本文数据源来自北大开放研究数据平台。DVN/WBO7LK_2020\n使用 SAS 逐渐让我失去的耐心，极其臃肿和笨重，Vintage Car，交互页面也很糟糕，用起来很令人烦躁，遂改用 R 对数据进行分析。\n2025-03-06 R 也没那么好用，反而觉得 Stata 的简便也是一种优势所在。",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html#数据导出",
    "href": "Guide/R/2025-02-22-CLHLS.html#数据导出",
    "title": "CLHLS Data Analysis by R",
    "section": "\n2.1 数据导出",
    "text": "2.1 数据导出\n\nlibrary(writexl)  \n# 12. 保存描述性统计表格  \nwrite_xlsx(final_summary, \"C:/Users/asus/Desktop/test/CLHLS/Analysis-0214/Rsummary0223.xlsx\")  \n# 13. 保存结果  \nwrite_xlsx(final_data, \"C:/Users/asus/Desktop/test/CLHLS/Analysis-0214/final_data0223.xlsx\")",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html#描述性统计",
    "href": "Guide/R/2025-02-22-CLHLS.html#描述性统计",
    "title": "CLHLS Data Analysis by R",
    "section": "\n2.2 描述性统计",
    "text": "2.2 描述性统计",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html#构建logistic回归方程",
    "href": "Guide/R/2025-02-22-CLHLS.html#构建logistic回归方程",
    "title": "CLHLS Data Analysis by R",
    "section": "\n3.1 构建Logistic回归方程",
    "text": "3.1 构建Logistic回归方程",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "R introduction",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/R/R-intro.html",
    "href": "Guide/R/R-intro.html",
    "title": "R introduction",
    "section": "",
    "text": "R语言是一种自由软件编程语言与操作环境，主要用于统计分析、绘图以及数据挖掘。R由新西兰奥克兰大学的统计学家罗斯·伊哈卡和罗伯特·杰特曼开发，现在由R核心小组负责开发，同时也有其他用户编写了诸多外挂的软件包。R以S语言为基础，其词法作用域语义来自Scheme。R的后台程序大多由C语言、FORTRAN语言和R自己写成。\nR 语言是为数学研究工作者设计的一种数学编程语言，主要用于统计分析、绘图、数据挖掘。\n如果你是一个计算机程序的初学者并且急切地想了解计算机的通用编程，R 语言不是一个很理想的选择，可以选择 Python、C 或 Java。\nR 语言与 C 语言都是贝尔实验室的研究成果，但两者有不同的侧重领域，R 语言是一种解释型的面向数学理论研究工作者的语言，而 C 语言是为计算机软件工程师设计的。\nR 语言是解释运行的语言（与 C 语言的编译运行不同），它的执行速度比 C 语言慢得多，不利于优化。但它在语法层面提供了更加丰富的数据结构操作并且能够十分方便地输出文字和图形信息，所以它广泛应用于数学尤其是统计学领域https://www.runoob.com/r/r-tutorial.html\n近些年 R 的发展也是极为迅速，在 Rstudio 改名为 Posit 后，R 的生态在快速发展。\n本网站就是其中一个分支的成果： Quarto\n\nAn open-source scientific and technical publishing system\n\n其他的还有诸如：\n\n0.1 Posit Workbench（数据科学家协同开发平台）\n\nJupyter, RStudio, and VS Code environments centrally maintained and ready to use\n\n\n\n0.2 MLOps（机器学习模型部署）\n\nMachine learning operations, or MLOps, is a set of practices to deploy and maintain machine learning models in production reliably and efficiently. The vetiver framework is for MLOps tasks in Python and R.\n\n\n\n0.3 Shiny for python\n\nEffortless Python web applications with the power of reactive programming.",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "R"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html",
    "href": "Guide/SAS/SAS-install.html",
    "title": "SAS Install",
    "section": "",
    "text": "SAS Install",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "SAS introduction",
      "SAS Install"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "医学统计学习笔记",
    "section": "",
    "text": "本书的前身是开始于23年夏天的Rmarkdown笔记的制作，在考研的过程中，一些无聊又不想学习的时间被用来整理这些笔记和制作文档与网页，由于Quarto的出现和发展，感觉到它的强大与便利，遂用Quarto重制。\n现在重制这本学习笔记，按照初步构想，会继续完善卫生/医学统计学部分的内容，从低阶往高阶完善；再就是记录相关程序的使用技巧和学习记录（R、Python、SAS etc.）；还有有一部分的项目分享（如果顺利的话）和读书报告。\nEpidemiologists, in response to a health emergency or as a result of systematic surveillance, first obtain and analyze observed data. They use data, observations, science, and theory as they work at identifying a pathogen (when unknown) behind an observed disease outbreak or as they proceed to plan or implement policies that ameliorates its impact.\n按照大类再细分的话会有如下：\n\n统计学\n\n医学统计学（基础篇）\n贝叶斯统计\n\n统计软件的使用\n\nR\nPython\nSAS\nStata\nQuarto\n\n项目记录\n\nKaggle\n\n读书笔记\n\n\n\n引用格式BibTeX@online{2025,\n  author = {, simonzhou},\n  title = {医学统计学习笔记},\n  date = {2025-05-08},\n  url = {https://github.com/zhoulvbang/Med-Stat-Notes},\n  langid = {zh}\n}\n请按如下格式引用：\nsimonzhou. 2025. “医学统计学习笔记.” May 8, 2025. https://github.com/zhoulvbang/Med-Stat-Notes.",
    "crumbs": [
      "Home",
      "主页"
    ]
  },
  {
    "objectID": "Learn/Basic/01-preview.html",
    "href": "Learn/Basic/01-preview.html",
    "title": "关于卫生统计学",
    "section": "",
    "text": "卫生统计学是一门致力于收集、分析和解释与健康相关的数据的学科。它的目标是通过统计方法来评估和改善人群的健康状况，从而提高公共卫生水平。卫生统计学将统计学原理应用于医学和公共卫生领域，以支持健康决策的制定和实施。\n\n\n\n卫生统计学在以下几个方面发挥着重要作用：\n\n疾病监测与控制： 通过收集和分析疾病发病率、死亡率和流行病学数据，卫生统计学帮助识别疾病的流行趋势，并制定相应的预防和控制策略。\n健康政策制定： 卫生统计学提供了评估不同健康政策和干预措施效果的方法，为政策制定者提供了决策支持。\n卫生服务评估： 通过分析卫生服务的覆盖范围、质量和效率，卫生统计学评估卫生系统的运作情况，并提供改进建议。\n流行病学研究： 卫生统计学在研究人群健康与疾病之间的关系方面发挥着关键作用，帮助揭示疾病的发病机制和影响因素。\n\n\n\n\n随着数据科学和人工智能技术的发展，卫生统计学将迎来新的机遇和挑战：\n\n大数据与人工智能： 大数据技术使得收集、整合和分析海量的健康数据成为可能，而人工智能技术则提供了更高效、精确的数据处理和预测能力，为卫生统计学研究提供了新的方法和工具。\n个性化医疗： 基于个体遗传信息和生活方式数据的个性化医疗将成为未来的发展趋势，卫生统计学将在个体化医疗决策和健康管理中发挥更加重要的作用。\n跨学科合作： 卫生统计学将与流行病学、遗传学、生物信息学等学科交叉融合，形成跨学科合作的新模式，共同解决健康领域的复杂问题。\n公众参与与健康促进： 卫生统计学将更加注重公众参与和社区健康促进，通过社会行为和环境因素的分析，促进健康政策的制定和实施，推动社会健康公平。\n\n随着社会的发展和健康需求的不断变化，卫生统计学将继续发挥着重要的作用，为促进全民健康、预防疾病、改善医疗服务质量和推动卫生政策提供支持和指导。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "关于卫生统计学"
    ]
  },
  {
    "objectID": "Learn/Basic/01-preview.html#什么是卫生统计学",
    "href": "Learn/Basic/01-preview.html#什么是卫生统计学",
    "title": "关于卫生统计学",
    "section": "",
    "text": "卫生统计学是一门致力于收集、分析和解释与健康相关的数据的学科。它的目标是通过统计方法来评估和改善人群的健康状况，从而提高公共卫生水平。卫生统计学将统计学原理应用于医学和公共卫生领域，以支持健康决策的制定和实施。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "关于卫生统计学"
    ]
  },
  {
    "objectID": "Learn/Basic/01-preview.html#卫生统计学的重要性",
    "href": "Learn/Basic/01-preview.html#卫生统计学的重要性",
    "title": "关于卫生统计学",
    "section": "",
    "text": "卫生统计学在以下几个方面发挥着重要作用：\n\n疾病监测与控制： 通过收集和分析疾病发病率、死亡率和流行病学数据，卫生统计学帮助识别疾病的流行趋势，并制定相应的预防和控制策略。\n健康政策制定： 卫生统计学提供了评估不同健康政策和干预措施效果的方法，为政策制定者提供了决策支持。\n卫生服务评估： 通过分析卫生服务的覆盖范围、质量和效率，卫生统计学评估卫生系统的运作情况，并提供改进建议。\n流行病学研究： 卫生统计学在研究人群健康与疾病之间的关系方面发挥着关键作用，帮助揭示疾病的发病机制和影响因素。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "关于卫生统计学"
    ]
  },
  {
    "objectID": "Learn/Basic/01-preview.html#卫生统计学的未来展望",
    "href": "Learn/Basic/01-preview.html#卫生统计学的未来展望",
    "title": "关于卫生统计学",
    "section": "",
    "text": "随着数据科学和人工智能技术的发展，卫生统计学将迎来新的机遇和挑战：\n\n大数据与人工智能： 大数据技术使得收集、整合和分析海量的健康数据成为可能，而人工智能技术则提供了更高效、精确的数据处理和预测能力，为卫生统计学研究提供了新的方法和工具。\n个性化医疗： 基于个体遗传信息和生活方式数据的个性化医疗将成为未来的发展趋势，卫生统计学将在个体化医疗决策和健康管理中发挥更加重要的作用。\n跨学科合作： 卫生统计学将与流行病学、遗传学、生物信息学等学科交叉融合，形成跨学科合作的新模式，共同解决健康领域的复杂问题。\n公众参与与健康促进： 卫生统计学将更加注重公众参与和社区健康促进，通过社会行为和环境因素的分析，促进健康政策的制定和实施，推动社会健康公平。\n\n随着社会的发展和健康需求的不断变化，卫生统计学将继续发挥着重要的作用，为促进全民健康、预防疾病、改善医疗服务质量和推动卫生政策提供支持和指导。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "关于卫生统计学"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html",
    "href": "Learn/Basic/03-Random-events-probabilities.html",
    "title": "随机事件的概率",
    "section": "",
    "text": "不确定的知识+所含不确定性度量的知识=可用的知识\n\n\n\n确定性现象（deterministic phenomenon）：在一定条件下必然会发生的现象；\n随机现象（random phenomenon）：在同一条件下具有不确定结果的现象。\n\n对随机现象获得一个观察或进行一次测量的过程称为随机试验，简称试验，用\\(E\\)表示。\n用\\(\\omega\\)表示试验\\(E\\)的一个可能的结果，则称\\(\\omega\\)为\\(E\\)的一个基本事件（elementary event）；基本事件是指不能在分解为更简单结果的事件。\n\n\n\n\n包含关系：对任意两事件A和B，如果事件B发生，则事件A必发生，则称事件A包含事件B，记作\\(A\\supseteq B\\)或\\(B\\subseteq A\\)，符号\\(\\supseteq\\)和 \\(\\subseteq\\)分别表示包含与被包含。\n相等关系：如果事件A和事件B满足以下关系：\\(A\\supseteq B 且B\\supseteq A\\)，则称A和B相等，记作\\(A=B\\)\n事件的和：在一次试验中，对任意两事件A和B，“A和B中至少有一个发生”也是一个事件，称此事件为A和B的“和或并”，记作\\(A\\cup B\\)，也可以表示为\\(A+B\\)。\n事件的交：在一次试验中，对任意两事件A和B，“A和B同时发生”也是一个事件，称此事件为A和B的“交”，记作\\(A\\cap B\\)，也可以表示为\\(AB\\)。\n事件的差：在一次试验中，“A发生且B不发生”也是一个事件，称此事件为A和B的“差”，记作\\(A-B\\)。\n互不相容事件：在一次试验中，“A与B不能同时发生”，即\\(A\\cup B=\\emptyset\\)，则称此事件A和事件B为互不相容事件，也称互斥事件。\n对立事件：是一种特殊的互不相容事件，若“事件A与事件B不能同时发生，且他们的和组成样本空间”，即\\(A\\cap B=\\emptyset 且A\\cup B=\\Omega\\)，则称A和B互为对立事件（complementary events）。\n\n\n\n\n\n频率（frequency）：设E为一随机试验，A是其中一事件，在同样条件下把E重复的做n次，以m表示事件A在这n次试验中发生的次数，则称比值\\(m/n\\)为事件A发生的频率，记为F(A)： \\[F(A)=\\frac{m}{n}\\]\n概率（probability）：设在同一条件下，重复进行n次试验，随机事件A发生m次，若试验次数n无限大时，频率\\(m/n\\)将在某一确定值p的附近摆动，则称p为事件A的概率，记为P(A)： \\[P(A)=p \\approx \\frac{m}{n}\\]\n概率有时也被称为相对频率方法；概率是事物固有的属性，不以人的主观意志为转移。\n概率的描述性定义\n概率的公理化定义：\n\n\n非负性:对于任意事件A，有\\(P(A)\\geq 0\\)\n规范性:\\(P(\\Omega)=1\\)\n可加可列性:\\(P(\\bigcup \\limits_{i=1}^{\\infty}A_i)=\\sum \\limits_{i=1}^{\\infty}P(A_i)\\)\n\n\n\n\n若A和B是样本空间\\(\\Omega\\)中两个互不相容的事件，则事件和的概率等于两事件概率之和，即：\n\\[P(A+B)=P(A)+P(B)\\] 这称之为概率的加法公式。此公式要求事件A和事件B互不相容。\n易知，A的对立事件\\(\\bar A\\)的概率\\(P(\\bar A)=1-P(A)\\)，加法公式可自然推广到多个事件的情形。\n对于任意事件A和B，即事件A和不是互不相容，更一般有： \\[P(A+B)=P(A)+P(B)-P(AB)\\] 因为当A和B互不相容时，有\\(P(AB)=0\\)。\n\n\n\n当存在某些可能影响结果的条件时，事件发生的概率可能会改变，我们称这种情况下的概率为条件概率。\n\n条件概率\n\n在已知事件A发生的条件下，事件B发生的概率成为条件概率（conditional probability），记为\\(P(B\\mid A)\\)，公式为： \\[P(B\\mid A)=\\frac{P(AB)}{P(A)},P(A)&gt;0\\] 即条件概率等于事件A和B同时发生的概率除以事件A发生的概率。\n\n概率乘法公式\n\n\\[P(AB)=P(A)P(B\\mid A),P(A)&gt;0\\\\\nP(AB)=P(B)P(A\\mid B),P(B)&gt;0\\]\n\n独立事件\n\n如果事件B发生的概率不受事件A发生概率的影响，即\\(P(B\\mid A)=P(B)\\)，则称事件B对事件A独立。由于两事件间的独立总是相互的，故也有\\(P(A\\mid B)=P(A)\\)。\n根据\\(P(AB)=P(B)P(A\\mid B),P(B)&gt;0\\)，若事件A、B独立，则有 \\[P(AB)=P(A)P(B)\\]\n在医学研究中，可以根据试验条件及生物学知识判断事物之间的独立性。\n\n\n\n\n全概率公式\n\n如果事件组\\(A_1,A_2,\\dots,A_n\\)满足以下两个条件：\n\n\\(A_1,A_2,\\dots,A_n\\)互不相容，且\\(P(A_i)&gt;0(i=1,2,\\dots,n)\\);\n\\(A_1+A_2+\\dots+A_n=\\Omega\\)\n\n那么对于任意事件B，都有\n\\[P(B)=\\sum_\\limits{i=1}^{n}P(A_i)P(B\\mid A_i)\\]\n此公式成为全概率公式（total probability formula），即B的概率可以表示为在给定\\(A_i\\)发生条件下B发生的条件概率的加权平均。\n\nBayes公式\n\n对于n个互不相容的事件\\(A_1,A_2,\\dots,A_n\\)，且他们的和为必然事件，则在时间B发生的前提下事件\\(A_k(k=1,2,\\dots,n)\\)发生的概率为：\n\\[P(A_k\\mid B)=\\frac{P(A_k)P(B\\mid A_k)}{\\sum_\\limits{i=1}^{n}P(A_i)P(B\\mid A_i)},(k=1,2,\\dots,n),P(B)&gt;0\\]\nBayes公式的意义在于，它可以改变条件概率结论的方向，即在知道结果的情况下来推断原因，用式子\\(P(A_k\\mid B)\\)表示，称为后验概率(posterior probability)；而\\(P(A_k)\\)表示各种原因出现可能性的大小，一般是过去经验的总结，称为先验概率(prior probability)。\n\n\n在流行病学领域，研究人员一直在寻找提高分析准确性和可靠性的方法。一种越来越流行的方法是使用贝叶斯方法，该方法为处理复杂数据和整合先验知识提供了独特的视角。\n\n为什么使用贝叶斯方法？\n\n贝叶斯方法允许流行病学家纳入关于该疾病的现有知识或信念或风险因素纳入分析，从而获得更准确、更翔实的结果。\n\n处理复杂模型\n\n贝叶斯方法非常适合分析复杂的流行病学模型具有许多参数和相互作用，因为它们可以解释这些参数的不确定性。\n\n处理缺失数据\n\n贝叶斯方法可以将缺失数据视为另一个需要估计的未知参数来处理，与排除或归咎于缺失数据的传统方法相比，这可以产生更准确的结果。\n\n提供概率解释\n\n贝叶斯方法以概率分布的形式提供结果，比频率学派方法获得的点估计和置信区间更直观、更具信息量。\n\n\n\n在人卫版《流行病学》第8版中，第七章-筛检，关于预测值的计算，该指标反映了筛检试验实际应用到人群筛查后，获得的首医收益大小。\n在医院开展的基于病例-非病例设计的筛查试验研究，病例组和非病例组的构成比不能代表目标人群的现患与未患比例，因此不能直接计算预测值。此时，可以根据灵敏度、特异度、现患率与预测值的关系式（Bayes公式）来估算预测值。\n[PPV=] [NPV=]\n公式中，PPV是阳性预测值，NPV是阴性预测值；SE是灵敏度，SP是特异度，P是患病率。\n流行病学中的贝叶斯方法:简介\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#随机事件与样本空间",
    "href": "Learn/Basic/03-Random-events-probabilities.html#随机事件与样本空间",
    "title": "随机事件的概率",
    "section": "",
    "text": "确定性现象（deterministic phenomenon）：在一定条件下必然会发生的现象；\n随机现象（random phenomenon）：在同一条件下具有不确定结果的现象。\n\n对随机现象获得一个观察或进行一次测量的过程称为随机试验，简称试验，用\\(E\\)表示。\n用\\(\\omega\\)表示试验\\(E\\)的一个可能的结果，则称\\(\\omega\\)为\\(E\\)的一个基本事件（elementary event）；基本事件是指不能在分解为更简单结果的事件。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#事件的运算",
    "href": "Learn/Basic/03-Random-events-probabilities.html#事件的运算",
    "title": "随机事件的概率",
    "section": "",
    "text": "包含关系：对任意两事件A和B，如果事件B发生，则事件A必发生，则称事件A包含事件B，记作\\(A\\supseteq B\\)或\\(B\\subseteq A\\)，符号\\(\\supseteq\\)和 \\(\\subseteq\\)分别表示包含与被包含。\n相等关系：如果事件A和事件B满足以下关系：\\(A\\supseteq B 且B\\supseteq A\\)，则称A和B相等，记作\\(A=B\\)\n事件的和：在一次试验中，对任意两事件A和B，“A和B中至少有一个发生”也是一个事件，称此事件为A和B的“和或并”，记作\\(A\\cup B\\)，也可以表示为\\(A+B\\)。\n事件的交：在一次试验中，对任意两事件A和B，“A和B同时发生”也是一个事件，称此事件为A和B的“交”，记作\\(A\\cap B\\)，也可以表示为\\(AB\\)。\n事件的差：在一次试验中，“A发生且B不发生”也是一个事件，称此事件为A和B的“差”，记作\\(A-B\\)。\n互不相容事件：在一次试验中，“A与B不能同时发生”，即\\(A\\cup B=\\emptyset\\)，则称此事件A和事件B为互不相容事件，也称互斥事件。\n对立事件：是一种特殊的互不相容事件，若“事件A与事件B不能同时发生，且他们的和组成样本空间”，即\\(A\\cap B=\\emptyset 且A\\cup B=\\Omega\\)，则称A和B互为对立事件（complementary events）。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#概率的定义",
    "href": "Learn/Basic/03-Random-events-probabilities.html#概率的定义",
    "title": "随机事件的概率",
    "section": "",
    "text": "频率（frequency）：设E为一随机试验，A是其中一事件，在同样条件下把E重复的做n次，以m表示事件A在这n次试验中发生的次数，则称比值\\(m/n\\)为事件A发生的频率，记为F(A)： \\[F(A)=\\frac{m}{n}\\]\n概率（probability）：设在同一条件下，重复进行n次试验，随机事件A发生m次，若试验次数n无限大时，频率\\(m/n\\)将在某一确定值p的附近摆动，则称p为事件A的概率，记为P(A)： \\[P(A)=p \\approx \\frac{m}{n}\\]\n概率有时也被称为相对频率方法；概率是事物固有的属性，不以人的主观意志为转移。\n概率的描述性定义\n概率的公理化定义：\n\n\n非负性:对于任意事件A，有\\(P(A)\\geq 0\\)\n规范性:\\(P(\\Omega)=1\\)\n可加可列性:\\(P(\\bigcup \\limits_{i=1}^{\\infty}A_i)=\\sum \\limits_{i=1}^{\\infty}P(A_i)\\)",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#概率的加法公式",
    "href": "Learn/Basic/03-Random-events-probabilities.html#概率的加法公式",
    "title": "随机事件的概率",
    "section": "",
    "text": "若A和B是样本空间\\(\\Omega\\)中两个互不相容的事件，则事件和的概率等于两事件概率之和，即：\n\\[P(A+B)=P(A)+P(B)\\] 这称之为概率的加法公式。此公式要求事件A和事件B互不相容。\n易知，A的对立事件\\(\\bar A\\)的概率\\(P(\\bar A)=1-P(A)\\)，加法公式可自然推广到多个事件的情形。\n对于任意事件A和B，即事件A和不是互不相容，更一般有： \\[P(A+B)=P(A)+P(B)-P(AB)\\] 因为当A和B互不相容时，有\\(P(AB)=0\\)。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#概率的乘法公式",
    "href": "Learn/Basic/03-Random-events-probabilities.html#概率的乘法公式",
    "title": "随机事件的概率",
    "section": "",
    "text": "当存在某些可能影响结果的条件时，事件发生的概率可能会改变，我们称这种情况下的概率为条件概率。\n\n条件概率\n\n在已知事件A发生的条件下，事件B发生的概率成为条件概率（conditional probability），记为\\(P(B\\mid A)\\)，公式为： \\[P(B\\mid A)=\\frac{P(AB)}{P(A)},P(A)&gt;0\\] 即条件概率等于事件A和B同时发生的概率除以事件A发生的概率。\n\n概率乘法公式\n\n\\[P(AB)=P(A)P(B\\mid A),P(A)&gt;0\\\\\nP(AB)=P(B)P(A\\mid B),P(B)&gt;0\\]\n\n独立事件\n\n如果事件B发生的概率不受事件A发生概率的影响，即\\(P(B\\mid A)=P(B)\\)，则称事件B对事件A独立。由于两事件间的独立总是相互的，故也有\\(P(A\\mid B)=P(A)\\)。\n根据\\(P(AB)=P(B)P(A\\mid B),P(B)&gt;0\\)，若事件A、B独立，则有 \\[P(AB)=P(A)P(B)\\]\n在医学研究中，可以根据试验条件及生物学知识判断事物之间的独立性。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/03-Random-events-probabilities.html#全概率公式与bayes公式",
    "href": "Learn/Basic/03-Random-events-probabilities.html#全概率公式与bayes公式",
    "title": "随机事件的概率",
    "section": "",
    "text": "全概率公式\n\n如果事件组\\(A_1,A_2,\\dots,A_n\\)满足以下两个条件：\n\n\\(A_1,A_2,\\dots,A_n\\)互不相容，且\\(P(A_i)&gt;0(i=1,2,\\dots,n)\\);\n\\(A_1+A_2+\\dots+A_n=\\Omega\\)\n\n那么对于任意事件B，都有\n\\[P(B)=\\sum_\\limits{i=1}^{n}P(A_i)P(B\\mid A_i)\\]\n此公式成为全概率公式（total probability formula），即B的概率可以表示为在给定\\(A_i\\)发生条件下B发生的条件概率的加权平均。\n\nBayes公式\n\n对于n个互不相容的事件\\(A_1,A_2,\\dots,A_n\\)，且他们的和为必然事件，则在时间B发生的前提下事件\\(A_k(k=1,2,\\dots,n)\\)发生的概率为：\n\\[P(A_k\\mid B)=\\frac{P(A_k)P(B\\mid A_k)}{\\sum_\\limits{i=1}^{n}P(A_i)P(B\\mid A_i)},(k=1,2,\\dots,n),P(B)&gt;0\\]\nBayes公式的意义在于，它可以改变条件概率结论的方向，即在知道结果的情况下来推断原因，用式子\\(P(A_k\\mid B)\\)表示，称为后验概率(posterior probability)；而\\(P(A_k)\\)表示各种原因出现可能性的大小，一般是过去经验的总结，称为先验概率(prior probability)。\n\n\n在流行病学领域，研究人员一直在寻找提高分析准确性和可靠性的方法。一种越来越流行的方法是使用贝叶斯方法，该方法为处理复杂数据和整合先验知识提供了独特的视角。\n\n为什么使用贝叶斯方法？\n\n贝叶斯方法允许流行病学家纳入关于该疾病的现有知识或信念或风险因素纳入分析，从而获得更准确、更翔实的结果。\n\n处理复杂模型\n\n贝叶斯方法非常适合分析复杂的流行病学模型具有许多参数和相互作用，因为它们可以解释这些参数的不确定性。\n\n处理缺失数据\n\n贝叶斯方法可以将缺失数据视为另一个需要估计的未知参数来处理，与排除或归咎于缺失数据的传统方法相比，这可以产生更准确的结果。\n\n提供概率解释\n\n贝叶斯方法以概率分布的形式提供结果，比频率学派方法获得的点估计和置信区间更直观、更具信息量。\n\n\n\n在人卫版《流行病学》第8版中，第七章-筛检，关于预测值的计算，该指标反映了筛检试验实际应用到人群筛查后，获得的首医收益大小。\n在医院开展的基于病例-非病例设计的筛查试验研究，病例组和非病例组的构成比不能代表目标人群的现患与未患比例，因此不能直接计算预测值。此时，可以根据灵敏度、特异度、现患率与预测值的关系式（Bayes公式）来估算预测值。\n[PPV=] [NPV=]\n公式中，PPV是阳性预测值，NPV是阴性预测值；SE是灵敏度，SP是特异度，P是患病率。\n流行病学中的贝叶斯方法:简介\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "随机事件的概率"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html",
    "title": "连续型随机变量的概率分布",
    "section": "",
    "text": "若随机变量X的密度函数是\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, (-\\infty&lt;x&lt;+\\infty)\\] 则称X服从正态分布，记为\\(X\\sim N(\\mu,\\sigma^2)\\)。\n\n\n\n\nNormal Curve comparsion\n\n\n\n\n正态分布(Normal Distribution)：正态分布是最重要的连续型分布，随机变量\\(X\\)服从均数为\\(\\mu\\)，标准差为\\(\\sigma\\)的正态分布，记为\\(X\\sim N(\\mu,\\sigma^{2})\\)。\n正态曲线（Normal curve）：即正态分布曲线，\\(\\mu\\)和\\(\\sigma\\)是正态分布的两个参数。\n\n\n\n\n\nNormal Curve\n\n\n\n\n性质\n\n\n正态曲线在横轴上方均数处最高\n\n正态分布以均数为中心，左右对称\n\n正态分布有两个参数，即位置参数\\(\\mu\\)和形态参数\\(\\sigma\\)\n\n固定\\(\\sigma\\)，改变\\(\\mu\\)值，形态不变，曲线沿着\\(X\\)轴平行移动\n固定\\(\\mu\\)，改变\\(\\sigma\\)值，中心在\\(X\\)轴的位置不变\n\n\n\\(\\sigma\\)越小，曲线越陡峭\\(\\to\\)瘦高\n\n\\(\\sigma\\)越大，曲线越低平\\(\\to\\)矮胖\n\n\n正态分布的可加性，当随机变量X服从正态分布\\(N(\\mu_1,\\sigma_1^2)\\)，Y服从正态分布\\(N(\\mu_2,\\sigma_2^2)\\)，X与Y独立，则\\(X-Y\\)服从\\(N(\\mu_1-\\mu_2,\\sigma_1^2+\\sigma_2^2)\\)的正态分布\n\n\n\n\n\n\n\nDifferent Normal Curve\n\n\n\n\n标准正态随机变量U的密度函数用\\(\\varphi(u)\\)表示，为： \\[\\varphi(u)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{u^2}{2}},(-\\infty&lt;x&lt;+\\infty)\\]\n\n标准正态分布（Standard normal distribution）：是一种特殊的正态分布，通常用\\(U\\)或\\(Z\\)表示服从标准正态分布的变量，此时称随机变量\\(X\\)服从均数为0，标准差为1的标准正态分布，记为\\(X \\sim N(0,1)\\)\n\n\n\n正态分布：一簇曲线\n标准正态分布：一条曲线\n\n\n标准正态变换：Z变换、U变换\n\n\n疑难1：Z值到底表达什么意思？\n\n个体值到均值的距离，有多少个标准差 \\(Z = \\frac{X-\\mu}{\\sigma}\\)\n\n只有正态分布的资料才能通过Z变换变成标准正态分布\n\n\n疑难2：标准化变换的公式如何理解？\n\n个体值减去均值，除以标准差，均数和标准差由\\(\\mu,\\sigma\\)变为\\(0,1\\)\n\n\n\n\n\n\nTableGrob (1 x 2) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (1-1,2-2) arrange gtable[layout]\n\n\n\n\nNormalized Transformation\n\n\n\n\n\n正态分布的68-95-99.7法则\n\n\n\n\n\nNormal\n\n\n\n\n标准化转换，涉及到以下两个互逆计算\n\n\n估计某个随机变量在一定取值范围内的观测值个数占全部观测值数量的百分比\n通过已知的百分比，估计总体变量值的分布范围（本质同医学参考值范围的计算）\n\n\n运用正态近似法计算医学参考值范围\n\n\n\nMedical reference range\n\n\n运用正态近似法计算置信区间\n正态分布是很多统计学分析方法的理论基础\n\nnotice:\n\n正态曲线上的拐点所对应的横坐标为\\(\\mu ±\\sigma\\)。\n设随机变量\\(X\\)的概率密度曲线为\\(f(x)=\\frac{1}{2\\sqrt{p}}e^{\\frac{(x+2)^2}{4}}\\)，若要将\\(X\\)转化为服从标准正态分布的变量\\(\\mu\\)，则所采用的标准化变换为：\\(\\frac{X-2}{\\sqrt{2}}\\)（其原式为：\\(f(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\)，题目和原式中：\\(p=\\pi\\)）\n\n\n\n\n\nSkewed Curves\n\n\n\nnotice：\n\n左偏，左边尾长，平均数靠近左侧，平均数小于中位数小于众数，负偏态；\n右偏，右边尾长，平均数靠近右侧，平均数大于中位数大于众数，正偏态。\n\n\n\nconversion relationship",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#正态分布normal-distribution",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#正态分布normal-distribution",
    "title": "连续型随机变量的概率分布",
    "section": "",
    "text": "若随机变量X的密度函数是\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, (-\\infty&lt;x&lt;+\\infty)\\] 则称X服从正态分布，记为\\(X\\sim N(\\mu,\\sigma^2)\\)。\n\n\n\n\nNormal Curve comparsion\n\n\n\n\n正态分布(Normal Distribution)：正态分布是最重要的连续型分布，随机变量\\(X\\)服从均数为\\(\\mu\\)，标准差为\\(\\sigma\\)的正态分布，记为\\(X\\sim N(\\mu,\\sigma^{2})\\)。\n正态曲线（Normal curve）：即正态分布曲线，\\(\\mu\\)和\\(\\sigma\\)是正态分布的两个参数。\n\n\n\n\n\nNormal Curve\n\n\n\n\n性质\n\n\n正态曲线在横轴上方均数处最高\n\n正态分布以均数为中心，左右对称\n\n正态分布有两个参数，即位置参数\\(\\mu\\)和形态参数\\(\\sigma\\)\n\n固定\\(\\sigma\\)，改变\\(\\mu\\)值，形态不变，曲线沿着\\(X\\)轴平行移动\n固定\\(\\mu\\)，改变\\(\\sigma\\)值，中心在\\(X\\)轴的位置不变\n\n\n\\(\\sigma\\)越小，曲线越陡峭\\(\\to\\)瘦高\n\n\\(\\sigma\\)越大，曲线越低平\\(\\to\\)矮胖\n\n\n正态分布的可加性，当随机变量X服从正态分布\\(N(\\mu_1,\\sigma_1^2)\\)，Y服从正态分布\\(N(\\mu_2,\\sigma_2^2)\\)，X与Y独立，则\\(X-Y\\)服从\\(N(\\mu_1-\\mu_2,\\sigma_1^2+\\sigma_2^2)\\)的正态分布\n\n\n\n\n\n\n\nDifferent Normal Curve",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#标准正态分布",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#标准正态分布",
    "title": "连续型随机变量的概率分布",
    "section": "",
    "text": "标准正态随机变量U的密度函数用\\(\\varphi(u)\\)表示，为： \\[\\varphi(u)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{u^2}{2}},(-\\infty&lt;x&lt;+\\infty)\\]\n\n标准正态分布（Standard normal distribution）：是一种特殊的正态分布，通常用\\(U\\)或\\(Z\\)表示服从标准正态分布的变量，此时称随机变量\\(X\\)服从均数为0，标准差为1的标准正态分布，记为\\(X \\sim N(0,1)\\)\n\n\n\n正态分布：一簇曲线\n标准正态分布：一条曲线\n\n\n标准正态变换：Z变换、U变换\n\n\n疑难1：Z值到底表达什么意思？\n\n个体值到均值的距离，有多少个标准差 \\(Z = \\frac{X-\\mu}{\\sigma}\\)\n\n只有正态分布的资料才能通过Z变换变成标准正态分布\n\n\n疑难2：标准化变换的公式如何理解？\n\n个体值减去均值，除以标准差，均数和标准差由\\(\\mu,\\sigma\\)变为\\(0,1\\)\n\n\n\n\n\n\nTableGrob (1 x 2) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (1-1,2-2) arrange gtable[layout]\n\n\n\n\nNormalized Transformation\n\n\n\n\n\n正态分布的68-95-99.7法则\n\n\n\n\n\nNormal\n\n\n\n\n标准化转换，涉及到以下两个互逆计算\n\n\n估计某个随机变量在一定取值范围内的观测值个数占全部观测值数量的百分比\n通过已知的百分比，估计总体变量值的分布范围（本质同医学参考值范围的计算）\n\n\n运用正态近似法计算医学参考值范围\n\n\n\nMedical reference range\n\n\n运用正态近似法计算置信区间\n正态分布是很多统计学分析方法的理论基础\n\nnotice:\n\n正态曲线上的拐点所对应的横坐标为\\(\\mu ±\\sigma\\)。\n设随机变量\\(X\\)的概率密度曲线为\\(f(x)=\\frac{1}{2\\sqrt{p}}e^{\\frac{(x+2)^2}{4}}\\)，若要将\\(X\\)转化为服从标准正态分布的变量\\(\\mu\\)，则所采用的标准化变换为：\\(\\frac{X-2}{\\sqrt{2}}\\)（其原式为：\\(f(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\)，题目和原式中：\\(p=\\pi\\)）\n\n\n\n\n\nSkewed Curves\n\n\n\nnotice：\n\n左偏，左边尾长，平均数靠近左侧，平均数小于中位数小于众数，负偏态；\n右偏，右边尾长，平均数靠近右侧，平均数大于中位数大于众数，正偏态。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#小结",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#小结",
    "title": "连续型随机变量的概率分布",
    "section": "",
    "text": "conversion relationship",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#t分布",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#t分布",
    "title": "连续型随机变量的概率分布",
    "section": "\n2.1 t分布",
    "text": "2.1 t分布\n说起t分布，首先要提一句u分布，正态分布（Normal Distribution）是许多统计方法的理论基础。\n正态分布的两个参数\\(\\mu\\)和\\(\\sigma\\)决定了正态分布的位置和形态。为了应用方便，常将一般的正态变量X通过u变换\\([(X-\\mu)/\\sigma]\\)转化成标准正态变量u，以使原来各种形态的正态分布都转换为\\(\\mu=0,\\sigma=1\\)的标准正态分布(Standard Normal Distribution)，亦称u分布。\n根据中心极限定理，通过抽样模拟试验表明，在正态分布总体中以固定 n 抽取若干个样本时，样本均数的分布仍服从正态分布，即\\(N(\\mu,\\sigma)\\)。所以，对样本均数的分布进行u变换，也可变换为标准正态分布\\(N(0,1)\\)。\n由于在实际工作中，往往\\(\\sigma^2\\)(总体方差)是未知的，常用\\(s^2\\)(样本方差)作为\\(\\sigma^2\\)的估计值，为了与u变换区别，称为 t 变换，统计量 t 值的分布称为 t 分布。\n\n\n\n\nt-distribution Curves\n\n\n\n\nt 分布是英国统计学家 W.S. Gosset 在 1908 年以笔名 Student发表的论文中提出的, 故后人称为 “学生氏 (Student) 分布” 或 “t 分 布”。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#f分布",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#f分布",
    "title": "连续型随机变量的概率分布",
    "section": "\n2.2 F分布",
    "text": "2.2 F分布\n\n\n\n\nF-distribution Curves\n\n\n\n\n2.2.1 F分布的应用\n\n方差的同质性检验 组与组之间的差异称组间变异（variation between classes），反映在各组的平均数不同。同一组内部被试（个体）之间的差异称组内变异（variation within class），反映在每一个个体之间的差异。\n总变异的分解：\n\n\n总变异 = 组间变异+组内变异\n组间变异 = 实验条件 + 随机误差\n组内变异 = 个体差异 + 实验误差 。组内误差都是随机误差。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/05-random-variable-of-continuous-type.html#chi2分布",
    "href": "Learn/Basic/05-random-variable-of-continuous-type.html#chi2分布",
    "title": "连续型随机变量的概率分布",
    "section": "\n2.3 \\(\\chi^2\\)分布",
    "text": "2.3 \\(\\chi^2\\)分布\n\n\n\n\nChi-square Distribution Curves\n\n\n\n\n2.3.1 卡方检验应用\n\n检验连续变量的分布是否与某种理论分布一致。\n检验某个分类变量各类的出现概率是否等于指定概率。\n检验某两种方法的结果是否一致。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "连续型随机变量的概率分布"
    ]
  },
  {
    "objectID": "Learn/Basic/07-parameter-test.html",
    "href": "Learn/Basic/07-parameter-test.html",
    "title": "参数检验",
    "section": "",
    "text": "维度\n参数检验（Parameter test）\n非参数检验（Non-parameter tests）\n\n\n\n定义\n以特定的总体分布为前提\\(\\rightarrow\\)?\n不依赖于总体分布特征\\(\\rightarrow\\)?\n\n\n举例\n\n\\(Z\\)检验、\\(t\\)分布、\\(F\\)检验\n秩和检验（Rank sum test）、卡方检验\n\n\n优点\n1. 直接利用原始观测值计算统计量，检验效能高；2.可对总体参数做出估计\n1. 适用范围广、收集资料方便；2. 多数非参数检验方法比较简便、易于掌握\n\n\n缺点\n对数据分布有特定要求，适用范围窄\n1. 没有充分利用原始数据，检验效能低；2. 不能对总体参数做出推断\n\n\n适用范围\n必须符合相应的要求，如两样本t检验要求：独立、正态、方差齐\n1. 总体分布形式未知、分布类型不明确、偏态分布数据；2. 等级资料；3. 不满足参数检验条件的数据；4. 数据一段或两端为无法测量的数值等。\n\n\n选用原则\n1. 如果数据符合参数检验条件，或经过变换后符合参数检验的条件，最好用参数检验；2. 参数检验误用为非参数检验，会导致检验效能降低。\n\n\n\n\n\n\n\n\n\n\n类目\n\n\\(t\\)分布\n\n\n\n概念\n设从正态分布\\(N(\\mu,\\sigma^2)\\)随机抽取含量为n的样本，样本均数为\\(\\bar x\\)、标准差为\\(s\\)、则\\(t=\\frac{\\bar x-\\mu}{s_{\\bar x}}=\\frac{\\bar x-\\mu}{s/\\sqrt{n}}\\)，自由度为\\(n-1\\)。\n\n\n图形特点\n一簇以0为中心，左右对称的单峰曲线； 但随着自由度的增加，\\(t\\)分布曲线将越来越接近于标准正态分布曲线\n\n\n统计量值\n\n\\(t\\)的取值范围\\(-\\infty \\sim +\\infty\\)\n\n\n\n自由度\n\\(v=n-1\\)\n\n\n\n\n\n\n\nt-Distribution Curves vs. Standard Normal Curve\n\n\n\n\n\n\n\n\n正态（或正态近似法）\nt分布法\n\n\n\n\n样本均数的中心极限定理。从任意均数等于\\(\\mu\\)，方差等于\\(\\sigma^2\\)的一个总体中抽取样本量为\\(n\\)的简单随机样本，当样本量\\(n\\)很大时，无论总体分布形态如何，样本均数的抽样分布近似服从正态分布。\n样本率的中心极限定理。从“成功”率为\\(\\pi\\)的总体中随机抽取样本量为\\(n\\)的样本，其样本“成功”率用\\(p\\)表示，当\\(n\\pi&gt;5\\)且\\(n(1-\\pi)&gt;5\\)时，样本率\\(p\\)近似服从正态分布。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/07-parameter-test.html#参数检验和非参数检验的区别",
    "href": "Learn/Basic/07-parameter-test.html#参数检验和非参数检验的区别",
    "title": "参数检验",
    "section": "",
    "text": "维度\n参数检验（Parameter test）\n非参数检验（Non-parameter tests）\n\n\n\n定义\n以特定的总体分布为前提\\(\\rightarrow\\)?\n不依赖于总体分布特征\\(\\rightarrow\\)?\n\n\n举例\n\n\\(Z\\)检验、\\(t\\)分布、\\(F\\)检验\n秩和检验（Rank sum test）、卡方检验\n\n\n优点\n1. 直接利用原始观测值计算统计量，检验效能高；2.可对总体参数做出估计\n1. 适用范围广、收集资料方便；2. 多数非参数检验方法比较简便、易于掌握\n\n\n缺点\n对数据分布有特定要求，适用范围窄\n1. 没有充分利用原始数据，检验效能低；2. 不能对总体参数做出推断\n\n\n适用范围\n必须符合相应的要求，如两样本t检验要求：独立、正态、方差齐\n1. 总体分布形式未知、分布类型不明确、偏态分布数据；2. 等级资料；3. 不满足参数检验条件的数据；4. 数据一段或两端为无法测量的数值等。\n\n\n选用原则\n1. 如果数据符合参数检验条件，或经过变换后符合参数检验的条件，最好用参数检验；2. 参数检验误用为非参数检验，会导致检验效能降低。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/07-parameter-test.html#t分布",
    "href": "Learn/Basic/07-parameter-test.html#t分布",
    "title": "参数检验",
    "section": "",
    "text": "类目\n\n\\(t\\)分布\n\n\n\n概念\n设从正态分布\\(N(\\mu,\\sigma^2)\\)随机抽取含量为n的样本，样本均数为\\(\\bar x\\)、标准差为\\(s\\)、则\\(t=\\frac{\\bar x-\\mu}{s_{\\bar x}}=\\frac{\\bar x-\\mu}{s/\\sqrt{n}}\\)，自由度为\\(n-1\\)。\n\n\n图形特点\n一簇以0为中心，左右对称的单峰曲线； 但随着自由度的增加，\\(t\\)分布曲线将越来越接近于标准正态分布曲线\n\n\n统计量值\n\n\\(t\\)的取值范围\\(-\\infty \\sim +\\infty\\)\n\n\n\n自由度\n\\(v=n-1\\)\n\n\n\n\n\n\n\nt-Distribution Curves vs. Standard Normal Curve",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/07-parameter-test.html#一个正态总体参数的估计",
    "href": "Learn/Basic/07-parameter-test.html#一个正态总体参数的估计",
    "title": "参数检验",
    "section": "",
    "text": "正态（或正态近似法）\nt分布法",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/07-parameter-test.html#小结",
    "href": "Learn/Basic/07-parameter-test.html#小结",
    "title": "参数检验",
    "section": "",
    "text": "样本均数的中心极限定理。从任意均数等于\\(\\mu\\)，方差等于\\(\\sigma^2\\)的一个总体中抽取样本量为\\(n\\)的简单随机样本，当样本量\\(n\\)很大时，无论总体分布形态如何，样本均数的抽样分布近似服从正态分布。\n样本率的中心极限定理。从“成功”率为\\(\\pi\\)的总体中随机抽取样本量为\\(n\\)的样本，其样本“成功”率用\\(p\\)表示，当\\(n\\pi&gt;5\\)且\\(n(1-\\pi)&gt;5\\)时，样本率\\(p\\)近似服从正态分布。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html",
    "href": "Learn/Basic/09-non-parameter-test.html",
    "title": "非参数检验",
    "section": "",
    "text": "资料特征\n数据特征\n\n完全随机设计\n\n配对设计\n随机区组\n\n\n\n\n\n\n单组\n两组\n多组\n\n\n\n\n分类资料\n无序分类资料\n二项分布直接计算概率法、正态近似法（Z检验）、率的正态近似\n独立四格表\\(\\chi^2\\)检验、Fisher确切概率法\nR×C交叉表\\(\\chi^2\\)检验、Fisher确切概率法\n配对四格表\\(\\chi^2\\)检验，配对R×R列联表\\(\\chi^2\\)检验\n/\n\n\n\n等级资料\nWilcoxon符合秩和检验\nwilcoxon秩和检验\nKruskal-Wallis H检验\nWilcoxon符合秩和检验\nFriedman M秩和检验\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n内容\n\n\n\n\n确切概率法\n1. 适用情形：样本量较小或\\(\\pi_0\\)不靠近0.5时作单侧检验的情形。2. 计算公式：(1)最多有k例阳性的概率：\\(Pr(X\\le k)\\)(2)最少有k例阳性的概率：\\(Pr(X\\ge k)\\)\n\n\n正态近似法\n1. 适用情形：样本量较大时，\\(n\\pi,n(1-\\pi)\\)均大于5；2. 计算公式：分子为\\(p-\\pi_0\\)，分母为率的标准误\n\n\n\nnotice：上式中p为样本率，\\(\\pi_0\\)为给的总体率（常为理论值或标准值），n为样本含量。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n独立四格表卡方检验\n\\(n\\ge 40\\)且所有的\\(T\\ge 5\\)\\(n\\ge 40\\)且任一理论频数有\\(1\\le T&lt; 5\\)当\\(n&lt;40\\)，或任一一个格子理论频数\\(T&lt;1\\)时\n卡方基本公式、独立四格表专用公式同上、但是需要校正用四格表资料的Fisher确切概率法\n\n\n正态近似法\n\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)均大于5\n分子为样本率之差，分母为样本率差的标准误\\(S_{p1-p2}\\)为两个样本率之差的标准误，\\(p_c=\\frac{x_1+x_2}{n_1+n_2}\\)为两样本的合并率\n\n\n校正样本率的正态近似法\n当\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)不太大时\n同上，但是需要对样本率实施“分子+2、分母+4”的校正\n\n\n\nnotice：\n\n正态近似法与卡方检验结果是很接近的。在日常计算时，因为计算简便，故常用卡方检验公式。\n四格表的自由度为1。\n四格表实际频数变动时，若周边合计数保持不变，则理论频数将不会产生变化。\n用\\(n_R\\)和\\(n_C\\)和n分别表示行合计、列合计和总合计，则计算每格理论数的公式为：\\(T_{RC}=\\frac{n_R×n_C}{n}\\)。\n\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}\\)。\n校正的\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|A-T|-0.5)^2}{T}\\)。\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n配对四格表卡方检验\n当\\((b+c)\\ge 40\\)时当\\((b+c)&lt;40\\)时\n配对卡方检验专用公式校正配对卡方检验专用公式\n\n\n配对R×R交叉表数据的\\(\\chi^2\\)检验\nR（\\(R\\ge2\\)）\n\\(T=\\frac{k-1}{k}\\sum_{i=1}^{k}\\frac{(n_i-m_i)^2}{n_i+m_i-2A_{ii}}\\)\n\n\n\nnotice：\n\n配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}=\\frac{(b-c)^2}{b+c}\\)。\n若b+c&lt;40,使用校正的配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|b-c|-1)^2}{b+c}\\)。\n\n\n\n\n\n\n\n\n建立假设检验，确定检验水准 \\(H_0\\):两变量之间相互独立 \\(H_1\\):两变量之间相互独立 \\(\\alpha=0.05\\)\n计算检验统计量 [^2=_{i,j} ]\n确定P值，做出推断\n关联系数的计算 [r=]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n内容\n\n\n\n\n假设检验\n\\(H_0\\)：各组总体率（或构成比）相同。\\(H_1\\)：各组总体率（或构成比）不同（不全相同）。\n\n\n计算公式\n卡方检验基本公式，自由度为：\\(v=(R-1)(C-1)\\)\n\n\n数据要求\n1. 应用条件：不能有理论频数小于1的格子，或者不能有1/5以上的理论频数大于等于1且小于5 2. 不能进行卡方检验时的解决办法：①增加样本量；②合并或删除理论频数比较小的行或列；③采用Fisher确切概率法\n\n\n卡方分割\n多个率或多个频率分布比较的卡方检验，当结论为拒绝\\(H_0\\)时，仅表示多组之间是有差别的。若需要明确研究是那两组之间存在差别，可做率的多重比较，将R×C表分割为若干个小的四格表进行检验，并且需要根据比较的次数合理地修正检验水准\\(\\alpha\\)，否则将人为地增大犯第一类错误的概率\n\n\n\nnotice:\n\n多个独立样本率的比较，根据R个独立样本的频率分布，是检验R个二项分布总体的概率是否相同，。假设对四个样本率进行比较，进行\\(\\chi^2\\)检验，则它的行数为4，列数为2，其自由度为\\(v=(R-1)×(C-1)=(4-1)(2-1)=3\\)。\n针对行列表资料的\\(\\chi^2\\)检验，若有\\(1/5\\)格子以上的理论频数小于5，即\\(1\\le T\\le5\\)时，应考虑增加样本量，或结合专业知识对行或列进行合并。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html#卡方检验",
    "href": "Learn/Basic/09-non-parameter-test.html#卡方检验",
    "title": "非参数检验",
    "section": "",
    "text": "资料特征\n数据特征\n\n完全随机设计\n\n配对设计\n随机区组\n\n\n\n\n\n\n单组\n两组\n多组\n\n\n\n\n分类资料\n无序分类资料\n二项分布直接计算概率法、正态近似法（Z检验）、率的正态近似\n独立四格表\\(\\chi^2\\)检验、Fisher确切概率法\nR×C交叉表\\(\\chi^2\\)检验、Fisher确切概率法\n配对四格表\\(\\chi^2\\)检验，配对R×R列联表\\(\\chi^2\\)检验\n/\n\n\n\n等级资料\nWilcoxon符合秩和检验\nwilcoxon秩和检验\nKruskal-Wallis H检验\nWilcoxon符合秩和检验\nFriedman M秩和检验\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n内容\n\n\n\n\n确切概率法\n1. 适用情形：样本量较小或\\(\\pi_0\\)不靠近0.5时作单侧检验的情形。2. 计算公式：(1)最多有k例阳性的概率：\\(Pr(X\\le k)\\)(2)最少有k例阳性的概率：\\(Pr(X\\ge k)\\)\n\n\n正态近似法\n1. 适用情形：样本量较大时，\\(n\\pi,n(1-\\pi)\\)均大于5；2. 计算公式：分子为\\(p-\\pi_0\\)，分母为率的标准误\n\n\n\nnotice：上式中p为样本率，\\(\\pi_0\\)为给的总体率（常为理论值或标准值），n为样本含量。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html#率的比较",
    "href": "Learn/Basic/09-non-parameter-test.html#率的比较",
    "title": "非参数检验",
    "section": "",
    "text": "方法\n情形\n计算公式\n\n\n\n\n独立四格表卡方检验\n\\(n\\ge 40\\)且所有的\\(T\\ge 5\\)\\(n\\ge 40\\)且任一理论频数有\\(1\\le T&lt; 5\\)当\\(n&lt;40\\)，或任一一个格子理论频数\\(T&lt;1\\)时\n卡方基本公式、独立四格表专用公式同上、但是需要校正用四格表资料的Fisher确切概率法\n\n\n正态近似法\n\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)均大于5\n分子为样本率之差，分母为样本率差的标准误\\(S_{p1-p2}\\)为两个样本率之差的标准误，\\(p_c=\\frac{x_1+x_2}{n_1+n_2}\\)为两样本的合并率\n\n\n校正样本率的正态近似法\n当\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)不太大时\n同上，但是需要对样本率实施“分子+2、分母+4”的校正\n\n\n\nnotice：\n\n正态近似法与卡方检验结果是很接近的。在日常计算时，因为计算简便，故常用卡方检验公式。\n四格表的自由度为1。\n四格表实际频数变动时，若周边合计数保持不变，则理论频数将不会产生变化。\n用\\(n_R\\)和\\(n_C\\)和n分别表示行合计、列合计和总合计，则计算每格理论数的公式为：\\(T_{RC}=\\frac{n_R×n_C}{n}\\)。\n\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}\\)。\n校正的\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|A-T|-0.5)^2}{T}\\)。\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n配对四格表卡方检验\n当\\((b+c)\\ge 40\\)时当\\((b+c)&lt;40\\)时\n配对卡方检验专用公式校正配对卡方检验专用公式\n\n\n配对R×R交叉表数据的\\(\\chi^2\\)检验\nR（\\(R\\ge2\\)）\n\\(T=\\frac{k-1}{k}\\sum_{i=1}^{k}\\frac{(n_i-m_i)^2}{n_i+m_i-2A_{ii}}\\)\n\n\n\nnotice：\n\n配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}=\\frac{(b-c)^2}{b+c}\\)。\n若b+c&lt;40,使用校正的配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|b-c|-1)^2}{b+c}\\)。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html#独立性检验",
    "href": "Learn/Basic/09-non-parameter-test.html#独立性检验",
    "title": "非参数检验",
    "section": "",
    "text": "建立假设检验，确定检验水准 \\(H_0\\):两变量之间相互独立 \\(H_1\\):两变量之间相互独立 \\(\\alpha=0.05\\)\n计算检验统计量 [^2=_{i,j} ]\n确定P值，做出推断\n关联系数的计算 [r=]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n内容\n\n\n\n\n假设检验\n\\(H_0\\)：各组总体率（或构成比）相同。\\(H_1\\)：各组总体率（或构成比）不同（不全相同）。\n\n\n计算公式\n卡方检验基本公式，自由度为：\\(v=(R-1)(C-1)\\)\n\n\n数据要求\n1. 应用条件：不能有理论频数小于1的格子，或者不能有1/5以上的理论频数大于等于1且小于5 2. 不能进行卡方检验时的解决办法：①增加样本量；②合并或删除理论频数比较小的行或列；③采用Fisher确切概率法\n\n\n卡方分割\n多个率或多个频率分布比较的卡方检验，当结论为拒绝\\(H_0\\)时，仅表示多组之间是有差别的。若需要明确研究是那两组之间存在差别，可做率的多重比较，将R×C表分割为若干个小的四格表进行检验，并且需要根据比较的次数合理地修正检验水准\\(\\alpha\\)，否则将人为地增大犯第一类错误的概率\n\n\n\nnotice:\n\n多个独立样本率的比较，根据R个独立样本的频率分布，是检验R个二项分布总体的概率是否相同，。假设对四个样本率进行比较，进行\\(\\chi^2\\)检验，则它的行数为4，列数为2，其自由度为\\(v=(R-1)×(C-1)=(4-1)(2-1)=3\\)。\n针对行列表资料的\\(\\chi^2\\)检验，若有\\(1/5\\)格子以上的理论频数小于5，即\\(1\\le T\\le5\\)时，应考虑增加样本量，或结合专业知识对行或列进行合并。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html#秩和检验",
    "href": "Learn/Basic/09-non-parameter-test.html#秩和检验",
    "title": "非参数检验",
    "section": "2.1 秩和检验",
    "text": "2.1 秩和检验\n秩和检验（Rank-Sum Test）是一种非参数检验方法，用于比较两个独立样本的分布是否存在显著差异。它无需对数据分布作正态性假设，适用于数据偏离正态分布、样本量较小或数据为序数型变量的场景。\n常见的秩和检验包括：\n\nMann-Whitney U 检验（也称Wilcoxon秩和检验）：用于比较两个独立样本的中位数是否相等。\nWilcoxon 符号秩检验：用于两个配对样本的比较（类似配对t检验，但无需正态性假设）。\n\n\n2.1.1 秩和检验的公式\n\nMann-Whitney U 检验公式\n\n假设两组独立样本分别为 \\(X\\) 和 \\(Y\\)，样本量分别为 \\(n_1\\) 和 \\(n_2\\)。\n对两组样本合并并按大小排序，赋予秩次。计算两组的秩次和 \\(R_1\\) 和 \\(R_2\\)（分别为 $ X$ 和 \\(Y\\) 的秩次总和）。\n\n确定统计量T值：\n\n假设两组样本量 \\(n_1&lt;n_2\\)，一般情况下以样本量较小者\\(n_1\\)对应的秩和\\(T_1\\)为检验统计量\\(T\\)，当样本相等时可以选择任一组的秩和为\\(T\\)。1\n当两组中样本量较小者不低于10时，在\\(H_0\\)成立假设下，统计量\\(T\\)的抽样分布近似于正态分布，有\n\\[T\\approx N\\left(\\frac{n_1(n+1)}{2},\\frac{n_1 n_2(n+1)}{12} \\right)\\] 此时，Wilcoxon 秩和统计量在\\(H_0\\)下关于\\(\\mu=\\frac{n_1(n+1)}{2}\\)对称。\n如果没有或存在较少的“结”，将\\(T\\)标准化后为：\n\\[U=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2(n+1)}{12}}}\\approx N(0,1)\\]\n其中，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n若“结”的比例较多（&gt;25%），则用以下公式校正：\n\\[U_c=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2}{12}[(n+1)-\\sum_\\limits{i=1}^{g}\\frac{t_i^3-t_i}{n(n-1)}]}}\\approx N(0,1)\\]\n\nWilcoxon 符号秩检验公式\n\n对配对样本 \\((X_i, Y_i)\\)，计算差值 \\(D_i = X_i - Y_i\\)，取非零差值的绝对值并排序（若差值为0则舍去不计，且减去相应的个数），赋予秩次 \\(R_i\\)。再根据差值的符号计算符号秩次和 \\(W\\)：\n\\[W = \\sum R_i \\cdot \\text{sign}(D_i)\\]\n检验统计量 \\(T\\) 是 \\(W\\) 的绝对值，依据表或正态分布计算显著性。\n正态近似法：\n当\\(n\\ge 30\\)时，有中心极限定理可知，当\\(H_0\\)成立时统计量\\(T\\)的抽样分布近似正态分布，有\n\\[T\\approx N \\left(\\frac{n(n+1)}{4},\\frac{n(n+1)(2n+1)}{24}\\right)\\] 其中，均数\\(\\mu=\\frac{n(n+1)}{4}\\)，方差\\(\\sigma^2=\\frac{n(n+1)(2n+1)}{24}\\)。 将T标准化后，近似服从标准正态分布，有\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}}}\\approx N(0,1)\\] 其中，n是差值不为0的对子数，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n当N较大时，样本中可能存在较多的“结”，（如“结”所占比例大于25%），此时需要使用校正公式：\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}-\\frac{\\sum_\\limits{i=1}^g(t_i^3-t_i)}{48}}}\\approx N(0,1)\\] 其中，\\(t_i\\)为\\(i\\)个“结”中有相同秩次的个数，\\(g\\)是“结”的个数。\nWilcoxon符号秩检验的前提条件为数据是连续的且差值分布是对称的。\nnotice：秩和秩和的区别：秩是指全部观察值按某种顺序排列的位序，在一定程度上反映了等级的高低；而秩和则表示同组秩次之和，在一定程度上反映了等级的分布。2\n\n\n2.1.2 应用场景\n\nMann-Whitney U 检验：\n\n比较两个独立样本的中位数是否存在显著差异。\n\n适用于非正态分布数据或含有极端值的样本。\n\n示例：比较两种治疗方法的疗效（不同受试者组）。\n\nWilcoxon 符号秩检验：\n\n比较两个配对样本的中位数差异。\n\n适用于重复测量数据或实验设计中存在配对关系的场景。\n\n示例：同一批受试者在治疗前后血压的变化。\n\n\n\n\n2.1.3 注意事项\n\n秩和检验是非参数方法，对数据分布假设少，但效率可能低于参数方法（如t检验）在满足条件时的效果，如果满足参数检验的条件，应优先考虑使用参数检验的方法，否则会增加犯二类错误的概率。\n数据需要满足独立性假设，否则检验结果可能不准确。\n\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-non-parameter-test.html#footnotes",
    "href": "Learn/Basic/09-non-parameter-test.html#footnotes",
    "title": "非参数检验",
    "section": "脚注",
    "text": "脚注\n\n\n不是说一定要选择样本量较小者对应的秩和作为检验统计量，只是长期的使用习惯，造成了这一惯例。如果取较小的秩和计算后得到的\\(U&lt;u_{\\alpha/2}\\)，则表示拒绝\\(H_0\\)；相反，如果取较大的秩和计算后得到的\\(U&gt;u_{1-\\alpha/2}\\)，也会表示拒绝\\(H_0\\)，他们都表示检验统计量落在了拒绝域中。↩︎\n尽管非参数方法对总体分布形式未做要求，但如果我们知道总体的一些性质而不去利用，就会浪费许多有用的信息，最常见的就是分布的对称性，配对设计的 Wilcoxon 符号秩检验充分利用了差值分布对称性这一信息，这与尽可能地采用有效方法，利用尽可能多的信息进行统计分析的大原则相一致。↩︎",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Bayes/00-About-Bayes.html",
    "href": "Learn/Bayes/00-About-Bayes.html",
    "title": "贝叶斯与概率推理",
    "section": "",
    "text": "Bayes系列的笔记主要基于Learning-Probabilistic-Graphical-Models-in-R.\n皮埃尔-西蒙·拉普拉斯(Pierre-Simon Laplace,1749-1827)，法国数学家，也是有史以来最伟大的科学家之被认为是第一批理解数据收集重要性的人:他发现了数 -，不据不可靠，有不确定性，也就是今天说的有噪声。他也是第一个研究使用概率来处理不确定性等问题，并表示事件或信息信念度的人。\n在他的论文《概率的哲学》​（Essai philosophique sur lesprobabilités,1814）中，拉普拉斯给出了最初的支持新老数据推理的数学系统，其中的用户信念会在新数据可用的时候得到更新和改进。\n概率是表示和处理不确定性的严密的数学方法。\n概率是一种量化常识推理和信念程度的工具。\n概率图模型，从数学的角度看，是一种表示几个变量概率分布的方法，也叫作联合概率分布。换句话说，它是一种表示几个变量共同出现的数值信念的工具。基于这种理解，虽然概率图模型看起来很简单，但是概率图模型强调的是对于许多变量概率分布的表示。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理"
    ]
  },
  {
    "objectID": "Learn/Bayes/00-About-Bayes.html#联合概率分布",
    "href": "Learn/Bayes/00-About-Bayes.html#联合概率分布",
    "title": "贝叶斯与概率推理",
    "section": "\n1 联合概率分布",
    "text": "1 联合概率分布\n当我们同时考虑两个试验（投掷硬币2次和投掷一个骰子）的时候，我们对同时获得0、1或2的概率以及1、2、3、4、5或6的点数概率更感兴趣。这两个同时考虑的随机变量的概率分布写作 \\(P(N, D)\\) ，称作联合概率分布。\n一个概率图模型就是一个联合概率分布。除此之外，别无他物。\n联合概率分布的最后一个重要概念是边缘化（marginalization）。\n联合分布\\(P(X,Y)\\)的边缘分布\\(P(X)\\)可以通过下列操作获得：\n\\[P(X)=\\sum_y P(X,Y)\\]\n当Y值是连续值是，边缘化可以写作：\n\\(P(X)=\\int_{y} P(X,y) \\mathrm{d}y\\)。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理"
    ]
  },
  {
    "objectID": "Learn/Bayes/00-About-Bayes.html#贝叶斯规则",
    "href": "Learn/Bayes/00-About-Bayes.html#贝叶斯规则",
    "title": "贝叶斯与概率推理",
    "section": "\n2 贝叶斯规则",
    "text": "2 贝叶斯规则\n\n2.1 条件概率\n条件概率是指在知道其他时间发生的条件下当前事件的概率。很明显，两个事件必须某种程度的依赖，否则一个事件的发生不会影响另一个事件。\n条件概率转化为公式如下：\n\\(P(X|Y)=\\frac{P(X,Y)}{P(Y)}\\)和\\(P(Y|X)=\\frac{P(X,Y)}{P(X)}\\)\n\n2.2 贝叶斯公式\n从上述两个公式推导出贝叶斯公式：\n\\(P(X|Y)=\\frac{{P(Y|X)}·P(X)}{P(Y)}\\)\n在这个公式中，我们把 \\(P(X|Y)\\) 叫做是给定\\(Y\\)下\\(X\\)的后验分布，因此，\\(P(X)\\) 叫做后验分布。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理"
    ]
  },
  {
    "objectID": "Learn/Bayes/00-About-Bayes.html#例子机器与灯泡",
    "href": "Learn/Bayes/00-About-Bayes.html#例子机器与灯泡",
    "title": "贝叶斯与概率推理",
    "section": "\n3 例子：机器与灯泡",
    "text": "3 例子：机器与灯泡\n在构建贝叶斯统计的时候，我们总是需要建立两个部件：\n\n先验分布\n似然率\n\n先验分布使我们关于机器工作状态的初试信念。我们确定了第一个刻画机器状态的随机变量 \\(M\\) 。这个随机变量有两个状态 \\({working,broken}\\) 。我们相信机器是好的，是可以正常工作的，所以先验分布如下：\n\n\\(P(M=working)=0.99\\)\n\\(P(M=broken)=0.01\\)\n\n\n3.1 R代码的实现\n先验分布、似然率和数据序列\n\nprior &lt;- c(working=0.99, broken=0.01) \nlikelihood &lt;- rbind( \n    working = c(good = 0.99, bad = 0.01),broken = c(good = 0.6, bad = 0.4) \n) \ndata &lt;- c(\"bad\", \"bad\", \"bad\", \"bad\") \n\n贝叶斯更新函数\n\nbayes &lt;-function(prior, likelihood, data)\n{\nposterior &lt;-matrix(0, nrow =length(data), ncol =length(prior))\ndimnames(posterior) &lt;-list(data, names(prior))\ninitial_prior &lt;-prior\nfor (i in 1:length(data))\n{\nposterior[i, ] &lt;-\nprior *likelihood[, data[i]]/\nsum(prior *likelihood[, data[i]])\nprior &lt;-posterior[i, ]\n}\nreturn(rbind(initial_prior, posterior))\n}\n\n\n创建一个矩阵，存储后验分布的连续计算结果。\n然后对于每一个数据，给定当前先验概率计算后验概率：和之前的一样，你可以看到贝叶斯公式的R代码。\n最后，新的先验概率是当前的后验概率，而且同样的过程可以迭代。\n分布的演化情况\n\nmatplot(bayes(prior, likelihood, data), t ='b', \n        lty =1, pch =20,\n        col =c(3, 2))\n\n\n\n\n\n\n\n随着坏灯泡的增多，机器正常的概率快速下降（绿色线）。\n如果我们换一个先验分布，假设我们不知道机器是否可以正常工作，即好坏参半，我们给定如下概率：\n\nprior &lt;- c(working = 0.5, broken = 0.5)\nmatplot(bayes(prior, likelihood, data), t ='b', \n        lty =1, pch =20,\n        col =c(3, 2))\n\n\n\n\n\n\n\n这个曲线快速收敛，机器有问题的概率很高。\n再对数据变换一下，假设机器正常工作的概率是99%，我们观察10个灯泡，其中一个灯泡是坏的：\n\nprior =c(working =0.99, broken =0.01)\ndata =c(\"bad\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\",\"good\", \"good\", \"good\")\nmatplot(bayes(prior, likelihood, data), t ='b', pch =20, col =c(3, 2))\n\n\n\n\n\n\n\n算法在第一个灯泡处犹豫了一下，因为这么好的机器不太可能生产出一个坏灯泡。然后它又收敛到很高的概率，因为好灯泡的序列不会预示任何问题。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理"
    ]
  },
  {
    "objectID": "navbar/book.html",
    "href": "navbar/book.html",
    "title": "公共卫生相关的书籍",
    "section": "",
    "text": "读的书有点乱，主要都在微信读书看，部分是纸质书，介绍主要以我看过或在看的书为主。\n\n1 Maybe you will be interested\n对于刚接触统计的朋友，可以先看看《女士品茶：统计学如何变革了科学和生活》这本书，也会他能激发你的一些对于统计学（家们）的兴趣，通过一些有趣的小故事，带你走进统计学的世界。\n2025-03-30，出于对 THE LADY TASTING TEA 的兴趣，后续将会整理一下出现的主要统计学家和他的主要成果（公式）。\n\n\n2 Basic\n考研的话，除了人卫第八版是一个通用教材，其他的院校各有不同，比如说赵耐青、方积乾和贺佳，根据院校给的参考书目来选择，这几个人的书都买了看过一下，个人主要用过人卫版和姜晶梅的《医学统计学》，各有见长。\n\n\n3 Advance\n提升部分可以看看 Springer 出版的 Mathematical Models in Epidemiology ，2023年科学出版社出版了中文版 《流行病学中的数学模型》\n\nThe book is a comprehensive, self-contained introduction to the mathematical modeling and analysis of disease transmission models. It includes (i) an introduction to the main concepts of compartmental models including models with heterogeneous mixing of individuals and models for vector-transmitted diseases, (ii) a detailed analysis of models for important specific diseases, including tuberculosis, HIV/AIDS, influenza, Ebola virus disease, malaria, dengue fever and the Zika virus, (iii) an introduction to more advanced mathematical topics, including age structure, spatial structure, and mobility, and (iv) some challenges and opportunities for the future.\nThere are exercises of varying degrees of difficulty, and projects leading to new research directions. For the benefit of public health professionals whose contact with mathematics may not be recent, there is an appendix covering the necessary mathematical background. There are indications which sections require a strong mathematical background so that the book can be useful for both mathematical modelers and public health professionals.\n\nend."
  },
  {
    "objectID": "Quarto/quarto-intro.html",
    "href": "Quarto/quarto-intro.html",
    "title": "Quarto introduction",
    "section": "",
    "text": "Quarto introduction\n\nAn open-source scientific and technical publishing system\n\nAuthor using Jupyter notebooks or with plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nShare knowledge and insights organization-wide by publishing to Posit Connect, Confluence, or other publishing systems.\nWrite using Pandoc markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\n\n\nAnalyze. Share. Reproduce. You have a story to tell with data—tell it with Quarto.\nFrom https://quarto.org/\nend.",
    "crumbs": [
      "Home",
      "Quarto introduction"
    ]
  },
  {
    "objectID": "Guide/R/2025-02-22-CLHLS.html#构建logistic回归模型",
    "href": "Guide/R/2025-02-22-CLHLS.html#构建logistic回归模型",
    "title": "CLHLS Data Analysis by R",
    "section": "\n3.1 构建Logistic回归模型",
    "text": "3.1 构建Logistic回归模型\nLogistic 回归分析最大的一个优势可能就是广泛涉及优势比（？），在这里介绍一下优势比的概念。\n\n3.1.1 优势比\n优势（Odds）是指某事件的发生概率 \\(\\pi\\) 与该事件不发生的概率 \\(1-\\pi\\) 之比，亦称为比，记为 Odds ，某事件在两种不同条件下的优势之比称为优势比（Odds Ratio，OR）。优势比在流行病学中的病例对照研究中被普遍应用（当然不仅限于病例对照研究）。\n设某事件的两种不同暴露的发生概率分别为 \\(\\pi_1\\) 和 \\(\\pi_0\\) ，对应的 \\(Odds_1= \\pi_1 /（1-\\pi_1）\\)，\\(Odds_0 =\\pi_0 /（1-\\pi_0）\\) ，两个 \\(Odds\\) 之比定义为 \\(OR=Odds_1/Odds_0\\)。\n由于 \\(Odds=\\pi /(1-\\pi)=(\\pi-1+1)/(1-\\pi)=－1+1 /(1-\\pi)\\) 以及 \\(0＜ \\pi ＜1\\)，所以 \\(\\pi\\) 越大，\\(Odds\\) 就越大，反之 \\(\\pi\\) 越小，\\(Odds\\) 就越小，特別当 \\(\\pi\\) 越接近 0时，\\(Odds\\) 也越接近 0，因此优势和优势比具有下列性质。\n\n如果 \\(\\pi_1 = \\pi_0\\)。，对应有 \\(Odds_1=Odds_0\\) , \\(OR=1\\) 。\n如果 \\(\\pi_1&gt;\\pi_0\\) ，则 \\(Odds_1&gt;Odds_o\\) ，相应有 $ OR&gt;1$ 。同理如果 \\(\\pi_1&lt;\\pi_0\\)，则 \\(Odds_1&lt;Odds_0\\) ，相应有 \\(OR &lt; 1\\)。\n\n由于概率 \\(\\pi_1\\) 也可以用 \\(Odds\\) 表示，\\(\\pi=\\frac{Odds}{1+Odds}\\) ，所以也可以通过比较 \\(Odds_1\\) 与 \\(Odds_0\\) 的大小来推断 \\(\\pi_1\\) 和 \\(\\pi_0\\) 的大小关系，即可以用 \\(OR&gt;1\\), \\(OR=1\\) 或 \\(OR&lt;1\\) 推断 \\(\\pi_1\\) 和 \\(\\pi_0\\) 的大小关系。\n\n3.1.2 Logistic 回归模型\nLogistic 回归模型的因变量必须为分类变量，主要有三种：二分类 Logistic 回归模型、有序分类 Logistic 回归模型、无序分类 Logistic 回归模型。其中二分类 Logistic 回归模型最为常用。自变量则无要求，可以是定量变量、有序分类变量和无序分类变量。\n假如研究所关注的事件（如死亡或痊愈等）是否发生用因变量 \\(Y\\) 表示，\\(Y=1\\) 表示该结局事件发生，反之，\\(Y=0\\) 表示该结局事件未发生。\n那么可构建如下方程式：\n\\[logit(\\pi(Y=1))=ln\\frac{\\pi(Y=1)}{\\pi(Y=0)}=ln\\frac{\\pi(Y=1)}{1-\\pi(Y=1)}=\\beta_0 + \\beta_1 X_1 + \\cdot \\beta_p X_p\\]\n\n3.1.3 共线性检验\n共线性(Colinearity)指的是自变量之间存在高度相关性。这种情况会导致回归系数的不稳定，并使得对模型参数 的估计变得不可靠。在 Logistic 回归中，严重的共线性可能导致模型性能下降，甚至可能导致预测结果难以解释\n方差膨胀因子是统计学中用于衡量多元线性回归模型中自变量之间共线性程度的指标，提供了一种定量的方式来评估自变量之间的共线性。\n方差膨胀因子的解释标准通常如下：如果VIF值小于5，表示自变量之间的共线性程度较低，可以接受。如果VIF值在5到10之间， 表示自变量之间存在一定程度的共线性，但尚可接受。如果VIF值大于10，表示自变量之间存在严重的共线性问题， 需要考虑进行变量选择或者采取其他方法来处理共线性。\n自评健康作为 因变量 ，自变量是 经济支持、生活支持和情绪支持，在前序的处理过程，只有原始的的经济支持变量（加总金额）是定量变量，其他两个都是做的二分类变量处理，如何做共线性检验，看起来并不明朗，因为三个变量看起来没有太多的关系，但是控制变量有些可能存在相关性，但是，是否有对控制变量做共线性检验的必要？\n\n3.1.4 2025-03-07 共线性检验\n使用car包对共线性进行检验。\n理论上，共线性检验较为简单，但是在处理此数据时，碰壁较多。\n首先核查数据的完整性，删除全NA和单一值变量，再将分类变量转换为因子类型，经济变量这里也是用“经济分组”这一变量处理，即所有的变量都是分类变量。（全部为分类变量在这里处理其实可能有些问题，我暂未找到合适的处理办法，寻找相关的信息也没有较为清晰地答案）\n\n# library(car)\n# 数据完整性检查，删除全NA和单一值变量\nvalid_vars &lt;- all_predictors[sapply(final_data[, all_predictors, drop = FALSE], function(x) !(all(is.na(x)) || length(unique(na.omit(x))) == 1))]\nfinal_data &lt;- final_data[, c(outcome, valid_vars)]\n\n# 仅将二分类变量转换为因子类型\nbinary_vars &lt;- valid_vars[sapply(final_data[, valid_vars], function(x) length(unique(na.omit(x))) == 2)]\nfinal_data &lt;- final_data %&gt;% mutate(across(all_of(binary_vars), as.factor))\n\n# VIF计算\nif (length(valid_vars) &gt; 1) {\n  formula_vif &lt;- as.formula(paste(outcome, \"~\", paste(valid_vars, collapse = \" + \")))\n  lm_model &lt;- lm(formula_vif, data = final_data)\n  vif_values &lt;- vif(lm_model)\n  \n  vif_df &lt;- data.frame(\n    Variable = names(vif_values),\n    VIF = round(vif_values, 3)\n  )\n  mean_vif &lt;- mean(vif_values, na.rm = TRUE)\n  # 结果保留两位小数\n  mean_vif &lt;- round(mean_vif, 3)\n  \n  # 保存结果\n  writeLines(c(\"=== 多重共线性检验 (VIF Results) ===\", \n               paste(\"平均 VIF 值:\", round(mean_vif, 3)), \"\",\n               capture.output(print(vif_df))), con = output_vif_file)\n  \n  write_xlsx(list(VIF_Results = vif_df, Mean_VIF = data.frame(Statistic = \"平均 VIF 值\", Value = round(mean_vif, 3))), \n             output_vif_excel)\n  \n  cat(\"VIF 检验完成，结果已保存至:\", output_vif_file, \"和\", output_vif_excel, \"\\n\")\n} else {\n  cat(\"错误：自变量数量不足，无法计算 VIF\\n\")\n}\n\n\n3.1.5 logistic 回归\n\n# 5. Logistic 回归分析\nlogistic_results &lt;- list()\n\nfor (outcome in outcomes) {\n  formula &lt;- as.formula(\n    paste(outcome, \"~\", paste(independents, collapse = \" + \"), \"+\", paste(controls, collapse = \" + \"))\n  )\n  \n  model &lt;- glm(formula, data = final_data, family = binomial(link = \"logit\"))\n  \n  # 提取回归结果，保留三位小数\n  model_summary &lt;- summary(model)\n  coef_table &lt;- as.data.frame(coef(model_summary)) %&gt;%\n    mutate(\n      Variable = rownames(.),\n      OR = round(exp(Estimate), 3),\n      OR_Lower = round(exp(Estimate - 1.96 * `Std. Error`), 3),\n      OR_Upper = round(exp(Estimate + 1.96 * `Std. Error`), 3),\n      `Wald χ²` = round((Estimate / `Std. Error`)^2, 3),\n      df = 1\n    ) %&gt;%\n    select(Variable, df, Estimate, `Std. Error`, `Wald χ²`, `Pr(&gt;|z|)`, OR, OR_Lower, OR_Upper) %&gt;%\n    rename(\n      `回归系数` = Estimate,\n      `标准误` = `Std. Error`,\n      `P 值` = `Pr(&gt;|z|)`\n    ) %&gt;%\n    mutate(across(c(`回归系数`, `标准误`, `P 值`), ~ round(.x, 3)))\n  \n  logistic_results[[outcome]] &lt;- coef_table\n}\n\n# 6. 生成 Word 文档\ndoc &lt;- read_docx()\n\nfor (outcome in outcomes) {\n  doc &lt;- doc %&gt;%\n    body_add_par(value = paste(\"Logistic 回归分析结果：因变量 =\", outcome), style = \"heading 1\") %&gt;%\n    body_add_flextable(flextable(logistic_results[[outcome]]) %&gt;%\n                         set_table_properties(width = 1, layout = \"autofit\"))\n}\n\nprint(doc, target = output_logistic_file)\ncat(\"Logistic 回归分析结果已保存至:\", output_logistic_file, \"\\n\")",
    "crumbs": [
      "Home",
      "统计软件",
      "R",
      "CLHLS Data Analysis by R"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html",
    "href": "Guide/Python/25-03-09-cos-reg.html",
    "title": "用Python做生存分析和COX回归",
    "section": "",
    "text": "生存分析（survival analysis）是一种统计方法，用于分析时间数据， 主要研究生存时间和结局的分布及其影响因素的统计方法。在生存分析中，每个研究对象的结局变量由 “time”（生存时间） 和 “status”（生存状态）组成。生存时间是指从某个特定时间点开始，到某个事件的节点时，事件数据是指某个事件是否发生。 生存时间是一个非负实数，生存状态是一个二元变量，通常用1表示事件发生，0表示事件未发生。\n生存分析的主要应用领域是医学、生物学、工程学、经济学等。\n\n\n生存函数（survival function）是生存分析的基本概念之一，它是一个函数，用于刻画研究对象在某个时刻 t 内存存活的概率。 生存函数通常用 \\(S(t)\\) 表示。\n风险函数（hazard function）是生存分析的另一个基本概念，用于刻画研究对象在某个时刻 t 还存活但是极短的时间内死亡的风险。 风险函数通常用 \\(h(t)\\) 表示。\n如果记寿命分布的密度为 \\(f(t)\\)，则有： \\(h(t) = f(t) / S(t)\\) 。\n\n\n\n这里使用 R 语言 survival 包中的 ovarian 数据集，该数据集来自一项比较卵巢癌患者在两种治疗方式下的生存率比较的随机对照试验。\n首先找到 ovarian 数据集，你可以从互联网上寻找相关资源；或者从 R 的 survival 包中导出这一数据集，操作如下：\n# install.packages(\"survival\")\nlibrary(survival)\novarian\ndata(cancer, package=\"survival\")\n\ndf &lt;- ovarian\nwrite.csv(df, \"your-file-path\\\\ovarian.csv\", row.names = FALSE)\n将数据集下载到你的工作目录，然后使用 Pandas 导入与读取：\n\nimport pandas as pd\novarian = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\R\\quarto\\Med-Stat-Notes\\Data\\ovarian.csv\")\novarian.head()\n\n\n\n\n\n\n\n\nfutime\nfustat\nage\nresid.ds\nrx\necog.ps\n\n\n\n\n0\n59\n1\n72.3315\n2\n1\n1\n\n\n1\n115\n1\n74.4932\n2\n1\n1\n\n\n2\n156\n1\n66.4658\n2\n1\n2\n\n\n3\n421\n0\n53.3644\n2\n2\n1\n\n\n4\n431\n1\n50.3397\n2\n1\n1\n\n\n\n\n\n\n\n数据集包括 26 个观测值，6 个变量。变量如下：futime（随访时间），fustat（研究结束时的状态：0 表示存活，1表示死亡），age（患者的年龄），resid.ds（疾病残留情况：1 表示有残留，2 表示没有残留），rx（治疗方式：1 表示环磷酰胺，2 表示环磷酰胺+阿霉素）和 ecog.ps（患者的 ECOG 评分：1 表示较好，2 表示较差）。\n对年龄进行分组，分为 &lt;50 和 &gt;50 两组，并将其他三个变量的各水平加上相应的标签。\n\n# 查看 ovarian(DataFrame)的变量名，因为不同的渠道下载的 Dataset 可能会有区别\nprint(ovarian.columns)\n\n# 将 age 列转换为数值类型\novarian['age'] = pd.to_numeric(ovarian['age'], errors='coerce')\n\novarian.age = pd.cut(ovarian.age, [0,50,75], labels = ['&lt;=50','&gt;50'])\n\novarian['resid.ds'] = ovarian['resid.ds'].map({1: \"NO\", 2: \"Yes\"})\novarian['rx'] = ovarian['rx'].map({1: \"A\", 2: \"B\"})\novarian['ecog.ps'] = ovarian['ecog.ps'].map({1: \"Good\", 2: \"Bad\"})\n\nIndex(['futime', 'fustat', 'age', 'resid.ds', 'rx', 'ecog.ps'], dtype='object')\n\n\n\n\n\n生存率的 Kaplan-Meier 估计的计算可以调用 lifelines 库中的 KaplanMeierFitter 函数实现。\npip 安装：pip install lifelines\nconda 安装： conda install lifelines\n拟合 fit :\n\nfrom lifelines import KaplanMeierFitter\nkmf = KaplanMeierFitter()\nfit = kmf.fit(ovarian.futime,ovarian.fustat)\nfit\n\n&lt;lifelines.KaplanMeierFitter:\"KM_estimate\", fitted with 26 total observations, 14 right-censored observations&gt;\n\n\n拟合结果 fit 包含了很多属性，我们可以通过点操作符单独提取其中的属性。例如，查看中位生存时间：\n\nfit.median_survival_time_\n\n638.0\n\n\n中位数生存时间表示，有 50% 的患者生存时间达到了 638 天。还可以提取寿命表和生存函数等属性，通过以下方式实现合并查看：\n\npd.concat([fit.event_table,fit.survival_function_], axis = 1)\n\n\n\n\n\n\n\n\nremoved\nobserved\ncensored\nentrance\nat_risk\nKM_estimate\n\n\n\n\n0.0\n0\n0\n0\n26\n26\n1.000000\n\n\n59.0\n1\n1\n0\n0\n26\n0.961538\n\n\n115.0\n1\n1\n0\n0\n25\n0.923077\n\n\n156.0\n1\n1\n0\n0\n24\n0.884615\n\n\n268.0\n1\n1\n0\n0\n23\n0.846154\n\n\n329.0\n1\n1\n0\n0\n22\n0.807692\n\n\n353.0\n1\n1\n0\n0\n21\n0.769231\n\n\n365.0\n1\n1\n0\n0\n20\n0.730769\n\n\n377.0\n1\n0\n1\n0\n19\n0.730769\n\n\n421.0\n1\n0\n1\n0\n18\n0.730769\n\n\n431.0\n1\n1\n0\n0\n17\n0.687783\n\n\n448.0\n1\n0\n1\n0\n16\n0.687783\n\n\n464.0\n1\n1\n0\n0\n15\n0.641931\n\n\n475.0\n1\n1\n0\n0\n14\n0.596078\n\n\n477.0\n1\n0\n1\n0\n13\n0.596078\n\n\n563.0\n1\n1\n0\n0\n12\n0.546405\n\n\n638.0\n1\n1\n0\n0\n11\n0.496732\n\n\n744.0\n1\n0\n1\n0\n10\n0.496732\n\n\n769.0\n1\n0\n1\n0\n9\n0.496732\n\n\n770.0\n1\n0\n1\n0\n8\n0.496732\n\n\n803.0\n1\n0\n1\n0\n7\n0.496732\n\n\n855.0\n1\n0\n1\n0\n6\n0.496732\n\n\n1040.0\n1\n0\n1\n0\n5\n0.496732\n\n\n1106.0\n1\n0\n1\n0\n4\n0.496732\n\n\n1129.0\n1\n0\n1\n0\n3\n0.496732\n\n\n1206.0\n1\n0\n1\n0\n2\n0.496732\n\n\n1227.0\n1\n0\n1\n0\n1\n0.496732\n\n\n\n\n\n\n\n\n\nKaplan-Meier 法估计的生存率是一个阶梯状的函数，其阶梯跳跃点是给定的时间点，我们可以通过调用 plot 方法绘制生存曲线，如图所示：\n\nimport matplotlib.pyplot as plt\nfit.plot(show_censors = True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n在生存分析中，经常需要比较不同情形下的生存率。在本例中，想要比较不同治疗方式下生存率，可以输入下面的命令：\n\ng1 = ovarian.rx == \"A\"\ng2 = ovarian.rx == \"B\"\nkmf_A = KaplanMeierFitter()\nkmf_A.fit(ovarian.futime[g1],ovarian.fustat[g1],label = \"Treatmeat A\")\nkmf_B = KaplanMeierFitter()\nkmf_B.fit(ovarian.futime[g2],ovarian.fustat[g2],label = \"Treatmeat B\")\n\n&lt;lifelines.KaplanMeierFitter:\"Treatmeat B\", fitted with 13 total observations, 8 right-censored observations&gt;\n\n\n\n\n可以单独提取两组的生存函数进行比较，但在同一个图中显示多条生存曲线更有助于生存率的比较。\n\nfig, axes = plt.subplots()\nkmf_A.plot(ax = axes,show_censors = True)\nkmf_B.plot(ax = axes,show_censors = True)\nplt.show()\n\n\n\n\n\n\n\n\n从上图中可以看出，治疗方式 “B” 的生存率高于治疗方式 “A” 的生存率，但是这种差异是由随机误差引起还是真是的治疗方式的不同所造成的差异，需要做进一步的统计学检验。\n\n\n\n因果关联的推断步骤\n\n\n\n\n\n生存分析中常用的统计学检验是 时序检验（log rank test），其基本思想是先计算出不同时间两种治疗方式的暴露人数和死亡人数，并由此在两种治疗方式效果相同的假设下计算出预期死亡人数，如果不拒绝零假设（ \\(H_0\\) ：两种治疗方式的效果相同，即预期死亡人数一致），则实际观测值和期望值的差异不会很大，如果差异过大则不能认为该差异是由随机误差引起的。\n对此，用 \\(\\chi^2\\) 检验做判断。时序检验可以用 lifetimes 库的函数 logrank_test 实现。\n\nfrom lifelines.statistics import logrank_test\nlr = logrank_test(ovarian.futime[g1], ovarian.futime[g2],\n                  ovarian.fustat[g1], ovarian.fustat[g2])\nlr.p_value\n\n0.3025911169890923\n\n\n这里得到的结果为 \\(P&gt;0.05\\) ，在一般情况下，我们会认为这是没有统计学意义的，即无法排除差异是由随机误差引起的。\n这种结果不显著的情况下，我们可以做一些思考，即是否有其他因素干预了结果的显著性，以及是否是样本量过小，导致差异不显著。（样本量的大小会影响检验效能，如果检验效能太低，即使有差异，也很难被检验方法发现）\n这里我们无法改变样本量的大小，但是可以考虑其他混杂因素的影响，并且使用更全面的模型进行检验。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#生存函数",
    "href": "Guide/Python/25-03-09-cos-reg.html#生存函数",
    "title": "用Python做生存分析和COX回归",
    "section": "",
    "text": "生存函数（survival function）是生存分析的基本概念之一，它是一个函数，用于刻画研究对象在某个时刻 t 内存存活的概率。 生存函数通常用 \\(S(t)\\) 表示。\n风险函数（hazard function）是生存分析的另一个基本概念，用于刻画研究对象在某个时刻 t 还存活但是极短的时间内死亡的风险。 风险函数通常用 \\(h(t)\\) 表示。\n如果记寿命分布的密度为 \\(f(t)\\)，则有： \\(h(t) = f(t) / S(t)\\) 。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#数据集及来源",
    "href": "Guide/Python/25-03-09-cos-reg.html#数据集及来源",
    "title": "用Python做生存分析和COX回归",
    "section": "",
    "text": "这里使用 R 语言 survival 包中的 ovarian 数据集，该数据集来自一项比较卵巢癌患者在两种治疗方式下的生存率比较的随机对照试验。\n首先找到 ovarian 数据集，你可以从互联网上寻找相关资源；或者从 R 的 survival 包中导出这一数据集，操作如下：\n# install.packages(\"survival\")\nlibrary(survival)\novarian\ndata(cancer, package=\"survival\")\n\ndf &lt;- ovarian\nwrite.csv(df, \"your-file-path\\\\ovarian.csv\", row.names = FALSE)\n将数据集下载到你的工作目录，然后使用 Pandas 导入与读取：\n\nimport pandas as pd\novarian = pd.read_csv(r\"C:\\Users\\asus\\Desktop\\R\\quarto\\Med-Stat-Notes\\Data\\ovarian.csv\")\novarian.head()\n\n\n\n\n\n\n\n\nfutime\nfustat\nage\nresid.ds\nrx\necog.ps\n\n\n\n\n0\n59\n1\n72.3315\n2\n1\n1\n\n\n1\n115\n1\n74.4932\n2\n1\n1\n\n\n2\n156\n1\n66.4658\n2\n1\n2\n\n\n3\n421\n0\n53.3644\n2\n2\n1\n\n\n4\n431\n1\n50.3397\n2\n1\n1\n\n\n\n\n\n\n\n数据集包括 26 个观测值，6 个变量。变量如下：futime（随访时间），fustat（研究结束时的状态：0 表示存活，1表示死亡），age（患者的年龄），resid.ds（疾病残留情况：1 表示有残留，2 表示没有残留），rx（治疗方式：1 表示环磷酰胺，2 表示环磷酰胺+阿霉素）和 ecog.ps（患者的 ECOG 评分：1 表示较好，2 表示较差）。\n对年龄进行分组，分为 &lt;50 和 &gt;50 两组，并将其他三个变量的各水平加上相应的标签。\n\n# 查看 ovarian(DataFrame)的变量名，因为不同的渠道下载的 Dataset 可能会有区别\nprint(ovarian.columns)\n\n# 将 age 列转换为数值类型\novarian['age'] = pd.to_numeric(ovarian['age'], errors='coerce')\n\novarian.age = pd.cut(ovarian.age, [0,50,75], labels = ['&lt;=50','&gt;50'])\n\novarian['resid.ds'] = ovarian['resid.ds'].map({1: \"NO\", 2: \"Yes\"})\novarian['rx'] = ovarian['rx'].map({1: \"A\", 2: \"B\"})\novarian['ecog.ps'] = ovarian['ecog.ps'].map({1: \"Good\", 2: \"Bad\"})\n\nIndex(['futime', 'fustat', 'age', 'resid.ds', 'rx', 'ecog.ps'], dtype='object')",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#生存率的-kaplan-meier-估计",
    "href": "Guide/Python/25-03-09-cos-reg.html#生存率的-kaplan-meier-估计",
    "title": "用Python做生存分析和COX回归",
    "section": "",
    "text": "生存率的 Kaplan-Meier 估计的计算可以调用 lifelines 库中的 KaplanMeierFitter 函数实现。\npip 安装：pip install lifelines\nconda 安装： conda install lifelines\n拟合 fit :\n\nfrom lifelines import KaplanMeierFitter\nkmf = KaplanMeierFitter()\nfit = kmf.fit(ovarian.futime,ovarian.fustat)\nfit\n\n&lt;lifelines.KaplanMeierFitter:\"KM_estimate\", fitted with 26 total observations, 14 right-censored observations&gt;\n\n\n拟合结果 fit 包含了很多属性，我们可以通过点操作符单独提取其中的属性。例如，查看中位生存时间：\n\nfit.median_survival_time_\n\n638.0\n\n\n中位数生存时间表示，有 50% 的患者生存时间达到了 638 天。还可以提取寿命表和生存函数等属性，通过以下方式实现合并查看：\n\npd.concat([fit.event_table,fit.survival_function_], axis = 1)\n\n\n\n\n\n\n\n\nremoved\nobserved\ncensored\nentrance\nat_risk\nKM_estimate\n\n\n\n\n0.0\n0\n0\n0\n26\n26\n1.000000\n\n\n59.0\n1\n1\n0\n0\n26\n0.961538\n\n\n115.0\n1\n1\n0\n0\n25\n0.923077\n\n\n156.0\n1\n1\n0\n0\n24\n0.884615\n\n\n268.0\n1\n1\n0\n0\n23\n0.846154\n\n\n329.0\n1\n1\n0\n0\n22\n0.807692\n\n\n353.0\n1\n1\n0\n0\n21\n0.769231\n\n\n365.0\n1\n1\n0\n0\n20\n0.730769\n\n\n377.0\n1\n0\n1\n0\n19\n0.730769\n\n\n421.0\n1\n0\n1\n0\n18\n0.730769\n\n\n431.0\n1\n1\n0\n0\n17\n0.687783\n\n\n448.0\n1\n0\n1\n0\n16\n0.687783\n\n\n464.0\n1\n1\n0\n0\n15\n0.641931\n\n\n475.0\n1\n1\n0\n0\n14\n0.596078\n\n\n477.0\n1\n0\n1\n0\n13\n0.596078\n\n\n563.0\n1\n1\n0\n0\n12\n0.546405\n\n\n638.0\n1\n1\n0\n0\n11\n0.496732\n\n\n744.0\n1\n0\n1\n0\n10\n0.496732\n\n\n769.0\n1\n0\n1\n0\n9\n0.496732\n\n\n770.0\n1\n0\n1\n0\n8\n0.496732\n\n\n803.0\n1\n0\n1\n0\n7\n0.496732\n\n\n855.0\n1\n0\n1\n0\n6\n0.496732\n\n\n1040.0\n1\n0\n1\n0\n5\n0.496732\n\n\n1106.0\n1\n0\n1\n0\n4\n0.496732\n\n\n1129.0\n1\n0\n1\n0\n3\n0.496732\n\n\n1206.0\n1\n0\n1\n0\n2\n0.496732\n\n\n1227.0\n1\n0\n1\n0\n1\n0.496732\n\n\n\n\n\n\n\n\n\nKaplan-Meier 法估计的生存率是一个阶梯状的函数，其阶梯跳跃点是给定的时间点，我们可以通过调用 plot 方法绘制生存曲线，如图所示：\n\nimport matplotlib.pyplot as plt\nfit.plot(show_censors = True)\nplt.show()",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#生存率的比较",
    "href": "Guide/Python/25-03-09-cos-reg.html#生存率的比较",
    "title": "用Python做生存分析和COX回归",
    "section": "",
    "text": "在生存分析中，经常需要比较不同情形下的生存率。在本例中，想要比较不同治疗方式下生存率，可以输入下面的命令：\n\ng1 = ovarian.rx == \"A\"\ng2 = ovarian.rx == \"B\"\nkmf_A = KaplanMeierFitter()\nkmf_A.fit(ovarian.futime[g1],ovarian.fustat[g1],label = \"Treatmeat A\")\nkmf_B = KaplanMeierFitter()\nkmf_B.fit(ovarian.futime[g2],ovarian.fustat[g2],label = \"Treatmeat B\")\n\n&lt;lifelines.KaplanMeierFitter:\"Treatmeat B\", fitted with 13 total observations, 8 right-censored observations&gt;\n\n\n\n\n可以单独提取两组的生存函数进行比较，但在同一个图中显示多条生存曲线更有助于生存率的比较。\n\nfig, axes = plt.subplots()\nkmf_A.plot(ax = axes,show_censors = True)\nkmf_B.plot(ax = axes,show_censors = True)\nplt.show()\n\n\n\n\n\n\n\n\n从上图中可以看出，治疗方式 “B” 的生存率高于治疗方式 “A” 的生存率，但是这种差异是由随机误差引起还是真是的治疗方式的不同所造成的差异，需要做进一步的统计学检验。\n\n\n\n因果关联的推断步骤\n\n\n\n\n\n生存分析中常用的统计学检验是 时序检验（log rank test），其基本思想是先计算出不同时间两种治疗方式的暴露人数和死亡人数，并由此在两种治疗方式效果相同的假设下计算出预期死亡人数，如果不拒绝零假设（ \\(H_0\\) ：两种治疗方式的效果相同，即预期死亡人数一致），则实际观测值和期望值的差异不会很大，如果差异过大则不能认为该差异是由随机误差引起的。\n对此，用 \\(\\chi^2\\) 检验做判断。时序检验可以用 lifetimes 库的函数 logrank_test 实现。\n\nfrom lifelines.statistics import logrank_test\nlr = logrank_test(ovarian.futime[g1], ovarian.futime[g2],\n                  ovarian.fustat[g1], ovarian.fustat[g2])\nlr.p_value\n\n0.3025911169890923\n\n\n这里得到的结果为 \\(P&gt;0.05\\) ，在一般情况下，我们会认为这是没有统计学意义的，即无法排除差异是由随机误差引起的。\n这种结果不显著的情况下，我们可以做一些思考，即是否有其他因素干预了结果的显著性，以及是否是样本量过小，导致差异不显著。（样本量的大小会影响检验效能，如果检验效能太低，即使有差异，也很难被检验方法发现）\n这里我们无法改变样本量的大小，但是可以考虑其他混杂因素的影响，并且使用更全面的模型进行检验。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#建立模型",
    "href": "Guide/Python/25-03-09-cos-reg.html#建立模型",
    "title": "用Python做生存分析和COX回归",
    "section": "2.1 建立模型",
    "text": "2.1 建立模型\n在建立模型前，需要对分类变量进行哑变量处理：\n\ndf_dummy = pd.get_dummies(ovarian,drop_first = True)\ndf_dummy.head()\n\n\n\n\n\n\n\n\nfutime\nfustat\nage_&gt;50\nresid.ds_Yes\nrx_B\necog.ps_Good\n\n\n\n\n0\n59\n1\nTrue\nTrue\nFalse\nTrue\n\n\n1\n115\n1\nTrue\nTrue\nFalse\nTrue\n\n\n2\n156\n1\nTrue\nTrue\nFalse\nFalse\n\n\n3\n421\n0\nTrue\nTrue\nTrue\nTrue\n\n\n4\n431\n1\nTrue\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n使用 drop_first = True 参数是为了去掉各个参考类别。\n下面将所有的协变量都纳入，建立 Cox 回归模型：\n\nfrom lifelines import CoxPHFitter\ncox = CoxPHFitter()\ncox.fit(df_dummy,duration_col = 'futime', event_col = 'fustat')\ncox.print_summary()\n\n\n\n\n\n\n\nmodel\nlifelines.CoxPHFitter\n\n\nduration col\n'futime'\n\n\nevent col\n'fustat'\n\n\nbaseline estimation\nbreslow\n\n\nnumber of observations\n26\n\n\nnumber of events observed\n12\n\n\npartial log-likelihood\n-28.89\n\n\ntime fit was run\n2025-03-10 05:46:54 UTC\n\n\n\n\n\n\n\n\n\ncoef\nexp(coef)\nse(coef)\ncoef lower 95%\ncoef upper 95%\nexp(coef) lower 95%\nexp(coef) upper 95%\ncmp to\nz\np\n-log2(p)\n\n\n\n\nage_&gt;50\n2.20\n9.04\n1.11\n0.03\n4.37\n1.03\n79.10\n0.00\n1.99\n0.05\n4.42\n\n\nresid.ds_Yes\n1.45\n4.25\n0.73\n0.02\n2.88\n1.02\n17.75\n0.00\n1.98\n0.05\n4.40\n\n\nrx_B\n-1.38\n0.25\n0.64\n-2.65\n-0.12\n0.07\n0.89\n0.00\n-2.14\n0.03\n4.96\n\n\necog.ps_Good\n-0.59\n0.56\n0.63\n-1.83\n0.65\n0.16\n1.92\n0.00\n-0.93\n0.35\n1.50\n\n\n\n\n\n\n\n\n\nConcordance\n0.79\n\n\nPartial AIC\n65.78\n\n\nlog-likelihood ratio test\n12.19 on 4 df\n\n\n-log2(p) of ll-ratio test\n5.97\n\n\n\n\n\n\n\n结果显示，在调整了协变量后，两种治疗方式的死亡风险的差异具有统计学意义（P&lt;0.05）。\n模型的回归系数及其置信区间可以通过 plot 方法进行直观展示：\n\ncox.plot()\n\n\n\n\n\n\n\n\nCox 回归是一种半参数回归模型，也像多元线性回归一样，存在变量选择的问题。通常可以用 AIC 进行变量选择。1\n查看当前模型的 AIC 值：\n\ncox.AIC_partial_\n\n65.77513405570859\n\n\n根据前序的结果，变量 ecog.ps 对应的 P 值最大，对其进行剔除后再次拟合模型：\n\ncox1 = CoxPHFitter()\ndf_dummy_sub = df_dummy.drop('ecog.ps_Good', axis = 1)\ncox1.fit(df_dummy_sub, duration_col = 'futime', event_col = 'fustat')\ncox1.print_summary()\ncox1.AIC_partial_\n\n\n\n\n\n\n\nmodel\nlifelines.CoxPHFitter\n\n\nduration col\n'futime'\n\n\nevent col\n'fustat'\n\n\nbaseline estimation\nbreslow\n\n\nnumber of observations\n26\n\n\nnumber of events observed\n12\n\n\npartial log-likelihood\n-29.33\n\n\ntime fit was run\n2025-03-10 05:46:54 UTC\n\n\n\n\n\n\n\n\n\ncoef\nexp(coef)\nse(coef)\ncoef lower 95%\ncoef upper 95%\nexp(coef) lower 95%\nexp(coef) upper 95%\ncmp to\nz\np\n-log2(p)\n\n\n\n\nage_&gt;50\n2.11\n8.29\n1.09\n-0.02\n4.25\n0.98\n70.32\n0.00\n1.94\n0.05\n4.25\n\n\nresid.ds_Yes\n1.25\n3.50\n0.69\n-0.10\n2.61\n0.90\n13.58\n0.00\n1.81\n0.07\n3.83\n\n\nrx_B\n-1.28\n0.28\n0.62\n-2.50\n-0.06\n0.08\n0.94\n0.00\n-2.06\n0.04\n4.67\n\n\n\n\n\n\n\n\n\nConcordance\n0.77\n\n\nPartial AIC\n64.66\n\n\nlog-likelihood ratio test\n11.31 on 3 df\n\n\n-log2(p) of ll-ratio test\n6.62\n\n\n\n\n\n\n\n64.65793573545281\n\n\n剔除 ecog_ps 后的模型， AIC 值有所下降，但是不多（谨慎对待），能认为新模型优于原有模型。",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-09-cos-reg.html#footnotes",
    "href": "Guide/Python/25-03-09-cos-reg.html#footnotes",
    "title": "用Python做生存分析和COX回归",
    "section": "脚注",
    "text": "脚注\n\n\n赤池信息量准则，即Akaike information criterion，简称AIC，是衡量统计模型拟合优良性的一种标准，是由日本统计学家赤池弘次创立和发展的。赤池信息量准则建立在熵的概念基础上。\nAIC越小，模型越好，通常选择AIC最小的模型↩︎",
    "crumbs": [
      "Home",
      "统计软件使用指南",
      "Python",
      "用Python做生存分析和COX回归"
    ]
  },
  {
    "objectID": "Guide/Stata/Stata-intro.html",
    "href": "Guide/Stata/Stata-intro.html",
    "title": "Stata",
    "section": "",
    "text": "关于 Stata 大二上统计学课的时候，老师在课上提了一嘴，说：“等你们以后读研了，就不用 SPSS 这种工具了，就会开始用 Stata、R 这些工具了“。也确实，本科期间确实就是一个 SPSS 管了四年，因为实在脱离他的应用场景，加之，老师就只会 SPSS ，那就没办法咯。\n想起那时候，专业两个班的 SPSS 软件基本上都是我去装的，老师弄不会，同学们更不会，我比较喜欢摸索，所以摸索出了这些，找到了安装包和密钥，然后拷在 U盘 里，课前课后课中就是给他们装软件，有时候，有些系统还装不上，某为就是，同学的某为一直装不上，当时看是因为缺 Java 环境，但是装了 Java JDK 还是不行，遂放弃。\n等到毕业的时候，想着看能不能用 Stata 做一下毕业论文的数据分析，最后太忙，没时间也没精力，用了 SPSS 结束。\n老师也是到了我大四的时候在哪里自学 Stata ，不过要说的是，在 AI 成熟以前，没有 AI 的辅助情况下，从0开始去学一门技能或程序，没有捷径，耗时耗力。现在逐渐理解，因为自己当时抱着 Python 的几本书，看了两三年也没有啥进展，等到 AI 出来了，不懂的就问 AI，节省了很多时间和精力；也和理解力的提升有关，进展迅速。\n回归正题，Stata 是一款用于数据科学的统计软件，其功能强大，但是对比 Python、R、MATLAB等程序或软件，还是略显不足，但是对于一般情况的数据分析，Stata 是够用的，其主要的优点是语法简洁和有诸多可以拿来即用的包，同时作为一款商业软件，其价格相较于 SAS 是很低的（但是换算RMB仍然很高），其支持相较于 R 等也可以说是较为丰富的（庞大的社区），还有跨平台使用等优势，这里不一一列举。\nStata 的安装很简单，互联网上有很多教程，但是关键的一点是获得 Key (许可证和激活秘钥)，当然互联网亦有诸多的资源可供选择。\n然后根据研究目的，选择合适的模型，找到前人写的代码，拿来增删改查，AI 大法，跑通，分析。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html",
    "href": "Guide/Stata/25-03-11-ITSA.html",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "Interrupted Time Series Analysis (ITSA) 是一种常用的时间序列分析方法，用于评估某个干预措施对某个事件或趋势的影响。\n\n\nITSA 模型的基本形式如下：\n\\[\nY_t = \\beta_0 + \\beta_1 \\cdot T_t + \\beta_2 \\cdot X_t + \\beta_3 \\cdot T_t \\cdot X_t + \\epsilon_t\n\\]\n公式中各代码的含义分别为：\n\n\\(Y_t\\)：因变量，时间序列的观测值\n\\(T_t\\)：时间变量（序列），表示时间点 \\(t\\) 距离干预前的时间长度\n\\(X_t\\)：干预变量（哑变量），表示干预措施的状态，通常为 0 或 1\n\\(\\beta_0\\)：截距，即常数项\n\\(\\beta_1\\)：时间变量的系数，表示时间的趋势（改革前的变化趋势）\n\\(\\beta_2\\)：干预变量的系数，表示干预的效应\n\\(\\beta_3\\)：交互项系数，表示改革后与改革前斜率的差值，故改革后的斜率值为 \\(\\beta_1 + \\beta_3\\)\n\\(\\epsilon_t\\)：误差项\n\n\n\n\n这里使用 Stata 中的 nlswork 数据集，该数据集包含了 1987 年和 1988 年的 个体数据。 首先找到 nlswork 数据集，你可以从互联网上寻找相关资源；或者从 Stata 的 nlswork 包中导出这一数据集，操作如下：\nsysuse nlswork, clear\nsave \"your-file-path\\nlswork.dta\", replace\n\n\n\n\n\nssc install itsa\nssc install actest\n\n\n\n\n需要分析的变量很多，但是我们可以首先从次均住院费用开始，这是最直接的一组数据，根据 DIP政策 实施的时间点来划分时间段。\n这里是2018-2023年六年的费用数据，我们首先对其按年进行处理，得到一个费用均数，实际上大多数论文都是按照月份进行处理，这样更合理也更详细一些，这里也可以按月份来，但是需要重新清洗数据，还是按照年份先试一下。\n但是也有个问题就是，2022年开始DIP改革，数据只截止到2023，因此2022-2023无法进行回归，只能用截距代替一下（数据不稳定，谨慎对待）。\n操作代码如下：\nclear all\nuse \"C:\\Users\\asus\\ITSA\\ITSA-PRE.dta\", replace\n\n// 按年份聚合数据，取平均值\ncollapse (mean) Cost, by(year)\n\n// 设置时间变量\ntsset year\n\n// 定义时间变量和干预变量\ngen time = year - 2017\ngen DIP = (year &gt;= 2022) // 2022年及以后为1，之前为0\ngen time_post = (time - 5) * DIP // 干预后时间变量\n\n// 计算政策前的趋势（2018-2021）\nregress Cost time if year &lt;= 2021\nlocal beta0 = _b[_cons]\nlocal beta1 = _b[time]\n\n// 计算 2022 年的预测值\nlocal cost_2022_pred = `beta0' + `beta1' * 5\ngen Cost_pred_2022 = `cost_2022_pred' if year == 2022\n\n// 计算政策前趋势线（2018-2022）\ngenerate Cost_pred_2018_2022 = `beta0' + `beta1' * time if year &lt;= 2022\n\n// 计算 2022 和 2023 年的真实值\ngen cost_2022_actual = Cost if year == 2022\ngen cost_2023_actual = Cost if year == 2023\negen cost_2022_real = max(cost_2022_actual)\negen cost_2023_real = max(cost_2023_actual)\nlocal cost_2022_actual = cost_2022_real\nlocal cost_2023_actual = cost_2023_real\n\n// 计算政策后趋势斜率\nlocal beta_post_1 = (`cost_2023_actual' - `cost_2022_actual') / (6 - 5)\n\n// 计算政策后趋势线（从 2022 真实值开始）\ngenerate Cost_pred_post = `cost_2022_actual' + `beta_post_1' * (time - 5) if year &gt;= 2022\n\n// 显示关键信息\ndisplay \"政策前趋势 β1: `beta1'\"\ndisplay \"2022 预测值: `cost_2022_pred'\"\ndisplay \"2022 真实值: `cost_2022_actual'\"\ndisplay \"2023 真实值: `cost_2023_actual'\"\ndisplay \"政策后斜率 β_post_1: `beta_post_1'\"\n\n// 画图\ntwoway (scatter Cost year, msize(small)) /// 观察值\n       (line Cost_pred_2018_2022 year if year &lt;= 2022, lcolor(blue)) /// 政策前趋势\n       (line Cost_pred_post year if year &gt;= 2022, lcolor(red)) /// 政策后趋势\n       (scatter Cost_pred_2022 year if year == 2022, mcolor(green) msize(medium)) /// 2022预测值\n       (pcarrow Cost_pred_2022 year Cost year if year == 2022, lcolor(green) mcolor(green)), /// \n       xline(2022, lpattern(dash)) ///\n       title(\"Cost Time Series Analysis\") ///\n       subtitle(\"Intervention at 2022\") ///\n       xlabel(2018(1)2023) ///\n       legend(label(1 \"Observed\") label(2 \"Pre-intervention trend\") ///\n              label(3 \"Post-intervention trend\") label(4 \"2022 Prediction\") label(5 \"Level change at 2022\"))\n\n// 保存图形\ngraph save \"C:\\Users\\asus\\test\\cost_time_series_graph.gph\", replace\n\n// 计算回归系数表\nregress Cost time DIP time_post\noutreg2 using \"C:\\Users\\asus\\test\\itsa_results.doc\", replace word\n这里用\n\n2018年的数据作为起始数据（ITSA方程的截距，即 \\(\\beta_0\\) ）；\n2018-2021年的数据拟合政策前 次均费用 随时间变化的趋势，即 \\(\\beta_1\\) ；\n然后用 \\(\\beta_1\\) 计算2022年的预测值(\\(cost_{2022pred} = \\beta_0 +\\beta_1 * 5\\) )；\n2022年到2023年的数据直接使用真实值，得到政策后的趋势直线，如果数据足够多（≥3），则可以对政策后的数据进行回归得到 \\(\\beta_3\\)；\n使用2022年的真实值减去2022年的预测值，得到政策变化对 cost 造成的水平影响，即 \\(\\beta_2\\) 。\n\n\n\n\n上述代码运行后输出 中断时间序列分析 趋势图：\n\n\n\nITSA-TREND\n\n\n\n\n\n代码的最后，做回归系数表，得到如下结果：\n\n\n\n回归系数表\n\n\n\n政策前的时间趋势为：\\(\\beta_1=331.6185\\)；\n政策实施时的瞬时变化为：\\(\\beta_2=-219.9033\\);\n政策实施后的变化趋势为：\\(\\beta_3=-1628.044\\) 。\n\n其他变量也就可以按照此种模式进行一一计算，当然也可以用循环的模式计算，以后再论。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#stsa-公式",
    "href": "Guide/Stata/25-03-11-ITSA.html#stsa-公式",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "ITSA 模型的基本形式如下：\n\\[\nY_t = \\beta_0 + \\beta_1 \\cdot T_t + \\beta_2 \\cdot X_t + \\beta_3 \\cdot T_t \\cdot X_t + \\epsilon_t\n\\]\n公式中各代码的含义分别为：\n\n\\(Y_t\\)：因变量，时间序列的观测值\n\\(T_t\\)：时间变量（序列），表示时间点 \\(t\\) 距离干预前的时间长度\n\\(X_t\\)：干预变量（哑变量），表示干预措施的状态，通常为 0 或 1\n\\(\\beta_0\\)：截距，即常数项\n\\(\\beta_1\\)：时间变量的系数，表示时间的趋势（改革前的变化趋势）\n\\(\\beta_2\\)：干预变量的系数，表示干预的效应\n\\(\\beta_3\\)：交互项系数，表示改革后与改革前斜率的差值，故改革后的斜率值为 \\(\\beta_1 + \\beta_3\\)\n\\(\\epsilon_t\\)：误差项",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#数据集及来源",
    "href": "Guide/Stata/25-03-11-ITSA.html#数据集及来源",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "这里使用 Stata 中的 nlswork 数据集，该数据集包含了 1987 年和 1988 年的 个体数据。 首先找到 nlswork 数据集，你可以从互联网上寻找相关资源；或者从 Stata 的 nlswork 包中导出这一数据集，操作如下：\nsysuse nlswork, clear\nsave \"your-file-path\\nlswork.dta\", replace",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#stata-准备",
    "href": "Guide/Stata/25-03-11-ITSA.html#stata-准备",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "ssc install itsa\nssc install actest",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#对次均住院费用进行分析",
    "href": "Guide/Stata/25-03-11-ITSA.html#对次均住院费用进行分析",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "需要分析的变量很多，但是我们可以首先从次均住院费用开始，这是最直接的一组数据，根据 DIP政策 实施的时间点来划分时间段。\n这里是2018-2023年六年的费用数据，我们首先对其按年进行处理，得到一个费用均数，实际上大多数论文都是按照月份进行处理，这样更合理也更详细一些，这里也可以按月份来，但是需要重新清洗数据，还是按照年份先试一下。\n但是也有个问题就是，2022年开始DIP改革，数据只截止到2023，因此2022-2023无法进行回归，只能用截距代替一下（数据不稳定，谨慎对待）。\n操作代码如下：\nclear all\nuse \"C:\\Users\\asus\\ITSA\\ITSA-PRE.dta\", replace\n\n// 按年份聚合数据，取平均值\ncollapse (mean) Cost, by(year)\n\n// 设置时间变量\ntsset year\n\n// 定义时间变量和干预变量\ngen time = year - 2017\ngen DIP = (year &gt;= 2022) // 2022年及以后为1，之前为0\ngen time_post = (time - 5) * DIP // 干预后时间变量\n\n// 计算政策前的趋势（2018-2021）\nregress Cost time if year &lt;= 2021\nlocal beta0 = _b[_cons]\nlocal beta1 = _b[time]\n\n// 计算 2022 年的预测值\nlocal cost_2022_pred = `beta0' + `beta1' * 5\ngen Cost_pred_2022 = `cost_2022_pred' if year == 2022\n\n// 计算政策前趋势线（2018-2022）\ngenerate Cost_pred_2018_2022 = `beta0' + `beta1' * time if year &lt;= 2022\n\n// 计算 2022 和 2023 年的真实值\ngen cost_2022_actual = Cost if year == 2022\ngen cost_2023_actual = Cost if year == 2023\negen cost_2022_real = max(cost_2022_actual)\negen cost_2023_real = max(cost_2023_actual)\nlocal cost_2022_actual = cost_2022_real\nlocal cost_2023_actual = cost_2023_real\n\n// 计算政策后趋势斜率\nlocal beta_post_1 = (`cost_2023_actual' - `cost_2022_actual') / (6 - 5)\n\n// 计算政策后趋势线（从 2022 真实值开始）\ngenerate Cost_pred_post = `cost_2022_actual' + `beta_post_1' * (time - 5) if year &gt;= 2022\n\n// 显示关键信息\ndisplay \"政策前趋势 β1: `beta1'\"\ndisplay \"2022 预测值: `cost_2022_pred'\"\ndisplay \"2022 真实值: `cost_2022_actual'\"\ndisplay \"2023 真实值: `cost_2023_actual'\"\ndisplay \"政策后斜率 β_post_1: `beta_post_1'\"\n\n// 画图\ntwoway (scatter Cost year, msize(small)) /// 观察值\n       (line Cost_pred_2018_2022 year if year &lt;= 2022, lcolor(blue)) /// 政策前趋势\n       (line Cost_pred_post year if year &gt;= 2022, lcolor(red)) /// 政策后趋势\n       (scatter Cost_pred_2022 year if year == 2022, mcolor(green) msize(medium)) /// 2022预测值\n       (pcarrow Cost_pred_2022 year Cost year if year == 2022, lcolor(green) mcolor(green)), /// \n       xline(2022, lpattern(dash)) ///\n       title(\"Cost Time Series Analysis\") ///\n       subtitle(\"Intervention at 2022\") ///\n       xlabel(2018(1)2023) ///\n       legend(label(1 \"Observed\") label(2 \"Pre-intervention trend\") ///\n              label(3 \"Post-intervention trend\") label(4 \"2022 Prediction\") label(5 \"Level change at 2022\"))\n\n// 保存图形\ngraph save \"C:\\Users\\asus\\test\\cost_time_series_graph.gph\", replace\n\n// 计算回归系数表\nregress Cost time DIP time_post\noutreg2 using \"C:\\Users\\asus\\test\\itsa_results.doc\", replace word\n这里用\n\n2018年的数据作为起始数据（ITSA方程的截距，即 \\(\\beta_0\\) ）；\n2018-2021年的数据拟合政策前 次均费用 随时间变化的趋势，即 \\(\\beta_1\\) ；\n然后用 \\(\\beta_1\\) 计算2022年的预测值(\\(cost_{2022pred} = \\beta_0 +\\beta_1 * 5\\) )；\n2022年到2023年的数据直接使用真实值，得到政策后的趋势直线，如果数据足够多（≥3），则可以对政策后的数据进行回归得到 \\(\\beta_3\\)；\n使用2022年的真实值减去2022年的预测值，得到政策变化对 cost 造成的水平影响，即 \\(\\beta_2\\) 。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#绘制itsa趋势图",
    "href": "Guide/Stata/25-03-11-ITSA.html#绘制itsa趋势图",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "上述代码运行后输出 中断时间序列分析 趋势图：\n\n\n\nITSA-TREND",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-11-ITSA.html#输出统计结果",
    "href": "Guide/Stata/25-03-11-ITSA.html#输出统计结果",
    "title": "11-Stata 做 ITSA 分析",
    "section": "",
    "text": "代码的最后，做回归系数表，得到如下结果：\n\n\n\n回归系数表\n\n\n\n政策前的时间趋势为：\\(\\beta_1=331.6185\\)；\n政策实施时的瞬时变化为：\\(\\beta_2=-219.9033\\);\n政策实施后的变化趋势为：\\(\\beta_3=-1628.044\\) 。\n\n其他变量也就可以按照此种模式进行一一计算，当然也可以用循环的模式计算，以后再论。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-Stata 做 ITSA 分析"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html",
    "title": "用Python做桑基图",
    "section": "",
    "text": "桑基图（Sankey diagram），即桑基能量分流图，也叫桑基能量平衡图。它是一种特定类型的流程图，概述图中延伸的分支的宽度对应数据流量的大小，通常应用于能源、材料成分、金融等数据的可视化分析。因1898年Matthew Henry Phineas Riall Sankey绘制的“蒸汽机的能源效率图”而闻名，此后便以其名字命名为“桑基图”。\nSankey diagrams are a data visualisation technique or flow diagram that emphasizes flow/movement/change from one state to another or one time to another, in which the width of the arrows is proportional to the flow rate of the depicted extensive property. The arrows being connected are called nodes and the connections are called links.\nSankey diagrams can also visualize the energy accounts, material flow accounts on a regional or national level, and cost breakdowns.The diagrams are often used in the visualization of material flow analysis.\nSankey diagrams emphasize the major transfers or flows within a system. They help locate the most important contributions to a flow. They often show conserved quantities within defined system boundaries.(Wikipedia contributors 2025)\n\n\n桑基图常用于可持续能源、物流、人口流动、资源分配等领域的数据可视化。它可以帮助用户直观地理解和分析复杂的流动和关系，从而支持决策和策划过程。\n\n\n\n\n节点：桑基图由一系列节点组成，每个节点代表一个特定的实体或类别。例如，节点可以代表不同的时间、地点和部门等。\n箭头：箭头表示流动的路径，从一个节点流向另一个节点。箭头的宽度通常表示流量或数量的大小。\n流量量级：桑基图可以显示不同节点之间的流量量级，通过箭头的宽度来表示。宽度越大，表示流量或数量越大。\n路径：桑基图可以显示多个节点之间的复杂路径，通过连接不同的节点和箭头来表示。\n颜色编码：桑基图可以使用颜色来编码不同的节点或流动路径，以帮助用户更好地理解和区分不同的实体或类别。\n\n\n\n\n在设计桑基图图表时，以下是一些需要注意的事项：\n\n数据准备：确保数据准备充分，包括节点和流量的数据。节点应该清晰明确，流量数据应该准确可靠。\n简洁明了：桑基图应该保持简洁明了，避免过多的节点和复杂的路径。过多的节点和路径可能会导致图表混乱不清晰，难以理解。\n良好的布局：选择合适的布局方式，使得节点和箭头的排列有一定的逻辑性。可以按照流动的方向或重要性进行布局。\n色彩选择：选择合适的色彩来区分不同的节点和流动路径。颜色应该鲜明对比，以便用户能够清晰地区分不同的实体或类别。\n箭头宽度控制：根据流量的大小，合理调整箭头的宽度。宽度应该能够直观地反映流量的差异，但也不能过于夸张。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html#桑基图的主要应用场景",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html#桑基图的主要应用场景",
    "title": "用Python做桑基图",
    "section": "",
    "text": "桑基图常用于可持续能源、物流、人口流动、资源分配等领域的数据可视化。它可以帮助用户直观地理解和分析复杂的流动和关系，从而支持决策和策划过程。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html#桑基图的特点",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html#桑基图的特点",
    "title": "用Python做桑基图",
    "section": "",
    "text": "节点：桑基图由一系列节点组成，每个节点代表一个特定的实体或类别。例如，节点可以代表不同的时间、地点和部门等。\n箭头：箭头表示流动的路径，从一个节点流向另一个节点。箭头的宽度通常表示流量或数量的大小。\n流量量级：桑基图可以显示不同节点之间的流量量级，通过箭头的宽度来表示。宽度越大，表示流量或数量越大。\n路径：桑基图可以显示多个节点之间的复杂路径，通过连接不同的节点和箭头来表示。\n颜色编码：桑基图可以使用颜色来编码不同的节点或流动路径，以帮助用户更好地理解和区分不同的实体或类别。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html#设计的注意事项",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html#设计的注意事项",
    "title": "用Python做桑基图",
    "section": "",
    "text": "在设计桑基图图表时，以下是一些需要注意的事项：\n\n数据准备：确保数据准备充分，包括节点和流量的数据。节点应该清晰明确，流量数据应该准确可靠。\n简洁明了：桑基图应该保持简洁明了，避免过多的节点和复杂的路径。过多的节点和路径可能会导致图表混乱不清晰，难以理解。\n良好的布局：选择合适的布局方式，使得节点和箭头的排列有一定的逻辑性。可以按照流动的方向或重要性进行布局。\n色彩选择：选择合适的色彩来区分不同的节点和流动路径。颜色应该鲜明对比，以便用户能够清晰地区分不同的实体或类别。\n箭头宽度控制：根据流量的大小，合理调整箭头的宽度。宽度应该能够直观地反映流量的差异，但也不能过于夸张。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html#本地python",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html#本地python",
    "title": "用Python做桑基图",
    "section": "2.1 本地Python",
    "text": "2.1 本地Python\n安装相关的包和库：\n\npip install dash\npip install numpy\n\n\n2.1.1 本地运行示例\n在 Python 终端或编辑器运行后，可以在浏览器中输入 http://127.0.0.1:8051/ 进行查看。\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Mar 14 17:07:53 2025\n\n@author: asus\n\"\"\"\n\nimport dash\nfrom dash import html, dcc, Input, Output\nimport plotly.graph_objects as go\nimport dash_bootstrap_components as dbc\nimport numpy as np\nimport plotly.express as px\n \n \ndef create_complex_sankey():\n    # 示例数据\n    labels = [\"能源\", \"电力\", \"运输\", \"工业\", \"住宅\", \"商业\", \"损失\", \"可再生\", \"化石燃料\", \"核能\"]\n    sources = [0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6]\n    targets = [1, 2, 3, 4, 5, 6, 4, 5, 6, 7, 7, 8, 8, 9, 9, 7]\n    values = [8, 4, 2, 8, 4, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1]\n \n    # 创建桑基图\n    sankey_fig = go.Figure(data=[go.Sankey(\n        node=dict(\n            pad=15,\n            thickness=20,\n            line=dict(color=\"black\", width=0.5),\n            label=labels,\n            color=[\"#FF9999\", \"#66B3FF\", \"#99FF99\", \"#FFCC99\", \"#FF6666\", \"#66FF66\", \"#6666FF\", \"#FF66FF\", \"#66FFFF\", \"#FFFF66\"]\n        ),\n        link=dict(\n            source=sources,\n            target=targets,\n            value=values,\n            color=[\"rgba(255, 153, 153, 0.6)\", \"rgba(102, 179, 255, 0.6)\", \"rgba(153, 255, 153, 0.6)\", \"rgba(255, 204, 153, 0.6)\",\n                   \"rgba(255, 102, 102, 0.6)\", \"rgba(102, 255, 102, 0.6)\", \"rgba(102, 102, 255, 0.6)\", \"rgba(255, 102, 255, 0.6)\",\n                   \"rgba(102, 255, 255, 0.6)\", \"rgba(255, 255, 102, 0.6)\", \"rgba(255, 153, 153, 0.6)\", \"rgba(102, 179, 255, 0.6)\",\n                   \"rgba(153, 255, 153, 0.6)\", \"rgba(255, 204, 153, 0.6)\", \"rgba(255, 102, 102, 0.6)\", \"rgba(102, 255, 102, 0.6)\"]\n        )\n    )])\n \n    # 更新布局\n    sankey_fig.update_layout(\n        title='复杂桑基图示例',\n        font_size=10,\n        template='plotly_white'\n    )\n \n    return sankey_fig\n \napp = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n \napp.layout = html.Div([\n    html.H3(\"桑基图展示\", className=\"text-center mt-4 mb-3\"),\n    dcc.Graph(figure=create_complex_sankey())\n])\n \nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=8051)",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Guide/Python/25-03-14-Sankey-diagram.html#jupyter-实现",
    "href": "Guide/Python/25-03-14-Sankey-diagram.html#jupyter-实现",
    "title": "用Python做桑基图",
    "section": "2.2 Jupyter 实现",
    "text": "2.2 Jupyter 实现\n为了能在本网页中显示桑基图，需要在 jupyter 中运行该程序，那么除了相关的 jupyter 包需要被安装外，需要更换 dash 包为 jupyter-dash 。\n\n2.2.1 安装 jupyter-dash\n\npip install jupyter-dash\n\n\n\n2.2.2 运行程序\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Mar 14 17:07:53 2025\n\n@author: asus\n\"\"\"\n\nfrom jupyter_dash import JupyterDash  # 替换 dash\nfrom dash import html, dcc, Input, Output\nimport plotly.graph_objects as go\nimport dash_bootstrap_components as dbc\nimport numpy as np\nimport plotly.express as px\n\ndef create_complex_sankey():\n    labels = [\"能源\", \"电力\", \"运输\", \"工业\", \"住宅\", \"商业\", \"损失\", \"可再生\", \"化石燃料\", \"核能\"]\n    sources = [0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6]\n    targets = [1, 2, 3, 4, 5, 6, 4, 5, 6, 7, 7, 8, 8, 9, 9, 7]\n    values = [8, 4, 2, 8, 4, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1]\n\n    sankey_fig = go.Figure(data=[go.Sankey(\n        node=dict(\n            pad=15,\n            thickness=20,\n            line=dict(color=\"black\", width=0.5),\n            label=labels,\n            color=[\"#FF9999\", \"#66B3FF\", \"#99FF99\", \"#FFCC99\", \"#FF6666\", \"#66FF66\", \"#6666FF\", \"#FF66FF\", \"#66FFFF\", \"#FFFF66\"]\n        ),\n        link=dict(\n            source=sources,\n            target=targets,\n            value=values,\n            color=[\"rgba(255, 153, 153, 0.6)\", \"rgba(102, 179, 255, 0.6)\", \"rgba(153, 255, 153, 0.6)\", \"rgba(255, 204, 153, 0.6)\",\n                   \"rgba(255, 102, 102, 0.6)\", \"rgba(102, 255, 102, 0.6)\", \"rgba(102, 102, 255, 0.6)\", \"rgba(255, 102, 255, 0.6)\",\n                   \"rgba(102, 255, 255, 0.6)\", \"rgba(255, 255, 102, 0.6)\", \"rgba(255, 153, 153, 0.6)\", \"rgba(102, 179, 255, 0.6)\",\n                   \"rgba(153, 255, 153, 0.6)\", \"rgba(255, 204, 153, 0.6)\", \"rgba(255, 102, 102, 0.6)\", \"rgba(102, 255, 102, 0.6)\"]\n        )\n    )])\n\n    sankey_fig.update_layout(\n        title='复杂桑基图示例',\n        font_size=10,\n        template='plotly_white'\n    )\n\n    return sankey_fig\n\n# 使用 JupyterDash 替代 Dash\napp = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n\napp.layout = html.Div([\n    html.H3(\"桑基图展示\", className=\"text-center mt-4 mb-3\"),\n    dcc.Graph(figure=create_complex_sankey())\n])\n\n# 在 Jupyter 中运行，mode='inline' 将图形嵌入笔记本\napp.run_server(mode='inline')\n\n\n        \n        \n\n\n\n        \n        \n\n\n\n\n2.2.3 网页无法正确显示\n因为 GitHub 只支持静态网页，但是 桑基图 是一个通过 Flask 生成的动态 Web 应用，因为当本地生成后托管到 GitHub 后，是无法正确显示该图形。\n解决办法：\n\n将网页托管到支持动态图像的服务器上，如 Render 等\n改用静态图形\n\n\nimport plotly.graph_objects as go\n\ndef create_complex_sankey():\n    labels = [\"能源\", \"电力\", \"运输\", \"工业\", \"住宅\", \"商业\", \"损失\", \"可再生\", \"化石燃料\", \"核能\"]\n    sources = [0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6]\n    targets = [1, 2, 3, 4, 5, 6, 4, 5, 6, 7, 7, 8, 8, 9, 9, 7]\n    values = [8, 4, 2, 8, 4, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1]\n    sankey_fig = go.Figure(data=[go.Sankey(\n        node=dict(pad=15, thickness=20, line=dict(color=\"black\", width=0.5), label=labels),\n        link=dict(source=sources, target=targets, value=values)\n    )])\n    sankey_fig.update_layout(title='复杂桑基图示例', font_size=10, template='plotly_white')\n    return sankey_fig\n\n# 保存为静态 HTML 文件\nfig = create_complex_sankey()\nfig.write_html(\"sankey.html\", include_plotlyjs='cdn')",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "用Python做桑基图"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#假设检验",
    "href": "Learn/Basic/10-regression-correlation.html#假设检验",
    "title": "简单线性相关和回归",
    "section": "\n2.1 假设检验",
    "text": "2.1 假设检验\n\n2.1.1 F检验\n\\(y_i\\)的总离均差平方和为：\n\\[SS_{yy}=\\sum_{i}(y_i-\\bar y)^2\\] 对其做分解，得到等式：\n\\[SS_{yy}=\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\] \\(\\sum_{i}^{n}(\\hat y_i-\\bar y)^2\\)为回归平方和（regression sum of squares），记为\\(SS_R\\)，表示回归估计值\\(\\hat y_i\\)与均数\\(\\bar y\\)的离差平方和，其公式为：\n\\[\n\\begin{align}\nSS_{yy} &= \\sum_{i=1}^{n}(\\hat y_i - \\bar y)^2 \\\\\n        &= \\sum_{i=1}^{n}[a + bx_i - (a + b\\bar x)]^2 \\\\\n        &= SS_{xx}b^2 \\\\\n        &= SS_{xy}b\n\\end{align}\n\\] 显然，回归平方和\\(SS_{R}\\)反映的是在y的总变异中由x与y的直线回归关系解释的那部分变异。\\(SS_R\\)值越大，说明回归直线的拟合效果就越好。\n\\(\\sum_{i}^{n}(y_i-\\hat y_i)^2\\)为残差平方和（residual sum of squares），记为\\(SS_E\\)，表示观测值\\(y_i\\)与回归估计值\\(\\hat y_i\\)的离差平方和，其公式为： \\[SS_E=\\sum_{i=1}^{n}(y_i-\\hat y_i)^2\\] \\(SS_E\\)反映了在总变异中扣除自变量x对因变量y的线性影响以后的其他因素（包括x对y的非线性影响和随机误差等）对y变异的影响，也就是在总平方和中无法用y和x线性回归关系解释的部分。\\(SS_E\\)值越小，说明回归直线的拟合效果就越好。\n对公式进行简化： \\[\\begin{align}\nSS_{yy}=&\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\\\\n=&SS_R+SS_E\n\\end{align}\\] 上述三个平方和，各有其相应的自由度\\(v\\)，并有如下关系： \\[v_{yy}=v_R+v_E\\\\\nv_{yy}=n-1,v_R=1,v_E=n-2\\]\n在\\(H_0\\)成立的条件下，有： \\[\\frac{SS_R}{\\sigma^2}\\sim \\chi^2(v_R),\\frac{SS_E}{\\sigma^2}\\sim \\chi^2(v_E)\\] 且\\(SS_R\\)和\\(SS_E\\)相互独立。\n检验统计量：\n\\[F=\\frac{SS_R/v_R}{SS_E/v_E}\\] 服从自由度\\(v_R=1,v_E=n-2\\)的F分布。如果y和x确实存在直线回归关系，那么回归所解释的变异\\(SS_R\\)应大于其他因素所解释的变异\\(SS_E\\)。由此可见，F检验正是建立在这个基础上的。\n对于给定的检验水准\\(\\alpha\\)， 如果\\(F&gt;F_{(v_R,v_E),1-\\alpha}\\)，则拒绝\\(H_0\\)，认为直线回归方程有统计学显著性；\n如果\\(F\\leq F_{(v_R,v_E),1-\\alpha}\\)，则不拒绝\\(H_0\\)，尚不能认为直线回归方程有统计学显著性。\n\n2.1.2 t检验法\n回归直线方程的稳定性程度取决于 \\(b\\) 的波动大小，即 \\(S_b\\) 的大小，这里的 \\(S_b\\) 为样本回归系数 \\(b\\) 的标准误的估计值。由于统计量 \\(b\\) 来自正态总体，故可从 \\(b\\) 的抽样分布出发构造 \\(t\\) 统计量对其进行假设检验。\n当 \\(H_0\\) ：\\(\\beta = 0\\) 成立时，检验统计量服从自由度 \\(v_E=n-2\\) 的 \\(t\\) 分布。\n\\[\nt=\\frac{b-0}{S_b}\\sim t(v_E)\n\\]\n其中：\n\\[\nS_b=\\frac{S_{yx}}{\\sqrt{SS_{xx}}}=\\sqrt{\\frac{S_{yx}/(n-2)}{SS_{xx}}}\n\\]\n\\(S_{yx}=\\sqrt{SS_E/(n-2)}\\) 为剩余标准差（residual standard deviation），是指扣除 \\(x\\) 对 \\(y\\) 的线性影响后，衡量观测值 \\(y\\) 对回归直线的平均离散程度，即回归直线 \\(\\hat y\\) 估计的精度。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#直线回归方程的区间估计",
    "href": "Learn/Basic/10-regression-correlation.html#直线回归方程的区间估计",
    "title": "简单线性相关和回归",
    "section": "\n2.2 直线回归方程的区间估计",
    "text": "2.2 直线回归方程的区间估计\n\n2.2.1 总体回归系数的置信区间\n\\[\nt = \\frac{b-\\beta}{S_b}\\sim t(v_E)\n\\]",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#数学定义",
    "href": "Learn/Basic/10-regression-correlation.html#数学定义",
    "title": "简单线性相关和回归",
    "section": "\n3.1 数学定义",
    "text": "3.1 数学定义\n均值向量：对于双元正态变量 \\((X, Y)\\)，均值向量为：\n\\[\\mu = \\begin{pmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{pmatrix}\\]\n其中 \\(\\mu_X\\) 和 \\(\\mu_Y\\) 分别是随机变量 \\(X\\) 和 \\(Y\\) 的均值。 2. 协方差矩阵：协方差矩阵为：\n\\[\\Sigma = \\begin{pmatrix}\n\\sigma_X^2 & \\sigma_{XY} \\\\\n\\sigma_{XY} & \\sigma_Y^2\n\\end{pmatrix}\\]\n其中 \\(\\sigma_X^2\\) 和 \\(\\sigma_Y^2\\) 是 \\(X\\) 和 \\(Y\\) 的方差，\\(\\sigma_{XY}\\) 是它们之间的协方差。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#性质",
    "href": "Learn/Basic/10-regression-correlation.html#性质",
    "title": "简单线性相关和回归",
    "section": "\n3.2 性质",
    "text": "3.2 性质\n\n边缘分布：\\(X\\) 和 \\(Y\\) 的边缘分布也是正态分布。\n条件分布：给定 \\(X\\) 的值，\\(Y\\) 的条件分布也是正态分布。\n相关性：协方差矩阵的值可以用来判断 X 和 Y 之间的相关性。如果 \\(\\sigma_{XY} &gt; 0\\)，则两者正相关；如果 \\(\\sigma_{XY} &lt; 0\\)，则负相关。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-regression-correlation.html#示例",
    "href": "Learn/Basic/10-regression-correlation.html#示例",
    "title": "简单线性相关和回归",
    "section": "\n3.3 示例",
    "text": "3.3 示例\n假设有一组数据描述学生的身高 \\(X\\) 和体重 \\(Y\\)，并且假设 \\((X, Y)\\) 服从双元正态分布：\n\n3.3.1 均值向量：\n\\[\\mu = \\begin{pmatrix}\n170 \\\\\n65\n\\end{pmatrix}\\]\n表示身高的均值为 170 厘米，体重的均值为 65 千克。 •\n\n3.3.2 协方差矩阵：\n\\[\\Sigma = \\begin{pmatrix}\n100 & 20 \\\\\n20 & 25\n\\end{pmatrix}\\]\n这里，身高的方差为 100，体重的方差为 25，协方差为 20，表示身高和体重之间存在正相关关系。\n在这个示例中，如果我们知道某个学生的身高为 180 厘米，我们可以利用条件分布来预测他的体重，这个体重的预测值也是正态分布。\n我们用一个图形来展示：\n\n## 安装和加载所需的包\n#install.packages(\"plotly\")\n#install.packages(\"mvtnorm\")\nlibrary(plotly)\nlibrary(mvtnorm)\nlibrary(webshot2)\n\n# 创建网格数据\nx &lt;- seq(150, 190, length.out = 100)\n#身高150-190，等距的100个值\ny &lt;- seq(50, 80, length.out = 100)\n#体重50-80，等距的100个值\ngrid &lt;- expand.grid(X = x, Y = y)\n#生成 x 和 y 的所有组合，用于构建一个网格数据框，以便计算多元正态分布的概率密度。\n\n# 设置均值和协方差矩阵\nmu &lt;- c(170, 65)\n#设置双元正态分布的均值向量，表示均值分别为身高 170 cm 和体重 65 kg\n\nsigma &lt;- matrix(c(100, 20, 20, 25), nrow = 2)\n#设置协方差矩阵，表示身高的方差为 100，体重的方差为 25，身高和体重之间的协方差为 20\n\n# 计算概率密度\nz &lt;- dmvnorm(as.matrix(grid), mean = mu, sigma = sigma)\n#计算每个网格点上双元正态分布的概率密度。\n\n# 将概率密度矩阵转换为适合绘图的形状\nz_matrix &lt;- matrix(z, nrow = 100, ncol = 100)\n\n# 绘制三维表面图\nplot_ly(x = x, y = y, z = z_matrix, type = \"surface\") %&gt;%\n  layout(title = list(text = \"双元正态分布的三维概率密度图\", y=0.95),\n         scene = list(xaxis = list(title = \"身高 (cm)\"),\n                      yaxis = list(title = \"体重 (kg)\"),\n                      zaxis = list(title = \"概率密度\")))\n\n\nBinary normal distribution\n\n\n注：上述图像在被转换为PDF文件时，会发生报错：Quarto 文档中包含了一些生成 HTML 输出的函数（比如交互式图表或其他 HTML 小部件），但你当前的目标输出格式是 PDF。由于 PDF 是静态格式，无法直接渲染 HTML 内容，Quarto 会报错并停止执行。\n解决方案，此章节不转换为PDF格式，或者：\n如果你仍想输出 PDF，但希望将 HTML 小部件作为静态截图嵌入，可以安装 R 的 webshot 或 webshot2 包。Quarto 会利用它们将 HTML 内容转换为图片。\n需要安装：\n\ninstall.packages(\"webshot2\")\n\n然后在这段程序的前部导入该包：library(webshot2)。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/10-non-parameter-test.html",
    "href": "Learn/Basic/10-non-parameter-test.html",
    "title": "非参数检验",
    "section": "",
    "text": "秩和检验（Rank-Sum Test）是一种非参数检验方法，用于比较两个独立样本的分布是否存在显著差异。它无需对数据分布作正态性假设，适用于数据偏离正态分布、样本量较小或数据为序数型变量的场景。\n常见的秩和检验包括：\n\nMann-Whitney U 检验（也称Wilcoxon秩和检验）：用于比较两个独立样本的中位数是否相等。\nWilcoxon 符号秩检验：用于两个配对样本的比较（类似配对t检验，但无需正态性假设）。\n\n\n\n\nMann-Whitney U 检验公式\n\n假设两组独立样本分别为 \\(X\\) 和 \\(Y\\)，样本量分别为 \\(n_1\\) 和 \\(n_2\\)。\n对两组样本合并并按大小排序，赋予秩次。计算两组的秩次和 \\(R_1\\) 和 \\(R_2\\)（分别为 $ X$ 和 \\(Y\\) 的秩次总和）。\n\n确定统计量T值：\n\n假设两组样本量 \\(n_1&lt;n_2\\)，一般情况下以样本量较小者\\(n_1\\)对应的秩和\\(T_1\\)为检验统计量\\(T\\)，当样本相等时可以选择任一组的秩和为\\(T\\)。1\n当两组中样本量较小者不低于10时，在\\(H_0\\)成立假设下，统计量\\(T\\)的抽样分布近似于正态分布，有\n\\[T\\approx N\\left(\\frac{n_1(n+1)}{2},\\frac{n_1 n_2(n+1)}{12} \\right)\\] 此时，Wilcoxon 秩和统计量在\\(H_0\\)下关于\\(\\mu=\\frac{n_1(n+1)}{2}\\)对称。\n如果没有或存在较少的“结”，将\\(T\\)标准化后为：\n\\[U=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2(n+1)}{12}}}\\approx N(0,1)\\]\n其中，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n若“结”的比例较多（&gt;25%），则用以下公式校正：\n\\[U_c=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2}{12}[(n+1)-\\sum_\\limits{i=1}^{g}\\frac{t_i^3-t_i}{n(n-1)}]}}\\approx N(0,1)\\]\n\nWilcoxon 符号秩检验公式\n\n对配对样本 \\((X_i, Y_i)\\)，计算差值 \\(D_i = X_i - Y_i\\)，取非零差值的绝对值并排序（若差值为0则舍去不计，且减去相应的个数），赋予秩次 \\(R_i\\)。再根据差值的符号计算符号秩次和 \\(W\\)：\n\\[W = \\sum R_i \\cdot \\text{sign}(D_i)\\]\n检验统计量 \\(T\\) 是 \\(W\\) 的绝对值，依据表或正态分布计算显著性。\n正态近似法：\n当\\(n\\ge 30\\)时，有中心极限定理可知，当\\(H_0\\)成立时统计量\\(T\\)的抽样分布近似正态分布，有\n\\[T\\approx N \\left(\\frac{n(n+1)}{4},\\frac{n(n+1)(2n+1)}{24}\\right)\\] 其中，均数\\(\\mu=\\frac{n(n+1)}{4}\\)，方差\\(\\sigma^2=\\frac{n(n+1)(2n+1)}{24}\\)。 将T标准化后，近似服从标准正态分布，有\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}}}\\approx N(0,1)\\] 其中，n是差值不为0的对子数，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n当N较大时，样本中可能存在较多的“结”，（如“结”所占比例大于25%），此时需要使用校正公式：\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}-\\frac{\\sum_\\limits{i=1}^g(t_i^3-t_i)}{48}}}\\approx N(0,1)\\] 其中，\\(t_i\\)为\\(i\\)个“结”中有相同秩次的个数，\\(g\\)是“结”的个数。\nWilcoxon符号秩检验的前提条件为数据是连续的且差值分布是对称的。\nnotice：秩和秩和的区别：秩是指全部观察值按某种顺序排列的位序，在一定程度上反映了等级的高低；而秩和则表示同组秩次之和，在一定程度上反映了等级的分布。2\n\n\n\n\nMann-Whitney U 检验：\n\n比较两个独立样本的中位数是否存在显著差异。\n\n适用于非正态分布数据或含有极端值的样本。\n\n示例：比较两种治疗方法的疗效（不同受试者组）。\n\nWilcoxon 符号秩检验：\n\n比较两个配对样本的中位数差异。\n\n适用于重复测量数据或实验设计中存在配对关系的场景。\n\n示例：同一批受试者在治疗前后血压的变化。\n\n\n\n\n\n\n秩和检验是非参数方法，对数据分布假设少，但效率可能低于参数方法（如t检验）在满足条件时的效果，如果满足参数检验的条件，应优先考虑使用参数检验的方法，否则会增加犯二类错误的概率。\n数据需要满足独立性假设，否则检验结果可能不准确。\n\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/10-non-parameter-test.html#秩和检验",
    "href": "Learn/Basic/10-non-parameter-test.html#秩和检验",
    "title": "非参数检验",
    "section": "",
    "text": "秩和检验（Rank-Sum Test）是一种非参数检验方法，用于比较两个独立样本的分布是否存在显著差异。它无需对数据分布作正态性假设，适用于数据偏离正态分布、样本量较小或数据为序数型变量的场景。\n常见的秩和检验包括：\n\nMann-Whitney U 检验（也称Wilcoxon秩和检验）：用于比较两个独立样本的中位数是否相等。\nWilcoxon 符号秩检验：用于两个配对样本的比较（类似配对t检验，但无需正态性假设）。\n\n\n\n\nMann-Whitney U 检验公式\n\n假设两组独立样本分别为 \\(X\\) 和 \\(Y\\)，样本量分别为 \\(n_1\\) 和 \\(n_2\\)。\n对两组样本合并并按大小排序，赋予秩次。计算两组的秩次和 \\(R_1\\) 和 \\(R_2\\)（分别为 $ X$ 和 \\(Y\\) 的秩次总和）。\n\n确定统计量T值：\n\n假设两组样本量 \\(n_1&lt;n_2\\)，一般情况下以样本量较小者\\(n_1\\)对应的秩和\\(T_1\\)为检验统计量\\(T\\)，当样本相等时可以选择任一组的秩和为\\(T\\)。1\n当两组中样本量较小者不低于10时，在\\(H_0\\)成立假设下，统计量\\(T\\)的抽样分布近似于正态分布，有\n\\[T\\approx N\\left(\\frac{n_1(n+1)}{2},\\frac{n_1 n_2(n+1)}{12} \\right)\\] 此时，Wilcoxon 秩和统计量在\\(H_0\\)下关于\\(\\mu=\\frac{n_1(n+1)}{2}\\)对称。\n如果没有或存在较少的“结”，将\\(T\\)标准化后为：\n\\[U=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2(n+1)}{12}}}\\approx N(0,1)\\]\n其中，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n若“结”的比例较多（&gt;25%），则用以下公式校正：\n\\[U_c=\\frac{T-\\frac{n_1(n+1)}{2}+C}{\\sqrt{\\frac{n_1 n_2}{12}[(n+1)-\\sum_\\limits{i=1}^{g}\\frac{t_i^3-t_i}{n(n-1)}]}}\\approx N(0,1)\\]\n\nWilcoxon 符号秩检验公式\n\n对配对样本 \\((X_i, Y_i)\\)，计算差值 \\(D_i = X_i - Y_i\\)，取非零差值的绝对值并排序（若差值为0则舍去不计，且减去相应的个数），赋予秩次 \\(R_i\\)。再根据差值的符号计算符号秩次和 \\(W\\)：\n\\[W = \\sum R_i \\cdot \\text{sign}(D_i)\\]\n检验统计量 \\(T\\) 是 \\(W\\) 的绝对值，依据表或正态分布计算显著性。\n正态近似法：\n当\\(n\\ge 30\\)时，有中心极限定理可知，当\\(H_0\\)成立时统计量\\(T\\)的抽样分布近似正态分布，有\n\\[T\\approx N \\left(\\frac{n(n+1)}{4},\\frac{n(n+1)(2n+1)}{24}\\right)\\] 其中，均数\\(\\mu=\\frac{n(n+1)}{4}\\)，方差\\(\\sigma^2=\\frac{n(n+1)(2n+1)}{24}\\)。 将T标准化后，近似服从标准正态分布，有\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}}}\\approx N(0,1)\\] 其中，n是差值不为0的对子数，C为连续性校正系数，当\\(T&gt;\\frac{n(n+1)}{4}\\)时，\\(C=-0.5\\)，当\\(T&lt;\\frac{n(n+1)}{4}\\)时，\\(C=0.5\\)，当\\(T=\\frac{n(n+1)}{4}\\)时，\\(C=0\\)。\n当N较大时，样本中可能存在较多的“结”，（如“结”所占比例大于25%），此时需要使用校正公式：\n\\[U=\\frac{T-\\frac{n(n+1)}{4}+C}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}-\\frac{\\sum_\\limits{i=1}^g(t_i^3-t_i)}{48}}}\\approx N(0,1)\\] 其中，\\(t_i\\)为\\(i\\)个“结”中有相同秩次的个数，\\(g\\)是“结”的个数。\nWilcoxon符号秩检验的前提条件为数据是连续的且差值分布是对称的。\nnotice：秩和秩和的区别：秩是指全部观察值按某种顺序排列的位序，在一定程度上反映了等级的高低；而秩和则表示同组秩次之和，在一定程度上反映了等级的分布。2\n\n\n\n\nMann-Whitney U 检验：\n\n比较两个独立样本的中位数是否存在显著差异。\n\n适用于非正态分布数据或含有极端值的样本。\n\n示例：比较两种治疗方法的疗效（不同受试者组）。\n\nWilcoxon 符号秩检验：\n\n比较两个配对样本的中位数差异。\n\n适用于重复测量数据或实验设计中存在配对关系的场景。\n\n示例：同一批受试者在治疗前后血压的变化。\n\n\n\n\n\n\n秩和检验是非参数方法，对数据分布假设少，但效率可能低于参数方法（如t检验）在满足条件时的效果，如果满足参数检验的条件，应优先考虑使用参数检验的方法，否则会增加犯二类错误的概率。\n数据需要满足独立性假设，否则检验结果可能不准确。\n\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/10-non-parameter-test.html#footnotes",
    "href": "Learn/Basic/10-non-parameter-test.html#footnotes",
    "title": "非参数检验",
    "section": "脚注",
    "text": "脚注\n\n\n不是说一定要选择样本量较小者对应的秩和作为检验统计量，只是长期的使用习惯，造成了这一惯例。如果取较小的秩和计算后得到的\\(U&lt;u_{\\alpha/2}\\)，则表示拒绝\\(H_0\\)；相反，如果取较大的秩和计算后得到的\\(U&gt;u_{1-\\alpha/2}\\)，也会表示拒绝\\(H_0\\)，他们都表示检验统计量落在了拒绝域中。↩︎\n尽管非参数方法对总体分布形式未做要求，但如果我们知道总体的一些性质而不去利用，就会浪费许多有用的信息，最常见的就是分布的对称性，配对设计的 Wilcoxon 符号秩检验充分利用了差值分布对称性这一信息，这与尽可能地采用有效方法，利用尽可能多的信息进行统计分析的大原则相一致。↩︎",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "非参数检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-chi2-test.html",
    "href": "Learn/Basic/09-chi2-test.html",
    "title": "卡方检验",
    "section": "",
    "text": "资料特征\n数据特征\n\n完全随机设计\n\n配对设计\n随机区组\n\n\n\n\n\n\n单组\n两组\n多组\n\n\n\n\n分类资料\n无序分类资料\n二项分布直接计算概率法、正态近似法（Z检验）、率的正态近似\n独立四格表\\(\\chi^2\\)检验、Fisher确切概率法\nR×C交叉表\\(\\chi^2\\)检验、Fisher确切概率法\n配对四格表\\(\\chi^2\\)检验，配对R×R列联表\\(\\chi^2\\)检验\n/\n\n\n\n等级资料\nWilcoxon符合秩和检验\nwilcoxon秩和检验\nKruskal-Wallis H检验\nWilcoxon符合秩和检验\nFriedman M秩和检验\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n内容\n\n\n\n\n确切概率法\n1. 适用情形：样本量较小或\\(\\pi_0\\)不靠近0.5时作单侧检验的情形。2. 计算公式：(1)最多有k例阳性的概率：\\(Pr(X\\le k)\\)(2)最少有k例阳性的概率：\\(Pr(X\\ge k)\\)\n\n\n正态近似法\n1. 适用情形：样本量较大时，\\(n\\pi,n(1-\\pi)\\)均大于5；2. 计算公式：分子为\\(p-\\pi_0\\)，分母为率的标准误\n\n\n\nnotice：上式中p为样本率，\\(\\pi_0\\)为给的总体率（常为理论值或标准值），n为样本含量。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n独立四格表卡方检验\n\\(n\\ge 40\\)且所有的\\(T\\ge 5\\)\\(n\\ge 40\\)且任一理论频数有\\(1\\le T&lt; 5\\)当\\(n&lt;40\\)，或任一一个格子理论频数\\(T&lt;1\\)时\n卡方基本公式、独立四格表专用公式同上、但是需要校正用四格表资料的Fisher确切概率法\n\n\n正态近似法\n\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)均大于5\n分子为样本率之差，分母为样本率差的标准误\\(S_{p1-p2}\\)为两个样本率之差的标准误，\\(p_c=\\frac{x_1+x_2}{n_1+n_2}\\)为两样本的合并率\n\n\n校正样本率的正态近似法\n当\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)不太大时\n同上，但是需要对样本率实施“分子+2、分母+4”的校正\n\n\n\nnotice：\n\n正态近似法与卡方检验结果是很接近的。在日常计算时，因为计算简便，故常用卡方检验公式。\n四格表的自由度为1。\n四格表实际频数变动时，若周边合计数保持不变，则理论频数将不会产生变化。\n用\\(n_R\\)和\\(n_C\\)和n分别表示行合计、列合计和总合计，则计算每格理论数的公式为：\\(T_{RC}=\\frac{n_R×n_C}{n}\\)。\n\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}\\)。\n校正的\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|A-T|-0.5)^2}{T}\\)。\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n配对四格表卡方检验\n当\\((b+c)\\ge 40\\)时当\\((b+c)&lt;40\\)时\n配对卡方检验专用公式校正配对卡方检验专用公式\n\n\n配对R×R交叉表数据的\\(\\chi^2\\)检验\nR（\\(R\\ge2\\)）\n\\(T=\\frac{k-1}{k}\\sum_{i=1}^{k}\\frac{(n_i-m_i)^2}{n_i+m_i-2A_{ii}}\\)\n\n\n\nnotice：\n\n配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}=\\frac{(b-c)^2}{b+c}\\)。\n若b+c&lt;40,使用校正的配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|b-c|-1)^2}{b+c}\\)。\n\n\n\n\n\n\n\n\n建立假设检验，确定检验水准 \\(H_0\\):两变量之间相互独立 \\(H_1\\):两变量之间相互独立 \\(\\alpha=0.05\\)\n计算检验统计量 [^2=_{i,j} ]\n确定P值，做出推断\n关联系数的计算 [r=]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n内容\n\n\n\n\n假设检验\n\\(H_0\\)：各组总体率（或构成比）相同。\\(H_1\\)：各组总体率（或构成比）不同（不全相同）。\n\n\n计算公式\n卡方检验基本公式，自由度为：\\(v=(R-1)(C-1)\\)\n\n\n数据要求\n1. 应用条件：不能有理论频数小于1的格子，或者不能有1/5以上的理论频数大于等于1且小于5 2. 不能进行卡方检验时的解决办法：①增加样本量；②合并或删除理论频数比较小的行或列；③采用Fisher确切概率法\n\n\n卡方分割\n多个率或多个频率分布比较的卡方检验，当结论为拒绝\\(H_0\\)时，仅表示多组之间是有差别的。若需要明确研究是那两组之间存在差别，可做率的多重比较，将R×C表分割为若干个小的四格表进行检验，并且需要根据比较的次数合理地修正检验水准\\(\\alpha\\)，否则将人为地增大犯第一类错误的概率\n\n\n\nnotice:\n\n多个独立样本率的比较，根据R个独立样本的频率分布，是检验R个二项分布总体的概率是否相同，。假设对四个样本率进行比较，进行\\(\\chi^2\\)检验，则它的行数为4，列数为2，其自由度为\\(v=(R-1)×(C-1)=(4-1)(2-1)=3\\)。\n针对行列表资料的\\(\\chi^2\\)检验，若有\\(1/5\\)格子以上的理论频数小于5，即\\(1\\le T\\le5\\)时，应考虑增加样本量，或结合专业知识对行或列进行合并。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "卡方检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-chi2-test.html#卡方检验",
    "href": "Learn/Basic/09-chi2-test.html#卡方检验",
    "title": "卡方检验",
    "section": "",
    "text": "资料特征\n数据特征\n\n完全随机设计\n\n配对设计\n随机区组\n\n\n\n\n\n\n单组\n两组\n多组\n\n\n\n\n分类资料\n无序分类资料\n二项分布直接计算概率法、正态近似法（Z检验）、率的正态近似\n独立四格表\\(\\chi^2\\)检验、Fisher确切概率法\nR×C交叉表\\(\\chi^2\\)检验、Fisher确切概率法\n配对四格表\\(\\chi^2\\)检验，配对R×R列联表\\(\\chi^2\\)检验\n/\n\n\n\n等级资料\nWilcoxon符合秩和检验\nwilcoxon秩和检验\nKruskal-Wallis H检验\nWilcoxon符合秩和检验\nFriedman M秩和检验\n\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n内容\n\n\n\n\n确切概率法\n1. 适用情形：样本量较小或\\(\\pi_0\\)不靠近0.5时作单侧检验的情形。2. 计算公式：(1)最多有k例阳性的概率：\\(Pr(X\\le k)\\)(2)最少有k例阳性的概率：\\(Pr(X\\ge k)\\)\n\n\n正态近似法\n1. 适用情形：样本量较大时，\\(n\\pi,n(1-\\pi)\\)均大于5；2. 计算公式：分子为\\(p-\\pi_0\\)，分母为率的标准误\n\n\n\nnotice：上式中p为样本率，\\(\\pi_0\\)为给的总体率（常为理论值或标准值），n为样本含量。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "卡方检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-chi2-test.html#率的比较",
    "href": "Learn/Basic/09-chi2-test.html#率的比较",
    "title": "卡方检验",
    "section": "",
    "text": "方法\n情形\n计算公式\n\n\n\n\n独立四格表卡方检验\n\\(n\\ge 40\\)且所有的\\(T\\ge 5\\)\\(n\\ge 40\\)且任一理论频数有\\(1\\le T&lt; 5\\)当\\(n&lt;40\\)，或任一一个格子理论频数\\(T&lt;1\\)时\n卡方基本公式、独立四格表专用公式同上、但是需要校正用四格表资料的Fisher确切概率法\n\n\n正态近似法\n\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)均大于5\n分子为样本率之差，分母为样本率差的标准误\\(S_{p1-p2}\\)为两个样本率之差的标准误，\\(p_c=\\frac{x_1+x_2}{n_1+n_2}\\)为两样本的合并率\n\n\n校正样本率的正态近似法\n当\\(n_1p_1,n_1(1-p_1),n_2p_2,n_2(1-p_2)\\)不太大时\n同上，但是需要对样本率实施“分子+2、分母+4”的校正\n\n\n\nnotice：\n\n正态近似法与卡方检验结果是很接近的。在日常计算时，因为计算简便，故常用卡方检验公式。\n四格表的自由度为1。\n四格表实际频数变动时，若周边合计数保持不变，则理论频数将不会产生变化。\n用\\(n_R\\)和\\(n_C\\)和n分别表示行合计、列合计和总合计，则计算每格理论数的公式为：\\(T_{RC}=\\frac{n_R×n_C}{n}\\)。\n\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}\\)。\n校正的\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|A-T|-0.5)^2}{T}\\)。\n\n\n\n\n\n\n\n\n\n\n\n\n方法\n情形\n计算公式\n\n\n\n\n配对四格表卡方检验\n当\\((b+c)\\ge 40\\)时当\\((b+c)&lt;40\\)时\n配对卡方检验专用公式校正配对卡方检验专用公式\n\n\n配对R×R交叉表数据的\\(\\chi^2\\)检验\nR（\\(R\\ge2\\)）\n\\(T=\\frac{k-1}{k}\\sum_{i=1}^{k}\\frac{(n_i-m_i)^2}{n_i+m_i-2A_{ii}}\\)\n\n\n\nnotice：\n\n配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(A-T)^2}{T}=\\frac{(b-c)^2}{b+c}\\)。\n若b+c&lt;40,使用校正的配对\\(\\chi^2\\)检验的基本公式：\\(\\chi^2=\\sum \\frac{(|b-c|-1)^2}{b+c}\\)。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "卡方检验"
    ]
  },
  {
    "objectID": "Learn/Basic/09-chi2-test.html#独立性检验",
    "href": "Learn/Basic/09-chi2-test.html#独立性检验",
    "title": "卡方检验",
    "section": "",
    "text": "建立假设检验，确定检验水准 \\(H_0\\):两变量之间相互独立 \\(H_1\\):两变量之间相互独立 \\(\\alpha=0.05\\)\n计算检验统计量 [^2=_{i,j} ]\n确定P值，做出推断\n关联系数的计算 [r=]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n类目\n内容\n\n\n\n\n假设检验\n\\(H_0\\)：各组总体率（或构成比）相同。\\(H_1\\)：各组总体率（或构成比）不同（不全相同）。\n\n\n计算公式\n卡方检验基本公式，自由度为：\\(v=(R-1)(C-1)\\)\n\n\n数据要求\n1. 应用条件：不能有理论频数小于1的格子，或者不能有1/5以上的理论频数大于等于1且小于5 2. 不能进行卡方检验时的解决办法：①增加样本量；②合并或删除理论频数比较小的行或列；③采用Fisher确切概率法\n\n\n卡方分割\n多个率或多个频率分布比较的卡方检验，当结论为拒绝\\(H_0\\)时，仅表示多组之间是有差别的。若需要明确研究是那两组之间存在差别，可做率的多重比较，将R×C表分割为若干个小的四格表进行检验，并且需要根据比较的次数合理地修正检验水准\\(\\alpha\\)，否则将人为地增大犯第一类错误的概率\n\n\n\nnotice:\n\n多个独立样本率的比较，根据R个独立样本的频率分布，是检验R个二项分布总体的概率是否相同，。假设对四个样本率进行比较，进行\\(\\chi^2\\)检验，则它的行数为4，列数为2，其自由度为\\(v=(R-1)×(C-1)=(4-1)(2-1)=3\\)。\n针对行列表资料的\\(\\chi^2\\)检验，若有\\(1/5\\)格子以上的理论频数小于5，即\\(1\\le T\\le5\\)时，应考虑增加样本量，或结合专业知识对行或列进行合并。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "卡方检验"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html",
    "href": "Learn/Basic/11-regression-correlation.html",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "two variables relationship\n\n\n\n\n\nThe basic process of straight-line regression analysis\n\n\n\n\n\n\n\n\n名称\n适用条件\n\n\n\nPearson直线相关系数\n双变量正态分布的资料\\(\\rightarrow\\)定量\\(\\rightarrow\\)类比t检验、方差分析\n\n\n列联系数\n非等级资料\\(\\rightarrow\\)分类\\(\\rightarrow\\)类比卡方检验\n\n\nSpearman秩相关系数\n不满足双变量正态分布、分布未知、等级资料\\(\\rightarrow\\)定量+分类\\(\\rightarrow\\)类比秩和检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#两变量关系分析",
    "href": "Learn/Basic/11-regression-correlation.html#两变量关系分析",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "two variables relationship",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#常见相关系数",
    "href": "Learn/Basic/11-regression-correlation.html#常见相关系数",
    "title": "简单线性相关和回归",
    "section": "",
    "text": "The basic process of straight-line regression analysis\n\n\n\n\n\n\n\n\n名称\n适用条件\n\n\n\nPearson直线相关系数\n双变量正态分布的资料\\(\\rightarrow\\)定量\\(\\rightarrow\\)类比t检验、方差分析\n\n\n列联系数\n非等级资料\\(\\rightarrow\\)分类\\(\\rightarrow\\)类比卡方检验\n\n\nSpearman秩相关系数\n不满足双变量正态分布、分布未知、等级资料\\(\\rightarrow\\)定量+分类\\(\\rightarrow\\)类比秩和检验",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#假设检验",
    "href": "Learn/Basic/11-regression-correlation.html#假设检验",
    "title": "简单线性相关和回归",
    "section": "\n2.1 假设检验",
    "text": "2.1 假设检验\n\n2.1.1 F检验\n\\(y_i\\)的总离均差平方和为：\n\\[SS_{yy}=\\sum_{i}(y_i-\\bar y)^2\\] 对其做分解，得到等式：\n\\[SS_{yy}=\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\] \\(\\sum_{i}^{n}(\\hat y_i-\\bar y)^2\\)为回归平方和（regression sum of squares），记为\\(SS_R\\)，表示回归估计值\\(\\hat y_i\\)与均数\\(\\bar y\\)的离差平方和，其公式为：\n\\[\n\\begin{align}\nSS_{yy} &= \\sum_{i=1}^{n}(\\hat y_i - \\bar y)^2 \\\\\n        &= \\sum_{i=1}^{n}[a + bx_i - (a + b\\bar x)]^2 \\\\\n        &= SS_{xx}b^2 \\\\\n        &= SS_{xy}b\n\\end{align}\n\\] 显然，回归平方和\\(SS_{R}\\)反映的是在y的总变异中由x与y的直线回归关系解释的那部分变异。\\(SS_R\\)值越大，说明回归直线的拟合效果就越好。\n\\(\\sum_{i}^{n}(y_i-\\hat y_i)^2\\)为残差平方和（residual sum of squares），记为\\(SS_E\\)，表示观测值\\(y_i\\)与回归估计值\\(\\hat y_i\\)的离差平方和，其公式为： \\[SS_E=\\sum_{i=1}^{n}(y_i-\\hat y_i)^2\\] \\(SS_E\\)反映了在总变异中扣除自变量x对因变量y的线性影响以后的其他因素（包括x对y的非线性影响和随机误差等）对y变异的影响，也就是在总平方和中无法用y和x线性回归关系解释的部分。\\(SS_E\\)值越小，说明回归直线的拟合效果就越好。\n对公式进行简化： \\[\\begin{align}\nSS_{yy}=&\\sum_{i}^{n}(\\hat y_i-\\bar y)^2+\\sum_{i}^{n}(y_i-\\hat y_i)^2\\\\\n=&SS_R+SS_E\n\\end{align}\\] 上述三个平方和，各有其相应的自由度\\(v\\)，并有如下关系： \\[v_{yy}=v_R+v_E\\\\\nv_{yy}=n-1,v_R=1,v_E=n-2\\]\n在\\(H_0\\)成立的条件下，有： \\[\\frac{SS_R}{\\sigma^2}\\sim \\chi^2(v_R),\\frac{SS_E}{\\sigma^2}\\sim \\chi^2(v_E)\\] 且\\(SS_R\\)和\\(SS_E\\)相互独立。\n检验统计量：\n\\[F=\\frac{SS_R/v_R}{SS_E/v_E}\\] 服从自由度\\(v_R=1,v_E=n-2\\)的F分布。如果y和x确实存在直线回归关系，那么回归所解释的变异\\(SS_R\\)应大于其他因素所解释的变异\\(SS_E\\)。由此可见，F检验正是建立在这个基础上的。\n对于给定的检验水准\\(\\alpha\\)， 如果\\(F&gt;F_{(v_R,v_E),1-\\alpha}\\)，则拒绝\\(H_0\\)，认为直线回归方程有统计学显著性；\n如果\\(F\\leq F_{(v_R,v_E),1-\\alpha}\\)，则不拒绝\\(H_0\\)，尚不能认为直线回归方程有统计学显著性。\n\n2.1.2 t检验法\n回归直线方程的稳定性程度取决于 \\(b\\) 的波动大小，即 \\(S_b\\) 的大小，这里的 \\(S_b\\) 为样本回归系数 \\(b\\) 的标准误的估计值。由于统计量 \\(b\\) 来自正态总体，故可从 \\(b\\) 的抽样分布出发构造 \\(t\\) 统计量对其进行假设检验。\n当 \\(H_0\\) ：\\(\\beta = 0\\) 成立时，检验统计量服从自由度 \\(v_E=n-2\\) 的 \\(t\\) 分布。\n\\[\nt=\\frac{b-0}{S_b}\\sim t(v_E)\n\\]\n其中：\n\\[\nS_b=\\frac{S_{yx}}{\\sqrt{SS_{xx}}}=\\sqrt{\\frac{S_{yx}/(n-2)}{SS_{xx}}}\n\\]\n\\(S_{yx}=\\sqrt{SS_E/(n-2)}\\) 为剩余标准差（residual standard deviation），是指扣除 \\(x\\) 对 \\(y\\) 的线性影响后，衡量观测值 \\(y\\) 对回归直线的平均离散程度，即回归直线 \\(\\hat y\\) 估计的精度。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#直线回归方程的区间估计",
    "href": "Learn/Basic/11-regression-correlation.html#直线回归方程的区间估计",
    "title": "简单线性相关和回归",
    "section": "\n2.2 直线回归方程的区间估计",
    "text": "2.2 直线回归方程的区间估计\n\n2.2.1 总体回归系数的置信区间\n\\[\nt = \\frac{b-\\beta}{S_b}\\sim t(v_E)\n\\]",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#直线相关与直线回归的比较",
    "href": "Learn/Basic/11-regression-correlation.html#直线相关与直线回归的比较",
    "title": "简单线性相关和回归",
    "section": "\n2.3 直线相关与直线回归的比较",
    "text": "2.3 直线相关与直线回归的比较\n\n\n\n\n\n\n\n区别与联系\n类目\n内容\n\n\n\n区别\n资料要求\n1. 线性相关要求X,Y服从双变量正态分布，对这种资料进行回归分析称为\\(\\textrm{II}\\)型回归，即可以把X当自变量，也可以当因变量，反之亦然。2. 线性回归要求Y在给定X值时服从正态分布，X可以是精确测量和严格控制的变量，这时的回归称为型回归，即不可以把X当因变量，Y当自变量进行回归分析。\n\n\n\n\n应用\n1. 线性相关用来表达两个变量间的互依关系，两个变量的研究地位是相等的，谁做X，谁做Y都可以；2. 线性回归用来表达两个变量间的依存变化的数量关系，即一个变量（为因变量Y）如何依存于另一个变量（为自变量X）而变化，两个变量的研究地位是不相等的。\n\n\n\n意义\n1. 相关系数r说明具有线性关系的两个变量之间的密切程度和相关方向；2. 回归系数b表示X每变化一个单位所导致的Y的平均变化量。\n\n\n\nr和b的取值范围\nr没有单位，而b有单位（其单位是：Y的单位/X的单位），所以导致两者的取值范围不同；\\(-1 \\le r \\le 1\\),\\(-\\infty&lt;b&lt;+\\infty\\)\n\n\n\n\nr和b的计算公式不同\n\n\\(r=\\frac{l_{xy}}{\\sqrt{l_{xx}l_{yy}}}\\),\\(b=\\frac{SS_{xy}}{SS_{xx}}\\)\n\n\n\n联系\n符号\n对于既可以做相关又可作回归的同一组资料，计算出r与b的正负号相同\n\n\n\n假设检验\n对于同一组资料，相关系数和回归系数的假设检验等价。即有：\\(t_b=t_r\\)\n\n\n\n\n相互换算\n对于同一组资料，相关系数和回归系数可通过下式换算：\\(b=r\\frac{S_Y}{S_X}\\)，式中的\\(S_X,S_Y\\)分别是\\(X,Y\\)的标准差\n\n\n\n用回归解释相关\n又决定系数\\(R^2=\\frac{SS_{回}}{SS_{总}}\\in [0,1]\\)当总平方和的大小决定了相关的密切程度，回归平方和越接近总平方和，则\\(R^2\\)越接近1，相关的效果越好，说明回归效果越好，相关的密切程度也越高。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#数学定义",
    "href": "Learn/Basic/11-regression-correlation.html#数学定义",
    "title": "简单线性相关和回归",
    "section": "\n3.1 数学定义",
    "text": "3.1 数学定义\n均值向量：对于双元正态变量 \\((X, Y)\\)，均值向量为：\n\\[\\mu = \\begin{pmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{pmatrix}\\]\n其中 \\(\\mu_X\\) 和 \\(\\mu_Y\\) 分别是随机变量 \\(X\\) 和 \\(Y\\) 的均值。 2. 协方差矩阵：协方差矩阵为：\n\\[\\Sigma = \\begin{pmatrix}\n\\sigma_X^2 & \\sigma_{XY} \\\\\n\\sigma_{XY} & \\sigma_Y^2\n\\end{pmatrix}\\]\n其中 \\(\\sigma_X^2\\) 和 \\(\\sigma_Y^2\\) 是 \\(X\\) 和 \\(Y\\) 的方差，\\(\\sigma_{XY}\\) 是它们之间的协方差。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#性质",
    "href": "Learn/Basic/11-regression-correlation.html#性质",
    "title": "简单线性相关和回归",
    "section": "\n3.2 性质",
    "text": "3.2 性质\n\n边缘分布：\\(X\\) 和 \\(Y\\) 的边缘分布也是正态分布。\n条件分布：给定 \\(X\\) 的值，\\(Y\\) 的条件分布也是正态分布。\n相关性：协方差矩阵的值可以用来判断 X 和 Y 之间的相关性。如果 \\(\\sigma_{XY} &gt; 0\\)，则两者正相关；如果 \\(\\sigma_{XY} &lt; 0\\)，则负相关。",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Basic/11-regression-correlation.html#示例",
    "href": "Learn/Basic/11-regression-correlation.html#示例",
    "title": "简单线性相关和回归",
    "section": "\n3.3 示例",
    "text": "3.3 示例\n假设有一组数据描述学生的身高 \\(X\\) 和体重 \\(Y\\)，并且假设 \\((X, Y)\\) 服从双元正态分布：\n\n3.3.1 均值向量：\n\\[\\mu = \\begin{pmatrix}\n170 \\\\\n65\n\\end{pmatrix}\\]\n表示身高的均值为 170 厘米，体重的均值为 65 千克。 •\n\n3.3.2 协方差矩阵：\n\\[\\Sigma = \\begin{pmatrix}\n100 & 20 \\\\\n20 & 25\n\\end{pmatrix}\\]\n这里，身高的方差为 100，体重的方差为 25，协方差为 20，表示身高和体重之间存在正相关关系。\n在这个示例中，如果我们知道某个学生的身高为 180 厘米，我们可以利用条件分布来预测他的体重，这个体重的预测值也是正态分布。\n我们用一个图形来展示：\n\n## 安装和加载所需的包\n#install.packages(\"plotly\")\n#install.packages(\"mvtnorm\")\nlibrary(plotly)\nlibrary(mvtnorm)\nlibrary(webshot2)\n\n# 创建网格数据\nx &lt;- seq(150, 190, length.out = 100)\n#身高150-190，等距的100个值\ny &lt;- seq(50, 80, length.out = 100)\n#体重50-80，等距的100个值\ngrid &lt;- expand.grid(X = x, Y = y)\n#生成 x 和 y 的所有组合，用于构建一个网格数据框，以便计算多元正态分布的概率密度。\n\n# 设置均值和协方差矩阵\nmu &lt;- c(170, 65)\n#设置双元正态分布的均值向量，表示均值分别为身高 170 cm 和体重 65 kg\n\nsigma &lt;- matrix(c(100, 20, 20, 25), nrow = 2)\n#设置协方差矩阵，表示身高的方差为 100，体重的方差为 25，身高和体重之间的协方差为 20\n\n# 计算概率密度\nz &lt;- dmvnorm(as.matrix(grid), mean = mu, sigma = sigma)\n#计算每个网格点上双元正态分布的概率密度。\n\n# 将概率密度矩阵转换为适合绘图的形状\nz_matrix &lt;- matrix(z, nrow = 100, ncol = 100)\n\n# 绘制三维表面图\nplot_ly(x = x, y = y, z = z_matrix, type = \"surface\") %&gt;%\n  layout(title = list(text = \"双元正态分布的三维概率密度图\", y=0.95),\n         scene = list(xaxis = list(title = \"身高 (cm)\"),\n                      yaxis = list(title = \"体重 (kg)\"),\n                      zaxis = list(title = \"概率密度\")))\n\n\nBinary normal distribution\n\n\n注：上述图像在被转换为PDF文件时，会发生报错：Quarto 文档中包含了一些生成 HTML 输出的函数（比如交互式图表或其他 HTML 小部件），但你当前的目标输出格式是 PDF。由于 PDF 是静态格式，无法直接渲染 HTML 内容，Quarto 会报错并停止执行。\n解决方案，此章节不转换为PDF格式，或者：\n如果你仍想输出 PDF，但希望将 HTML 小部件作为静态截图嵌入，可以安装 R 的 webshot 或 webshot2 包。Quarto 会利用它们将 HTML 内容转换为图片。\n需要安装：\n\ninstall.packages(\"webshot2\")\n\n然后在这段程序的前部导入该包：library(webshot2)。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "医学统计学基础",
      "简单线性相关和回归"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html#灯泡机的简单概率图模型",
    "href": "Learn/Bayes/01-Bayes-PGM.html#灯泡机的简单概率图模型",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "",
    "text": "首先，为每一个节点定义取值：\n\nmachine_val &lt;- c(\"working\", \"broken\")\nlight_bulb_val &lt;- c(\"good\", \"bad\")\n\n为两个随机变量定义百分比数值：\n\nmachine_val &lt;- c(99,1)\nlight_bulb_val &lt;- c(99,1,60,40)\n\n使用 gRain 定义随机变量：\n\nlibrary(gRain)\nM &lt;- cptable(~machine, values = machine_prob,\n            levels = machine_val)\nL &lt;- cptable(~light_bulb | machine,\n            values = light_bulb_prob,\n            levels = light_bulb_val)\n\n这里的 cptable 表示条件概率表1：它是离散型随机变量概率分布的内存表示2。\n\n\nplist &lt;- compileCPT(list(M,L))\nplist\n\n输出结果如上，这里可以清楚地看到之前定义的概率分布\n2025.04.05 再次尝试复现程序，失败，暂时停止概率图R程序的复现工作。\nend.",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Learn/Bayes/01-Bayes-PGM.html#footnotes",
    "href": "Learn/Bayes/01-Bayes-PGM.html#footnotes",
    "title": "贝叶斯与概率图模型（PGM）",
    "section": "脚注",
    "text": "脚注\n\n条件概率表（Conditional Probability Table, CPT），是一种表格形式的数据结构，用来描述离散型随机变量之间的概率关系。通常用于表示一个变量（或一组变量）的概率分布，可能依赖于其他变量（条件变量）。↩︎\n“内存表示”指的是在计算机程序中，这种概率分布被组织和存储为一种数据结构（例如表格、数组或矩阵），以便程序可以高效地访问和操作这些概率值。具体来说，cptable 函数会根据你提供的参数（如 values 和 levels）生成一个对象，这个对象在内存中以某种形式存储了变量的概率分布。↩︎",
    "crumbs": [
      "Home",
      "医学统计学基础",
      "贝叶斯与概率推理",
      "贝叶斯与概率图模型（PGM）"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID-test.html",
    "href": "Guide/Python/25-04-28-DID-test.html",
    "title": "How to Use the DID in Python",
    "section": "",
    "text": "双重差分回归 (DID) 用于评估一个事件的因果效应，其方法是比较事件发生的单元集合（处理组）与事件未发生的单元集合（控制组）。\nDID 背后的逻辑是，如果事件从未发生，处理组和控制组之间的差异应该随着时间的推移保持不变。\nDID 通过比较处理组和控制组在事件发生前后的差异来估计事件的因果效应。\nDID 法是一种无法随机分配样本情况下的替代方法，主要应用于区域行的策略评估问题。\n目标：获取相对同质的策略组和控制组，这个“相对”是指除策略影响外，策略组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n对于相对同质的策略组和控制组，DID法通过第一次的差分消除这个基本固定的差异，通过第二次的差分消除时间趋势的影响，评估策略带来的实际效应。\n从DID 法的目标中可知，该方法面对的实验数据是面板数据（多个时间点的截面数据组成面板数据），即在策略干预时间点前，至少有两个时间点的数据。\n\\[\ny = \\alpha_0 +\\alpha_1g +\\alpha_2T + \\alpha_3gT + \\epsilon\n\\] \\(\\alpha_0\\)为常数项，\\(\\alpha_1\\)为处理组和控制组的差异，\\(\\alpha_2\\)为时间效应，\\(\\epsilon\\)为误差项。 \\(\\alpha_3\\)为交互项的系数，表示处理组和控制组在事件发生前后的差异。\n其中，\\(y\\)为结果变量，\\(g\\)为处理组和控制组的虚拟变量，\\(T\\)为时间虚拟变量，\\(gT\\)为交互项。 \\(\\alpha_3\\)为DID估计量，表示处理组和控制组在事件发生前后的差异。\nDID 模型的有效性检验\n为了保证该模型的有效性，在试验设计时需要满足平行趋势假设：在事件发生前，处理组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n平行趋势，即策略组和控制组在干预前保持相同的变化趋势。\n3种常见的平行趋势的检验方法：\n\n画图法：画出处理组和控制组在事件发生前后的结果变量的变化趋势图，观察两组的变化趋势是否平行。\n统计检验法：使用t检验或F检验等统计方法，检验处理组和控制组在事件发生前的结果变量的差异是否显著。\n伪DID法：在事件发生前，随机选择一个时间点，将处理组和控制组的结果变量进行差分，检验差分后的结果变量是否显著。",
    "crumbs": [
      "Home",
      "统计软件",
      "Python",
      "How to Use the DID in Python"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID-test.html#did双重差分回归",
    "href": "Guide/Python/25-04-28-DID-test.html#did双重差分回归",
    "title": "How to Use the DID in Python",
    "section": "",
    "text": "双重差分回归 (DID) 用于评估一个事件的因果效应，其方法是比较事件发生的单元集合（处理组）与事件未发生的单元集合（控制组）。\nDID 背后的逻辑是，如果事件从未发生，处理组和控制组之间的差异应该随着时间的推移保持不变。\nDID 通过比较处理组和控制组在事件发生前后的差异来估计事件的因果效应。\nDID 法是一种无法随机分配样本情况下的替代方法，主要应用于区域行的策略评估问题。\n目标：获取相对同质的策略组和控制组，这个“相对”是指除策略影响外，策略组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n对于相对同质的策略组和控制组，DID法通过第一次的差分消除这个基本固定的差异，通过第二次的差分消除时间趋势的影响，评估策略带来的实际效应。\n从DID 法的目标中可知，该方法面对的实验数据是面板数据（多个时间点的截面数据组成面板数据），即在策略干预时间点前，至少有两个时间点的数据。\n\\[\ny = \\alpha_0 +\\alpha_1g +\\alpha_2T + \\alpha_3gT + \\epsilon\n\\] \\(\\alpha_0\\)为常数项，\\(\\alpha_1\\)为处理组和控制组的差异，\\(\\alpha_2\\)为时间效应，\\(\\epsilon\\)为误差项。 \\(\\alpha_3\\)为交互项的系数，表示处理组和控制组在事件发生前后的差异。\n其中，\\(y\\)为结果变量，\\(g\\)为处理组和控制组的虚拟变量，\\(T\\)为时间虚拟变量，\\(gT\\)为交互项。 \\(\\alpha_3\\)为DID估计量，表示处理组和控制组在事件发生前后的差异。\nDID 模型的有效性检验\n为了保证该模型的有效性，在试验设计时需要满足平行趋势假设：在事件发生前，处理组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n平行趋势，即策略组和控制组在干预前保持相同的变化趋势。\n3种常见的平行趋势的检验方法：\n\n画图法：画出处理组和控制组在事件发生前后的结果变量的变化趋势图，观察两组的变化趋势是否平行。\n统计检验法：使用t检验或F检验等统计方法，检验处理组和控制组在事件发生前的结果变量的差异是否显著。\n伪DID法：在事件发生前，随机选择一个时间点，将处理组和控制组的结果变量进行差分，检验差分后的结果变量是否显著。",
    "crumbs": [
      "Home",
      "统计软件",
      "Python",
      "How to Use the DID in Python"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID.html",
    "href": "Guide/Python/25-04-28-DID.html",
    "title": "DID 双重差分模型",
    "section": "",
    "text": "双重差分回归 (DID) 用于评估一个事件的因果效应，其方法是比较事件发生的单元集合（处理组）与事件未发生的单元集合（控制组）。\nDID 背后的逻辑是，如果事件从未发生，处理组和控制组之间的差异应该随着时间的推移保持不变。\nDID 通过比较处理组和控制组在事件发生前后的差异来估计事件的因果效应。\nDID 法是一种无法随机分配样本情况下的替代方法，主要应用于区域行的策略评估问题。\n目标：获取相对同质的策略组和控制组，这个“相对”是指除策略影响外，策略组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n对于相对同质的策略组和控制组，DID法通过第一次的差分消除这个基本固定的差异，通过第二次的差分消除时间趋势的影响，评估策略带来的实际效应。\n从DID 法的目标中可知，该方法面对的实验数据是面板数据（多个时间点的截面数据组成面板数据），即在策略干预时间点前，至少有两个时间点的数据。\n\n\n\n\n\n\\[\ny = \\alpha_0 +\\alpha_1g +\\alpha_2T + \\alpha_3gT + \\epsilon\n\\]\n其中，\\(\\alpha_0\\)为常数项，\\(\\alpha_1\\)为处理组和控制组的差异，\\(\\alpha_2\\)为时间效应，\\(\\epsilon\\)为误差项。\n\\(y\\)为结果变量，\\(g\\)为处理组和控制组的虚拟变量，\\(T\\)为时间虚拟变量，\\(gT\\)为交互项。\n\\(\\alpha_3\\)为交互项的系数，也就是DID的估计量，当交互项 \\(gT\\) 与结果变量 \\(y\\) 显著相关时，\\(\\alpha_3\\) 为评估的实际的策略效应，表示处理组和控制组在事件发生前后的差异。\n\n\n\n\n\n\nDID模型示意图\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n组别\n干预前（T=0）\n干预后（T=1)\n差分（干预后非策略差异）\n\n\n\n\n策略组(g=1)\n\\(\\alpha_0+\\alpha_1\\)\n\\(\\alpha_0+\\alpha_1+\\alpha_2+\\alpha_3\\)\n\\(\\alpha_2+\\alpha_3\\)\n\n\n控制组(g=0)\n\\(\\alpha_0\\)（基准值）\n\\(\\alpha_0+\\alpha_2\\)\n\\(\\alpha_2\\)\n\n\n差分(组间差异)\n\\(\\alpha_1\\)\n\\(\\alpha_1+\\alpha_3\\)\n\\(\\alpha_3\\)（策略效应）\n\n\n\nDID模型的原理很清晰，在无法获取完全同质的策略组和控制组的情况下，替代地获取存在固定组间差异的试验数据以抵抗混杂的影响，最后通过回归系数的显著性检验来评估策略实施的净效应。但是，理论很简洁，现实很残酷，获取 @ref(DID-models) 所示的理想数据并不简单，为了保证DID模型可以有效地评估策略效应，需要一些必要的前提检验。下面介绍这些关于DID模型的有效性检验。\n\n\n\n\n为了保证该模型的有效性，在试验设计时需要满足平行趋势假设：在事件发生前，处理组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n平行趋势，即策略组和控制组在干预前保持相同的变化趋势。\n3种常见的平行趋势的检验方法：\n\n画图法：画出处理组和控制组在事件发生前后的结果变量的变化趋势图，观察两组的变化趋势是否平行。\n统计检验法（差异性检验）：使用t检验或F检验等统计方法，检验处理组和控制组在事件发生前的结果变量的差异是否显著。\n伪DID法（交互项显著性检验）：在事件发生前，随机选择一个时间点，将处理组和控制组的结果变量进行差分，检验差分后的结果变量是否显著。\n\n\n\n\n数据来自 princeton 的 Oscar Torres-Reyna 教授构建的虚拟数据集。\n案例数据示例如下表所示：",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "DID 双重差分模型"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID.html#did-定义与目标",
    "href": "Guide/Python/25-04-28-DID.html#did-定义与目标",
    "title": "DID 双重差分模型",
    "section": "",
    "text": "双重差分回归 (DID) 用于评估一个事件的因果效应，其方法是比较事件发生的单元集合（处理组）与事件未发生的单元集合（控制组）。\nDID 背后的逻辑是，如果事件从未发生，处理组和控制组之间的差异应该随着时间的推移保持不变。\nDID 通过比较处理组和控制组在事件发生前后的差异来估计事件的因果效应。\nDID 法是一种无法随机分配样本情况下的替代方法，主要应用于区域行的策略评估问题。\n目标：获取相对同质的策略组和控制组，这个“相对”是指除策略影响外，策略组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n对于相对同质的策略组和控制组，DID法通过第一次的差分消除这个基本固定的差异，通过第二次的差分消除时间趋势的影响，评估策略带来的实际效应。\n从DID 法的目标中可知，该方法面对的实验数据是面板数据（多个时间点的截面数据组成面板数据），即在策略干预时间点前，至少有两个时间点的数据。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "DID 双重差分模型"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID.html#did-模型的原理",
    "href": "Guide/Python/25-04-28-DID.html#did-模型的原理",
    "title": "DID 双重差分模型",
    "section": "",
    "text": "\\[\ny = \\alpha_0 +\\alpha_1g +\\alpha_2T + \\alpha_3gT + \\epsilon\n\\]\n其中，\\(\\alpha_0\\)为常数项，\\(\\alpha_1\\)为处理组和控制组的差异，\\(\\alpha_2\\)为时间效应，\\(\\epsilon\\)为误差项。\n\\(y\\)为结果变量，\\(g\\)为处理组和控制组的虚拟变量，\\(T\\)为时间虚拟变量，\\(gT\\)为交互项。\n\\(\\alpha_3\\)为交互项的系数，也就是DID的估计量，当交互项 \\(gT\\) 与结果变量 \\(y\\) 显著相关时，\\(\\alpha_3\\) 为评估的实际的策略效应，表示处理组和控制组在事件发生前后的差异。\n\n\n\n\n\n\nDID模型示意图\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n组别\n干预前（T=0）\n干预后（T=1)\n差分（干预后非策略差异）\n\n\n\n\n策略组(g=1)\n\\(\\alpha_0+\\alpha_1\\)\n\\(\\alpha_0+\\alpha_1+\\alpha_2+\\alpha_3\\)\n\\(\\alpha_2+\\alpha_3\\)\n\n\n控制组(g=0)\n\\(\\alpha_0\\)（基准值）\n\\(\\alpha_0+\\alpha_2\\)\n\\(\\alpha_2\\)\n\n\n差分(组间差异)\n\\(\\alpha_1\\)\n\\(\\alpha_1+\\alpha_3\\)\n\\(\\alpha_3\\)（策略效应）\n\n\n\nDID模型的原理很清晰，在无法获取完全同质的策略组和控制组的情况下，替代地获取存在固定组间差异的试验数据以抵抗混杂的影响，最后通过回归系数的显著性检验来评估策略实施的净效应。但是，理论很简洁，现实很残酷，获取 @ref(DID-models) 所示的理想数据并不简单，为了保证DID模型可以有效地评估策略效应，需要一些必要的前提检验。下面介绍这些关于DID模型的有效性检验。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "DID 双重差分模型"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID.html#did-模型的有效性检验",
    "href": "Guide/Python/25-04-28-DID.html#did-模型的有效性检验",
    "title": "DID 双重差分模型",
    "section": "",
    "text": "为了保证该模型的有效性，在试验设计时需要满足平行趋势假设：在事件发生前，处理组和控制组的结果变量随时间的变化存在一个基本固定的差异。\n平行趋势，即策略组和控制组在干预前保持相同的变化趋势。\n3种常见的平行趋势的检验方法：\n\n画图法：画出处理组和控制组在事件发生前后的结果变量的变化趋势图，观察两组的变化趋势是否平行。\n统计检验法（差异性检验）：使用t检验或F检验等统计方法，检验处理组和控制组在事件发生前的结果变量的差异是否显著。\n伪DID法（交互项显著性检验）：在事件发生前，随机选择一个时间点，将处理组和控制组的结果变量进行差分，检验差分后的结果变量是否显著。",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "DID 双重差分模型"
    ]
  },
  {
    "objectID": "Guide/Python/25-04-28-DID.html#did-案例分析",
    "href": "Guide/Python/25-04-28-DID.html#did-案例分析",
    "title": "DID 双重差分模型",
    "section": "",
    "text": "数据来自 princeton 的 Oscar Torres-Reyna 教授构建的虚拟数据集。\n案例数据示例如下表所示：",
    "crumbs": [
      "Home",
      "统计软件使用历程",
      "Python",
      "DID 双重差分模型"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html",
    "href": "Guide/Stata/25-04-29-Stata-PY.html",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "在Stata官网冲浪的时候，看到他们推出了Python与Stata联合使用的功能，于是就想试试。\nStata是一个强大的统计分析软件，广泛应用于社会科学、经济学、医学等领域。它提供了丰富的统计分析功能和数据处理工具，适合进行复杂的数据分析和建模工作。\n在使用Stata的时候，我们可能会需要在Python中调用Stata的功能。PyStata就是一个可以让我们在Python中使用Stata的工具。它允许我们在Python中运行Stata命令，并且可以将结果返回到Python中进行进一步的分析和处理。\n\n\n\nStata 官网关于 pystata 的介绍\nPyStata 是 Stata17 中引入的一个新概念，它涵盖了所有 Stata 和 Python 的交互方式。\n事实上从 Stata16 开始，我们就可以在 Stata 中调用 Python 代码，并通过 Stata 函数接口（ sfi 模块）实现 Python 与 Stata 核心功能的交互；但 Stata17 通过允许我们通过导入一个新的 Python 包（pystata）从一个独立的 Python 环境中调用 Stata ，这大大扩展二者的交互功能，使我们可以在基于或支持 IPython 内核的环境中（例如：Jupyter Notebook 、Jupyter Lab 、Spyder 、PyCharm 、VScode 等）更加方便地调用 Stata 和 Mata。\n\n\n自 Stata17 之后，官方推出了一种 StataS与 Python 的全新交互方式，而 stata_kernel 是一个第三方项目。\n即通过在 Python 环境中直接安装 pystata 模块，便能在 Python 环境中直接调用 Stata17 的命令。而 stata_kernel 是通过在 Jupyter Notebook 中安装 stata_kernel 模块，来实现 Python 与 Stata 的交互。\n在使用Quarto制作本网站时，可以编译 .qmd 和 .ipynb 文件，生成 .html 和 .pdf 文件，就想是否可以 Quarto 中利用 .ipynb 文件来调用 Stata 然后编译成 .html 文件再在网站中展示出来。\n于是有了这篇笔记。\n步骤主要分为四部分：\n\n将 Stata 添加到系统环境变量中\n在 Python 中安装 PyStata\n在 Jupyter Notebook 中使用PyStata\n在 Quarto 中使用 PyStata\n\n\n\n\n\n\n请先安装好 Stata 17 或更高版本，且最好是无限制的版本，因为有很多网上的资源是破解的，可能会有一些限制。\nStata17/18/19 软件必须具备有效的许可证，否则无法调用\n拥有基于或支持 IPython 内核的 Python 环境（建议使用 Jupyter Lab 或 VScode）\nPython 3.7 或更高版本（建议使用 Anaconda 进行安装和管理）\n\n\n\n\n要使用 pystata 包的完整功能，需要安装以下 Python 包：\n\nNumPy 1.9 或更高版本\npandas 0.15 或更高版本\n\n如果您仅计划通过调用 stata 模块中的 run() 方法执行 Stata 命令，则无需安装 NumPy 和 pandas 包。\n然而，如果需要调用 stata 模块中用于在 Stata 和 Python 之间传递数据和结果的方法，则必须安装这些包。\n\nIPython 5.0 或更高版本\n\n如果您想使用魔法命令，则需要安装 IPython 包。\n\n\n\n\n\n\n\n\n找到 Stata 的安装目录，通常在 C:\\Program Files\\Stata18 或 C:\\Program Files (x86)\\Stata18。\n复制该目录的路径。\n右键点击“此电脑”或“计算机”，选择“属性”。\n点击“高级系统设置”。\n在“系统属性”窗口中，点击“环境变量”。\n在“系统变量”部分，找到名为“Path”的变量，选中它并点击“编辑”。\n在“编辑环境变量”窗口中，点击“新建”，然后粘贴 Stata 的安装目录路径。\n点击“确定”保存更改，关闭所有窗口。\n重新启动命令提示符或 PowerShell，以使更改生效。\n在命令提示符中输入 stata，如果 Stata 启动，则说明添加成功。\n\n\n\n\n\n打开命令提示符或 PowerShell。\n输入以下命令，将 C:\\Program Files\\Stata18 替换为 Stata 的安装目录：\n\nsetx PATH \"%PATH%;C:\\Program Files\\Stata17\"\n\n按下回车键执行命令。\n关闭命令提示符或 PowerShell。\n重新打开命令提示符或 PowerShell，以使更改生效。\n在命令提示符中输入 stata，如果 Stata 启动，则说明添加成功。\n\n\n\n\n这个办法也有一定的普及程度，但是可能不太好用，对于新手来说会有些难以理解。可以参考连玉君的珠联璧合：Jupyter Notebook 和 Stata 之融合。\n主要步骤如下：\n\n找到 Stata 的安装目录，通常在 C:\\Program Files\\Stata18 或 C:\\Program Files (x86)\\Stata18。\n在该目录下找到 StataMP.exe 或 StataSE.exe 文件。\n将该文件的路径复制下来。\n以管理员身份打开电脑的 Windows Powershell 。\n在 PS C:\\WINDOWS\\system32&gt; 后输入以下命令（将路径转到 Stata 安装目录下）：\n\n在 Windows PowerShell 执行 cd 命令，以进入 stata 程序安装的路径。cd 命令后接上步所获取的 stata 安装路径。根据个人电脑安装路径不同有所差异。路径请以英文引号包围，这样可以避免路径文件夹名称中包含空格导致无法顺利进入目标路径。\ncd \"C:\\Program Files\\Stata18\"\n实际效果应该如下：\nPS C:\\WINDOWS\\system32&gt; cd 'C:\\Program Files\\Stata18'\nPS C:\\Program Files\\Stata18&gt;\n\n输入以下命令来将 stata 添加到命令行注册表中：\n\n.\\StataMP-64.exe /Register\n需要注意的是： .\\StataMP-64.exe /Register 中的 .\\StataMP-64.exe 部分，根据个人电脑安装 Stata17+ 版本有所差异。我电脑安装的是 MP 版，所以为 .\\StataMP-64.exe。如果安装的是 SE 版，应该为 .\\StataSE-64.exe。\n这里实测效果不太好，不知道为什么，注册成功但是命令行输入 stata 还是无法打开 Stata。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#前言",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#前言",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "在Stata官网冲浪的时候，看到他们推出了Python与Stata联合使用的功能，于是就想试试。\nStata是一个强大的统计分析软件，广泛应用于社会科学、经济学、医学等领域。它提供了丰富的统计分析功能和数据处理工具，适合进行复杂的数据分析和建模工作。\n在使用Stata的时候，我们可能会需要在Python中调用Stata的功能。PyStata就是一个可以让我们在Python中使用Stata的工具。它允许我们在Python中运行Stata命令，并且可以将结果返回到Python中进行进一步的分析和处理。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#关于pystata",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#关于pystata",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "Stata 官网关于 pystata 的介绍\nPyStata 是 Stata17 中引入的一个新概念，它涵盖了所有 Stata 和 Python 的交互方式。\n事实上从 Stata16 开始，我们就可以在 Stata 中调用 Python 代码，并通过 Stata 函数接口（ sfi 模块）实现 Python 与 Stata 核心功能的交互；但 Stata17 通过允许我们通过导入一个新的 Python 包（pystata）从一个独立的 Python 环境中调用 Stata ，这大大扩展二者的交互功能，使我们可以在基于或支持 IPython 内核的环境中（例如：Jupyter Notebook 、Jupyter Lab 、Spyder 、PyCharm 、VScode 等）更加方便地调用 Stata 和 Mata。\n\n\n自 Stata17 之后，官方推出了一种 StataS与 Python 的全新交互方式，而 stata_kernel 是一个第三方项目。\n即通过在 Python 环境中直接安装 pystata 模块，便能在 Python 环境中直接调用 Stata17 的命令。而 stata_kernel 是通过在 Jupyter Notebook 中安装 stata_kernel 模块，来实现 Python 与 Stata 的交互。\n在使用Quarto制作本网站时，可以编译 .qmd 和 .ipynb 文件，生成 .html 和 .pdf 文件，就想是否可以 Quarto 中利用 .ipynb 文件来调用 Stata 然后编译成 .html 文件再在网站中展示出来。\n于是有了这篇笔记。\n步骤主要分为四部分：\n\n将 Stata 添加到系统环境变量中\n在 Python 中安装 PyStata\n在 Jupyter Notebook 中使用PyStata\n在 Quarto 中使用 PyStata",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#前置要求",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#前置要求",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "请先安装好 Stata 17 或更高版本，且最好是无限制的版本，因为有很多网上的资源是破解的，可能会有一些限制。\nStata17/18/19 软件必须具备有效的许可证，否则无法调用\n拥有基于或支持 IPython 内核的 Python 环境（建议使用 Jupyter Lab 或 VScode）\nPython 3.7 或更高版本（建议使用 Anaconda 进行安装和管理）",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#依赖项",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#依赖项",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "要使用 pystata 包的完整功能，需要安装以下 Python 包：\n\nNumPy 1.9 或更高版本\npandas 0.15 或更高版本\n\n如果您仅计划通过调用 stata 模块中的 run() 方法执行 Stata 命令，则无需安装 NumPy 和 pandas 包。\n然而，如果需要调用 stata 模块中用于在 Stata 和 Python 之间传递数据和结果的方法，则必须安装这些包。\n\nIPython 5.0 或更高版本\n\n如果您想使用魔法命令，则需要安装 IPython 包。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#stata-添加到系统环境变量中",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#stata-添加到系统环境变量中",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "",
    "text": "找到 Stata 的安装目录，通常在 C:\\Program Files\\Stata18 或 C:\\Program Files (x86)\\Stata18。\n复制该目录的路径。\n右键点击“此电脑”或“计算机”，选择“属性”。\n点击“高级系统设置”。\n在“系统属性”窗口中，点击“环境变量”。\n在“系统变量”部分，找到名为“Path”的变量，选中它并点击“编辑”。\n在“编辑环境变量”窗口中，点击“新建”，然后粘贴 Stata 的安装目录路径。\n点击“确定”保存更改，关闭所有窗口。\n重新启动命令提示符或 PowerShell，以使更改生效。\n在命令提示符中输入 stata，如果 Stata 启动，则说明添加成功。\n\n\n\n\n\n打开命令提示符或 PowerShell。\n输入以下命令，将 C:\\Program Files\\Stata18 替换为 Stata 的安装目录：\n\nsetx PATH \"%PATH%;C:\\Program Files\\Stata17\"\n\n按下回车键执行命令。\n关闭命令提示符或 PowerShell。\n重新打开命令提示符或 PowerShell，以使更改生效。\n在命令提示符中输入 stata，如果 Stata 启动，则说明添加成功。\n\n\n\n\n这个办法也有一定的普及程度，但是可能不太好用，对于新手来说会有些难以理解。可以参考连玉君的珠联璧合：Jupyter Notebook 和 Stata 之融合。\n主要步骤如下：\n\n找到 Stata 的安装目录，通常在 C:\\Program Files\\Stata18 或 C:\\Program Files (x86)\\Stata18。\n在该目录下找到 StataMP.exe 或 StataSE.exe 文件。\n将该文件的路径复制下来。\n以管理员身份打开电脑的 Windows Powershell 。\n在 PS C:\\WINDOWS\\system32&gt; 后输入以下命令（将路径转到 Stata 安装目录下）：\n\n在 Windows PowerShell 执行 cd 命令，以进入 stata 程序安装的路径。cd 命令后接上步所获取的 stata 安装路径。根据个人电脑安装路径不同有所差异。路径请以英文引号包围，这样可以避免路径文件夹名称中包含空格导致无法顺利进入目标路径。\ncd \"C:\\Program Files\\Stata18\"\n实际效果应该如下：\nPS C:\\WINDOWS\\system32&gt; cd 'C:\\Program Files\\Stata18'\nPS C:\\Program Files\\Stata18&gt;\n\n输入以下命令来将 stata 添加到命令行注册表中：\n\n.\\StataMP-64.exe /Register\n需要注意的是： .\\StataMP-64.exe /Register 中的 .\\StataMP-64.exe 部分，根据个人电脑安装 Stata17+ 版本有所差异。我电脑安装的是 MP 版，所以为 .\\StataMP-64.exe。如果安装的是 SE 版，应该为 .\\StataSE-64.exe。\n这里实测效果不太好，不知道为什么，注册成功但是命令行输入 stata 还是无法打开 Stata。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#方法一使用-pip-配置-pystata",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#方法一使用-pip-配置-pystata",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "2.1 方法一：使用 pip 配置 pystata",
    "text": "2.1 方法一：使用 pip 配置 pystata\nPyStata 可以通过 pip 安装。Windows 可以使用以下命令进行安装：\npip install --upgrade --user stata_setup\nmacOS 或 Unix 系统可以使用以下命令进行安装：\n$ pip install --upgrade --user stata_setup\n\n2.1.1 配置 Stata\n假设你的 Stata 安装在 STATA_SYSDIR 目录下，并且使用的是 Stata/MP 版本。你可以在 Python 环境中按如下方式配置 Stata：\n如果 Stata 配置正确，stata_setup.config() 将返回如下的启动画面，其中包含 Stata 的徽标和初始化消息。\n\n\n代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp')\n\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       MP—Parallel Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user 2-core network, expiring  8 Apr 2026\nSerial number: 501809376090\n  Licensed to: ausa\n               NJU\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. More than 2 billion observations are allowed; see help obs_advice.\n      3. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\n如果你不想看到初始化的信息，可以使用如下命令将其隐藏：\nstata_setup.config('YOUR_STATA_SYSDIR', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-04-29-Stata-PY.html#方法二将-pystata-添加到-sys.path",
    "href": "Guide/Stata/25-04-29-Stata-PY.html#方法二将-pystata-添加到-sys.path",
    "title": "01-Use Stata in Quarto(ipynb)",
    "section": "2.2 方法二：将 pystata 添加到 sys.path",
    "text": "2.2 方法二：将 pystata 添加到 sys.path\n找到 pystata 包的最直接方法是将 pystata 子目录的位置添加到 Python 的模块搜索路径中。在你的 Python 环境中，你可以输入\nimport sys\nsys.path.append('STATA_SYSDIR/utilities')\nfrom pystata import config\nconfig.init('mp')\n如果配置正确，config.init() 应该返回无错误，并显示同上面一样的的启动画面，其中包含 Stata 的徽标和初始化消息。如果您想隐藏这些消息，可以将 splash 参数设置为 False。\n更多的安装和配置信息可以访问：pystata",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "01-Use Stata in Quarto(ipynb)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html",
    "href": "Guide/Stata/25-05-02-stata-obs.html",
    "title": "02-数据的初步观测",
    "section": "",
    "text": "describe  // 描述数据集的基本信息\nsummarize  // 描述变量的基本信息\nlist  // 列出数据集的所有观测值\nbrowse  // 浏览数据集的所有观测值\ninspect  // 检查变量的分布情况\ntabulate  // 生成频数表\nhistogram  // 生成直方图\nscatter  // 生成散点图",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#数据集的基本信息",
    "href": "Guide/Stata/25-05-02-stata-obs.html#数据集的基本信息",
    "title": "01-数据的初步观测",
    "section": "",
    "text": "Stata内置的 1978 automobile data（数据集名为 auto）是一个经典的示例数据集，常用于演示统计分析、回归建模等操作。\n\n\n代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)\n\n\n\n\n代码\n%%stata\nsysuse auto, clear  \n// 加载内置数据集\n\n\n\n. sysuse auto, clear  \n(1978 automobile data)\n\n. // 加载内置数据集\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "01-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#describe",
    "href": "Guide/Stata/25-05-02-stata-obs.html#describe",
    "title": "02-数据的初步观测",
    "section": "2.2 describe",
    "text": "2.2 describe\n展示变量类型、格式、和任何的赋值/变量标签\n语法: describe [varlist]\n注意: [] 意味着[varlist]是可选的，varlist 是变量列表，可以指定一个或多个变量。如果不指定，Stata将显示数据集中所有变量的信息。\n\n\n代码\n%%stata\ndescribe\n// 描述数据集的变量信息\n\n\n\n. describe\n\nContains data from C:\\Program Files\\Stata18/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2022 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n. // 描述数据集的变量信息\n. \n\n\n\n\n代码\n%%stata\n// 简要描述数据集的变量信息\ndescribe ,short\n\n\n\n. // 简要描述数据集的变量信息\n. describe ,short\n\nContains data from C:\\Program Files\\Stata18/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2022 17:45\nSorted by: foreign\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#count",
    "href": "Guide/Stata/25-05-02-stata-obs.html#count",
    "title": "02-数据的初步观测",
    "section": "2.3 count",
    "text": "2.3 count\n计算数据集中观测值的数量\n语法: count [if] [in]\n\n\n代码\n%%stata\n// 计算数据集中的观测值数量\ncount\n// 计算数据集中观测值价格大于4000的数量\ncount if price &gt; 4000\n\n\n\n. // 计算数据集中的观测值数量\n. count\n  74\n\n. // 计算数据集中观测值价格大于4000的数量\n. count if price &gt; 4000\n  63\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#isid",
    "href": "Guide/Stata/25-05-02-stata-obs.html#isid",
    "title": "02-数据的初步观测",
    "section": "2.4 isid",
    "text": "2.4 isid\n检查数据集中是否有重复的观测值（是否唯一标识符）\n语法: isid varlist\n如果报错就说明不是，如果没有报错就说明是唯一标识符。\n\n\n代码\n%%stata\n// 检查变量mpg是否是唯一标识符\nisid mpg\n\n\n\n---------------------------------------------------------------------------\nSystemError                               Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 get_ipython().run_cell_magic('stata', '', '// 检查变量mpg是否是唯一标识符\\nisid mpg\\n')\n\nFile c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2543, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2541 with self.builtin_trap:\n   2542     args = (magic_arg_s, cell)\n-&gt; 2543     result = fn(*args, **kwargs)\n   2545 # The code below prevents the output from being displayed\n   2546 # when using magics with decorator @output_can_be_silenced\n   2547 # when the last Python token in the expression is a ';'.\n   2548 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile C:\\Program Files/Stata18\\utilities\\pystata\\ipython\\stpymagic.py:276, in PyStataMagic.stata(self, line, cell, local_ns)\n    274     _stata.run(cell, quietly=True, inline=_config.stconfig['grshow'])\n    275 else:\n--&gt; 276     _stata.run(cell, quietly=False, inline=_config.stconfig['grshow'])\n    278 if '-gw' in args or '-gh' in args:\n    279     _config.set_graph_size(gwidth, gheight)\n\nFile C:\\Program Files/Stata18\\utilities\\pystata\\stata.py:325, in run(cmd, quietly, echo, inline)\n    323         _stata_wrk2(\"qui include \" + tmpf, None, False, 1)\n    324     else:\n--&gt; 325         _stata_wrk2(\"include \" + tmpf, None, False, 1)\n    327 if inline:\n    328     if config.get_stipython()&gt;=3:\n\nFile C:\\Program Files/Stata18\\utilities\\pystata\\stata.py:116, in _stata_wrk2(cmd, real_cmd, colon, mode)\n    114         err = callback[0]\n    115         callback.clear()\n--&gt; 116         raise SystemError(err)\n    117 except KeyboardInterrupt:\n    118     outputter.done()\n\nSystemError: \n. // 检查变量mpg是否是唯一标识符\n. isid mpg\nvariable mpg does not uniquely identify the observations\nr(459);\nr(459);\n\n\n\n\n\n\n代码\n%%stata\n// 检查变量price是否是唯一标识符\nisid price\n// 检查变量mpg和price的组合是否是唯一标识符\nisid mpg price\n\n\n\n. isid price\n\n. // 检查变量price是否是唯一标识符\n. isid mpg price\n\n. // 检查变量mpg和price的组合是否是唯一标识符\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#unique",
    "href": "Guide/Stata/25-05-02-stata-obs.html#unique",
    "title": "02-数据的初步观测",
    "section": "2.5 unique",
    "text": "2.5 unique\nunique 是一个用户自定义命令，用于检查数据集中变量的唯一值，它不是自带的，需要用户进行安装。\n语法: unique varlist\n它和 isid 的区别在于，isid 只检查唯一标识符，而 unique 可以检查任意变量的唯一值。\nisid 在遇到重复值时会报错，而 unique 会返回一个包含唯一值的列表。\n\n\n代码\n%%stata\nssc install unique\n\n\nchecking unique consistency and verifying not already installed...\ninstalling into C:\\Users\\asus\\ado\\plus\\...\ninstallation complete.\n\n\n\n\n代码\n%%stata\nunique mpg\nunique weight\nunique price\nunique mpg weight\n\n\n\n. unique mpg\nNumber of unique values of mpg is  21\nNumber of records is  74\n\n. unique weight\nNumber of unique values of weight is  64\nNumber of records is  74\n\n. unique price\nNumber of unique values of price is  74\nNumber of records is  74\n\n. unique mpg weight\nNumber of unique values of mpg weight is  74\nNumber of records is  74\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#summarize",
    "href": "Guide/Stata/25-05-02-stata-obs.html#summarize",
    "title": "02-数据的初步观测",
    "section": "2.6 summarize",
    "text": "2.6 summarize\n展示变量的基本统计信息，包括均值、标准差、最小值、最大值等。\n语法: summarize [varlist]\n\n\n代码\n%%stata\n// 只查看 `auto` 数据集中的 `price` 变量\nsummarize price\n// 查看 `auto` 数据集中所有的变量信息\nsummarize\n\n\n\n. // 只查看 `auto` 数据集中的 `price` 变量\n. summarize price\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. // 查看 `auto` 数据集中所有的变量信息\n. summarize\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        make |          0\n       price |         74    6165.257    2949.496       3291      15906\n         mpg |         74     21.2973    5.785503         12         41\n       rep78 |         69    3.405797    .9899323          1          5\n    headroom |         74    2.993243    .8459948        1.5          5\n-------------+---------------------------------------------------------\n       trunk |         74    13.75676    4.277404          5         23\n      weight |         74    3019.459    777.1936       1760       4840\n      length |         74    187.9324    22.26634        142        233\n        turn |         74    39.64865    4.399354         31         51\ndisplacement |         74    197.2973    91.83722         79        425\n-------------+---------------------------------------------------------\n  gear_ratio |         74    3.014865    .4562871       2.19       3.89\n     foreign |         74    .2972973    .4601885          0          1\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#list",
    "href": "Guide/Stata/25-05-02-stata-obs.html#list",
    "title": "02-数据的初步观测",
    "section": "2.7 list",
    "text": "2.7 list\n列出数据集的所有观测值\n语法: list [varlist] [if] [in]\n如果不指定 varlist，Stata 将列出数据集中的所有变量。 如果不指定 if 和 in，Stata 将列出数据集中的所有观测值。\n如果指定了 if 和 in，Stata 将只列出满足条件的观测值。\n\n\n代码\n%%stata\n// 只查看 `auto` 数据集中的 `price` 变量，显示详细信息\nsummarize price, detail\n\n// 查看 `auto` 数据集中所有的变量信息，限制显示前10行\nlist in 1/10\n\n\n\n. // 只查看 `auto` 数据集中的 `price` 变量，显示详细信息\n. summarize price, detail\n\n                            Price\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%         3291           3291\n 5%         3748           3299\n10%         3895           3667       Obs                  74\n25%         4195           3748       Sum of wgt.          74\n\n50%       5006.5                      Mean           6165.257\n                        Largest       Std. dev.      2949.496\n75%         6342          13466\n90%        11385          13594       Variance        8699526\n95%        13466          14500       Skewness       1.653434\n99%        15906          15906       Kurtosis       4.819188\n\n. \n. // 查看 `auto` 数据集中所有的变量信息，限制显示前10行\n. list in 10\n\n     +-----------------------------------------------------------------+\n 10. | make          | price | mpg | rep78 | headroom | trunk | weight |\n     | Buick Skylark | 4,082 |  19 |     3 |      3.5 |    13 |  3,400 |\n     |-----------------------------------------------------------------|\n     |  length   |  turn   |  displa~t   |   gear_r~o   |    foreign   |\n     |     200   |    42   |       231   |       3.08   |   Domestic   |\n     +-----------------------------------------------------------------+\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#tabulate",
    "href": "Guide/Stata/25-05-02-stata-obs.html#tabulate",
    "title": "02-数据的初步观测",
    "section": "2.8 tabulate",
    "text": "2.8 tabulate\n用于生成频数表\n语法: tabulate varlist [if] [in]\n命令可以简写为 tab。\n如果不指定 if 和 in，Stata 将列出数据集中的所有观测值。 如果指定了 if 和 in，Stata 将只列出满足条件的观测值。\n\n\n代码\n%%stata\n// 对 `auto` 数据集生成频数表\ntabulate foreign\n\n// 对 `auto` 数据集生成二维交叉表\ntab foreign rep78\n\n\n\n. // 对 `auto` 数据集生成频数表\n. tabulate foreign\n\n Car origin |      Freq.     Percent        Cum.\n------------+-----------------------------------\n   Domestic |         52       70.27       70.27\n    Foreign |         22       29.73      100.00\n------------+-----------------------------------\n      Total |         74      100.00\n\n. \n. // 对 `auto` 数据集生成二维交叉表\n. tab foreign rep78\n\n           |                   Repair record 1978\nCar origin |         1          2          3          4          5 |     Total\n-----------+-------------------------------------------------------+----------\n  Domestic |         2          8         27          9          2 |        48 \n   Foreign |         0          0          3          9          9 |        21 \n-----------+-------------------------------------------------------+----------\n     Total |         2          8         30         18         11 |        69 \n\n. \n\n\n\n\n代码\n%%stata\n// 将缺失值定位某一类别, missing 可以简写为 m\ntab foreign, m\n\n// 不显示频率结果, nofreq 可以简写为 nof\ntab foreign, nofreq\n\n// 不显示标签值, nolabel 可以简写为 nol\ntab foreign, nolabel\n\n\n\n. // 将缺失值定位某一类别, missing 可以简写为 m\n. tab foreign, m\n\n Car origin |      Freq.     Percent        Cum.\n------------+-----------------------------------\n   Domestic |         52       70.27       70.27\n    Foreign |         22       29.73      100.00\n------------+-----------------------------------\n      Total |         74      100.00\n\n. \n. // 不显示频率结果, nofreq 可以简写为 nof\n. tab foreign, nofreq\n\n. \n. // 不显示标签值, nolabel 可以简写为 nol\n. tab foreign, nolabel\n\n Car origin |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          0 |         52       70.27       70.27\n          1 |         22       29.73      100.00\n------------+-----------------------------------\n      Total |         74      100.00\n\n. \n\n\n\n\n代码\n%%stata\n// 生成相对频率的条形图\ntab foreign, p\n\n// 按照频率数对 `auto` 数据集进行排序\ntabulate foreign, sort\n\n\n\n. // 生成相对频率的条形图\n. tab foreign, p\n\n Car origin |      Freq.\n------------+------------+-----------------------------------------------------\n   Domestic |         52 |****************************************************\n    Foreign |         22 |**********************\n------------+------------+-----------------------------------------------------\n      Total |         74 \n\n. \n. // 按照频率数对 `auto` 数据集进行排序\n. tabulate foreign, sort\n\n Car origin |      Freq.     Percent        Cum.\n------------+-----------------------------------\n   Domestic |         52       70.27       70.27\n    Foreign |         22       29.73      100.00\n------------+-----------------------------------\n      Total |         74      100.00\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#histogram",
    "href": "Guide/Stata/25-05-02-stata-obs.html#histogram",
    "title": "02-数据的初步观测",
    "section": "2.9 histogram",
    "text": "2.9 histogram\n生成直方图，用于展示变量的分布情况\n语法: histogram varname [if] [in]\n如果不指定 if 和 in，Stata 将列出数据集中的所有观测值。\n\n\n代码\n%%stata\n// 生成 `price` 变量的直方图，并叠加正态分布曲线\nhistogram price, normal\n\n\n\n. // 生成 `price` 变量的直方图，并叠加正态分布曲线\n. histogram price, normal\n(bin=8, start=3291, width=1576.875)\n\n. \n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\n// 生成 `price` 变量的直方图，并叠加频率分布曲线\nhistogram price, normal frequency\n\n\n\n. // 生成 `price` 变量的直方图，并叠加频率分布曲线\n. histogram price, normal frequency\n(bin=8, start=3291, width=1576.875)\n\n. \n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\n// 生成 `price` 变量的直方图，并叠加频率分布曲线，设置标题和坐标轴标签\nhistogram price, normal frequency ytitle(\"Frequency\") xtitle(\"Price\") title(\"Histogram of Price\")\n\n\n\n. // 生成 `price` 变量的直方图，并叠加频率分布曲线，设置标题和坐标轴标签\n. histogram price, normal frequency ytitle(\"Frequency\") xtitle(\"Price\") title(\"\n&gt; Histogram of Price\")\n(bin=8, start=3291, width=1576.875)\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#scatter",
    "href": "Guide/Stata/25-05-02-stata-obs.html#scatter",
    "title": "02-数据的初步观测",
    "section": "2.10 scatter",
    "text": "2.10 scatter\n生成散点图，用于展示两个变量之间的关系\n语法: scatter yvar xvar [if] [in]\n\n\n代码\n%%stata\n// 生成 `mpg` 和 `weight` 变量的散点图\nscatter mpg weight\n\n\n\n. // 生成 `mpg` 和 `weight` 变量的散点图\n. scatter mpg weight\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html",
    "href": "Guide/Stata/25-05-03-describe-index.html",
    "title": "03-统计描述指标",
    "section": "",
    "text": "代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#codebook",
    "href": "Guide/Stata/25-05-03-describe-index.html#codebook",
    "title": "03-统计描述指标",
    "section": "1.1 codebook",
    "text": "1.1 codebook\n数据字典，可以用来描述数据集的基本信息，包括变量名称、变量类型、缺失值、变量描述等。\n语法： codebook [varlist] [if] [in] [, options]\n\n[] 表示可选项，不是必须的。\nvarlist 表示变量列表，可以指定一个或多个变量。\nif 和 in 是条件语句，可以用来筛选数据。\noptions 是可选项，可以用来指定其他参数，一些可以自定义的选项。\n\n\n\n代码\n%%stata\n// 载入数据集，使用 Stata 的内置数据集 auto.dta\n// 该数据集包含汽车的各种属性，如价格、重量、马力等\nsysuse auto, clear\ncodebook price\n\n\n\n. // 载入数据集，使用 Stata 的内置数据集 auto.dta\n. // 该数据集包含汽车的各种属性，如价格、重量、马力等\n. sysuse auto, clear\n(1978 automobile data)\n\n. codebook price\n\n-------------------------------------------------------------------------------\nprice                                                                     Price\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [3291,15906]                  Units: 1\n         Unique values: 74                        Missing .: 0/74\n\n                  Mean: 6165.26\n             Std. dev.:  2949.5\n\n           Percentiles:     10%       25%       50%       75%       90%\n                           3895      4195    5006.5      6342     11385\n\n. \n\n\n\n\n代码\n%%stata\ncodebook price if price &gt; 5000\n\n\n\n-------------------------------------------------------------------------------\nprice                                                                     Price\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [5079,15906]                  Units: 1\n         Unique values: 37                        Missing .: 0/37\n\n                  Mean: 8086.95\n             Std. dev.: 3142.58\n\n           Percentiles:     10%       25%       50%       75%       90%\n                           5189      5788      6342     10371     13466\n\n\n\n\n代码\n%%stata\ncodebook price in 10/20\n\n\n\n-------------------------------------------------------------------------------\nprice                                                                     Price\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [3299,15906]                  Units: 1\n         Unique values: 11                        Missing .: 0/11\n\n                  Mean: 6917.36\n             Std. dev.: 4668.09\n\n           Percentiles:     10%       25%       50%       75%       90%\n                           3667      3955      4504     11385     14500\n\n\n\n\n代码\n%%stata\nhelp codebook\n\n\n\n[D] codebook -- Describe data contents\n                (View complete PDF manual entry)\n\n\nSyntax\n------\n\n        codebook [varlist] [if] [in] [, options]\n\n    options                  Description\n    -------------------------------------------------------------------------\n    Options\n      all                    print complete report without missing values\n      header                 print dataset name and last saved date\n      notes                  print any notes attached to variables\n      mv                     report pattern of missing values\n      tabulate(#)            set tables/summary statistics threshold; default\n                               is tabulate(9)\n      problems               report potential problems in dataset\n      detail                 display detailed report on the variables; only\n                               with problems\n      compact                display compact report on the variables\n      dots                   display a dot for each variable processed; only\n                               with compact\n\n    Languages\n      languages[(namelist)]  use with multilingual datasets; see [D] label\n                               language for details\n    -------------------------------------------------------------------------\n    collect is allowed; see prefix.\n\n\nMenu\n----\n\n    Data &gt; Describe data &gt; Describe data contents (codebook)\n\n\nDescription\n-----------\n\n    codebook examines the variable names, labels, and data to produce a\n    codebook describing the dataset.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Quick start\n\n        Remarks and examples\n\n    The above sections are not included in this help file.\n\n\nOptions\n-------\n\n        +---------+\n    ----+ Options +----------------------------------------------------------\n\n    all is equivalent to specifying the header and notes options.  It\n        provides a complete report, which excludes only performing mv.\n\n    header adds to the top of the output a header that lists the dataset\n        name, the date that the dataset was last saved, etc.\n\n    notes lists any notes attached to the variables; see [D] notes.\n\n    mv specifies that codebook search the data to determine the pattern of\n        missing values.  This is a CPU-intensive task.\n\n    tabulate(#) specifies the number of unique values of the variables to use\n        to determine whether a variable is categorical or continuous.\n        Missing values are not included in this count.  The default is 9;\n        when there are more than nine unique values, the variable is\n        classified as continuous.  Extended missing values will be included\n        in the tabulation.\n\n    problems specifies that a summary report is produced describing potential\n        problems that have been diagnosed:\n\n        - Variables that are labeled with an undefined value label\n        - Incompletely value-labeled variables\n        - Variables that are constant, including always missing\n        - Leading, trailing, and embedded spaces in string variables\n        - Embedded binary 0 (\\0) in string variables\n        - Noninteger-valued date variables\n\n        See codebook problems for a discussion of these problems and advice\n        on overcoming them.\n\n    detail may be specified only with the problems option.  It specifies that\n        the detailed report on the variables not be suppressed.\n\n    compact specifies that a compact report on the variables be displayed.\n        compact may not be specified with any options other than dots.\n\n    dots specifies that a dot be displayed for every variable processed.\n        dots may be specified only with compact.\n\n        +-----------+\n    ----+ Languages +--------------------------------------------------------\n\n    languages[(namelist)] is for use with multilingual datasets; see [D]\n        label language.  It indicates that the codebook pertains to the\n        languages in namelist or to all defined languages if no such list is\n        specified as an argument to languages().  The output of codebook\n        lists the data label and variable labels in these languages and which\n        value labels are attached to variables in these languages.\n\n        Problems are diagnosed in all of these languages, as well.  The\n        problem report does not provide details in which language problems\n        occur.  We advise you to rerun codebook for problematic variables;\n        specify detail to produce the problem report again.\n\n        If you have a multilingual dataset but do not specify languages(),\n        all output, including the problem report, is shown in the \"active\"\n        language.\n\n\nExamples\n--------\n\n    With standard (monolingual) datasets,\n\n        -----------------------------------------------------------------------\n        Setup\n            . sysuse auto\n            . note rep78: \"investigate missing values\"\n            . label values rep78 repairlbl\n\n        Display codebook for all variables in dataset\n            . codebook\n\n        Same as above command\n            . codebook _all\n\n        Same as above command, but print dataset name, date last saved,\n        dataset label, number of variables and of observations, and dataset\n        size\n            . codebook, header\n\n        Display codebook for rep78 variable\n            . codebook rep78\n\n        Display codebook for rep78 variable, including notes attached to\n        rep78\n            . codebook rep78, notes\n\n        Report potential problems with dataset\n            . codebook, problems\n\n        Display compact report for all variables in dataset\n            . codebook, compact\n\n        -----------------------------------------------------------------------\n        Setup\n            . webuse citytemp, clear\n\n        Display codebook for cooldd, heatdd, tempjan, and tempjuly, and\n        report pattern of missing values\n            . codebook cooldd heatdd tempjan tempjuly, mv\n        -----------------------------------------------------------------------\n\n\n    With multilingual datasets, with languages en and es, and with active\n    language en,\n\n        Setup\n            . webuse autom\n\n        Display codebook for foreign in language en\n            . codebook foreign\n\n        Display codebook for foreign in language es\n            . codebook foreign, language(es)\n\n        Display codebook for foreign in both en and es\n            . codebook foreign, languages\n\n\nStored results\n--------------\n\n    codebook stores the following lists of variables with potential problems\n    in r():\n\n    Macros             \n      r(cons)                 constant (or missing)\n      r(labelnotfound)        undefined value labeled\n      r(notlabeled)           value labeled but with unlabeled categories\n      r(str_type)             compressible\n      r(str_leading)          leading blanks\n      r(str_trailing)         trailing blanks\n      r(str_embedded)         embedded blanks\n      r(str_embedded0)        embedded binary 0 (\\0)\n      r(realdate)             noninteger dates",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#summarize",
    "href": "Guide/Stata/25-05-03-describe-index.html#summarize",
    "title": "03-统计描述指标",
    "section": "1.2 summarize",
    "text": "1.2 summarize\n打印数据集的基本统计描述指标，包括均值、标准差、最小值、最大值等。 语法： summarize [varlist] [if] [in] [, options] 可以使用 sum 或 summ 来代替 summarize。\n\n\n代码\n%%stata\nsum price\nsumm price\n\n\n\n. sum price\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. summ price\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. \n\n\n\n1.2.1 codebook 与 summarize 的区别\n最大的区别就是 summarize 有一个 detail 选项，可以打印出更多的统计描述指标，比如四分位数、偏度、峰度等。\n\n\n代码\n%%stata\nhelp summarize\n\n\n\n[R] summarize -- Summary statistics\n                 (View complete PDF manual entry)\n\n\nSyntax\n------\n\n        summarize [varlist] [if] [in] [weight] [, options]\n\n    options           Description\n    -------------------------------------------------------------------------\n    Main\n      detail          display additional statistics\n      meanonly        suppress the display; calculate only the mean;\n                        programmer's option\n      format          use variable's display format\n      separator(#)    draw separator line after every # variables; default is\n                        separator(5)\n      display_options control spacing, line width, and base and empty cells\n\n    -------------------------------------------------------------------------\n    varlist may contain factor variables; see fvvarlist.\n    varlist may contain time-series operators; see tsvarlist.\n    by, collect, rolling, and statsby are allowed; see prefix.\n  \n    aweights, fweights, and iweights are allowed.  However, iweights may not\n      be used with the detail option; see weight.\n\n\nMenu\n----\n\n    Statistics &gt; Summaries, tables, and tests &gt; Summary and descriptive\n        statistics &gt; Summary statistics\n\n\nDescription\n-----------\n\n    summarize calculates and displays a variety of univariate summary\n    statistics.  If no varlist is specified, summary statistics are\n    calculated for all the variables in the dataset.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Quick start\n\n        Remarks and examples\n\n        Methods and formulas\n\n    The above sections are not included in this help file.\n\n\nOptions\n-------\n\n        +------+\n    ----+ Main +-------------------------------------------------------------\n\n    detail produces additional statistics, including skewness, kurtosis, the\n        four smallest and four largest values, and various percentiles.\n\n    meanonly, which is allowed only when detail is not specified, suppresses\n        the display of results and calculation of the variance.  Ado-file\n        writers will find this useful for fast calls.\n\n    format requests that the summary statistics be displayed using the\n        display formats associated with the variables rather than the default\n        g display format; see [D] format.\n\n    separator(#) specifies how often to insert separation lines into the\n        output.  The default is separator(5), meaning that a line is drawn\n        after every five variables.  separator(10) would draw a line after\n        every 10 variables.  separator(0) suppresses the separation line.\n\n    display_options:  vsquish, noemptycells, baselevels, allbaselevels,\n        nofvlabel, fvwrap(#), and fvwrapon(style); see [R] Estimation\n        options.\n\n\nExamples\n--------\n\n    . sysuse auto\n    . summarize\n    . summarize mpg weight\n    . summarize mpg weight if foreign\n    . summarize mpg weight if foreign, detail\n    . summarize i.rep78\n\n\nVideo example\n-------------\n\n    Descriptive statistics in Stata\n\n\nStored results\n--------------\n\n    summarize stores the following in r():\n\n    Scalars   \n      r(N)           number of observations\n      r(mean)        mean\n      r(skewness)    skewness (detail only)\n      r(min)         minimum\n      r(max)         maximum\n      r(sum_w)       sum of the weights\n      r(p1)          1st percentile (detail only)\n      r(p5)          5th percentile (detail only)\n      r(p10)         10th percentile (detail only)\n      r(p25)         25th percentile (detail only)\n      r(p50)         50th percentile (detail only)\n      r(p75)         75th percentile (detail only)\n      r(p90)         90th percentile (detail only)\n      r(p95)         95th percentile (detail only)\n      r(p99)         99th percentile (detail only)\n      r(Var)         variance\n      r(kurtosis)    kurtosis (detail only)\n      r(sum)         sum of variable\n      r(sd)          standard deviation\n\n\n\n\n代码\n%%stata\nsum price, detail\n\n\n\n                            Price\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%         3291           3291\n 5%         3748           3299\n10%         3895           3667       Obs                  74\n25%         4195           3748       Sum of wgt.          74\n\n50%       5006.5                      Mean           6165.257\n                        Largest       Std. dev.      2949.496\n75%         6342          13466\n90%        11385          13594       Variance        8699526\n95%        13466          14500       Skewness       1.653434\n99%        15906          15906       Kurtosis       4.819188",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#histogram",
    "href": "Guide/Stata/25-05-03-describe-index.html#histogram",
    "title": "03-统计描述指标",
    "section": "1.3 histogram",
    "text": "1.3 histogram\n绘制直方图，可以用来查看数据的分布情况。\n语法： histogram varname [if] [in] [weight] [,[continuous_opts][discrete_opts] options]\n\nvarname 是变量名称，可以指定一个变量。\nif 和 in 是条件语句，可以用来筛选数据。\noptions 是可选项，可以用来指定其他参数，比如直方图的颜色、边框、标题等。\nbin() 选项可以用来指定直方图的分组数，比如 bin(20) 表示将数据分成 20 组。\nnormal 选项可以用来绘制正态分布曲线，可以用来查看数据是否服从正态分布。\nfreq 选项可以用来绘制频率直方图，可以用来查看数据的频率分布情况。\npercent 选项可以用来绘制百分比直方图，可以用来查看数据的百分比分布情况。\ndensity 选项可以用来绘制密度直方图，可以用来查看数据的密度分布情况。\nstart() 选项可以用来指定直方图的起始值，比如 start(0) 表示从 0 开始。\nwidth() 选项可以用来指定直方图的宽度，比如 width(1) 表示每组的宽度为 1。\ngap() 选项可以用来指定直方图的间隔，比如 gap(0) 表示没有间隔。\nbarwidth() 选项可以用来指定直方图的条形宽度，比如 barwidth(0.5) 表示条形宽度为 0.5。\nbarcolor() 选项可以用来指定直方图的条形颜色，比如 barcolor(red) 表示条形颜色为红色。\nbarlabel() 选项可以用来指定直方图的条形标签，比如 barlabel(1) 表示条形标签为 1。\nbarlabelcolor() 选项可以用来指定直方图的条形标签颜色，比如 barlabelcolor(blue) 表示条形标签颜色为蓝色。\nbarlabelsize() 选项可以用来指定直方图的条形标签大小，比如 barlabelsize(10) 表示条形标签大小为 10。\nbarlabelposition() 选项可以用来指定直方图的条形标签位置，比如 barlabelposition(inside) 表示条形标签在条形内部， barlabelposition(outside) 表示条形标签在条形外部。\n\nhistogram 可以简写为 histo，也可以简写为 hist。\n\n\n代码\n%%stata\nhist price\n\n\n(bin=8, start=3291, width=1576.875)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nhist price, freq\n\n\n(bin=8, start=3291, width=1576.875)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nhist price, percent\n\n\n(bin=8, start=3291, width=1576.875)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nhist price, frac\n\n\n(bin=8, start=3291, width=1576.875)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nhist price, freq bin(5)\n\n\n(bin=5, start=3291, width=2523)\n\n\n\n\n\n\n\n\n\n\n1.3.1 添加密度曲线\n\nnormal 选项可以用来绘制正态分布曲线，可以用来查看数据是否服从正态分布。\nnormopts(line_options) 选项可以用来指定正态分布曲线的线条属性，比如 normopts(lcolor(red)) 表示正态分布曲线的颜色为红色。\nkdensity 选项可以用来绘制核密度曲线，可以用来查看数据的密度分布情况。\nkdenopts(line_options) 选项可以用来指定核密度曲线的线条属性，比如 kdenopts(lcolor(blue)) 表示核密度曲线的颜色为蓝色。\n\n\n\n代码\n%%stata\nhist price, freq bin(5) normal\n\n\n(bin=5, start=3291, width=2523)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nhist price, by(foreign)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#graph-box",
    "href": "Guide/Stata/25-05-03-describe-index.html#graph-box",
    "title": "03-统计描述指标",
    "section": "1.4 graph box",
    "text": "1.4 graph box\n绘制箱线图，可以用来查看数据的分布情况和异常值。\n箱线图的中位数、四分位数、最大值、最小值、异常值等信息。\n箱线图的中位数是箱子的中间线，四分位数是箱子的上下边界，最大值和最小值是箱子的上下须，异常值是箱子外的点。\n箱线图的优点是可以清晰地显示数据的分布情况和异常值，缺点是不能显示数据的具体数值。\n语法： graph box yvars [if] [in] [weight] [, options]\ngraph hbox yvars [if] [in] [weight] [, options]\n他们的区别在于 graph box 是绘制垂直箱线图，graph hbox 是绘制水平箱线图。\n\n[] 表示可选项，不是必须的。\nvarlist 表示变量列表，可以指定一个或多个变量。\nif 和 in 是条件语句，可以用来筛选数据。\noptions 是可选项，可以用来指定其他参数，一些可以自定义的选项。\n\n\n\n代码\n%%stata\ngraph box price\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\ngraph hbox price\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\ngraph box price, over(foreign)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#biolin-plot",
    "href": "Guide/Stata/25-05-03-describe-index.html#biolin-plot",
    "title": "03-统计描述指标",
    "section": "1.5 biolin plot",
    "text": "1.5 biolin plot\n小提琴图，类似于箱线图，可以用来查看数据的分布情况和异常值。\n语法： biolin [varlist] [if] [in] [, options] - [] 表示可选项，不是必须的。 - varlist 表示变量列表，可以指定一个或多个变量。 - if 和 in 是条件语句，可以用来筛选数据。 - options 是可选项，可以用来指定其他参数，一些可以自定义的选项。\n\n\n代码\n%%stata\n// 不是自带的命令，需要下载安装\nssc install vioplot\n\n\n\n. // 不是自带的命令，需要下载安装\n. ssc install vioplot\nchecking vioplot consistency and verifying not already installed...\ninstalling into C:\\Users\\asus\\ado\\plus\\...\ninstallation complete.\n\n. \n\n\n\n\n代码\n%%stata\nvioplot price\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\nvioplot price, over(foreign)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-02-stata-obs.html#以下数据集的基本信息",
    "href": "Guide/Stata/25-05-02-stata-obs.html#以下数据集的基本信息",
    "title": "02-数据的初步观测",
    "section": "2.1 以下数据集的基本信息",
    "text": "2.1 以下数据集的基本信息\nStata内置的 1978 automobile data（数据集名为 auto）是一个经典的示例数据集，常用于演示统计分析、回归建模等操作。\n\n\n代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)\n\n\n\n\n代码\n%%stata\nsysuse auto, clear  \n// 加载内置数据集\n\n\n\n. sysuse auto, clear  \n(1978 automobile data)\n\n. // 加载内置数据集\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "02-数据的初步观测"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html",
    "href": "Guide/Stata/25-05-04-CI.html",
    "title": "04-置信区间（CI）",
    "section": "",
    "text": "代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html#置信区间的定义",
    "href": "Guide/Stata/25-05-04-CI.html#置信区间的定义",
    "title": "04-置信区间（CI）",
    "section": "1 置信区间的定义",
    "text": "1 置信区间的定义\n置信区间是一个范围，用于估计总体参数的可能值。它是基于样本数据计算得出的，并且在一定的置信水平下，包含了总体参数的真实值。 置信区间通常用以下形式表示：\n\\[ CI = (\\hat{\\theta} - E, \\hat{\\theta} + E) \\]\n其中，\\(\\hat{\\theta}\\) 是样本统计量的估计值，\\(E\\) 是误差范围（也称为边际误差）。\n置信区间计算的一般形式为： \\[ CI = (\\hat{\\theta} - z_{\\alpha/2} \\cdot SE, \\hat{\\theta} + z_{\\alpha/2} \\cdot SE) \\]\n置信区间的宽度取决于样本大小、样本标准差和所选的置信水平。较大的样本通常会导致更窄的置信区间，而较高的置信水平则会导致更宽的置信区间。\n置信区间的计算通常涉及以下步骤：\n\n选择一个置信水平（例如，95%或99%）。\n计算样本统计量（例如，样本均值或样本比例）。\n计算样本标准差或标准误差。\n根据所选的置信水平，查找相应的临界值（例如，Z值或t值）。\n计算置信区间的边际误差。\n构建置信区间。\n解释置信区间的含义。\n报告置信区间的结果。\n进行假设检验时，置信区间可以用来判断是否拒绝原假设。\n在进行回归分析时，置信区间可以用来评估回归系数的显著性和可靠性。\n在进行方差分析时，置信区间可以用来评估组间差异的显著性和可靠性。\n\n\n\n代码\n%%stata\n// 载入数据集，使用 Stata 的内置数据集 auto.dta\nsysuse auto, clear\n\n\n\n. // 载入数据集，使用 Stata 的内置数据集 auto.dta\n. sysuse auto, clear\n(1978 automobile data)\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html#ci-mean",
    "href": "Guide/Stata/25-05-04-CI.html#ci-mean",
    "title": "04-置信区间（CI）",
    "section": "2 ci mean",
    "text": "2 ci mean\n连续变量 mean 的标准误（SE）和置信区间（CI）\n语法：\nci mean varname [if] [in] [weight] [,options]\n或者\ncii means #obs #mean #sd [,level(#)]\n默认置信水平为95%，可以通过 options 或 level(#) 选项更改。\n\n\n代码\n%%stata\nci mean mpg price,level(95)\n\n\n\n    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n-------------+---------------------------------------------------------------\n         mpg |         74     21.2973    .6725511         19.9569    22.63769\n       price |         74    6165.257    342.8719        5481.914      6848.6\n\n\n\n\n代码\n%%stata\ncii mean 144 19599 4389,level(95)\n\n\n\n    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n-------------+---------------------------------------------------------------\n             |        166       19599    340.6525         18926.4     20271.6",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html#分类变量的置信区间",
    "href": "Guide/Stata/25-05-04-CI.html#分类变量的置信区间",
    "title": "04-置信区间（CI）",
    "section": "3 分类变量的置信区间",
    "text": "3 分类变量的置信区间\n\n3.1 继续使用 ci 命令\nci 命令可以用于计算分类变量的置信区间。对于分类变量，通常使用比例（proportion）来表示其分布情况。ci 命令可以计算样本比例的置信区间。\nci priportion varname [if] [in] [weight] [,options]\n缺点：ci prop 只能用于计算 binary 变量的置信区间，不能用于多分类变量。\n\n\n代码\n%%stata\nci prop foreign\n\n\n\n                                                            Binomial exact   \n    Variable |        Obs  Proportion    Std. err.       [95% conf. interval]\n-------------+---------------------------------------------------------------\n     foreign |         74    .2972973    .0531331         .196584    .4148353\n\n\n\n\n代码\n%%stata\nci prop rep78\n\n\nno binary (0/1) variables found; nothing to compute\n\n\n\n\n3.2 proportion\nproportion 命令可以用于计算分类变量的置信区间。它可以处理多分类变量，并且可以计算每个类别的比例和置信区间。\n语法形式：\nproportion varname [if] [in] [weight] [,options]\n默认置信水平为95%，可以通过 options 选项更改。\n\n\n代码\n%%stata\nprop foreign\n\n\n\nProportion estimation                       Number of obs = 74\n\n--------------------------------------------------------------\n             |                                   Logit\n             | Proportion   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n     foreign |\n   Domestic  |   .7027027   .0531331      .5874215     .796909\n    Foreign  |   .2972973   .0531331       .203091    .4125785\n--------------------------------------------------------------\n\n\n\n\n代码\n%%stata\nprop rep78\n\n\n\nProportion estimation                       Number of obs = 69\n\n--------------------------------------------------------------\n             |                                   Logit\n             | Proportion   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n       rep78 |\n          1  |   .0289855   .0201966      .0070794    .1110924\n          2  |    .115942   .0385422       .058317    .2173648\n          3  |   .4347826   .0596787      .3214848    .5553295\n          4  |   .2608696   .0528625      .1695907    .3788629\n          5  |   .1594203   .0440694      .0895793     .267702\n--------------------------------------------------------------\n\n\n\n\n代码\n%%stata\nprop foreign rep78\n\n\n\nProportion estimation                       Number of obs = 69\n\n--------------------------------------------------------------\n             |                                   Logit\n             | Proportion   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n     foreign |\n   Domestic  |   .6956522   .0553932      .5755656     .793927\n    Foreign  |   .3043478   .0553932       .206073    .4244344\n             |\n       rep78 |\n          1  |   .0289855   .0201966      .0070794    .1110924\n          2  |    .115942   .0385422       .058317    .2173648\n          3  |   .4347826   .0596787      .3214848    .5553295\n          4  |   .2608696   .0528625      .1695907    .3788629\n          5  |   .1594203   .0440694      .0895793     .267702\n--------------------------------------------------------------\n\n\nprop 多个变量时，Stata 会默认去除缺失值。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html#pwcorr",
    "href": "Guide/Stata/25-05-04-CI.html#pwcorr",
    "title": "04-置信区间（CI）",
    "section": "4 pwcorr",
    "text": "4 pwcorr\npwcorr 命令用于计算变量之间的成对相关系数。它可以处理连续变量和分类变量，并且可以计算每对变量之间的相关系数和置信区间。\npwcorr [varlist] [if] [in] [weight] [,options]\n\n\n代码\n%%stata\npwcorr price headroom mpg displacement\n\n\n\n             |    price headroom      mpg displa~t\n-------------+------------------------------------\n       price |   1.0000 \n    headroom |   0.1145   1.0000 \n         mpg |  -0.4686  -0.4138   1.0000 \ndisplacement |   0.4949   0.4745  -0.7056   1.0000 \n\n\n\n\n代码\n%%stata\n// 展示P值版，并且用星号标记显著性水平为0.05的相关系数\npwcorr price headroom mpg displacement, sig star(0.05)\n\n\n\n. // 展示P值版\n. pwcorr price headroom mpg displacement, sig star(0.05)\n\n             |    price headroom      mpg displa~t\n-------------+------------------------------------\n       price |   1.0000 \n             |\n             |\n    headroom |   0.1145   1.0000 \n             |   0.3313\n             |\n         mpg |  -0.4686* -0.4138*  1.0000 \n             |   0.0000   0.0002\n             |\ndisplacement |   0.4949*  0.4745* -0.7056*  1.0000 \n             |   0.0000   0.0000   0.0000\n             |\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-04-CI.html#graph-matrix",
    "href": "Guide/Stata/25-05-04-CI.html#graph-matrix",
    "title": "04-置信区间（CI）",
    "section": "5 graph matrix",
    "text": "5 graph matrix\ngraph matrix 命令用于绘制变量之间的散点图矩阵。它可以处理连续变量和分类变量，并且可以计算每对变量之间的相关系数和置信区间。\ngraph matrix [varlist] [if] [in] [weight] [,options]\n\n\n代码\n%%stata\ngraph matrix price headroom mpg displacement\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\ngraph matrix price headroom mpg displacement, half",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "04-置信区间（CI）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-ttest.html",
    "href": "Guide/Stata/25-05-05-ttest.html",
    "title": "07-t检验（t-test）",
    "section": "",
    "text": "代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "07-t检验（t-test）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-ttest.html#数据导入",
    "href": "Guide/Stata/25-05-05-ttest.html#数据导入",
    "title": "07-t检验（t-test）",
    "section": "0.1 数据导入",
    "text": "0.1 数据导入\n\n\n代码\n%%stata\nwebuse auto.dta, clear\n\n\n(1978 automobile data)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "07-t检验（t-test）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-ttest.html#独立样本-t-检验-two-samples-t-test",
    "href": "Guide/Stata/25-05-05-ttest.html#独立样本-t-检验-two-samples-t-test",
    "title": "07-t检验（t-test）",
    "section": "1.1 独立样本 t 检验 (two-samples t-test)",
    "text": "1.1 独立样本 t 检验 (two-samples t-test)\n语法：\nttest varname, by(groupvar) [if] [in] [,level(#)]\n\n\n代码\n%%stata\nttest price, by(foreign)\n\n\n\nTwo-sample t test with equal variances\n------------------------------------------------------------------------------\n   Group |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\nDomestic |      52    6072.423    429.4911    3097.104    5210.184    6934.662\n Foreign |      22    6384.682    558.9942    2621.915     5222.19    7547.174\n---------+--------------------------------------------------------------------\nCombined |      74    6165.257    342.8719    2949.496    5481.914      6848.6\n---------+--------------------------------------------------------------------\n    diff |           -312.2587    754.4488               -1816.225    1191.708\n------------------------------------------------------------------------------\n    diff = mean(Domestic) - mean(Foreign)                         t =  -0.4139\nH0: diff = 0                                     Degrees of freedom =       72\n\n    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0\n Pr(T &lt; t) = 0.3401         Pr(|T| &gt; |t|) = 0.6802          Pr(T &gt; t) = 0.6599",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "07-t检验（t-test）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-ttest.html#两个变量的比较",
    "href": "Guide/Stata/25-05-05-ttest.html#两个变量的比较",
    "title": "07-t检验（t-test）",
    "section": "1.2 两个变量的比较",
    "text": "1.2 两个变量的比较\n\n1.2.1 非配对样本\nttest var1 == var2 [if] [in] ,unpaired [level(#)]\n\n\n代码\n%%stata\nwebuse fuel.dta, clear\nttest mpg1 == mpg2, unpaired\n\n\n\n. webuse fuel.dta, clear\n\n. ttest mpg1 == mpg2, unpaired\n\nTwo-sample t test with equal variances\n------------------------------------------------------------------------------\nVariable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\n    mpg1 |      12          21    .7881701    2.730301    19.26525    22.73475\n    mpg2 |      12       22.75    .9384465    3.250874    20.68449    24.81551\n---------+--------------------------------------------------------------------\nCombined |      24      21.875    .6264476    3.068954    20.57909    23.17091\n---------+--------------------------------------------------------------------\n    diff |               -1.75    1.225518               -4.291568    .7915684\n------------------------------------------------------------------------------\n    diff = mean(mpg1) - mean(mpg2)                                t =  -1.4280\nH0: diff = 0                                     Degrees of freedom =       22\n\n    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0\n Pr(T &lt; t) = 0.0837         Pr(|T| &gt; |t|) = 0.1673          Pr(T &gt; t) = 0.9163\n\n. \n\n\n\n\n1.2.2 配对样本 t 检验 (paired t-test)\n语法：\nttest var1 == var2 [if] [in] ,[level(#)]\n\n\n代码\n%%stata\nwebuse fuel.dta, clear\nttest mpg1 == mpg2\n\n\n\n. webuse fuel.dta, clear\n\n. ttest mpg1 == mpg2\n\nPaired t test\n------------------------------------------------------------------------------\nVariable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\n    mpg1 |      12          21    .7881701    2.730301    19.26525    22.73475\n    mpg2 |      12       22.75    .9384465    3.250874    20.68449    24.81551\n---------+--------------------------------------------------------------------\n    diff |      12       -1.75    .7797144     2.70101    -3.46614   -.0338602\n------------------------------------------------------------------------------\n     mean(diff) = mean(mpg1 - mpg2)                               t =  -2.2444\n H0: mean(diff) = 0                              Degrees of freedom =       11\n\n Ha: mean(diff) &lt; 0           Ha: mean(diff) != 0           Ha: mean(diff) &gt; 0\n Pr(T &lt; t) = 0.0232         Pr(|T| &gt; |t|) = 0.0463          Pr(T &gt; t) = 0.9768\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "07-t检验（t-test）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html",
    "href": "Guide/Stata/25-05-05-log-do.html",
    "title": "06-代码的可复现性与文件管理",
    "section": "",
    "text": "代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)\n代码\n%%stata\n// 载入数据集，使用 Stata 的内置数据集 auto.dta\nsysuse auto, clear\n\n\n\n. // 载入数据集，使用 Stata 的内置数据集 auto.dta\n. sysuse auto, clear\n(1978 automobile data)\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#代码规范",
    "href": "Guide/Stata/25-05-05-log-do.html#代码规范",
    "title": "06-代码的可复现性与文件管理",
    "section": "2.1 代码规范",
    "text": "2.1 代码规范\n/* README: \n   1. 创建时间：2025-05-05\n   2. 上次修改时间：2025-05-08\n   3. 本文档的目的：用于探索数据&绘图&制表&建模\n   4. 本文档的依赖项：\n      - Stata &gt; 17.0\n      - Data: data.dta or other.files\n      - packages: estout, outreg2, etc.\n   5. 输入数据：output.dta or other.files\n   6. 输出结果：results.txt\n   7. 如有问题，请联系作者或查看文档。\nNotes: \n   1. 本文档的内容仅供参考，作者不对其准确性和完整性负责。\n   2. 本文档的内容可能会随时更新，作者不保证其及时性和有效性。\n   3. 本文档的内容可能会受到版权保护，未经授权不得转载或引用。\n   4. 本文档的内容仅代表作者个人观点，不代表任何机构或组织的观点。\n   5. 其他信息：\n      - 作者：Simon Zhou\n      - 邮箱：\n*/\n\n* 代码如下：\n\n// 载入数据集，使用 Stata 的内置数据集 auto.dta\nsysuse auto, clear\n\n* 1. 数据探索\ndescribe\nsummarize\n\n* 2. 绘制散点图\ntwoway scatter mpg weight",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#append-和-replace-的区别",
    "href": "Guide/Stata/25-05-05-log-do.html#append-和-replace-的区别",
    "title": "06-代码的可复现性与文件管理",
    "section": "3.1 append 和 replace 的区别",
    "text": "3.1 append 和 replace 的区别\n\nappend：将新的输出追加到已有的 log file 中。\nreplace：替换已有的 log file，创建一个新的 log file。\n如果文件不存在，append 和 replace 的效果是一样的，都会创建一个新的 log file。\n如果文件已经存在，而用户没有指定 append 或 replace，Stata 会报错来询问用户如何选择。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#text-和-smcl-的区别",
    "href": "Guide/Stata/25-05-05-log-do.html#text-和-smcl-的区别",
    "title": "06-代码的可复现性与文件管理",
    "section": "3.2 text 和 smcl 的区别",
    "text": "3.2 text 和 smcl 的区别\n\ntext：创建一个纯文本格式的 log file，适合于在文本编辑器中查看和编辑，体积较小，只有单色。\nsmcl：创建一个 Stata 默认格式的 log file，适合于在 Stata 中查看和编辑，可以保存各种颜色。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#namelogname-的作用",
    "href": "Guide/Stata/25-05-05-log-do.html#namelogname-的作用",
    "title": "06-代码的可复现性与文件管理",
    "section": "3.3 name(logname) 的作用",
    "text": "3.3 name(logname) 的作用\n\nname(logname)：给 log file 起一个名字，方便后续的引用和管理。\n如果不指定，Stata 会自动生成一个默认的名字，通常是以日期和时间为基础的字符串。\n如果用户给 log file 起了名字（不是 filename），那么用户就可以同时打开好几个 log file。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#结束-log-file",
    "href": "Guide/Stata/25-05-05-log-do.html#结束-log-file",
    "title": "06-代码的可复现性与文件管理",
    "section": "3.4 结束 log file",
    "text": "3.4 结束 log file\n\n如果没有起名字，可以世界使用 log close 命令来结束 log file。\n如果用户没有结束 log file，Stata 会在退出时自动结束 log file。\n如果用户在 Stata 中打开了多个 log file，用户可以使用 log close logfile-name 命令来结束指定的 log file，或者使用 log close _all 命令来结束所有的 log file。\n\n\n\n代码\n%%stata\nlog close // 关闭现有的日志文件 \nlog using test,append\nlog close _all // 关闭所有日志文件\n\n\n\n. log close // 关闭现有的日志文件 \n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\asus\\Desktop\\test\\Stata-test\\Statatest.smcl\n  log type:  smcl\n closed on:   5 May 2025, 12:18:32\n-------------------------------------------------------------------------------\n\n. log using test,append\n(file C:\\Users\\asus\\Desktop\\test\\Stata-test\\test.smcl not found)\n-------------------------------------------------------------------------------\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\asus\\Desktop\\test\\Stata-test\\test.smcl\n  log type:  smcl\n opened on:   5 May 2025, 12:18:32\n\n. log close _all // 关闭所有日志文件\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\asus\\Desktop\\test\\Stata-test\\test.smcl\n  log type:  smcl\n closed on:   5 May 2025, 12:18:32\n-------------------------------------------------------------------------------\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-log-do.html#do-file-的基本语法",
    "href": "Guide/Stata/25-05-05-log-do.html#do-file-的基本语法",
    "title": "06-代码的可复现性与文件管理",
    "section": "4.1 Do file 的基本语法",
    "text": "4.1 Do file 的基本语法\n\n4.1.1 注释\n\n使用 * 或 // 来添加单行注释，* 作为一行开头时，这一行都是注释。\n\n在 Command 后添加注释使用 // 较好，并在 // 添加一些文字注释，以提醒或解释本行代码用途。\n\n使用 /* ... */ 来添加多行注释。\n注释可以放在代码的前面或后面，也可以单独成行。\n\n\n\n代码\n%%stata\nclear all // 清除所有变量和数据集\n\npwd // 显示当前工作目录\ncd \"C:\\Users\\asus\\Desktop\\test\\Stata-test\" // 更改工作目录到指定路径\n// 注意：请将 \"YourUsername\" 替换为你的实际用户名\n\n\n\n. clear all // 清除所有变量和数据集\n\n. \n. pwd // 显示当前工作目录\nC:\\Users\\asus\\Desktop\\test\\Stata-test\n\n. cd \"C:\\Users\\asus\\Desktop\\test\\Stata-test\" // 更改工作目录到指定路径\nC:\\Users\\asus\\Desktop\\test\\Stata-test\n\n. // 注意：请将 \"YourUsername\" 替换为你的实际用户名\n. \n\n\n\n\n代码\n%%stata\n*log close // 关闭现有的日志文件\nlog using Statatest, replace \n// 创建一个新的日志文件，替换现有的文件\n\n\n\n. *log close // 关闭现有的日志文件\n. log using Statatest, replace \n-------------------------------------------------------------------------------\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\asus\\Desktop\\test\\Stata-test\\Statatest.smcl\n  log type:  smcl\n opened on:   5 May 2025, 12:18:53\n\n. // 创建一个新的日志文件，替换现有的文件\n. \n\n\n\n\n代码\n%%stata\n// 载入数据集，使用 Stata 的内置数据集 auto.dta\nsysuse auto.dta, clear \n\n** 数据分析的探索性描述\nsum price\ncodebook mpg\n\n\n\n. // 载入数据集，使用 Stata 的内置数据集 auto.dta\n. sysuse auto.dta, clear \n(1978 automobile data)\n\n. \n. ** 数据分析的探索性描述\n. sum price\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       price |         74    6165.257    2949.496       3291      15906\n\n. codebook mpg\n\n-------------------------------------------------------------------------------\nmpg                                                               Mileage (mpg)\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [12,41]                       Units: 1\n         Unique values: 21                        Missing .: 0/74\n\n                  Mean: 21.2973\n             Std. dev.:  5.7855\n\n           Percentiles:     10%       25%       50%       75%       90%\n                             14        18        20        25        29\n\n. \n\n\n\n\n代码\n%%stata\nci mean rep78\ncorr weight length\n\n\n\n. ci mean rep78\n\n    Variable |        Obs        Mean    Std. err.       [95% conf. interval]\n-------------+---------------------------------------------------------------\n       rep78 |         69    3.405797    .1191738        3.167989    3.643605\n\n. corr weight length\n(obs=74)\n\n             |   weight   length\n-------------+------------------\n      weight |   1.0000\n      length |   0.9460   1.0000\n\n\n. \n\n\n\n\n4.1.2 copy the “Command”\n如果用户使用交互页面来选择相关的操作，Stata 会在命令窗口中显示用户操作所对应的命令。\n用户可以直接复制这些命令到 Do file 中，方便后续的修改和复现。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "06-代码的可复现性与文件管理"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html",
    "href": "Guide/Stata/25-05-05-scatter-plot.html",
    "title": "05-双变量作图",
    "section": "",
    "text": "代码\nimport stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#stata的scheme主题",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#stata的scheme主题",
    "title": "05-双变量作图",
    "section": "1.1 Stata的scheme（主题）",
    "text": "1.1 Stata的scheme（主题）\nStata提供了多种 scheme （style）来美化图形。可以使用set scheme命令来设置主题。以下是一些常用的主题：\n\ns1color：适用于需要强调数据点的情况，具有鲜艳的颜色。\ns2color：默认主题，适用于需要强调数据点的情况。\ns1mono：单色主题，适用于打印或黑白显示。\ns2mono：单色主题，适用于强调数据点的情况。\neconomist：适用于经济学和社会科学领域的主题。\njournal：适用于学术期刊的主题，具有简洁和专业的外观。\ns1manual：手动主题，适用于需要自定义颜色和样式的情况。\ns2manual：手动主题，适用于强调数据点的情况。\ns2color8：适用于需要强调数据点的情况，具有8种颜色的主题。\nplotplain：适用于需要强调数据点的情况，具有简单和清晰的外观。\n\n更好的主题？\nsimono：适用于需要强调数据点的情况，具有简洁和专业的外观。\n\n1.1.1 scheme 的设置\nset scheme s1mono : 当前会话中设置主题为s1mono。\nset scheme s1mono, perm : 永久设置主题为s1mono, perm 为 permanent 的缩写。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#scatter命令",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#scatter命令",
    "title": "05-双变量作图",
    "section": "1.2 scatter命令",
    "text": "1.2 scatter命令\nscatter命令用于绘制散点图。基本语法如下：\n[twoway] scatter yvar xvar [if] [in] [weight] [, options]\n最基础的形式：\ntwoway scatter yvar xvar\n进阶形式：\ntwoway scatter y1 y2 y3 x1 x2 x3 [if] [in] [weight], options\n\n\n代码\n%%stata\ntwoway scatter mpg weight\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\ntwoway scatter weight length price\n\n\n\n\n\n\n\n\n\n\n1.2.1 改变散点图的样式\ntwoway scatter y x,msymbol(oh) mcolor(red) msize(medium) scheme(s1mono)\n\nmsymbol:改变形状(help symbolstyle)\nmcolor:改变颜色(help colorstyle)\nmsize:改变大小(help markerstyle)\n\n\n\n代码\n%%stata\nhelp symbolstyle\n\n\n\n[G-4] symbolstyle -- Choices for the shape of markers\n                     (View complete PDF manual entry)\n\n\nSyntax\n------\n\n                        Synonym\n        symbolstyle     (if any)     Description\n        -------------------------------------------------------\n        circle             O         solid\n        diamond            D         solid\n        triangle           T         solid\n        square             S         solid\n        plus               +\n        X                  X\n        arrowf             A         filled arrow head\n        arrow              a\n        pipe               |\n        V                  V\n\n        smcircle           o         solid\n        smdiamond          d         solid\n        smsquare           s         solid\n        smtriangle         t         solid\n        smplus\n        smx                x\n        smv                v\n\n        circle_hollow      Oh        hollow\n        diamond_hollow     Dh        hollow\n        triangle_hollow    Th        hollow\n        square_hollow      Sh        hollow\n\n        smcircle_hollow    oh        hollow\n        smdiamond_hollow   dh        hollow\n        smtriangle_hollow  th        hollow\n        smsquare_hollow    sh        hollow\n\n        point              p         a small dot\n        none               i         a symbol that is invisible\n        -------------------------------------------------------\n\n        For a symbol palette displaying each of the above symbols, type\n\n            . palette symbolpalette [, scheme(schemename)]\n\n        Other symbolstyles may be available; type\n\n            . graph query symbolstyle\n\n        to obtain the complete list of symbolstyles installed on your\n        computer.\n\n\nDescription\n-----------\n\n    Markers are the ink used to mark where points are on a plot; see [G-3]\n    marker_options.  symbolstyle specifies the shape of the marker.\n\n    You specify the symbolstyle inside the msymbol() option allowed with many\n    of the graph commands:\n\n        . graph twoway ..., msymbol(symbolstyle) ...\n\n    Sometimes you will see that a symbolstylelist is allowed:\n\n        . scatter ..., msymbol(symbolstylelist) ...\n\n    A symbolstylelist is a sequence of symbolstyles separated by spaces.\n    Shorthands are allowed to make specifying the list easier; see [G-4]\n    stylelists.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Remarks and examples\n\n    The above sections are not included in this help file.\n\n\nRemarks\n-------\n\n    Remarks are presented under the following headings:\n\n        Typical use\n        Filled and hollow symbols\n        Size of symbols\n\n\nTypical use\n-----------\n\n    msymbol(symbolstyle) is one of the more commonly specified options.  For\n    instance, you may not be satisfied with the default rendition of\n\n        . scatter mpg weight if foreign ||\n          scatter mpg weight if !foreign\n\n    and prefer\n\n        . scatter mpg weight if foreign, msymbol(oh) ||\n          scatter mpg weight if !foreign, msymbol(x)\n\n    When you are graphing multiple y variables in the same plot, you can\n    specify a list of symbolstyles inside the msymbol() option:\n\n        . scatter mpg1 mpg2 weight, msymbol(oh x)\n\n    The result is the same as typing\n\n        . scatter mpg1 weight, msymbol(oh) ||\n          scatter mpg2 weight, msymbol(x)\n\n    Also, in the above, we specified the symbol-style synonyms.  Whether you\n    type\n\n        . scatter mpg1 weight, msymbol(oh) ||\n          scatter mpg2 weight, msymbol(x)\n\n    or\n\n        . scatter mpg1 weight, msymbol(smcircle_hollow) ||\n          scatter mpg2 weight, msymbol(smx)\n\n    makes no difference.\n\n\nFilled and hollow symbols\n-------------------------\n\n    The symbolstyle specifies the shape of the symbol, and in that sense, one\n    of the styles circle and hcircle -- and diamond and hdiamond, etc. -- is\n    unnecessary in that each is a different rendition of the same shape.  The\n    option mfcolor(colorstyle) (see [G-3] marker_options) specifies how the\n    inside of the symbol is to be filled.  hcircle(), hdiamond, etc., are\n    included for convenience and are equivalent to specifying\n\n        msymbol(Oh): msymbol(O) mfcolor(none)\n\n        msymbol(dh): msymbol(d) mfcolor(none)\n\n        etc.\n\n    Using mfcolor() to fill the inside of a symbol with different colors\n    sometimes creates what are effectively new symbols.  For instance, if you\n    take msymbol(O) and fill its interior with a lighter shade of the same\n    color used to outline the shape, you obtain a pleasing result.  For\n    instance, you might try\n\n        msymbol(O) mlcolor(yellow) mfcolor(.5*yellow)\n\n    or\n\n        msymbol(O) mlcolor(gs5) mfcolor(gs12)\n\n    as in\n\n        . scatter mpg weight, msymbol(O) mlcolor(gs5) mfcolor(gs14)\n          (click to run)\n\n\nSize of symbols\n---------------\n\n    Just as msymbol(O) and msymbol(Oh) differ only in mfcolor(), msymbol(O)\n    and msymbol(o) -- symbols circle and smcircle -- differ only in msize().\n    In particular,\n\n        msymbol(O): msymbol(O) msize(medium)\n\n        msymbol(o): msymbol(O) msize(small)\n\n    and the same is true for all the other large and small symbol pairs.\n\n    msize() is interpreted as being relative to the size of the graph region\n    (see [G-3] region_options), so the same symbol size will in fact be a\n    little different in\n\n        . scatter mpg weight\n\n    and\n\n        . scatter mpg weight, by(foreign total)\n\n\n\n\n代码\n%%stata\nhelp colorstyle\n\n\n\n[G-4] colorstyle -- Choices for color\n                    (View complete PDF manual entry)\n\n\nSyntax\n------\n\n    Set color of &lt;object&gt; to colorstyle\n\n        &lt;object&gt;color(colorstyle)\n\n\n    Set color of all affected objects to colorstyle\n\n        color(colorstyle)\n\n\n    Set opacity of &lt;object&gt; to #, where # is a percentage of 100% opacity\n\n        &lt;object&gt;color(%#)\n\n\n    Set opacity for all affected objects colors to #\n\n        color(%#)\n\n\n    Set both color and opacity of &lt;object&gt;\n\n        &lt;object&gt;color(colorstyle%#)\n\n\n    Set both color and opacity of all affected objects\n\n        &lt;object&gt;color(colorstyle%#)\n\n\n    colorstyle            Description\n    -------------------------------------------------------------------------\n    black                 \n\n    stc1                  color used by scheme stcolor\n    stc2                  color used by scheme stcolor\n    .                     \n    .                     \n    stc15                 color used by scheme stcolor\n    stblue                blue used by scheme stcolor\n    stgreen               green used by scheme stcolor\n    stred                 red used by scheme stcolor\n    styellow              yellow used by scheme stcolor\n\n    gs0                   gray scale: 0 = black\n    gs1                   gray scale: very dark gray\n    gs2                   \n    .                     \n    .                     \n    gs15                  gray scale: very light gray\n    gs16                  gray scale: 16 = white\n\n    white                 \n\n    blue                  \n    bluishgray            \n    brown                 \n    cranberry             \n    cyan                  \n    dimgray               between gs14 and gs15\n    dkgreen               dark green\n    dknavy                dark navy blue\n    dkorange              dark orange\n    eggshell              \n    emerald               \n    forest_green          \n    gold                  \n    gray                  equivalent to gs8\n    green                 \n    khaki                 \n    lavender              \n    lime                  \n    ltblue                light blue\n    ltbluishgray          light blue-gray, used by scheme s2color\n    ltkhaki               light khaki\n    magenta               \n    maroon                \n    midblue               \n    midgreen              \n    mint                  \n    navy                  \n    olive                 \n    olive_teal            \n    orange                \n    orange_red            \n    pink                  \n    purple                \n    red                   \n    sand                  \n    sandb                 bright sand\n    sienna                \n    stone                 \n    teal                  \n    yellow                \n\n                          colors used by The Economist magazine:\n    ebg                           background color\n    ebblue                        bright blue\n    edkblue                       dark blue\n    eltblue                       light blue\n    eltgreen                      light green\n    emidblue                      midblue\n    erose                         rose\n\n    none                  no color; invisible; draws nothing\n    background or bg      same color as background\n    foreground or fg      same color as foreground\n\n    \"# # #\"               RGB value; white = \"255 255 255\"\n\n    \"# # # #\"             CMYK value; yellow = \"0 0 255 0\"\n\n    \"hsv # # #\"           HSV value; white = \"hsv 0 0 1\"\n\n    colorstyle*#          color with adjusted intensity; #'s range from 0 to\n                            255\n\n    colorstyle%#          color with adjusted opacity; #s range from 0 to 100\n\n    *#                    default color with adjusted intensity\n    %#                    default color with adjusted opacity\n    -------------------------------------------------------------------------\n    When specifying RGB, CMYK, or HSV values, it is best to enclose the\n      values in quotes; type \"128 128 128\" not 128 128 128.\n\n\nDescription\n-----------\n\n    colorstyle sets the color and opacity of graph components such as lines,\n    backgrounds, and bars.  Some options allow a sequence of colorstyles with\n    colorstylelist; see [G-4] stylelists.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Remarks and examples\n\n    The above sections are not included in this help file.\n\n\nRemarks\n-------\n\n    colorstyle sets the color and opacity of graph components such as lines,\n    backgrounds, and bars.  Colors can be specified with a named color, such\n    as black, olive, and yellow, or with a color value in the RGB, CMYK, or\n    HSV format.  colorstyle can also set a component to match the background\n    color or foreground color.  Additionally, colorstyle can modify color\n    intensity, making the color lighter or darker.  Some options allow a\n    sequence of colorstyles with colorstylelist; see [G-4] stylelists.\n\n    To see a list of named colors, use graph query colorstyle.  See [G-2]\n    graph query.  For a color palette showing an individual color or\n    comparing two colors, use palette color.  See [G-2] palette.\n\n    Remarks are presented under the following headings:\n\n        Adjust opacity\n        Adjust intensity\n        Specify RGB values\n        Specify CMYK values\n        Specify HSV values\n        Export custom colors\n\n\nAdjust opacity\n--------------\n\n    Opacity is the percentage of a color that covers the background color.\n    That is, 100% means that the color fully hides the background, and 0%\n    means that the color has no coverage and is fully transparent.  If you\n    prefer to think about transparency, opacity is the inverse of\n    transparency.  Adjust opacity with the % modifier.  For example, type\n\n        green%50\n        \"0 255 0%50\"\n        %30\n\n    Omitting the color specification in the command adjusts the opacity of\n    the object while retaining the default color.  For instance, specify\n    mcolor(%30) with graph twoway scatter to use the default fill color at\n    30% opacity.\n\n    Specifying color%0 makes the object completely transparent and is\n    equivalent to color none.\n\n\nAdjust intensity\n----------------\n\n    Color intensity (brightness) can be modified by specifying a color, *,\n    and a multiplier value.  For example, type\n\n        green*.8\n        purple*1.5\n        \"0 255 255*1.2\"\n        \"hsv 240 1 1*.5\"\n\n    A value of 1 leaves the color unchanged, a value greater than 1 makes the\n    color darker, and a value less than 1 makes the color lighter.  Note that\n    there is no space between color and *, even when color is a numerical\n    value for RGB or CMYK.\n\n    Omitting the color specification in the command adjusts the intensity of\n    the object's default color.  For instance, specify bcolor(*.7) with graph\n    twoway bar to use the default fill color at reduced brightness, or\n    specify bcolor(*2) to increase the brightness of the default color.\n\n    Specifying color*0 makes the color as light as possible, but it is not\n    equivalent to color none.  color*255 makes the color as dark as possible,\n    although values much smaller than 255 usually achieve the same result.\n\n    For an example using the intensity adjustment, see Typical use in [G-2]\n    graph twoway kdensity.\n\n\nRGB values\n----------\n\n    In addition to specifying named colors such as yellow, you can specify\n    colors with RGB values.  An RGB value is a triplet of numbers ranging\n    from 0 to 255 that describes the level of red, green, and blue light that\n    must be emitted to produce a given color.  RGB is used to define colors\n    for on-screen display and in nonprofessional printing.  Examples of RGB\n    values are\n\n        red     =   255    0    0\n        green   =     0  255    0\n        blue    =     0    0  255\n        white   =   255  255  255\n        black   =     0    0    0\n        gray    =   128  128  128\n        navy    =    26   71  111\n\n\nCMYK values\n-----------\n\n    You can specify colors using CMYK values.  You will probably only use\n    CMYK values when they are provided by a journal or publisher.  You can\n    specify CMYK values either as integers from 0 to 255 or as proportions of\n    ink using real numbers from 0.0 to 1.0.  If all four values are 1 or\n    less, the numbers are taken to be proportions of ink.  For example,\n\n        red     =     0  255  255    0   or, equivalently,     0     1  1     0\n        green   =   255    0  255    0   or, equivalently,     1     0  1     0\n        blue    =   255  255    0    0   or, equivalently,     1     1  0     0\n        white   =     0    0    0    0   or, equivalently,     0     0  0     0\n        black   =     0    0    0  255   or, equivalently,     0     0  0     1\n        gray    =     0    0    0  128   or, equivalently,     0     0  0    .5\n        navy    =    85   40    0  144   or, equivalently,  .334  .157  0  .565\n\n\nHSV values\n----------\n\n    You can specify colors with HSV (hue, saturation, and value), also called\n    HSL (hue, saturation, and luminance) and HSB (hue, saturation, and\n    brightness).  HSV is often used in image editing software.  An HSV value\n    is a triplet of numbers.  So that Stata can differentiate them from RGB\n    values, HSV colors must be prefaced with hsv.  The first number specifies\n    the hue from 0 to 360, the second number specifies the saturation from 0\n    to 1, and the third number specifies the value (luminance or brightness)\n    from 0 to 1.  For example,\n\n        red     =   hsv   0     1     1\n        green   =   hsv 120     1  .502\n        blue    =   hsv 240     1     1\n        white   =   hsv   0     0     1\n        black   =   hsv   0     0     0\n        navy    =   hsv 209  .766  .435\n\n\nExport custom colors\n--------------------\n\n    graph export stores all colors as RGB+opacity values, that is, RGB values\n    0-255 and opacity values 0-1.  If you need color values from Stata in\n    CMYK format, use the graph export command with the cmyk(on) option, and\n    save the graph in one of the following formats: PostScript, Encapsulated\n    PostScript, or PDF.\n\n    You can set Stata to permanently use CMYK colors for PostScript export\n    files by typing translator set Graph2ps cmyk on and for EPS export files\n    by typing translator set Graph2eps cmyk on.\n\n    The CMYK values returned in graph export may differ from the CMYK values\n    that you entered.  This is because Stata normalizes CMYK values by\n    reducing all CMY values until one value is 0.  The difference is added to\n    the K (black) value.  For example, Stata normalizes the CMYK value 10 10\n    5 0 to 5 5 0 5.  Stata subtracts 5 from the CMY values so that Y is 0 and\n    then adds 5 to K.\n\n\nVideo example\n-------------\n\n        Transparency in Stata graphs\n\n\n\n\n代码\n%%stata\ntwoway scatter mpg weight,msymbol(D) mcolor(red) msize(medium)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#change-the-scheme-of-the-scatter-plot",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#change-the-scheme-of-the-scatter-plot",
    "title": "04-双变量作图",
    "section": "1.3 change the scheme of the scatter plot",
    "text": "1.3 change the scheme of the scatter plot\ntwoway scatter y x,msymbol(oh) mcolor(red) msize(medium) scheme(s1mono)\n\nmsymbol:改变形状(help symbolstyle)\nmcolor:改变颜色(help colorstyle)\nmsize:改变大小(help markerstyle)\n\n\n\n代码\n%%stata\nhelp symbolstyle\n\n\n\n[G-4] symbolstyle -- Choices for the shape of markers\n                     (View complete PDF manual entry)\n\n\nSyntax\n------\n\n                        Synonym\n        symbolstyle     (if any)     Description\n        -------------------------------------------------------\n        circle             O         solid\n        diamond            D         solid\n        triangle           T         solid\n        square             S         solid\n        plus               +\n        X                  X\n        arrowf             A         filled arrow head\n        arrow              a\n        pipe               |\n        V                  V\n\n        smcircle           o         solid\n        smdiamond          d         solid\n        smsquare           s         solid\n        smtriangle         t         solid\n        smplus\n        smx                x\n        smv                v\n\n        circle_hollow      Oh        hollow\n        diamond_hollow     Dh        hollow\n        triangle_hollow    Th        hollow\n        square_hollow      Sh        hollow\n\n        smcircle_hollow    oh        hollow\n        smdiamond_hollow   dh        hollow\n        smtriangle_hollow  th        hollow\n        smsquare_hollow    sh        hollow\n\n        point              p         a small dot\n        none               i         a symbol that is invisible\n        -------------------------------------------------------\n\n        For a symbol palette displaying each of the above symbols, type\n\n            . palette symbolpalette [, scheme(schemename)]\n\n        Other symbolstyles may be available; type\n\n            . graph query symbolstyle\n\n        to obtain the complete list of symbolstyles installed on your\n        computer.\n\n\nDescription\n-----------\n\n    Markers are the ink used to mark where points are on a plot; see [G-3]\n    marker_options.  symbolstyle specifies the shape of the marker.\n\n    You specify the symbolstyle inside the msymbol() option allowed with many\n    of the graph commands:\n\n        . graph twoway ..., msymbol(symbolstyle) ...\n\n    Sometimes you will see that a symbolstylelist is allowed:\n\n        . scatter ..., msymbol(symbolstylelist) ...\n\n    A symbolstylelist is a sequence of symbolstyles separated by spaces.\n    Shorthands are allowed to make specifying the list easier; see [G-4]\n    stylelists.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Remarks and examples\n\n    The above sections are not included in this help file.\n\n\nRemarks\n-------\n\n    Remarks are presented under the following headings:\n\n        Typical use\n        Filled and hollow symbols\n        Size of symbols\n\n\nTypical use\n-----------\n\n    msymbol(symbolstyle) is one of the more commonly specified options.  For\n    instance, you may not be satisfied with the default rendition of\n\n        . scatter mpg weight if foreign ||\n          scatter mpg weight if !foreign\n\n    and prefer\n\n        . scatter mpg weight if foreign, msymbol(oh) ||\n          scatter mpg weight if !foreign, msymbol(x)\n\n    When you are graphing multiple y variables in the same plot, you can\n    specify a list of symbolstyles inside the msymbol() option:\n\n        . scatter mpg1 mpg2 weight, msymbol(oh x)\n\n    The result is the same as typing\n\n        . scatter mpg1 weight, msymbol(oh) ||\n          scatter mpg2 weight, msymbol(x)\n\n    Also, in the above, we specified the symbol-style synonyms.  Whether you\n    type\n\n        . scatter mpg1 weight, msymbol(oh) ||\n          scatter mpg2 weight, msymbol(x)\n\n    or\n\n        . scatter mpg1 weight, msymbol(smcircle_hollow) ||\n          scatter mpg2 weight, msymbol(smx)\n\n    makes no difference.\n\n\nFilled and hollow symbols\n-------------------------\n\n    The symbolstyle specifies the shape of the symbol, and in that sense, one\n    of the styles circle and hcircle -- and diamond and hdiamond, etc. -- is\n    unnecessary in that each is a different rendition of the same shape.  The\n    option mfcolor(colorstyle) (see [G-3] marker_options) specifies how the\n    inside of the symbol is to be filled.  hcircle(), hdiamond, etc., are\n    included for convenience and are equivalent to specifying\n\n        msymbol(Oh): msymbol(O) mfcolor(none)\n\n        msymbol(dh): msymbol(d) mfcolor(none)\n\n        etc.\n\n    Using mfcolor() to fill the inside of a symbol with different colors\n    sometimes creates what are effectively new symbols.  For instance, if you\n    take msymbol(O) and fill its interior with a lighter shade of the same\n    color used to outline the shape, you obtain a pleasing result.  For\n    instance, you might try\n\n        msymbol(O) mlcolor(yellow) mfcolor(.5*yellow)\n\n    or\n\n        msymbol(O) mlcolor(gs5) mfcolor(gs12)\n\n    as in\n\n        . scatter mpg weight, msymbol(O) mlcolor(gs5) mfcolor(gs14)\n          (click to run)\n\n\nSize of symbols\n---------------\n\n    Just as msymbol(O) and msymbol(Oh) differ only in mfcolor(), msymbol(O)\n    and msymbol(o) -- symbols circle and smcircle -- differ only in msize().\n    In particular,\n\n        msymbol(O): msymbol(O) msize(medium)\n\n        msymbol(o): msymbol(O) msize(small)\n\n    and the same is true for all the other large and small symbol pairs.\n\n    msize() is interpreted as being relative to the size of the graph region\n    (see [G-3] region_options), so the same symbol size will in fact be a\n    little different in\n\n        . scatter mpg weight\n\n    and\n\n        . scatter mpg weight, by(foreign total)\n\n\n\n\n代码\n%%stata\nhelp colorstyle\n\n\n\n[G-4] colorstyle -- Choices for color\n                    (View complete PDF manual entry)\n\n\nSyntax\n------\n\n    Set color of &lt;object&gt; to colorstyle\n\n        &lt;object&gt;color(colorstyle)\n\n\n    Set color of all affected objects to colorstyle\n\n        color(colorstyle)\n\n\n    Set opacity of &lt;object&gt; to #, where # is a percentage of 100% opacity\n\n        &lt;object&gt;color(%#)\n\n\n    Set opacity for all affected objects colors to #\n\n        color(%#)\n\n\n    Set both color and opacity of &lt;object&gt;\n\n        &lt;object&gt;color(colorstyle%#)\n\n\n    Set both color and opacity of all affected objects\n\n        &lt;object&gt;color(colorstyle%#)\n\n\n    colorstyle            Description\n    -------------------------------------------------------------------------\n    black                 \n\n    stc1                  color used by scheme stcolor\n    stc2                  color used by scheme stcolor\n    .                     \n    .                     \n    stc15                 color used by scheme stcolor\n    stblue                blue used by scheme stcolor\n    stgreen               green used by scheme stcolor\n    stred                 red used by scheme stcolor\n    styellow              yellow used by scheme stcolor\n\n    gs0                   gray scale: 0 = black\n    gs1                   gray scale: very dark gray\n    gs2                   \n    .                     \n    .                     \n    gs15                  gray scale: very light gray\n    gs16                  gray scale: 16 = white\n\n    white                 \n\n    blue                  \n    bluishgray            \n    brown                 \n    cranberry             \n    cyan                  \n    dimgray               between gs14 and gs15\n    dkgreen               dark green\n    dknavy                dark navy blue\n    dkorange              dark orange\n    eggshell              \n    emerald               \n    forest_green          \n    gold                  \n    gray                  equivalent to gs8\n    green                 \n    khaki                 \n    lavender              \n    lime                  \n    ltblue                light blue\n    ltbluishgray          light blue-gray, used by scheme s2color\n    ltkhaki               light khaki\n    magenta               \n    maroon                \n    midblue               \n    midgreen              \n    mint                  \n    navy                  \n    olive                 \n    olive_teal            \n    orange                \n    orange_red            \n    pink                  \n    purple                \n    red                   \n    sand                  \n    sandb                 bright sand\n    sienna                \n    stone                 \n    teal                  \n    yellow                \n\n                          colors used by The Economist magazine:\n    ebg                           background color\n    ebblue                        bright blue\n    edkblue                       dark blue\n    eltblue                       light blue\n    eltgreen                      light green\n    emidblue                      midblue\n    erose                         rose\n\n    none                  no color; invisible; draws nothing\n    background or bg      same color as background\n    foreground or fg      same color as foreground\n\n    \"# # #\"               RGB value; white = \"255 255 255\"\n\n    \"# # # #\"             CMYK value; yellow = \"0 0 255 0\"\n\n    \"hsv # # #\"           HSV value; white = \"hsv 0 0 1\"\n\n    colorstyle*#          color with adjusted intensity; #'s range from 0 to\n                            255\n\n    colorstyle%#          color with adjusted opacity; #s range from 0 to 100\n\n    *#                    default color with adjusted intensity\n    %#                    default color with adjusted opacity\n    -------------------------------------------------------------------------\n    When specifying RGB, CMYK, or HSV values, it is best to enclose the\n      values in quotes; type \"128 128 128\" not 128 128 128.\n\n\nDescription\n-----------\n\n    colorstyle sets the color and opacity of graph components such as lines,\n    backgrounds, and bars.  Some options allow a sequence of colorstyles with\n    colorstylelist; see [G-4] stylelists.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Remarks and examples\n\n    The above sections are not included in this help file.\n\n\nRemarks\n-------\n\n    colorstyle sets the color and opacity of graph components such as lines,\n    backgrounds, and bars.  Colors can be specified with a named color, such\n    as black, olive, and yellow, or with a color value in the RGB, CMYK, or\n    HSV format.  colorstyle can also set a component to match the background\n    color or foreground color.  Additionally, colorstyle can modify color\n    intensity, making the color lighter or darker.  Some options allow a\n    sequence of colorstyles with colorstylelist; see [G-4] stylelists.\n\n    To see a list of named colors, use graph query colorstyle.  See [G-2]\n    graph query.  For a color palette showing an individual color or\n    comparing two colors, use palette color.  See [G-2] palette.\n\n    Remarks are presented under the following headings:\n\n        Adjust opacity\n        Adjust intensity\n        Specify RGB values\n        Specify CMYK values\n        Specify HSV values\n        Export custom colors\n\n\nAdjust opacity\n--------------\n\n    Opacity is the percentage of a color that covers the background color.\n    That is, 100% means that the color fully hides the background, and 0%\n    means that the color has no coverage and is fully transparent.  If you\n    prefer to think about transparency, opacity is the inverse of\n    transparency.  Adjust opacity with the % modifier.  For example, type\n\n        green%50\n        \"0 255 0%50\"\n        %30\n\n    Omitting the color specification in the command adjusts the opacity of\n    the object while retaining the default color.  For instance, specify\n    mcolor(%30) with graph twoway scatter to use the default fill color at\n    30% opacity.\n\n    Specifying color%0 makes the object completely transparent and is\n    equivalent to color none.\n\n\nAdjust intensity\n----------------\n\n    Color intensity (brightness) can be modified by specifying a color, *,\n    and a multiplier value.  For example, type\n\n        green*.8\n        purple*1.5\n        \"0 255 255*1.2\"\n        \"hsv 240 1 1*.5\"\n\n    A value of 1 leaves the color unchanged, a value greater than 1 makes the\n    color darker, and a value less than 1 makes the color lighter.  Note that\n    there is no space between color and *, even when color is a numerical\n    value for RGB or CMYK.\n\n    Omitting the color specification in the command adjusts the intensity of\n    the object's default color.  For instance, specify bcolor(*.7) with graph\n    twoway bar to use the default fill color at reduced brightness, or\n    specify bcolor(*2) to increase the brightness of the default color.\n\n    Specifying color*0 makes the color as light as possible, but it is not\n    equivalent to color none.  color*255 makes the color as dark as possible,\n    although values much smaller than 255 usually achieve the same result.\n\n    For an example using the intensity adjustment, see Typical use in [G-2]\n    graph twoway kdensity.\n\n\nRGB values\n----------\n\n    In addition to specifying named colors such as yellow, you can specify\n    colors with RGB values.  An RGB value is a triplet of numbers ranging\n    from 0 to 255 that describes the level of red, green, and blue light that\n    must be emitted to produce a given color.  RGB is used to define colors\n    for on-screen display and in nonprofessional printing.  Examples of RGB\n    values are\n\n        red     =   255    0    0\n        green   =     0  255    0\n        blue    =     0    0  255\n        white   =   255  255  255\n        black   =     0    0    0\n        gray    =   128  128  128\n        navy    =    26   71  111\n\n\nCMYK values\n-----------\n\n    You can specify colors using CMYK values.  You will probably only use\n    CMYK values when they are provided by a journal or publisher.  You can\n    specify CMYK values either as integers from 0 to 255 or as proportions of\n    ink using real numbers from 0.0 to 1.0.  If all four values are 1 or\n    less, the numbers are taken to be proportions of ink.  For example,\n\n        red     =     0  255  255    0   or, equivalently,     0     1  1     0\n        green   =   255    0  255    0   or, equivalently,     1     0  1     0\n        blue    =   255  255    0    0   or, equivalently,     1     1  0     0\n        white   =     0    0    0    0   or, equivalently,     0     0  0     0\n        black   =     0    0    0  255   or, equivalently,     0     0  0     1\n        gray    =     0    0    0  128   or, equivalently,     0     0  0    .5\n        navy    =    85   40    0  144   or, equivalently,  .334  .157  0  .565\n\n\nHSV values\n----------\n\n    You can specify colors with HSV (hue, saturation, and value), also called\n    HSL (hue, saturation, and luminance) and HSB (hue, saturation, and\n    brightness).  HSV is often used in image editing software.  An HSV value\n    is a triplet of numbers.  So that Stata can differentiate them from RGB\n    values, HSV colors must be prefaced with hsv.  The first number specifies\n    the hue from 0 to 360, the second number specifies the saturation from 0\n    to 1, and the third number specifies the value (luminance or brightness)\n    from 0 to 1.  For example,\n\n        red     =   hsv   0     1     1\n        green   =   hsv 120     1  .502\n        blue    =   hsv 240     1     1\n        white   =   hsv   0     0     1\n        black   =   hsv   0     0     0\n        navy    =   hsv 209  .766  .435\n\n\nExport custom colors\n--------------------\n\n    graph export stores all colors as RGB+opacity values, that is, RGB values\n    0-255 and opacity values 0-1.  If you need color values from Stata in\n    CMYK format, use the graph export command with the cmyk(on) option, and\n    save the graph in one of the following formats: PostScript, Encapsulated\n    PostScript, or PDF.\n\n    You can set Stata to permanently use CMYK colors for PostScript export\n    files by typing translator set Graph2ps cmyk on and for EPS export files\n    by typing translator set Graph2eps cmyk on.\n\n    The CMYK values returned in graph export may differ from the CMYK values\n    that you entered.  This is because Stata normalizes CMYK values by\n    reducing all CMY values until one value is 0.  The difference is added to\n    the K (black) value.  For example, Stata normalizes the CMYK value 10 10\n    5 0 to 5 5 0 5.  Stata subtracts 5 from the CMY values so that Y is 0 and\n    then adds 5 to K.\n\n\nVideo example\n-------------\n\n        Transparency in Stata graphs\n\n\n\n\n代码\n%%stata\ntwoway scatter mpg weight,msymbol(D) mcolor(red) msize(medium)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "04-双变量作图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#option-by",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#option-by",
    "title": "05-双变量作图",
    "section": "1.3 Option: by",
    "text": "1.3 Option: by\n按照某个变量分组绘制散点图，可以使用by选项。by选项可以将数据按照指定变量进行分组，并为每个组绘制单独的散点图。\ntwoway scatter y x, by(groupvar)\n\n\n代码\n%%stata\ntwoway scatter mpg weight,by(foreign)\n\n\n\n\n\n\n\n\n\n\n\n代码\n%%stata\ntwoway(scatter mpg weight if foreign ==0)(scatter mpg weight if foreign ==1),legend(label(1 \"Domestic\") label(2 \"Foreign\"))",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#sas-在-linux-的安装",
    "href": "Guide/SAS/SAS-install.html#sas-在-linux-的安装",
    "title": "00-SAS安装与vscode 扩展",
    "section": "2 SAS 在 Linux 的安装",
    "text": "2 SAS 在 Linux 的安装",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#sas-与-vscode-扩展",
    "href": "Guide/SAS/SAS-install.html#sas-与-vscode-扩展",
    "title": "00-SAS安装与vscode 扩展",
    "section": "3 SAS 与 vscode 扩展",
    "text": "3 SAS 与 vscode 扩展\nSAS VS Code 扩展轻量级，可在任何地方运行，并允许您集成 SAS 和其他语言。该工具还提供直接连接到 SAS Viya 和 SAS 9 并运行代码的功能。\n\nSAS 语法突出显示和帮助、代码完成和代码片段\n用于连接 SAS 和运行代码的配置文件配置\n支持 SAS Viya 和 SAS 9 连接\n访问 SAS 内容和库\n为 SAS、SQL、Python 和其他语言创建笔记本\n\n扩展程序可在 GitHub 上找到仓库与原代码：[vscode-sas-extension](https://github.com/sassoftware/vscode-sas-extension)\n更多关于 SAS 与 vscode 的信息可以访问：[SAS Extension for Visual Studio Code](https://developer.sas.com/programming/vs_code_extension)\n\n3.1 安装插件\n在 vscode 的扩展页面搜索 “sas” ，第一个 “official SAS ···“ 即为正确扩展：",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#配置路径",
    "href": "Guide/SAS/SAS-install.html#配置路径",
    "title": "00-SAS安装与vscode 扩展",
    "section": "4 配置路径",
    "text": "4 配置路径\nBefore you can run SAS code, you must configure the SAS extension to access your SAS 9.4 (remote or local) server or a SAS Viya server and add a connection profile.\n在运行 SAS 代码之前，您必须配置 SAS 扩展以访问 SAS 9.4（远程或本地）服务器或 SAS Viya 服务器。您必须获得 SAS 9.4 或 SAS Viya 的许可才能运行 SAS 代码。\n\n打开 SAS 程序文件。\n单击 VS Code 窗口左下方状态栏中的“无配置文件”。\n您还可以打开命令面板（F1，或Ctrl+Shift+P在 Windows 或 Linux 上，或Shift+CMD+P在 OSX 上）并找到SAS: Add New Connection Profile命令。\n按照“添加新连接配置文件”部分中的说明添加配置文件。\n创建配置文件后，状态栏项将从“无配置文件”更改为新配置文件的名称。\n\n\n更多设置可以查看[SAS Extension for Visual Studio Code Documentation](https://sassoftware.github.io/vscode-sas-extension/Configurations/Profiles/sas9local)",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#编译-sas-文件",
    "href": "Guide/SAS/SAS-install.html#编译-sas-文件",
    "title": "00-SAS安装与vscode 扩展",
    "section": "5 编译 SAS 文件",
    "text": "5 编译 SAS 文件\nSAS 文件右上角有一个 奔跑的小人 ，点击即可开始运行所选中的程序段落，并在右侧窗口输出结果。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#在-jupyter-notebook-中使用-sas",
    "href": "Guide/SAS/SAS-install.html#在-jupyter-notebook-中使用-sas",
    "title": "00-SAS安装与vscode 扩展",
    "section": "6 在 Jupyter Notebook 中使用 SAS",
    "text": "6 在 Jupyter Notebook 中使用 SAS\n\n6.1 环境准备\n安装 Anaconda 集成环境或 Python 和 SAS 软件，其中Anaconda要求Python3+；Python在Jupyter Notebook和SAS之间起一个桥梁的作用，Jupyter Notebook中的SAS代码会交给Python，Python负责将代码传递给SAS执行；然后将执行的结果返回给Jupyter Notebook显示。\nSAS版本要求9.4，也可以是 SAS Viya。\n\n\n6.2 安装SAS_KERNEL\n启动 cmd，输入命令：\n\n\n代码\npip install sas_kernel\n\n\n然后就会自动安装 sas_kernel 及其相应的依赖项。\n安装完成后可以输入命令：\n\n\n代码\njupyter kernelspec list\n\n\n来检测 sas_kernel 是否安装成功，如果成功，理论上会看到如下形式的输出：\nAvailable kernels:\n    python3    /home/sas/anaconda3/lib/python3.5/site-packages/ipykernel/resources\n    sas        /home/sas/.local/share/jupyter/kernels/sas\n\n\n6.3 修改 Python 配置文件\n安装好 sas_kernel 后找到 Anaconda 或 Python 的安装目录，会有一个相应的文件夹出现，例如我的文件路径如下：\nC:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\saspy\n在这个文件路径下找到 sascfg.py 文件，该文件中需要配置连接SAS的信息。可以配置连接本地机器的SAS；也可以配置连接远程机器的SAS Server，无论是Linux Server还是Windows Server都可以。此处就以连接本地SAS为例进行说明。\n\n打开该文件，首先是一大段注释；\n在这段注释后定义的第一个变量 SAS_config_names 用于指定连接SAS的配置方式，提供了 10 种方式：default, ssh, iomlinux, iomwin, winlocal, winiomlinux, winiomwin, httpsviya, httpviya, iomcom。默认为 default 方式。\n因为我们需要连接Windows机器本地的SAS，所以需要将 SAS_config_names 的值修改为 winlocal 。\n\n\n后续有一些安装步骤，但是大多是在2016-2020年更新的教程，无法找到复现的路径，可能相关的配置已被优化。\n包括这个 [SAS岩论 | 在Jupyter Notebook中使用SAS ](https://www.sohu.com/a/218339423_278472) 中写到的需要使用 cpW 定义 SAS 路径。\n\n\n6.4 修改系统变量\n将sas相关文件 sspiauth.dll 添加到系统环境变量，该文件很可能在如下目录：\nC:\\Program Files\\SASHome\\SASFoundation\\9.4\\core\\sasext\n（注意添加变量时不要包含 sspiauth.dll 文件本身）\nWarning: 环境变量添加完成后，要重启电脑才会生效。\n\n\n6.5 在 jupyter notebook 中使用 SAS\n新建文件，选择使用 SAS 内核，或者在 cell 中通过 magic command 指定内核。\n%%sas\n使用语法如下所示：\n\n\n代码\n%%sas\ndata iris;\n    set sashelp.iris;\nrun;\n\nproc print data=iris(obs=10);\nrun;\n\n\n在Notebook中写SAS代码了，跟Python一样，同样有代码提示、语法高亮的功能。但是你会注意到过程步的结果显示了，运行的日志去哪里了？\n如果代码运行错误或者没有输出（例如纯DATA步）的话，那么输出就是日志信息。\n能够正确运行且有输出结果的代码就不会显示日志了。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#安装sas日志组件",
    "href": "Guide/SAS/SAS-install.html#安装sas日志组件",
    "title": "00-SAS安装与vscode 扩展",
    "section": "7 安装SAS日志组件",
    "text": "7 安装SAS日志组件\n如果想要像SAS Base一样，随时查看所有程序运行的日志结果也没问题。安装一个Notebook的SAS日志扩展组件就可以了。打开Anaconda Prompt，输入以下命令安装：\njupyter nbextension install --py sas_kernel.showSASLog\n运行完毕后，输入以下命令启用SAS日志组件：\njupyter nbextension enable sas_kernel.showSASLog –py",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#连接sas-server",
    "href": "Guide/SAS/SAS-install.html#连接sas-server",
    "title": "00-SAS安装与vscode 扩展",
    "section": "8 连接SAS Server",
    "text": "8 连接SAS Server\n如果需要配置连接远程的SAS Server，如连接远程Windows机器的SAS Server，需在sascfg.py中做以下修改：\n\n将SAS_config_names的值改为“wintowin”；\n在wintowin连接方式中将参数iomhost的值修改为远程Windows机器的IP地址；将参数encoding的值修改为euc-cn；\n将cpW中5个Jar包的路径修改为远程Windows机器中SAS对应的目录。\n\n修改完毕后，启动Notebook，首次运行SAS代码时，会提示输入访问SAS Server的有效SAS用户和密码。1\n\n8.1 典型生态项目\n\nSASPy\nSASPy 是一个 Python 库，允许你通过 Python 代码与 SAS 进行交互。SAS Kernel 依赖于 SASPy，因此在使用 SAS Kernel 之前，你需要配置 SASPy。\n\n\nJupyterLab 扩展\nSAS Kernel 支持 JupyterLab 扩展，这些扩展可以提高你在 JupyterLab 中的编程效率。你可以通过以下命令安装这些扩展：\npip install sas_kernel[jlab_ext] \n\n\nNBGrader\nNBGrader 是一个用于分配和评分 Jupyter Notebook 的系统，它与 SAS Kernel 兼容。你可以使用 NBGrader 来创建和评分包含 SAS 代码的作业。\n通过这些生态项目，SAS Kernel 不仅扩展了 Jupyter Notebook 的功能，还增强了其在数据科学和分析领域的应用能力。2",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/SAS-install.html#footnotes",
    "href": "Guide/SAS/SAS-install.html#footnotes",
    "title": "00-SAS安装与vscode 扩展",
    "section": "脚注",
    "text": "脚注\n\n\nSAS岩论 | 在Jupyter Notebook中使用SAS↩︎\nSAS Kernel for Jupyter 安装与使用教程↩︎",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html",
    "href": "Guide/Stata/25-05-08-GLM.html",
    "title": "14-广义线性模型（GLM）",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)\n%%stata\nsysuse auto.dta,clear\n\n(1978 automobile data)\n大于15%会产生Bias的这个问题，只有在我们想用odds去estimaterisk的时候才会发生。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#什么是广义线性模型",
    "href": "Guide/Stata/25-05-08-GLM.html#什么是广义线性模型",
    "title": "14-广义线性模型（GLM）",
    "section": "1 什么是广义线性模型",
    "text": "1 什么是广义线性模型\n\n线性回归 Linear regression：\n\n\\(exp Y = \\beta_0 + \\beta_1*X_1+\\cdots + \\beta_n *X_n\\)\n\n广义线性回归 Generalized linear model：\n\n\\(f(exp Y)= β_0 + β_1*X_1 + \\cdots + \\beta_n* X_n\\)\nLinear regression is a special case of GLM\nSame for logistic regression, Poisson regression, Log-Binomial regression\nEven for Cox regression (Aug.31st & Sep.7th), GEE model (Sep.14th)\n\n总结：在GLM中，转化Y使得转化之后的Y和X们呈线性关系",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#simple-glm-vs.-multiple-glm",
    "href": "Guide/Stata/25-05-08-GLM.html#simple-glm-vs.-multiple-glm",
    "title": "14-广义线性模型（GLM）",
    "section": "2 Simple GLM vs. Multiple GLM",
    "text": "2 Simple GLM vs. Multiple GLM\n\n2.1 简单广义线性模型\n\\[f(exp Y)= β_0 + β_1*X_1\\]\n\n\n2.2 多元广义线性模型\n\\[f(exp Y)= β_0 + β_1*X_1 + \\cdots + \\beta_n* X_n\\]\nX变量可以是各种类型的变量（连续、分类）",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#specify-options",
    "href": "Guide/Stata/25-05-08-GLM.html#specify-options",
    "title": "14-广义线性模型（GLM）",
    "section": "3 Specify Options",
    "text": "3 Specify Options\n\n3.1 Family\nY 变量的分布类型\n\n\n3.2 Link\n把 Y 怎么转化，才和 X 成linear关系",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#linear-regression",
    "href": "Guide/Stata/25-05-08-GLM.html#linear-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "4 linear regression",
    "text": "4 linear regression\n\n4.1 Y var\n\n连续分布（continuous variable）\n如何分布? 高斯分布（Gaussian Distribution）\nModel：\n\n\\[f(exp Y)= β_0 + β_1*X_1 + \\cdots + \\beta_n* X_n\\]\nY 怎么转化才和 X 成 linear 关系？\n\n不用转化\nLink：Identity（恒等）",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#logistic-regression",
    "href": "Guide/Stata/25-05-08-GLM.html#logistic-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "5 Logistic regression",
    "text": "5 Logistic regression\n\n5.1 Y var\n\nBinary variable\n如何分布? 二项分布(Binomial)\nModel:\n\n\\[ln[P(Y=1)/P(Y=0)]= β_0 + β_1*X_1+\\cdots + β_n*X_n\\]\nExp Y = P(Y=1)\nY 怎么转化才和X成linear关系？Link：logit",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#poisson-regression",
    "href": "Guide/Stata/25-05-08-GLM.html#poisson-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "6 Poisson regression",
    "text": "6 Poisson regression\n\n6.1 Y var\n\nCount variable:整数(1,2,3,.n)\n如何分布? 泊松分布(Poisson)\nModel:\n\n\\[ln[risk of event)]= β_0 + β_1*X_1+\\cdots + β_n*X_n\\]\nY 怎么转化才和X成linear关系？Link：Log",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#log-binomial-regression",
    "href": "Guide/Stata/25-05-08-GLM.html#log-binomial-regression",
    "title": "14-广义线性模型（GLM）",
    "section": "7 Log-binomial Regression",
    "text": "7 Log-binomial Regression\n\n7.1 Y var\n\nBinary variable\n如何分布? 二项分布(Binomial)\nModel:\n\n\\[ln[P(Y=1)]= β_0 + β_1*X_1+\\cdots + β_n*X_n\\]\nExp Y = P(Y=1)\nY 怎么转化才和X成linear关系？Link：Log\n\n\n\n%%stata\nhelp glm\n\n\n[R] glm -- Generalized linear models\n           (View complete PDF manual entry)\n\n\nSyntax\n------\n\n        glm depvar [indepvars] [if] [in] [weight] [, options]\n\n    options                     Description\n    -------------------------------------------------------------------------\n    Model\n      family(familyname)        distribution of depvar; default is\n                                  family(gaussian)\n      link(linkname)            link function; default is canonical link for\n                                  family() specified\n\n    Model 2\n      noconstant                suppress constant term\n      exposure(varname)         include ln(varname) in model with coefficient\n                                  constrained to 1\n      offset(varname)           include varname in model with coefficient\n                                  constrained to 1\n      constraints(constraints)  apply specified linear constraints\n      asis                      retain perfect predictor variables\n      mu(varname)               use varname as the initial estimate for the\n                                  mean of depvar\n      init(varname)             synonym for mu(varname)\n\n    SE/Robust\n      vce(vcetype)              vcetype may be oim, robust, cluster clustvar,\n                                  eim, opg, bootstrap, jackknife, hac kernel,\n                                  jackknife1, or unbiased\n      vfactor(#)                multiply variance matrix by scalar #\n      disp(#)                   quasilikelihood multiplier\n      scale(x2|dev|#)           set the scale parameter\n\n    Reporting\n      level(#)                  set confidence level; default is level(95)\n      eform                     report exponentiated coefficients\n      nocnsreport               do not display constraints\n      display_options           control columns and column formats, row\n                                  spacing, line width, display of omitted\n                                  variables and base and empty cells, and\n                                  factor-variable labeling\n\n    Maximization\n      ml                        use maximum likelihood optimization; the\n                                  default\n      irls                      use iterated, reweighted least-squares\n                                  optimization of the deviance\n      maximize_options          control the maximization process; seldom used\n      fisher(#)                 use the Fisher scoring Hessian or expected\n                                  information matrix (EIM)\n      search                    search for good starting values\n\n      noheader                  suppress header table from above coefficient\n                                  table\n      notable                   suppress coefficient table\n      nodisplay                 suppress the output; iteration log is still\n                                  displayed\n      collinear                 keep collinear variables\n      coeflegend                display legend instead of statistics\n    -------------------------------------------------------------------------\n\n    familyname               Description\n    -------------------------------------------------------------------------\n    gaussian                 Gaussian (normal)\n    igaussian                inverse Gaussian\n    binomial[varnameN|#N]    Bernoulli/binomial\n    poisson                  Poisson\n    nbinomial[#k|ml]         negative binomial\n    gamma                    gamma\n    -------------------------------------------------------------------------\n\n    linkname                 Description\n    -------------------------------------------------------------------------\n    identity                 identity\n    log                      log\n    logit                    logit\n    probit                   probit\n    cloglog                  cloglog\n    power #                  power\n    opower #                 odds power\n    nbinomial                negative binomial\n    loglog                   log-log\n    logc                     log-complement\n    -------------------------------------------------------------------------\n\n    indepvars may contain factor variables; see fvvarlist.\n    depvar and indepvars may contain time-series operators; see tsvarlist.\n    bayes, bootstrap, by, collect, fmm, fp, jackknife, mfp, mi estimate,\n      nestreg, rolling, statsby, stepwise, and svy are allowed; see prefix.\n      For more details, see [BAYES] bayes: glm and [FMM] fmm: glm.\n    vce(bootstrap), vce(jackknife), and vce(jackknife1) are not allowed with\n      the mi estimate prefix.\n    Weights are not allowed with the bootstrap prefix.\n    aweights are not allowed with the jackknife prefix.\n    vce(), vfactor(), disp(), scale(), irls, fisher(), noheader, notable,\n      nodisplay, and weights are not allowed with the svy prefix.\n    fweights, aweights, iweights, and pweights are allowed; see weight.\n    noheader, notable, nodisplay, collinear, and coeflegend do not appear in\n      the dialog box.\n    See [R] glm postestimation for features available after estimation.\n\n\nMenu\n----\n\n    Statistics &gt; Generalized linear models &gt; Generalized linear models (GLM)\n\n\nDescription\n-----------\n\n    glm fits generalized linear models.  It can fit models by using either\n    IRLS (maximum quasilikelihood) or Newton-Raphson (maximum likelihood)\n    optimization, which is the default.\n\n    See [U] 27 Overview of Stata estimation commands for a description of all\n    of Stata's estimation commands, several of which fit models that can also\n    be fit using glm.\n\n\nLinks to PDF documentation\n--------------------------\n\n        Quick start\n\n        Remarks and examples\n\n        Methods and formulas\n\n    The above sections are not included in this help file.\n\n\nOptions\n-------\n\n        +-------+\n    ----+ Model +------------------------------------------------------------\n\n    family(familyname) specifies the distribution of depvar; family(gaussian)\n        is the default.\n\n    link(linkname) specifies the link function; the default is the canonical\n        link for the family() specified (except for family(nbinomial)).\n\n        +---------+\n    ----+ Model 2 +----------------------------------------------------------\n\n    noconstant, exposure(varname), offset(varname), constraints(constraints);\n        see [R] Estimation options.  constraints(constraints) is not allowed\n        with irls.\n\n    asis forces retention of perfect predictor variables and their\n        associated, perfectly predicted observations and may produce\n        instabilities in maximization; see [R] probit.  This option is\n        allowed only with option family(binomial) with a denominator of 1.\n\n    mu(varname) specifies varname as the initial estimate for the mean of \n        depvar.  This option can be useful with models that experience\n        convergence difficulties, such as family(binomial) models with power\n        or odds-power links.  init(varname) is a synonym.\n\n        +-----------+\n    ----+ SE/Robust +--------------------------------------------------------\n\n    vce(vcetype) specifies the type of standard error reported, which\n        includes types that are derived from asymptotic theory (oim, opg),\n        that are robust to some kinds of misspecification (robust), that\n        allow for intragroup correlation (cluster clustvar), and that use\n        bootstrap or jackknife methods (bootstrap, jackknife); see [R]\n        vce_option.\n\n        In addition to the standard vcetypes, glm allows the following\n        alternatives:\n\n        vce(eim) specifies that the EIM estimate of variance be used.\n\n        vce(jackknife1) specifies that the one-step jackknife estimate of\n            variance be used.\n\n        vce(hac kernel [#]) specifies that a heteroskedasticity- and\n            autocorrelation-consistent (HAC) variance estimate be used.  HAC\n            refers to the general form for combining weighted matrices to\n            form the variance estimate.  There are three kernels built into\n            glm.  kernel is a user-written program or one of\n\n                          nwest | gallant | anderson\n\n            # specifies the number of lags.  If # is not specified, N - 2 is\n            assumed.  If you wish to specify vce(hac ... ), you must tsset\n            your data before calling glm.\n\n        vce(unbiased) specifies that the unbiased sandwich estimate of\n            variance be used.\n\n    vfactor(#) specifies a scalar by which to multiply the resulting variance\n        matrix.  This option allows you to match output with other packages,\n        which may apply degrees of freedom or other small-sample corrections\n        to estimates of variance.\n\n    disp(#) multiplies the variance of depvar by # and divides the deviance\n        by #.  The resulting distributions are members of the quasilikelihood\n        family.  This option is allowed only with option irls.\n\n    scale(x2|dev|#) overrides the default scale parameter.  This option is\n        allowed only with Hessian (information matrix) variance estimates.\n\n        By default, scale(1) is assumed for the discrete distributions\n        (binomial, Poisson, and negative binomial), and scale(x2) is assumed\n        for the continuous distributions (Gaussian, gamma, and inverse\n        Gaussian).\n\n        scale(x2) specifies that the scale parameter be set to the Pearson\n        chi-squared (or generalized chi-squared) statistic divided by the\n        residual degrees of freedom, which is recommended by McCullagh and\n        Nelder (1989) as a good general choice for continuous distributions.\n\n        scale(dev) sets the scale parameter to the deviance divided by the\n        residual degrees of freedom.  This option provides an alternative to\n        scale(x2) for continuous distributions and overdispersed or\n        underdispersed discrete distributions.  This option is allowed only\n        with option irls.\n\n        scale(#) sets the scale parameter to #.  For example, using scale(1)\n        in family(gamma) models results in exponential-errors regression.\n        Additional use of link(log) rather than the default link(power -1)\n        for family(gamma) essentially reproduces Stata's streg, dist(exp)\n        nohr command (see [ST] streg) if all the observations are uncensored.\n\n        +-----------+\n    ----+ Reporting +--------------------------------------------------------\n\n    level(#); see [R] Estimation options.\n\n    eform displays the exponentiated coefficients and corresponding standard\n        errors and confidence intervals.  For family(binomial) link(logit)\n        (that is, logistic regression), exponentiation results are odds\n        ratios; for family(nbinomial) link(log) (that is, negative binomial\n        regression) and for family(poisson) link(log) (that is, Poisson\n        regression), exponentiated coefficients are incidence-rate ratios.\n\n    nocnsreport; see [R] Estimation options.\n\n    display_options:  noci, nopvalues, noomitted, vsquish, noemptycells,\n        baselevels, allbaselevels, nofvlabel, fvwrap(#), fvwrapon(style),\n        cformat(%fmt), pformat(%fmt), sformat(%fmt), and nolstretch; see [R]\n        Estimation options.\n\n        +--------------+\n    ----+ Maximization +-----------------------------------------------------\n\n    ml requests that optimization be carried out using Stata's ml commands\n        and is the default.\n\n    irls requests iterated, reweighted least-squares (IRLS) optimization of\n        the deviance instead of Newton-Raphson optimization of the log\n        likelihood.  If the irls option is not specified, the optimization is\n        carried out using Stata's ml commands, in which case all options of\n        ml maximize are also available.\n\n    maximize_options:  difficult, technique(algorithm_spec), iterate(#),\n        [no]log, trace, gradient, showstep, hessian, showtolerance,\n        tolerance(#), ltolerance(#), nrtolerance(#), nonrtolerance, and\n        from(init_specs); see [R] Maximize.  These options are seldom used.\n\n        Setting the optimization method to technique(bhhh) resets the default\n        vcetype to vce(opg).\n\n        If option irls is specified, only maximize_options iterate(), nolog,\n        trace, and ltolerance() are allowed.  With irls specified, the\n        convergence criterion is satisfied when the absolute change in\n        deviance from one iteration to the next is less than or equal to\n        ltolerance(), where ltolerance(1e-6) is the default.\n\n    fisher(#) specifies the number of Newton-Raphson steps that should use\n        the Fisher scoring Hessian or EIM before switching to the observed\n        information matrix (OIM).  This option is useful only for\n        Newton-Raphson optimization (and not when using irls).\n\n    search specifies that the command search for good starting values.  This\n        option is useful only for Newton-Raphson optimization (and not when\n        using irls).\n\n    The following options are available with glm but are not shown in the\n    dialog box:\n\n    noheader suppresses the header information from the output.  The\n        coefficient table is still displayed.\n\n    notable suppresses the table of coefficients from the output.  The header\n        information is still displayed.\n\n    nodisplay suppresses the output.  The iteration log is still displayed.\n\n    collinear, coeflegend; see [R] Estimation options.  collinear is not\n        allowed with irls.\n\n\nRemarks\n-------\n\n    Although glm can be used to fit linear regression (and, in fact, does so\n    by default), this should be viewed as an instructional feature; regress\n    produces such estimates more quickly, and many postestimation commands\n    are available to explore the adequacy of the fit; see [R] regress and [R]\n    regress postestimation.\n\n    In any case, you should specify the link function by using the link()\n    option and specify the distributional family by using family().  The\n    available link functions are\n\n                   Link function            glm option     \n                   ----------------------------------------\n                   identity                 link(identity) \n                   log                      link(log)      \n                   logit                    link(logit)    \n                   probit                   link(probit)   \n                   complementary log-log    link(cloglog)  \n                   odds power               link(opower #) \n                   power                    link(power #)  \n                   negative binomial        link(nbinomial)\n                   log-log                  link(loglog)   \n                   log-complement           link(logc)     \n\n    The available distributional families are\n\n                   Family                 glm option       \n                   ----------------------------------------\n                   Gaussian(normal)       family(gaussian) \n                   inverse Gaussian       family(igaussian)\n                   Bernoulli/binomial     family(binomial) \n                   Poisson                family(poisson)  \n                   negative binomial      family(nbinomial)\n                   gamma                  family(gamma)    \n\n    You do not have to specify both family() and link(); the default link()\n    is the canonical link for the specified family() (except for nbinomial):\n\n                    Family                  Default link  \n                    --------------------------------------\n                    family(gaussian)        link(identity)\n                    family(igaussian)       link(power -2)\n                    family(binomial)        link(logit)   \n                    family(poisson)         link(log)     \n                    family(nbinomial)       link(log)     \n                    family(gamma)           link(power -1)\n\n    If you specify both family() and link(), not all combinations make sense.\n    You may choose from the following combinations:\n\n          | id  log  logit  probit  clog  pow  opower  nbinomial  loglog  logc\n----------+-------------------------------------------------------------------\nGaussian  |  x   x                         x\ninv. Gau. |  x   x                         x\nbinomial  |  x   x     x      x       x    x     x                  x      x\nPoisson   |  x   x                         x\nneg. bin. |  x   x                         x              x\ngamma     |  x   x                         x\n\n\nExamples\n--------\n\n    ---------------------------------------------------------------------------\n    Setup\n        . webuse lbw\n\n    Generalized linear model with Bernoulli family and default logit link\n        . glm low age lwt i.race smoke ptl ht ui, family(binomial)\n\n    Replay results and report exponentiated coefficients\n        . glm, eform\n\n    ---------------------------------------------------------------------------\n    Setup\n        . webuse ldose\n\n    Generalized linear model with binomial family and default logit link\n        . glm r ldose, family(binomial n)\n\n    Generalized linear model with binomial family and cloglog link\n        . glm r ldose, family(binomial n) link(cloglog)\n\n    ---------------------------------------------------------------------------\n    Setup\n        . webuse beetle\n\n    Generalized linear model with binomial family and cloglog link\n        . glm r i.beetle ldose, family(binomial n) link(cloglog)\n\n    Replay results with 99% confidence intervals\n        . glm, level(99)\n    ---------------------------------------------------------------------------\n\n\nStored results\n--------------\n\n    glm, ml stores the following in e():\n\n    Scalars           \n      e(N)                   number of observations\n      e(k)                   number of parameters\n      e(k_eq)                number of equations in e(b)\n      e(k_eq_model)          number of equations in overall model test\n      e(k_dv)                number of dependent variables\n      e(df_m)                model degrees of freedom\n      e(df)                  residual degrees of freedom\n      e(phi)                 scale parameter\n      e(aic)                 model AIC\n      e(bic)                 model BIC\n      e(ll)                  log likelihood, if NR\n      e(N_clust)             number of clusters\n      e(chi2)                chi-squared\n      e(p)                   p-value for model test\n      e(deviance)            deviance\n      e(deviance_s)          scaled deviance\n      e(deviance_p)          Pearson deviance\n      e(deviance_ps)         scaled Pearson deviance\n      e(dispers)             dispersion\n      e(dispers_s)           scaled dispersion\n      e(dispers_p)           Pearson dispersion\n      e(dispers_ps)          scaled Pearson dispersion\n      e(nbml)                1 if negative binomial parameter estimated via\n                               ML, 0 otherwise\n      e(vf)                  factor set by vfactor(), 1 if not set\n      e(power)               power set by link(power #) or link(opower #)\n      e(rank)                rank of e(V)\n      e(ic)                  number of iterations\n      e(rc)                  return code\n      e(converged)           1 if converged, 0 otherwise\n\n    Macros            \n      e(cmd)                 glm\n      e(cmdline)             command as typed\n      e(depvar)              name of dependent variable\n      e(varfunc)             program to calculate variance function\n      e(varfunct)            variance title\n      e(varfuncf)            variance function\n      e(link)                program to calculate link function\n      e(linkt)               link title\n      e(linkf)               link function\n      e(m)                   number of binomial trials\n      e(wtype)               weight type\n      e(wexp)                weight expression\n      e(title)               title in estimation output\n      e(clustvar)            name of cluster variable\n      e(offset)              linear offset variable\n      e(chi2type)            Wald; type of model chi-squared test\n      e(cons)                noconstant, if specified\n      e(hac_kernel)          HAC kernel\n      e(hac_lag)             HAC lag\n      e(vce)                 vcetype specified in vce()\n      e(vcetype)             title used to label Std. err.\n      e(opt)                 ml or irls\n      e(opt1)                optimization title, line 1\n      e(opt2)                optimization title, line 2\n      e(which)               max or min; whether optimizer is to perform\n                               maximization or minimization\n      e(ml_method)           type of ml method\n      e(user)                name of likelihood-evaluator program\n      e(technique)           maximization technique\n      e(properties)          b V\n      e(predict)             program used to implement predict\n      e(marginsok)           predictions allowed by margins\n      e(marginsnotok)        predictions disallowed by margins\n      e(asbalanced)          factor variables fvset as asbalanced\n      e(asobserved)          factor variables fvset as asobserved\n\n    Matrices          \n      e(b)                   coefficient vector\n      e(Cns)                 constraints matrix\n      e(ilog)                iteration log (up to 20 iterations)\n      e(gradient)            gradient vector\n      e(V)                   variance-covariance matrix of the estimators\n      e(V_modelbased)        model-based variance\n\n    Functions         \n      e(sample)              marks estimation sample\n\n    In addition to the above, the following is stored in r():\n\n    Matrices          \n      r(table)               matrix containing the coefficients with their\n                               standard errors, test statistics, p-values,\n                               and confidence intervals\n\n    Note that results stored in r() are updated when the command is replayed\n    and will be replaced when any r-class command is run after the estimation\n    command.\n\n    glm, irls stores the following in e():\n\n    Scalars           \n      e(N)                   number of observations\n      e(k)                   number of parameters\n      e(k_eq_model)          number of equations in overall model test\n      e(df_m)                model degrees of freedom\n      e(df)                  residual degrees of freedom\n      e(phi)                 scale parameter\n      e(disp)                dispersion parameter\n      e(bic)                 model BIC\n      e(N_clust)             number of clusters\n      e(deviance)            deviance\n      e(deviance_s)          scaled deviance\n      e(deviance_p)          Pearson deviance\n      e(deviance_ps)         scaled Pearson deviance\n      e(dispers)             dispersion\n      e(dispers_s)           scaled dispersion\n      e(dispers_p)           Pearson dispersion\n      e(dispers_ps)          scaled Pearson dispersion\n      e(nbml)                1 if negative binomial parameter estimated via\n                               ML, 0 otherwise\n      e(vf)                  factor set by vfactor(), 1 if not set\n      e(power)               power set by link(power #) or link(opower #)\n      e(rank)                rank of e(V)\n      e(rc)                  return code\n\n    Macros            \n      e(cmd)                 glm\n      e(cmdline)             command as typed\n      e(depvar)              name of dependent variable\n      e(varfunc)             program to calculate variance function\n      e(varfunct)            variance title\n      e(varfuncf)            variance function\n      e(link)                program to calculate link function\n      e(linkt)               link title\n      e(linkf)               link function\n      e(m)                   number of binomial trials\n      e(wtype)               weight type\n      e(wexp)                weight expression\n      e(clustvar)            name of cluster variable\n      e(offset)              linear offset variable\n      e(cons)                noconstant, if specified\n      e(hac_kernel)          HAC kernel\n      e(hac_lag)             HAC lag\n      e(vce)                 vcetype specified in vce()\n      e(vcetype)             title used to label Std. err.\n      e(opt)                 ml or irls\n      e(opt1)                optimization title, line 1\n      e(opt2)                optimization title, line 2\n      e(properties)          b V\n      e(predict)             program used to implement predict\n      e(marginsok)           predictions allowed by margins\n      e(marginsnotok)        predictions disallowed by margins\n      e(asbalanced)          factor variables fvset as asbalanced\n      e(asobserved)          factor variables fvset as asobserved\n\n    Matrices          \n      e(b)                   coefficient vector\n      e(V)                   variance-covariance matrix of the estimators\n      e(V_modelbased)        model-based variance\n\n    Functions         \n      e(sample)              marks estimation sample\n\n    In addition to the above, the following is stored in r():\n\n    Matrices          \n      r(table)               matrix containing the coefficients with their\n                               standard errors, test statistics, p-values,\n                               and confidence intervals\n\n    Note that results stored in r() are updated when the command is replayed\n    and will be replaced when any r-class command is run after the estimation\n    command.\n\n\nReference\n---------\n\n    McCullagh, P., and J. A. Nelder. 1989.  Generalized Linear Models. 2nd\n        ed.  London: Chapman & Hall/CRC.\n\n\nglm command in Stata - Umbrella Command - linear regression: family(gaussian) link(identity) - logistic regression: family(binomial) link(logit) - poisson regression: family(poisson) link(log) - log-binomial regression: family(binomial) link(log)\n\n加粗部分表示，在实际代码中可以简写为加粗的字母或单词即可",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#代码的转换",
    "href": "Guide/Stata/25-05-08-GLM.html#代码的转换",
    "title": "14-广义线性模型（GLM）",
    "section": "8 代码的转换",
    "text": "8 代码的转换\n\n8.1 Linear regression:\n\nregress price weight length mpg i.rep78\nglm price weight length mpg i.rep78, family(gaussian)link(identity)\n\n\n\n8.2 Logistic regression:\n\nlogistic low age i.smoke i.race\nglm low age i.smoke i.race,family(binomial) link(logit)\nglm low age i.smoke i.race,family(binomial) link(logit) eform\n\n\neform 直接输出 OR 值，即 \\(e^{\\beta}\\)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#线性回归模型的比较",
    "href": "Guide/Stata/25-05-08-GLM.html#线性回归模型的比较",
    "title": "14-广义线性模型（GLM）",
    "section": "9 线性回归模型的比较",
    "text": "9 线性回归模型的比较\n\n%%stata\nregress price weight length mpg i.rep78\n\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(7, 61)        =      7.25\n       Model |   262008114         7  37429730.6   Prob &gt; F        =    0.0000\n    Residual |   314788844        61  5160472.86   R-squared       =    0.4542\n-------------+----------------------------------   Adj R-squared   =    0.3916\n       Total |   576796959        68  8482308.22   Root MSE        =    2271.7\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   5.186695   1.163383     4.46   0.000     2.860367    7.513022\n      length |  -124.1544   40.07637    -3.10   0.003     -204.292   -44.01671\n         mpg |  -126.8367   84.49819    -1.50   0.138    -295.8012    42.12791\n             |\n       rep78 |\n          2  |   1137.284   1803.332     0.63   0.531    -2468.701    4743.269\n          3  |   1254.642   1661.545     0.76   0.453    -2067.823    4577.108\n          4  |   2267.188   1698.018     1.34   0.187    -1128.208    5662.584\n          5  |   3850.759   1787.272     2.15   0.035     276.8886     7424.63\n             |\n       _cons |   14614.49   6155.842     2.37   0.021     2305.125    26923.86\n------------------------------------------------------------------------------\n\n\n\n%%stata\nglm price weight length mpg i.rep78, family(gaussian)link(identity)\n\n\nIteration 0:  Log likelihood = -626.90582  \n\nGeneralized linear models                         Number of obs   =         69\nOptimization     : ML                             Residual df     =         61\n                                                  Scale parameter =    5160473\nDeviance         =  314788844.4                   (1/df) Deviance =    5160473\nPearson          =  314788844.4                   (1/df) Pearson  =    5160473\n\nVariance function: V(u) = 1                       [Gaussian]\nLink function    : g(u) = u                       [Identity]\n\n                                                  AIC             =   18.40307\nLog likelihood   = -626.9058204                   BIC             =   3.15e+08\n\n------------------------------------------------------------------------------\n             |                 OIM\n       price | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   5.186695   1.163383     4.46   0.000     2.906506    7.466883\n      length |  -124.1544   40.07637    -3.10   0.002    -202.7026   -45.60612\n         mpg |  -126.8367   84.49819    -1.50   0.133    -292.4501    38.77675\n             |\n       rep78 |\n          2  |   1137.284   1803.332     0.63   0.528    -2397.182     4671.75\n          3  |   1254.642   1661.545     0.76   0.450    -2001.927    4511.211\n          4  |   2267.188   1698.018     1.34   0.182    -1060.866    5595.241\n          5  |   3850.759   1787.272     2.15   0.031     347.7711    7353.747\n             |\n       _cons |   14614.49   6155.842     2.37   0.018     2549.263    26679.72\n------------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#logistic-回归的比较",
    "href": "Guide/Stata/25-05-08-GLM.html#logistic-回归的比较",
    "title": "14-广义线性模型（GLM）",
    "section": "10 logistic 回归的比较",
    "text": "10 logistic 回归的比较\n重新导入数据：\n\n%%stata\nwebuse lbw,clear\n\n(Hosmer & Lemeshow data)\n\n\n\n%%stata\nlogistic low age i.smoke i.race\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(4)    =  15.81\n                                                        Prob &gt; chi2   = 0.0033\nLog likelihood = -109.4311                              Pseudo R2     = 0.0674\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9657186   .0322573    -1.04   0.296     .9045206    1.031057\n             |\n       smoke |\n     Smoker  |    3.00582   1.118001     2.96   0.003     1.449982    6.231081\n             |\n        race |\n      Black  |   2.749483   1.356659     2.05   0.040     1.045318    7.231924\n      Other  |   2.876948   1.167921     2.60   0.009     1.298314    6.375062\n             |\n       _cons |    .365111   .3146026    -1.17   0.242     .0674491    1.976395\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\n%%stata\nglm low age i.smoke i.race,family(binomial) link(logit) eform\n\n\nIteration 0:  Log likelihood = -109.53148  \nIteration 1:  Log likelihood = -109.43111  \nIteration 2:  Log likelihood =  -109.4311  \nIteration 3:  Log likelihood =  -109.4311  \n\nGeneralized linear models                         Number of obs   =        189\nOptimization     : ML                             Residual df     =        184\n                                                  Scale parameter =          1\nDeviance         =  218.8621974                   (1/df) Deviance =   1.189468\nPearson          =  182.9642078                   (1/df) Pearson  =   .9943707\n\nVariance function: V(u) = u*(1-u)                 [Bernoulli]\nLink function    : g(u) = ln(u/(1-u))             [Logit]\n\n                                                  AIC             =   1.210911\nLog likelihood   = -109.4310987                   BIC             =  -745.6193\n\n------------------------------------------------------------------------------\n             |                 OIM\n         low | Odds ratio   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9657186   .0322573    -1.04   0.296     .9045206    1.031057\n             |\n       smoke |\n     Smoker  |    3.00582   1.118001     2.96   0.003     1.449982    6.231081\n             |\n        race |\n      Black  |   2.749483   1.356659     2.05   0.040     1.045318    7.231924\n      Other  |   2.876948   1.167921     2.60   0.009     1.298314    6.375062\n             |\n       _cons |    .365111   .3146026    -1.17   0.242     .0674491    1.976395\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GLM.html#log-binomial-model",
    "href": "Guide/Stata/25-05-08-GLM.html#log-binomial-model",
    "title": "14-广义线性模型（GLM）",
    "section": "11 Log-Binomial Model",
    "text": "11 Log-Binomial Model\nglm low age i.smoke i.race,family(binomial) link(log)\n\nLogistic regression 的一种替代\nFail to converge\nPoisson regression with robust variance estimate\n\n为了避免出现 Fail to converge 的问题，采用稳健方差估计：\nglm low age i.smoke i.race,family(poisson) link(log) robust\n\nrobust 可以缩写为 r\n\n\n%%stata\nglm low age i.smoke i.race,family(poisson) link(log) robust\n\n\nIteration 0:  Log pseudolikelihood = -124.55888  \nIteration 1:  Log pseudolikelihood = -122.39663  \nIteration 2:  Log pseudolikelihood = -122.39591  \nIteration 3:  Log pseudolikelihood = -122.39591  \n\nGeneralized linear models                         Number of obs   =        189\nOptimization     : ML                             Residual df     =        184\n                                                  Scale parameter =          1\nDeviance         =   126.791811                   (1/df) Deviance =   .6890859\nPearson          =  124.4629927                   (1/df) Pearson  =   .6764293\n\nVariance function: V(u) = u                       [Poisson]\nLink function    : g(u) = ln(u)                   [Log]\n\n                                                  AIC             =   1.348105\nLog pseudolikelihood = -122.3959055               BIC             =  -837.6896\n\n------------------------------------------------------------------------------\n             |               Robust\n         low | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |  -.0246781   .0203034    -1.22   0.224    -.0644722    .0151159\n             |\n       smoke |\n     Smoker  |   .6972155    .211848     3.29   0.001     .2820011     1.11243\n             |\n        race |\n      Black  |    .639629   .2818353     2.27   0.023     .0872419    1.192016\n      Other  |   .6813692   .2428128     2.81   0.005     .2054649    1.157273\n             |\n       _cons |  -1.286544   .5405667    -2.38   0.017    -2.346035   -.2270526\n------------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "14-广义线性模型（GLM）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-multi-reg.html",
    "href": "Guide/Stata/25-05-07-multi-reg.html",
    "title": "12-多元线性回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-multi-reg.html#导入数据",
    "href": "Guide/Stata/25-05-07-multi-reg.html#导入数据",
    "title": "12-多元线性回归",
    "section": "1 导入数据",
    "text": "1 导入数据\n\n%%stata\nsysuse auto.dta,clear\n\n(1978 automobile data)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-multi-reg.html#区分简单线性回归和多元线性回归",
    "href": "Guide/Stata/25-05-07-multi-reg.html#区分简单线性回归和多元线性回归",
    "title": "12-多元线性回归",
    "section": "2 区分简单线性回归和多元线性回归",
    "text": "2 区分简单线性回归和多元线性回归\n简单线性回归形如：\\(y=\\beta_0+\\beta_1 x\\)，例如：\n\n%%stata\nreg price weight\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(1, 72)        =     29.42\n       Model |   184233937         1   184233937   Prob &gt; F        =    0.0000\n    Residual |   450831459        72  6261548.04   R-squared       =    0.2901\n-------------+----------------------------------   Adj R-squared   =    0.2802\n       Total |   635065396        73  8699525.97   Root MSE        =    2502.3\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   2.044063   .3768341     5.42   0.000     1.292857    2.795268\n       _cons |  -6.707353    1174.43    -0.01   0.995     -2347.89    2334.475\n------------------------------------------------------------------------------\n\n\n\\(\\beta_1\\)：汽车的重量每增加一个单位，售价增加 2.04 个单位(95%CI:1.29,2.80)\n_cons 是 \\(\\beta_0\\)，即是截距，但是要注意符号与实际的区别\n多元线性模型即不止一个自变量，形如：\\(y=\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 + \\dots\\)\n\n%%stata\nreg price weight length\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(2, 71)        =     18.91\n       Model |   220725280         2   110362640   Prob &gt; F        =    0.0000\n    Residual |   414340116        71  5835776.28   R-squared       =    0.3476\n-------------+----------------------------------   Adj R-squared   =    0.3292\n       Total |   635065396        73  8699525.97   Root MSE        =    2415.7\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   4.699065   1.122339     4.19   0.000     2.461184    6.936946\n      length |  -97.96031    39.1746    -2.50   0.015    -176.0722   -19.84838\n       _cons |   10386.54   4308.159     2.41   0.019     1796.316    18976.76\n------------------------------------------------------------------------------\n\n\n\\(\\beta_2\\)(length)：在控制 weight 变量后，汽车的 length 每增加一个单位，售价减少 97.96 个单位(95%CI:-176.07,2.-19.85)\n\n%%stata\nreg price weight length mpg\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(3, 70)        =     12.98\n       Model |   226957412         3  75652470.6   Prob &gt; F        =    0.0000\n    Residual |   408107984        70  5830114.06   R-squared       =    0.3574\n-------------+----------------------------------   Adj R-squared   =    0.3298\n       Total |   635065396        73  8699525.97   Root MSE        =    2414.6\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   4.364798   1.167455     3.74   0.000     2.036383    6.693213\n      length |  -104.8682   39.72154    -2.64   0.010    -184.0903   -25.64607\n         mpg |  -86.78928   83.94335    -1.03   0.305     -254.209    80.63046\n       _cons |   14542.43   5890.632     2.47   0.016      2793.94    26290.93\n------------------------------------------------------------------------------\n\n\n\n%%stata\nreg price weight length mpg rep78\n\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(4, 64)        =     12.68\n       Model |   255066807         4  63766701.9   Prob &gt; F        =    0.0000\n    Residual |   321730151        64  5027033.62   R-squared       =    0.4422\n-------------+----------------------------------   Adj R-squared   =    0.4074\n       Total |   576796959        68  8482308.22   Root MSE        =    2242.1\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   4.959534   1.119624     4.43   0.000     2.722827    7.196241\n      length |  -115.0177   38.56456    -2.98   0.004    -192.0592   -37.97612\n         mpg |  -106.7122   81.15836    -1.31   0.193    -268.8446    55.42027\n       rep78 |   910.9859   304.5274     2.99   0.004     302.6226    1519.349\n       _cons |   11934.51   5774.178     2.07   0.043     399.2604    23469.75\n------------------------------------------------------------------------------\n\n\n\\(\\beta_4\\) (rep78): 在控制了汽车的重量、长度、里程后，汽车的修理次数每增加一个单位，售价增加910.99个单位(95%CI:302.62,1519.35)\n这里 rep78 是一个连续变量，如果处理为多分类变量（哑变量）\n\n2.1 什么是哑变量？为什么要设置哑变量？\n当自变量X为多分类变量时，例如职业、学历、血型、疾病严重程度等等此时仅用一个回归系数来解释多分类变量之间的变化关系，及其对因变量Y的影响，就显得太不理想。\n\n\n2.2 如何设置哑变量\n\n在多分类变量前加上 “i.” 告诉Stata这不是连续变量\n形如：\n\n\n%%stata\nreg price weight length mpg i.rep78\n\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(7, 61)        =      7.25\n       Model |   262008114         7  37429730.6   Prob &gt; F        =    0.0000\n    Residual |   314788844        61  5160472.86   R-squared       =    0.4542\n-------------+----------------------------------   Adj R-squared   =    0.3916\n       Total |   576796959        68  8482308.22   Root MSE        =    2271.7\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |   5.186695   1.163383     4.46   0.000     2.860367    7.513022\n      length |  -124.1544   40.07637    -3.10   0.003     -204.292   -44.01671\n         mpg |  -126.8367   84.49819    -1.50   0.138    -295.8012    42.12791\n             |\n       rep78 |\n          2  |   1137.284   1803.332     0.63   0.531    -2468.701    4743.269\n          3  |   1254.642   1661.545     0.76   0.453    -2067.823    4577.108\n          4  |   2267.188   1698.018     1.34   0.187    -1128.208    5662.584\n          5  |   3850.759   1787.272     2.15   0.035     276.8886     7424.63\n             |\n       _cons |   14614.49   6155.842     2.37   0.021     2305.125    26923.86\n------------------------------------------------------------------------------\n\n\nrep78 2:在控制了汽车的重量、长度、里程后，汽 车的修理次数两次和一次相比，售价增加1137.28个单 位(95%CI:-2468.70,4743.27)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-multi-reg.html#回归分析需要注意的事情",
    "href": "Guide/Stata/25-05-07-multi-reg.html#回归分析需要注意的事情",
    "title": "12-多元线性回归",
    "section": "3 回归分析需要注意的事情",
    "text": "3 回归分析需要注意的事情\n\n时刻注意纳入模型的观测值数量\n\nCase-wise Deletion：只有纳入模型的变量都没有缺失值，这个观测值才会被纳入回归模型分析（有缺失会被系统默认剔除）\n\n如何让\\(\\beta_0\\)有意义？\n\n使用\\(y=\\beta_0+\\beta_1(x-\\bar x)\\)\n让\\(\\beta_0\\geq 0\\)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "12-多元线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html",
    "href": "Guide/Stata/25-05-07-ANOVA.html",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html#方差分析的假设",
    "href": "Guide/Stata/25-05-07-ANOVA.html#方差分析的假设",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.1 方差分析的假设",
    "text": "1.1 方差分析的假设\n\n假设1:y变量为连续变量\n假设2:有一个包含2个及以上分类、且组别间相互独立的x变量\n假设3:每组间和组内的观测值相互独立\n假设4:每组内没有明显异常值\n假设5:每组内y变量符合正态分布\n假设6:进行方差齐性检验，观察每组的方差是否相等\n\n总结为：独立、正态、方差齐",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html#数据导入",
    "href": "Guide/Stata/25-05-07-ANOVA.html#数据导入",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.2 数据导入",
    "text": "1.2 数据导入\n\n%%stata\nwebuse systolic,clear\n\n(Systolic blood pressure data)\n\n\n\n1.2.1 查看一下数据形式与结构\n\n%%stata\nlist\n\n\n     +---------------------------+\n     | drug   disease   systolic |\n     |---------------------------|\n  1. |    1         1         42 |\n  2. |    1         1         44 |\n  3. |    1         1         36 |\n  4. |    1         1         13 |\n  5. |    1         1         19 |\n     |---------------------------|\n  6. |    1         1         22 |\n  7. |    1         2         33 |\n  8. |    1         2         26 |\n  9. |    1         2         33 |\n 10. |    1         2         21 |\n     |---------------------------|\n 11. |    1         3         31 |\n 12. |    1         3         -3 |\n 13. |    1         3         25 |\n 14. |    1         3         25 |\n 15. |    1         3         24 |\n     |---------------------------|\n 16. |    2         1         28 |\n 17. |    2         1         23 |\n 18. |    2         1         34 |\n 19. |    2         1         42 |\n 20. |    2         1         13 |\n     |---------------------------|\n 21. |    2         2         34 |\n 22. |    2         2         33 |\n 23. |    2         2         31 |\n 24. |    2         2         36 |\n 25. |    2         3          3 |\n     |---------------------------|\n 26. |    2         3         26 |\n 27. |    2         3         28 |\n 28. |    2         3         32 |\n 29. |    2         3          4 |\n 30. |    2         3         16 |\n     |---------------------------|\n 31. |    3         1          1 |\n 32. |    3         1         29 |\n 33. |    3         1         19 |\n 34. |    3         2         11 |\n 35. |    3         2          9 |\n     |---------------------------|\n 36. |    3         2          7 |\n 37. |    3         2          1 |\n 38. |    3         2         -6 |\n 39. |    3         3         21 |\n 40. |    3         3          1 |\n     |---------------------------|\n 41. |    3         3          9 |\n 42. |    3         3          3 |\n 43. |    4         1         24 |\n 44. |    4         1          9 |\n 45. |    4         1         22 |\n     |---------------------------|\n 46. |    4         1         -2 |\n 47. |    4         1         15 |\n 48. |    4         2         27 |\n 49. |    4         2         12 |\n 50. |    4         2         12 |\n     |---------------------------|\n 51. |    4         2         -5 |\n 52. |    4         2         16 |\n 53. |    4         2         15 |\n 54. |    4         3         22 |\n 55. |    4         3          7 |\n     |---------------------------|\n 56. |    4         3         25 |\n 57. |    4         3          5 |\n 58. |    4         3         12 |\n     +---------------------------+\n\n\n\n%%stata\ncodebook drug\n\n\n-------------------------------------------------------------------------------\ndrug                                                                  Drug used\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (int)\n\n                 Range: [1,4]                         Units: 1\n         Unique values: 4                         Missing .: 0/58\n\n            Tabulation: Freq.  Value\n                           15  1\n                           15  2\n                           12  3\n                           16  4\n\n\n\n%%stata\nsum systolic,detail\n\n\n                 Increment in systolic b.p.\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%           -6             -6\n 5%           -3             -5\n10%            1             -3       Obs                  58\n25%            9             -2       Sum of wgt.          58\n\n50%           21                      Mean           18.87931\n                        Largest       Std. dev.      12.80087\n75%           28             36\n90%           34             42       Variance       163.8624\n95%           42             42       Skewness       -.094992\n99%           44             44       Kurtosis        2.13251",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html#假设4每组内没有明显的异常值",
    "href": "Guide/Stata/25-05-07-ANOVA.html#假设4每组内没有明显的异常值",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.3 假设4：每组内没有明显的异常值",
    "text": "1.3 假设4：每组内没有明显的异常值\n最直观的方式就是用箱型图进行展示数据分布\n\n%%stata\ngraph box systolic,over(drug)\n\n\n\n\n\n\n\n\n\n%%stata\ngraph box systolic",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html#假设5每组内y变量符合正态分布",
    "href": "Guide/Stata/25-05-07-ANOVA.html#假设5每组内y变量符合正态分布",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.4 假设5：每组内y变量符合正态分布",
    "text": "1.4 假设5：每组内y变量符合正态分布\n一般使用下面三种方法中的一种检验正态性就可以，一般使用 Shapiro-Wilk检验 较为普遍。\n\n1.4.1 偏度与峰度\n使用 sktest 检验偏度与峰度，P值大于0.05，就不能说不符合正态分布\n\n%%stata\nsktest systolic\n\n\nSkewness and kurtosis tests for normality\n                                                         ----- Joint test -----\n    Variable |       Obs   Pr(skewness)   Pr(kurtosis)   Adj chi2(2)  Prob&gt;chi2\n-------------+-----------------------------------------------------------------\n    systolic |        58         0.7452         0.0529          4.03     0.1331\n\n\n\n\n1.4.2 Shapiro-Wilk W test\nShapiro-Wilk检验，是一种基于相关性的算法。计算可得到一个相关系数，它越接近1就越表明数据和正态分布拟合得越好。\n它基于Shapiro和Wilk于1965年提出的检验统计量。以下是其基本原理和用途：\n基本原理：\n\n零假设（Null Hypothesis）：Shapiro-Wilk检验的零假设是数据集来自于正态分布。这意味着，如果数据确实服从正态分布，则零假设成立。\n计算Shapiro-Wilk统计量：检验首先计算Shapiro-Wilk统计量，这是一个衡量数据与正态分布拟合的度量。该统计量基于数据的观察值和正态分布的期望值之间的差异。//Shapiro-Wilk检验的统计量（W统计量）是通过与理论正态分布的期望值进行比较来判断样本数据是否符合正态分布。Shapiro-Wilk检验的原假设是样本数据符合正态分布。统计量的计算基于样本数据的排序顺序和回归分析的概念。W统计量越接近1，表示样本数据越接近正态分布。\n与临界值比较：接下来，Shapiro-Wilk统计量与临界值进行比较。临界值是根据所选的显著性水平（通常为5%）和数据集的大小计算得出的。如果Shapiro-Wilk统计量小于临界值，就意味着数据不太可能来自于正态分布。\n做出决策：根据统计量与临界值的比较，可以决定是否拒绝零假设。如果统计量足够小，小于临界值，通常会拒绝零假设，这意味着数据不服从正态分布。否则，不能拒绝零假设，这表示数据可能服从正态分布。\n\nNotices： Shapiro-Wilk正态性检验对检验样本大小有一定的要求。具体来说，Shapiro-Wilk检验在样本大小较小（通常小于大约50-200，具体取决于不同文献和实践）时可能不太适用，并且在这种情况下其效力可能会降低。\n\n%%stata\nswilk systolic\n\n\n                   Shapiro–Wilk W test for normal data\n\n    Variable |        Obs       W           V         z       Prob&gt;z\n-------------+------------------------------------------------------\n    systolic |         58    0.97803      1.162     0.323    0.37331\n\n\nShapiro-Francia test\nShapiro-Francia检验是一种用于检验数据是否来自正态分布的统计方法。它是Shapiro-Wilk检验的一个变种，通常适用于小到中等样本大小的数据集。Shapiro-Francia检验的核心思想是通过计算统计量来评估数据的正态性。\nShapiro-Francia检验的零假设是数据来自正态分布，而备择假设是数据不来自正态分布。检验的结果会生成一个p-value，如果p-value较小（通常小于0.05），则通常会拒绝零假设，表明数据不符合正态分布。如果p-value较大，则无法拒绝零假设，表明数据可能来自正态分布。\n这种检验方法对于小到中等样本大小的数据集通常效果良好，并且可以用于确定数据是否符合正态分布的假设。\n虽然Shapiro-Francia检验在小样本中通常效果较好，但它也依赖于权重参数的准确性，因此在某些情况下可能不如其他正态性检验方法稳健。因此，在使用Shapiro-Francia检验时，应谨慎选择合适的样本大小和检验方法，以确保可靠的结果。\n\n%%stata\nsfrancia systolic\n\n\n                  Shapiro–Francia W' test for normal data\n\n    Variable |       Obs       W'          V'        z       Prob&gt;z\n-------------+-----------------------------------------------------\n    systolic |        58    0.98522      0.866    -0.275    0.60824",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-ANOVA.html#单因素方差分析",
    "href": "Guide/Stata/25-05-07-ANOVA.html#单因素方差分析",
    "title": "10-单因素方差分析（ANOVA）",
    "section": "1.5 单因素方差分析",
    "text": "1.5 单因素方差分析\n语法：\noneway yvar xvar,[,option]\n如果得到的 Prob&gt;F 结果是 &gt;0.05 则说明至少有两个组的平均值是有显著差别的。\nProb&gt;chi2 = 得到的结果如果 &gt;0.05 则说明各组满足方差齐性。\n\n%%stata\noneway systolic drug\n\n\n                        Analysis of variance\n    Source              SS         df      MS            F     Prob &gt; F\n------------------------------------------------------------------------\nBetween groups      3133.23851      3   1044.41284      9.09     0.0001\n Within groups      6206.91667     54   114.942901\n------------------------------------------------------------------------\n    Total           9340.15517     57   163.862371\n\nBartlett's equal-variances test: chi2(3) =   1.0063    Prob&gt;chi2 = 0.800\n\n\n\n1.5.1 使用两两比较\n一般使用 bonferroni 比较\n\n%%stata\noneway systolic drug, bonferroni \n\n\n                        Analysis of variance\n    Source              SS         df      MS            F     Prob &gt; F\n------------------------------------------------------------------------\nBetween groups      3133.23851      3   1044.41284      9.09     0.0001\n Within groups      6206.91667     54   114.942901\n------------------------------------------------------------------------\n    Total           9340.15517     57   163.862371\n\nBartlett's equal-variances test: chi2(3) =   1.0063    Prob&gt;chi2 = 0.800\n\n            Comparison of Increment in systolic b.p. by Drug used\n                                (Bonferroni)\nRow Mean-|\nCol Mean |          1          2          3\n---------+---------------------------------\n       2 |   -.533333\n         |      1.000\n         |\n       3 |   -17.3167   -16.7833\n         |      0.001      0.001\n         |\n       4 |   -12.5667   -12.0333       4.75\n         |      0.012      0.017      1.000\n\n\n\n%%stata\noneway systolic drug, bonferroni tab\n\n\n            |  Summary of Increment in systolic\n            |                b.p.\n  Drug used |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n          1 |   26.066667   11.677002          15\n          2 |   25.533333    11.61813          15\n          3 |        8.75     10.0193          12\n          4 |        13.5   9.3238047          16\n------------+------------------------------------\n      Total |    18.87931   12.800874          58\n\n                        Analysis of variance\n    Source              SS         df      MS            F     Prob &gt; F\n------------------------------------------------------------------------\nBetween groups      3133.23851      3   1044.41284      9.09     0.0001\n Within groups      6206.91667     54   114.942901\n------------------------------------------------------------------------\n    Total           9340.15517     57   163.862371\n\nBartlett's equal-variances test: chi2(3) =   1.0063    Prob&gt;chi2 = 0.800\n\n            Comparison of Increment in systolic b.p. by Drug used\n                                (Bonferroni)\nRow Mean-|\nCol Mean |          1          2          3\n---------+---------------------------------\n       2 |   -.533333\n         |      1.000\n         |\n       3 |   -17.3167   -16.7833\n         |      0.001      0.001\n         |\n       4 |   -12.5667   -12.0333       4.75\n         |      0.012      0.017      1.000",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "10-单因素方差分析（ANOVA）"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-chi2-fisher-test.html",
    "href": "Guide/Stata/25-05-06-chi2-fisher-test.html",
    "title": "08-卡方检验与Fisher精确检验",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "08-卡方检验与Fisher精确检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-chi2-fisher-test.html#数据导入",
    "href": "Guide/Stata/25-05-06-chi2-fisher-test.html#数据导入",
    "title": "08-卡方检验与Fisher精确检验",
    "section": "1 数据导入",
    "text": "1 数据导入\n此章节使用 Stata 自带的 1988 年 美国的 National Longitudinal Study of Young Women Data 进行分析和示例。\n\n%%stata\nsysuse nlsw88.dta, clear\n\n(NLSW, 1988 extract)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "08-卡方检验与Fisher精确检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-chi2-fisher-test.html#chi2-test",
    "href": "Guide/Stata/25-05-06-chi2-fisher-test.html#chi2-test",
    "title": "08-卡方检验与Fisher精确检验",
    "section": "2 \\(\\chi^2\\) test",
    "text": "2 \\(\\chi^2\\) test\n卡方检验(\\(\\chi^2\\))，主要用于检验两个或多个分类变量之间的关联性。它通过比较实际观察频数与期望频数之间的差异来判断变量是否独立，从而揭示不同类别之间是否存在统计学上的显著关系。\n基本语法：\ntabulate var1 var2,chi2\ntabulate 可以简写为 tab，后续都用 tab 替代 tabulate\n\n%%stata\ntab race married, chi2\n\n\n           |        Married\n      Race |    Single    Married |     Total\n-----------+----------------------+----------\n     White |       487      1,150 |     1,637 \n     Black |       309        274 |       583 \n     Other |         8         18 |        26 \n-----------+----------------------+----------\n     Total |       804      1,442 |     2,246 \n\n          Pearson chi2(2) = 101.4215   Pr = 0.000\n\n\n\n2.1 查看每一个单元格对于 \\(\\chi^2\\) 检验的贡献:\n语法：\ntab var1 var2, cchi2 chi2\n\n%%stata\ntab race married, cchi2 chi2\n\n\n+-------------------+\n| Key               |\n|-------------------|\n|     frequency     |\n| chi2 contribution |\n+-------------------+\n\n           |        Married\n      Race |    Single    Married |     Total\n-----------+----------------------+----------\n     White |       487      1,150 |     1,637 \n           |      16.7        9.3 |      26.0 \n-----------+----------------------+----------\n     Black |       309        274 |       583 \n           |      48.2       26.9 |      75.1 \n-----------+----------------------+----------\n     Other |         8         18 |        26 \n           |       0.2        0.1 |       0.3 \n-----------+----------------------+----------\n     Total |       804      1,442 |     2,246 \n           |      65.1       36.3 |     101.4 \n\n          Pearson chi2(2) = 101.4215   Pr = 0.000\n\n\n\n\n2.2 查看期望频数（理论频数）\n语法：\ntab var1 var2, chi2 expected\n\n%%stata\ntab race married, chi2 expected\n\n\n+--------------------+\n| Key                |\n|--------------------|\n|     frequency      |\n| expected frequency |\n+--------------------+\n\n           |        Married\n      Race |    Single    Married |     Total\n-----------+----------------------+----------\n     White |       487      1,150 |     1,637 \n           |     586.0    1,051.0 |   1,637.0 \n-----------+----------------------+----------\n     Black |       309        274 |       583 \n           |     208.7      374.3 |     583.0 \n-----------+----------------------+----------\n     Other |         8         18 |        26 \n           |       9.3       16.7 |      26.0 \n-----------+----------------------+----------\n     Total |       804      1,442 |     2,246 \n           |     804.0    1,442.0 |   2,246.0 \n\n          Pearson chi2(2) = 101.4215   Pr = 0.000",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "08-卡方检验与Fisher精确检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-chi2-fisher-test.html#fishers-exact-test",
    "href": "Guide/Stata/25-05-06-chi2-fisher-test.html#fishers-exact-test",
    "title": "08-卡方检验与Fisher精确检验",
    "section": "3 Fisher’s exact test",
    "text": "3 Fisher’s exact test\n当列联表中理论频数(期望频数)&lt;5时，我们可以增加样本量、删去理论频数太少的行或列、或者合并某些行或列。\n当然也可以使用Fisher精确检验来检验，并且任何样本量都可以使用Fisher精确检验。\n语法：\ntab var1 var2, exact\n\n%%stata\ntab race married, exact\n\n\nEnumerating sample-space combinations:\nstage 3:  enumerations = 1\nstage 2:  enumerations = 27\nstage 1:  enumerations = 0\n\n           |        Married\n      Race |    Single    Married |     Total\n-----------+----------------------+----------\n     White |       487      1,150 |     1,637 \n     Black |       309        274 |       583 \n     Other |         8         18 |        26 \n-----------+----------------------+----------\n     Total |       804      1,442 |     2,246 \n\n           Fisher's exact =                 0.000",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "08-卡方检验与Fisher精确检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-chi2-fisher-test.html#notices",
    "href": "Guide/Stata/25-05-06-chi2-fisher-test.html#notices",
    "title": "08-卡方检验与Fisher精确检验",
    "section": "4 Notices",
    "text": "4 Notices\n在一般的统计学教程或书籍中，有如下的说明：\n\n总例数≥40，所有理论频数≥5，看Pearson Chi-Square结果;\n总例数≥40，出现1个理论频数≥1且&lt;5，\\(\\chi^2\\)检验需进行连续性校正，这时以Continuity Correction结果为准\n总例数≥40，至少2个理论频数≥1且&lt;5，看Fisher’s Exact Test结果;\n总例数&lt;40或者出现理论频数&lt;1，看Fisher’s Exact Test结果\n\n针对上述 第2点 中对\\(\\chi^2\\)检验需进行连续性校正，在 Stata 中一般不是必须的：\n\nStata不自带卡方检验的连续性矫正\nStata有用户自写的package可以实现连续性矫正，但是并不推荐:卡方检验的连续性矫正并不是必须的、也不是最推荐的方法\n在样本量足够大的时候，使用卡方检验时，是否使用卡方检验的连续性校正区别很小;使用Fisher精确检验也是没有问题的\n在样本量小的时候(通常是某个格子期望频&lt;5)，可直接使用Fisher精确检验，亦不需要使用“卡方检验+连续性矫正”",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "08-卡方检验与Fisher精确检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#lowess命令的语法",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#lowess命令的语法",
    "title": "05-双变量作图",
    "section": "2.1 lowess命令的语法：",
    "text": "2.1 lowess命令的语法：\nlowess yvar xvar [if] [in] [, options]\n其中，yvar 是因变量，xvar 是自变量。options 是一些可选参数，用来进一步调整光滑曲线的形状和拟合效果。例如，可以通过指定 span 参数来控制局部回归的窗口大小，较小的 span 值会导致更平滑的曲线，而较大的 span 值会导致更接近原始数据的曲线。\noptions 有如下选项：\n\n\n\nLowess-syntax\n\n\nlowess 命令生成的光滑曲线可以通过绘图命令来展示，例如使用 twoway scatter 命令可以在散点图上叠加绘制光滑曲线。lowess — Lowess smoothing",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-05-scatter-plot.html#lowess-命令的示例",
    "href": "Guide/Stata/25-05-05-scatter-plot.html#lowess-命令的示例",
    "title": "05-双变量作图",
    "section": "2.2 lowess 命令的示例：",
    "text": "2.2 lowess 命令的示例：\n\n\n代码\n%%stata\nsysuse auto\nlowess mpg weight, gen(lowess_mpg)\ntwoway scatter mpg weight || line lowess_mpg weight\n\n\n\n. sysuse auto\n(1978 automobile data)\n\n. lowess mpg weight, gen(lowess_mpg)\n\n. twoway scatter mpg weight || line lowess_mpg weight\n\n. \n\n\n\n\n\n\n\n\n\n上述代码首先载入Stata自带的汽车数据集auto，然后使用lowess命令生成一个光滑曲线，将结果保存在变量lowess_mpg中。最后利用twoway scatter命令绘制散点图，并使用line命令在散点图上叠加绘制光滑曲线。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "05-双变量作图"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "",
    "text": "代码\n%load_ext saspy.sas_magic",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#在-jupyter-notebook-中使用-sas",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#在-jupyter-notebook-中使用-sas",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "1 在 Jupyter Notebook 中使用 SAS",
    "text": "1 在 Jupyter Notebook 中使用 SAS\n\n1.1 环境准备\n安装 Anaconda 集成环境或 Python 和 SAS 软件，其中Anaconda要求Python3+；Python在Jupyter Notebook和SAS之间起一个桥梁的作用，Jupyter Notebook中的SAS代码会交给Python，Python负责将代码传递给SAS执行；然后将执行的结果返回给Jupyter Notebook显示。\nSAS版本要求9.4，也可以是 SAS Viya。\n\n\n1.2 安装SAS_KERNEL\n启动 cmd，输入命令：\npip install sas_kernel\n然后就会自动安装 sas_kernel 及其相应的依赖项。\n安装完成后可以输入命令：\njupyter kernelspec list\n来检测 sas_kernel 是否安装成功，如果成功，理论上会看到如下形式的输出：\nAvailable kernels:\n    python3    /home/sas/anaconda3/lib/python3.5/site-packages/ipykernel/resources\n    sas        /home/sas/.local/share/jupyter/kernels/sas\n\n\n1.3 修改 Python 配置文件\n安装好 sas_kernel 后找到 Anaconda 或 Python 的安装目录，会有一个相应的文件夹出现，例如我的文件路径如下：\nC:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\saspy\n在这个文件路径下找到 sascfg.py 文件，该文件中需要配置连接SAS的信息。可以配置连接本地机器的SAS；也可以配置连接远程机器的SAS Server，无论是Linux Server还是Windows Server都可以。此处就以连接本地SAS为例进行说明。\n\n打开该文件，首先是一大段注释；\n在这段注释后定义的第一个变量 SAS_config_names 用于指定连接SAS的配置方式，提供了 10 种方式：default, ssh, iomlinux, iomwin, winlocal, winiomlinux, winiomwin, httpsviya, httpviya, iomcom。默认为 default 方式。\n因为我们需要连接Windows机器本地的SAS，所以需要将 SAS_config_names 的值修改为 winlocal 。\n\n\n后续有一些安装步骤，但是大多是在2016-2020年更新的教程，无法找到复现的路径，可能相关的配置已被优化。\n包括这个 SAS岩论 | 在Jupyter Notebook中使用SAS 中写到的需要使用 cpW 定义 SAS 路径。\n目前（2025年），我摸索出来的办法是：\n修改 saspath 路径，指定 sas.exe 文件,路径形如：C:/Program Files/SASHome/SASFoundation/9.4/sas.exe\n\n\n\nsaspath\n\n\n修改 encoding 为 euc-cn: 将winlocal连接方式中的参数 encoding 的值修改为 euc-cn 。因为这是SAS在Windows下的默认编码方式。如果编码方式不对，中文会出现乱码。或者 utf8，如果和 SAS 软件有冲突，可能会造成 SAS 软件乱码。\n\n\n1.4 修改系统变量\n将sas相关文件 sspiauth.dll 添加到系统环境变量，该文件很可能在如下目录：\nC:\\Program Files\\SASHome\\SASFoundation\\9.4\\core\\sasext\n（注意添加变量时不要包含 sspiauth.dll 文件本身）\nWarning: 环境变量添加完成后，要重启电脑才会生效。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#在-jupyter-notebook-中使用-sas-1",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#在-jupyter-notebook-中使用-sas-1",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "2 在 jupyter notebook 中使用 SAS",
    "text": "2 在 jupyter notebook 中使用 SAS\n新建文件，选择使用 SAS 内核，或者在 cell 中通过 magic command 指定内核。\n%%SAS\nwarning: 在单元格中直接调用 SAS 之前，需要在文档中加载一次 magic 扩展 以定义 %%SAS\n%load_ext saspy.sas_magic\n使用语法如下所示：\n\n\n代码\n%%SAS\ndata prg1_1;\n    input x @@;\ndatalines;\n60 142 195 80 242 220 190 25 212 38 236 95\n;\nrun;\nproc means data=prg1_1;\n    var x;\n    quit;\n\n\nUsing SAS Config named: winlocal\nSAS Connection established. Subprocess id is 8788\n\n\n\n\n\n\n\n\nSAS 输出\n\n\n\n\n\nSAS 系统 \n\n\nMEANS PROCEDURE\n\n\n\n\n\n\n分析变量: x\n\n\n数目\n均值\n标准差\n最小值\n最大值\n\n\n\n\n12\n144.5833333\n80.9797487\n25.0000000\n242.0000000\n\n\n\n\n\n\n\n\n\n\n在Notebook中写SAS代码了，跟Python一样，同样有代码提示、语法高亮的功能。但是你会注意到过程步的结果显示了，运行的日志去哪里了？\n如果代码运行错误或者没有输出（例如纯DATA步）的话，那么输出就是日志信息。\n能够正确运行且有输出结果的代码就不会显示日志了。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#安装sas日志组件",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#安装sas日志组件",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "3 安装SAS日志组件",
    "text": "3 安装SAS日志组件\n如果想要像SAS Base一样，随时查看所有程序运行的日志结果也没问题。安装一个 Notebook 的 SAS 日志扩展组件就可以了。打开 Anaconda Prompt，输入以下命令安装：\njupyter nbextension install --py sas_kernel.showSASLog\n运行完毕后，输入以下命令启用 SAS 日志组件：\njupyter nbextension enable sas_kernel.showSASLog –py",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#连接sas-server",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#连接sas-server",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "4 连接SAS Server",
    "text": "4 连接SAS Server\n如果需要配置连接远程的SAS Server，如连接远程Windows机器的SAS Server，需在sascfg.py中做以下修改：\n\n将SAS_config_names的值改为“wintowin”；\n在wintowin连接方式中将参数iomhost的值修改为远程Windows机器的IP地址；将参数encoding的值修改为euc-cn；\n将cpW中5个Jar包的路径修改为远程Windows机器中SAS对应的目录。\n\n修改完毕后，启动Notebook，首次运行SAS代码时，会提示输入访问SAS Server的有效SAS用户和密码。1",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#典型生态项目",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#典型生态项目",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "5 典型生态项目",
    "text": "5 典型生态项目\n\n5.1 SASPy\nSASPy 是一个 Python 库，允许你通过 Python 代码与 SAS 进行交互。SAS Kernel 依赖于 SASPy，因此在使用 SAS Kernel 之前，你需要配置 SASPy。\n\n\n5.2 JupyterLab 扩展\nSAS Kernel 支持 JupyterLab 扩展，这些扩展可以提高你在 JupyterLab 中的编程效率。你可以通过以下命令安装这些扩展：\npip install sas_kernel[jlab_ext] \n\n\n5.3 NBGrader\nNBGrader 是一个用于分配和评分 Jupyter Notebook 的系统，它与 SAS Kernel 兼容。你可以使用 NBGrader 来创建和评分包含 SAS 代码的作业。\n通过这些生态项目，SAS Kernel 不仅扩展了 Jupyter Notebook 的功能，还增强了其在数据科学和分析领域的应用能力。2",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-05-05-SAS-install-connect.html#footnotes",
    "href": "Guide/SAS/25-05-05-SAS-install-connect.html#footnotes",
    "title": "02-SAS 在 Jupyter Notebook 中使用",
    "section": "脚注",
    "text": "脚注\n\n\nSAS岩论 | 在Jupyter Notebook中使用SAS↩︎\nSAS Kernel for Jupyter 安装与使用教程↩︎",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "02-SAS 在 Jupyter Notebook 中使用"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#sas-在-linux-的安装",
    "href": "Guide/SAS/25-03-11-SAS-install.html#sas-在-linux-的安装",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "2 SAS 在 Linux 的安装",
    "text": "2 SAS 在 Linux 的安装",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#sas-与-vscode-扩展",
    "href": "Guide/SAS/25-03-11-SAS-install.html#sas-与-vscode-扩展",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "3 SAS 与 vscode 扩展",
    "text": "3 SAS 与 vscode 扩展\nSAS VS Code 扩展轻量级，可在任何地方运行，并允许您集成 SAS 和其他语言。该工具还提供直接连接到 SAS Viya 和 SAS 9 并运行代码的功能。\n\nSAS 语法突出显示和帮助、代码完成和代码片段\n用于连接 SAS 和运行代码的配置文件配置\n支持 SAS Viya 和 SAS 9 连接\n访问 SAS 内容和库\n为 SAS、SQL、Python 和其他语言创建笔记本\n\n扩展程序可在 GitHub 上找到仓库与原代码：[vscode-sas-extension](https://github.com/sassoftware/vscode-sas-extension)\n更多关于 SAS 与 vscode 的信息可以访问：[SAS Extension for Visual Studio Code](https://developer.sas.com/programming/vs_code_extension)\n\n3.1 安装插件\n在 vscode 的扩展页面搜索 “sas” ，第一个 “official SAS ···“ 即为正确扩展：",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#配置路径",
    "href": "Guide/SAS/25-03-11-SAS-install.html#配置路径",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "4 配置路径",
    "text": "4 配置路径\nBefore you can run SAS code, you must configure the SAS extension to access your SAS 9.4 (remote or local) server or a SAS Viya server and add a connection profile.\n在运行 SAS 代码之前，您必须配置 SAS 扩展以访问 SAS 9.4（远程或本地）服务器或 SAS Viya 服务器。您必须获得 SAS 9.4 或 SAS Viya 的许可才能运行 SAS 代码。\n\n打开 SAS 程序文件。\n单击 VS Code 窗口左下方状态栏中的“无配置文件”。\n您还可以打开命令面板（F1，或Ctrl+Shift+P在 Windows 或 Linux 上，或Shift+CMD+P在 OSX 上）并找到SAS: Add New Connection Profile命令。\n按照“添加新连接配置文件”部分中的说明添加配置文件。\n创建配置文件后，状态栏项将从“无配置文件”更改为新配置文件的名称。\n\n\n更多设置可以查看[SAS Extension for Visual Studio Code Documentation](https://sassoftware.github.io/vscode-sas-extension/Configurations/Profiles/sas9local)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#编译-sas-文件",
    "href": "Guide/SAS/25-03-11-SAS-install.html#编译-sas-文件",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "5 编译 SAS 文件",
    "text": "5 编译 SAS 文件\nSAS 文件右上角有一个 奔跑的小人 ，点击即可开始运行所选中的程序段落，并在右侧窗口输出结果。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#在-jupyter-notebook-中使用-sas",
    "href": "Guide/SAS/25-03-11-SAS-install.html#在-jupyter-notebook-中使用-sas",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "6 在 Jupyter Notebook 中使用 SAS",
    "text": "6 在 Jupyter Notebook 中使用 SAS\n\n6.1 环境准备\n安装 Anaconda 集成环境或 Python 和 SAS 软件，其中Anaconda要求Python3+；Python在Jupyter Notebook和SAS之间起一个桥梁的作用，Jupyter Notebook中的SAS代码会交给Python，Python负责将代码传递给SAS执行；然后将执行的结果返回给Jupyter Notebook显示。\nSAS版本要求9.4，也可以是 SAS Viya。\n\n\n6.2 安装SAS_KERNEL\n启动 cmd，输入命令：\npip install sas_kernel\n然后就会自动安装 sas_kernel 及其相应的依赖项。\n安装完成后可以输入命令：\njupyter kernelspec list\n来检测 sas_kernel 是否安装成功，如果成功，理论上会看到如下形式的输出：\nAvailable kernels:\n    python3    /home/sas/anaconda3/lib/python3.5/site-packages/ipykernel/resources\n    sas        /home/sas/.local/share/jupyter/kernels/sas\n\n\n6.3 修改 Python 配置文件\n安装好 sas_kernel 后找到 Anaconda 或 Python 的安装目录，会有一个相应的文件夹出现，例如我的文件路径如下：\nC:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\saspy\n在这个文件路径下找到 sascfg.py 文件，该文件中需要配置连接SAS的信息。可以配置连接本地机器的SAS；也可以配置连接远程机器的SAS Server，无论是Linux Server还是Windows Server都可以。此处就以连接本地SAS为例进行说明。\n\n打开该文件，首先是一大段注释；\n在这段注释后定义的第一个变量 SAS_config_names 用于指定连接SAS的配置方式，提供了 10 种方式：default, ssh, iomlinux, iomwin, winlocal, winiomlinux, winiomwin, httpsviya, httpviya, iomcom。默认为 default 方式。\n因为我们需要连接Windows机器本地的SAS，所以需要将 SAS_config_names 的值修改为 winlocal 。\n\n\n后续有一些安装步骤，但是大多是在2016-2020年更新的教程，无法找到复现的路径，可能相关的配置已被优化。\n包括这个 [SAS岩论 | 在Jupyter Notebook中使用SAS ](https://www.sohu.com/a/218339423_278472) 中写到的需要使用 cpW 定义 SAS 路径。\n\n\n6.4 修改系统变量\n将sas相关文件 sspiauth.dll 添加到系统环境变量，该文件很可能在如下目录：\nC:\\Program Files\\SASHome\\SASFoundation\\9.4\\core\\sasext\n（注意添加变量时不要包含 sspiauth.dll 文件本身）\nWarning: 环境变量添加完成后，要重启电脑才会生效。\n\n\n6.5 在 jupyter notebook 中使用 SAS\n新建文件，选择使用 SAS 内核，或者在 cell 中通过 magic command 指定内核。\n%%sas\n使用语法如下所示：\n%%sas\ndata iris;\n    set sashelp.iris;\nrun;\n\nproc print data=iris(obs=10);\nrun;\n在Notebook中写SAS代码了，跟Python一样，同样有代码提示、语法高亮的功能。但是你会注意到过程步的结果显示了，运行的日志去哪里了？\n如果代码运行错误或者没有输出（例如纯DATA步）的话，那么输出就是日志信息。\n能够正确运行且有输出结果的代码就不会显示日志了。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#安装sas日志组件",
    "href": "Guide/SAS/25-03-11-SAS-install.html#安装sas日志组件",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "7 安装SAS日志组件",
    "text": "7 安装SAS日志组件\n如果想要像SAS Base一样，随时查看所有程序运行的日志结果也没问题。安装一个Notebook的SAS日志扩展组件就可以了。打开Anaconda Prompt，输入以下命令安装：\njupyter nbextension install --py sas_kernel.showSASLog\n运行完毕后，输入以下命令启用SAS日志组件：\njupyter nbextension enable sas_kernel.showSASLog –py",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#连接sas-server",
    "href": "Guide/SAS/25-03-11-SAS-install.html#连接sas-server",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "8 连接SAS Server",
    "text": "8 连接SAS Server\n如果需要配置连接远程的SAS Server，如连接远程Windows机器的SAS Server，需在sascfg.py中做以下修改：\n\n将SAS_config_names的值改为“wintowin”；\n在wintowin连接方式中将参数iomhost的值修改为远程Windows机器的IP地址；将参数encoding的值修改为euc-cn；\n将cpW中5个Jar包的路径修改为远程Windows机器中SAS对应的目录。\n\n修改完毕后，启动Notebook，首次运行SAS代码时，会提示输入访问SAS Server的有效SAS用户和密码。1\n\n8.1 典型生态项目\n\nSASPy\nSASPy 是一个 Python 库，允许你通过 Python 代码与 SAS 进行交互。SAS Kernel 依赖于 SASPy，因此在使用 SAS Kernel 之前，你需要配置 SASPy。\n\n\nJupyterLab 扩展\nSAS Kernel 支持 JupyterLab 扩展，这些扩展可以提高你在 JupyterLab 中的编程效率。你可以通过以下命令安装这些扩展：\npip install sas_kernel[jlab_ext]\n\n\nNBGrader\nNBGrader 是一个用于分配和评分 Jupyter Notebook 的系统，它与 SAS Kernel 兼容。你可以使用 NBGrader 来创建和评分包含 SAS 代码的作业。\n通过这些生态项目，SAS Kernel 不仅扩展了 Jupyter Notebook 的功能，还增强了其在数据科学和分析领域的应用能力。2",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/SAS/25-03-11-SAS-install.html#footnotes",
    "href": "Guide/SAS/25-03-11-SAS-install.html#footnotes",
    "title": "01-SAS 安装与vscode 扩展",
    "section": "脚注",
    "text": "脚注\n\n\nSAS岩论 | 在Jupyter Notebook中使用SAS↩︎\nSAS Kernel for Jupyter 安装与使用教程↩︎",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/SAS/SAS-intro.qmd",
      "01-SAS 安装与vscode 扩展"
    ]
  },
  {
    "objectID": "Guide/Stata/25-03-09-Stata-intro.html",
    "href": "Guide/Stata/25-03-09-Stata-intro.html",
    "title": "00-Stata Intro and Record",
    "section": "",
    "text": "关于 Stata 大二上统计学课的时候，老师在课上提了一嘴，说：“等你们以后读研了，就不用 SPSS 这种工具了，就会开始用 Stata、R 这些工具了“。也确实，本科期间确实就是一个 SPSS 管了四年，因为实在脱离他的应用场景，加之，老师就只会 SPSS ，那就没办法咯。\n想起那时候，专业两个班的 SPSS 软件基本上都是我去装的，老师弄不会，同学们更不会，我比较喜欢摸索，所以摸索出了这些，找到了安装包和密钥，然后拷在 U盘 里，课前课后课中就是给他们装软件，有时候，有些系统还装不上，某为就是，同学的某为一直装不上，当时看是因为缺 Java 环境，但是装了 Java JDK 还是不行，遂放弃。\n等到毕业的时候，想着看能不能用 Stata 做一下毕业论文的数据分析，最后太忙，没时间也没精力，用了 SPSS 结束。\n老师也是到了我大四的时候在哪里自学 Stata ，不过要说的是，在 AI 成熟以前，没有 AI 的辅助情况下，从0开始去学一门技能或程序，没有捷径，耗时耗力。现在逐渐理解，因为自己当时抱着 Python 的几本书，看了两三年也没有啥进展，等到 AI 出来了，不懂的就问 AI，节省了很多时间和精力；也和理解力的提升有关，进展迅速。\n回归正题，Stata 是一款用于数据科学的统计软件，其功能强大，但是对比 Python、R、MATLAB等程序或软件，还是略显不足，但是对于一般情况的数据分析，Stata 是够用的，其主要的优点是语法简洁和有诸多可以拿来即用的包，同时作为一款商业软件，其价格相较于 SAS 是很低的（但是换算RMB仍然很高），其支持相较于 R 等也可以说是较为丰富的（庞大的社区），还有跨平台使用等优势，这里不一一列举。\nStata 的安装很简单，互联网上有很多教程，但是关键的一点是获得 Key (许可证和激活秘钥)，当然互联网亦有诸多的资源可供选择。\n然后根据研究目的，选择合适的模型，找到前辈们写的代码，拿来增删改查，再结合 AI 的辅助，跑通，分析，写作。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "00-Stata Intro and Record"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-03-describe-index.html#stem-plot茎叶图",
    "href": "Guide/Stata/25-05-03-describe-index.html#stem-plot茎叶图",
    "title": "03-统计描述指标",
    "section": "1.6 stem plot(茎叶图)",
    "text": "1.6 stem plot(茎叶图)\n语法：\nstem var",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "03-统计描述指标"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-RR-CAL.html",
    "href": "Guide/Stata/25-05-06-RR-CAL.html",
    "title": "09-RR值与OR值的计算",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "09-RR值与OR值的计算"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-RR-CAL.html#rr值relative-risk",
    "href": "Guide/Stata/25-05-06-RR-CAL.html#rr值relative-risk",
    "title": "09-RR值与OR值的计算",
    "section": "1.1 RR值（Relative Risk）",
    "text": "1.1 RR值（Relative Risk）",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "09-RR值与OR值的计算"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-RR-CAL.html#数据导入",
    "href": "Guide/Stata/25-05-06-RR-CAL.html#数据导入",
    "title": "09-RR值与OR值的计算",
    "section": "1.2 数据导入",
    "text": "1.2 数据导入\n此章节使用网络数据，csxmpl\n\n%%stata\nwebuse csxmpl, clear\nlist\n\n\n. webuse csxmpl, clear\n\n. list\n\n     +------------------+\n     | case   exp   pop |\n     |------------------|\n  1. |    1     1     7 |\n  2. |    1     0    12 |\n  3. |    0     1     9 |\n  4. |    0     0     2 |\n     +------------------+\n\n. \n\n\n\n%%stata\ncs case exp [fweight = pop]\n\n\n                 |        Exposed         |\n                 |   Exposed   Unexposed  |      Total\n-----------------+------------------------+-----------\n           Cases |         7          12  |         19\n        Noncases |         9           2  |         11\n-----------------+------------------------+-----------\n           Total |        16          14  |         30\n                 |                        |\n            Risk |     .4375    .8571429  |   .6333333\n                 |                        |\n                 |      Point estimate    |    [95% conf. interval]\n                 |------------------------+------------------------\n Risk difference |        -.4196429       |   -.7240828   -.1152029 \n      Risk ratio |         .5104167       |    .2814332    .9257086 \n Prev. frac. ex. |         .4895833       |    .0742914    .7185668 \n Prev. frac. pop |         .2611111       |\n                 +-------------------------------------------------\n                               chi2(1) =     5.66  Pr&gt;chi2 = 0.0173\n\n\n\n1.2.1 RR 值计算\ncs var_case var_exp [if] [in] [weight] [,cs_options]\ncsi #a #b #c #d [,csi_options]\n计算 RR 值使用 cs 命令，它是 cohort study 的缩写\n上面 [fweight = pop] 中使用 pop 进行频数加权 fweight\n[,cs_options] 可以修改置信区间等",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "09-RR值与OR值的计算"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-06-RR-CAL.html#or值odds-ratio",
    "href": "Guide/Stata/25-05-06-RR-CAL.html#or值odds-ratio",
    "title": "09-RR值与OR值的计算",
    "section": "1.3 OR值（Odds Ratio）",
    "text": "1.3 OR值（Odds Ratio）\n\n1.3.1 RR值与OR值的区别\nRR值：Cohort study或者RCT中，研究者前瞻性地观察“暴露组”和“非暴露组”的发病情况，之后通过RR来评价“暴露组”研究对象的发病风险是“非暴露组”研究对象的多少倍?这个“多少倍”就是RR值\nOR值：在回顾性研究(如case-control)中，研究对象是已经患病的“病例组”和未患病的“对照组”，研究者回顾性地调查病例组和对照组的暴露情况，因此无法计算发病率等指标。\n想知道相对风险仍是我们最终的目的。\n因此我们可以使用 OR值 来近似估计 RR值。\n\n当终点事件发生率较低时，OR值可以近似为RR值（\\(&lt;15%\\)）\n当终点事件发生率较高时，OR会“夸大”RR值\n\nOR值相对于RR值“更远离1”\n当RR值大于1时，OR大于RR(\\(1&lt;RR&lt;OR\\))\n当RR值小于1时，OR小于RR(\\(OR&lt;RR&lt;1\\))\n终点事件发生率越高时，OR越会overestimate\n\n\n对于多列研究/RCT，可以报告OR值吗？\n\n可以，但是不够准确/精准\nRR值对于效应值的估计更加准确\nRR值对于临床意义的解释更加明确\nRegression model中：对于结局是二分类变量的研究，logistic回归只能提供OR值，不能提供RR值(当结局发生率高时，应该使用log-binomial回归或者使用带有稳健方差估计的泊松回归，直接提供RR值)\n\n\n\n1.3.2 OR值的计算\n语法：\ncc var_case var_exp [if] [in] [weight] [,cc_options]\ncci #a #b #c #d [,cci_options]\n\n\n1.3.3 数据载入\n\n%%stata\nwebuse ccxmpl,clear\nlist\n\n\n. webuse ccxmpl,clear\n\n. list\n\n     +-----------------------+\n     | case   exposed    pop |\n     |-----------------------|\n  1. |    1         1      4 |\n  2. |    1         0    386 |\n  3. |    0         1      4 |\n  4. |    0         0   1250 |\n     +-----------------------+\n\n. \n\n\n\n%%stata\ncc case exp [fweight = pop]\n\n                                                         Proportion\n                 |   Exposed   Unexposed  |      Total      exposed\n-----------------+------------------------+------------------------\n           Cases |         4         386  |        390       0.0103\n        Controls |         4        1250  |       1254       0.0032\n-----------------+------------------------+------------------------\n           Total |         8        1636  |       1644       0.0049\n                 |                        |\n                 |      Point estimate    |    [95% conf. interval]\n                 |------------------------+------------------------\n      Odds ratio |         3.238342       |    .5997233    17.45614 (exact)\n Attr. frac. ex. |            .6912       |   -.6674356    .9427136 (exact)\n Attr. frac. pop |         .0070892       |\n                 +-------------------------------------------------\n                               chi2(1) =     3.07  Pr&gt;chi2 = 0.0799\n\n\n\n%%stata\ncci 4 386 4 1250\n\n                                                         Proportion\n                 |   Exposed   Unexposed  |      Total      exposed\n-----------------+------------------------+------------------------\n           Cases |         4         386  |        390       0.0103\n        Controls |         4        1250  |       1254       0.0032\n-----------------+------------------------+------------------------\n           Total |         8        1636  |       1644       0.0049\n                 |                        |\n                 |      Point estimate    |    [95% conf. interval]\n                 |------------------------+------------------------\n      Odds ratio |         3.238342       |    .5997233    17.45614 (exact)\n Attr. frac. ex. |            .6912       |   -.6674356    .9427136 (exact)\n Attr. frac. pop |         .0070892       |\n                 +-------------------------------------------------\n                               chi2(1) =     3.07  Pr&gt;chi2 = 0.0799",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "09-RR值与OR值的计算"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html",
    "href": "Guide/Stata/25-05-07-log-binary.html",
    "title": "13-二分类Logistic回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html#什么时候该用-logistic-回归",
    "href": "Guide/Stata/25-05-07-log-binary.html#什么时候该用-logistic-回归",
    "title": "13-二分类Logistic回归",
    "section": "1 什么时候该用 Logistic 回归",
    "text": "1 什么时候该用 Logistic 回归\n当outcome发生率 &gt;15% 时logistic regression得出的OR值会overestimate实际的RR值\n\n传统的 Logistic Regression（得出OR值）\nMantel-Haenszel（得出RR值）\nPoisson Regression with robust variance estimate\n\n\n1.1 新方法\n\n1998 Zhang and Yu What’s the Relative Risk?\n\n\\[RR=\\frac{OR}{(1-P_0)+(P_0\\times OR)}\\]\n\n2003 McNutt Outcomes Estimating the Relative Risk in Cohort Studies and Clinical Trials of Common\n金标准：Log Binomial",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html#二分类-logistic-模型的假设",
    "href": "Guide/Stata/25-05-07-log-binary.html#二分类-logistic-模型的假设",
    "title": "13-二分类Logistic回归",
    "section": "2 二分类 Logistic 模型的假设",
    "text": "2 二分类 Logistic 模型的假设\n\n假设1:因变量(结局)是二分类变量。\n假设2:有至少1个自变量，自变量可以是连续变量，也可以是分类变量，\n假设3:每条观测间相互独立。分类变量(包括因变量和自变量)的分类必须全面且每一个分类间互斥\n假设4:最小样本量要求为自变量数目的15倍，但一些研究者认为样本量应达到自变量数目的50倍\n假设5:连续的自变量与因变量的logit转换值之间存在线性关系。\n假设6:自变量之间无多重共线性，\n假设7:没有明显的离群点、杠杆点和强影响点。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html#做-logistic-回归的要求",
    "href": "Guide/Stata/25-05-07-log-binary.html#做-logistic-回归的要求",
    "title": "13-二分类Logistic回归",
    "section": "3 做 Logistic 回归的要求",
    "text": "3 做 Logistic 回归的要求\n\nY是二分类变量\nY的发生率 &lt;15%",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html#导入数据",
    "href": "Guide/Stata/25-05-07-log-binary.html#导入数据",
    "title": "13-二分类Logistic回归",
    "section": "4 导入数据",
    "text": "4 导入数据\n变量 low 是我们的结局事件，我们想看什么因素和孩子的 low birthweight 相关\n\n%%stata\nwebuse lbw,clear\n\n(Hosmer & Lemeshow data)\n\n\n\n%%stata\ntab low\n\n\nBirthweight |\n     &lt;2500g |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          0 |        130       68.78       68.78\n          1 |         59       31.22      100.00\n------------+-----------------------------------\n      Total |        189      100.00\n\n\nDisclaimer: 本节的数据集中，结局事件发生率远远大于15%，应使用Log binomial模型进行分析。这里使用Logistic regression进行分析仅仅为了讲 解如何使用Stata进行操作、以及为下节的Log-binomial进行铺垫。",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-log-binary.html#logistic-regression",
    "href": "Guide/Stata/25-05-07-log-binary.html#logistic-regression",
    "title": "13-二分类Logistic回归",
    "section": "5 Logistic regression",
    "text": "5 Logistic regression\n语法：\nlogistic y x1 x2 x3 ...\n\n5.1 Model 1: \\(low=\\beta_0+\\beta_1 age\\)\n\n%%stata\nlogistic low age\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(1)    =   2.76\n                                                        Prob &gt; chi2   = 0.0966\nLog likelihood = -115.95598                             Pseudo R2     = 0.0118\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9501333   .0299423    -1.62   0.105     .8932232    1.010669\n       _cons |      1.469   1.075492     0.53   0.599     .3498129    6.168901\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\\(\\beta_1\\)：母亲的年龄每增加1岁，孩子低体重的风险是之前的0.95倍(95%CI:0.89,1.01)\n\n\n5.2 Model 2: \\(low=\\beta_0+\\beta_1 age+\\beta_2 smoke\\)\n\n%%stata\nlogistic low age i.smoke\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(2)    =   7.40\n                                                        Prob &gt; chi2   = 0.0248\nLog likelihood = -113.63815                             Pseudo R2     = 0.0315\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9514394   .0304194    -1.56   0.119     .8936482    1.012968\n             |\n       smoke |\n     Smoker  |   1.997405    .642777     2.15   0.032     1.063027    3.753081\n       _cons |   1.062798   .8048781     0.08   0.936     .2408901    4.689025\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\\(\\beta_1\\)：控制了母亲的吸烟状况以后，母亲的年龄每增加1岁孩子低体重的风险是之前的0.95倍(95% CI:0.89,1.01)\n\n\n5.3 Model 2: \\(low=\\beta_0+\\beta_1 age+\\beta_2 smoke+\\beta_3 race\\)\n\n%%stata\nlogistic low age i.smoke i.race\n\n\nLogistic regression                                     Number of obs =    189\n                                                        LR chi2(4)    =  15.81\n                                                        Prob &gt; chi2   = 0.0033\nLog likelihood = -109.4311                              Pseudo R2     = 0.0674\n\n------------------------------------------------------------------------------\n         low | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .9657186   .0322573    -1.04   0.296     .9045206    1.031057\n             |\n       smoke |\n     Smoker  |    3.00582   1.118001     2.96   0.003     1.449982    6.231081\n             |\n        race |\n      Black  |   2.749483   1.356659     2.05   0.040     1.045318    7.231924\n      Other  |   2.876948   1.167921     2.60   0.009     1.298314    6.375062\n             |\n       _cons |    .365111   .3146026    -1.17   0.242     .0674491    1.976395\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\\(\\beta_1\\)：控制了母亲的吸烟状况和种族以后，母亲的年龄每增加1岁孩子低体重的风险是之前的0.97倍(95%CI:0.90,1.03)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "13-二分类Logistic回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-simple-line-reg.html",
    "href": "Guide/Stata/25-05-07-simple-line-reg.html",
    "title": "11-简单线性回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-simple-line-reg.html#导入数据",
    "href": "Guide/Stata/25-05-07-simple-line-reg.html#导入数据",
    "title": "11-简单线性回归",
    "section": "1 导入数据",
    "text": "1 导入数据\n\n%%stata\nsysuse auto.dta,clear\n\n(1978 automobile data)",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-simple-line-reg.html#线性回归的假设",
    "href": "Guide/Stata/25-05-07-simple-line-reg.html#线性回归的假设",
    "title": "11-简单线性回归",
    "section": "2 线性回归的假设",
    "text": "2 线性回归的假设\n\n假设1:y是连续变量\n假设2:x可以被定义为连续变量（也可以是哑变量）\n假设3:y和x之间存在线性关系\n假设4:具有相互独立的观测值\n假设5:不存在显著的outlier\n假设6:等方差性\n假设7:residual近似正态分布\n\n总结而言：线性、独立、正态、方差齐\n\n2.1 假设3:y和x之间存在线性关系\n通过 scatter plot 或 Lowess plot 进行查看\n如果不符合线性关系怎么办？\n\nspline：一次方项，分段fit直线\nQuadratic：二次方项\nCubic：三次方项\nRestricted cubic：三次方项（头尾近乎直线）\n\n\n%%stata\ntwoway scatter mpg weight\n\n\n\n\n\n\n\n\n\n%%stata\ntwoway lowess mpg weight\n\n\n\n\n\n\n\n\nscatter plot和lowess plot输出在一起\n\n%%stata\nlowess mpg weight\n\n\n\n\n\n\n\n\n\n%%stata\ntwoway (scatter mpg weight)(lowess mpg weight)\n\n\n\n\n\n\n\n\n\n\n2.2 假设4:具有相互独立的观测值\n\n可以使用杜宾-瓦特森(Durbin-Watson)统计量\nStata对于非time series数据不设有这个统计量的检测\n更多情况下，不需要测量这个假设是否成立\n如果是相互关联的观测值：GEE模型、Multi-level模型\n\n\n\n2.3 假设5:不存在显著的outlier\n\nBoxplot 或 Violin Plot\n\n\n%%stata\ngraph box mpg\n\n\n\n\n\n\n\n\n\n%%stata\nvioplot mpg\n\n\n\n\n\n\n\n\n\n\n2.4 假设6:等方差性\n\n使用 Residual-versus-fitted plot\n代码:rvfplot\n\n做等方差之前需要做回归的分析，分析及结果如下：\n\n%%stata\nreg mpg weight\n\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(1, 72)        =    134.62\n       Model |   1591.9902         1   1591.9902   Prob &gt; F        =    0.0000\n    Residual |  851.469256        72  11.8259619   R-squared       =    0.6515\n-------------+----------------------------------   Adj R-squared   =    0.6467\n       Total |  2443.45946        73  33.4720474   Root MSE        =    3.4389\n\n------------------------------------------------------------------------------\n         mpg | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      weight |  -.0060087   .0005179   -11.60   0.000    -.0070411   -.0049763\n       _cons |   39.44028   1.614003    24.44   0.000     36.22283    42.65774\n------------------------------------------------------------------------------\n\n\n\n%%stata\nrvfplot\n\n\n\n\n\n\n\n\n从上图可以看出，并不是完全等方差，特别是在 Fitted value 在 25-30 区间内时\n\n\n2.5 假设7:residual近似正态分布\n\nStep 1:得到残差\nStep 2:使用正态分布的检验方法\n\n直方图\n使用 qq plot 观测\n偏度峰度、Shapiro-Wilk检验、Shapiro-Francia检验\n\n\n先建立一个 resid 变量：\n\n%%stata\npredict resid,residual\n\n再对 resid 变量进行查看和检验\n下面分别是 直方图 和 Q-Q图\n\n%%stata\nhist resid\n\n(bin=8, start=-6.9593482, width=2.5970982)\n\n\n\n\n\n\n\n\n\n\n%%stata\n//qq plot\nqnorm resid\n\n\n. //qq plot\n. qnorm resid\n\n. \n\n\n\n\n\n\n\n\n\n\n\n2.6 对残差使用偏度、峰度进行检验\n\n%%stata\nsktest resid\n\n\nSkewness and kurtosis tests for normality\n                                                         ----- Joint test -----\n    Variable |       Obs   Pr(skewness)   Pr(kurtosis)   Adj chi2(2)  Prob&gt;chi2\n-------------+-----------------------------------------------------------------\n       resid |        74         0.0000         0.0010         20.82     0.0000\n\n\n\n\n2.7 使用 Shapiro-Wilk 检验\n\n%%stata\nswilk resid\n\n\n                   Shapiro–Wilk W test for normal data\n\n    Variable |        Obs       W           V         z       Prob&gt;z\n-------------+------------------------------------------------------\n       resid |         74    0.89593      6.702     4.150    0.00002",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-07-simple-line-reg.html#简单线性回归",
    "href": "Guide/Stata/25-05-07-simple-line-reg.html#简单线性回归",
    "title": "11-简单线性回归",
    "section": "3 简单线性回归",
    "text": "3 简单线性回归\n\n3.1 语法\nregress depvar [indepvars] [if] [in] [weight] [,option]\n具体参见：regress — Linear regression",
    "crumbs": [
      "Home",
      "统计软件",
      "Guide/Stata/Stata-intro.qmd",
      "11-简单线性回归"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html",
    "href": "Guide/Stata/25-05-08-survival-ana.html",
    "title": "15-生存分析",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#生存分析",
    "href": "Guide/Stata/25-05-08-survival-ana.html#生存分析",
    "title": "15-生存分析",
    "section": "1 生存分析",
    "text": "1 生存分析\n\n描述一个组内个体的生存时间\n\n寿命表法(Life tables methods)\n非参数Kaplan-Meier曲线\n\n比较两个或多个组的生存时间\n\nLog-rank test\n\n研究生存时间和变量之间的关系\n\n半参数Cox比例风险模型\n参数生存分析模型",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#k-m曲线的历史",
    "href": "Guide/Stata/25-05-08-survival-ana.html#k-m曲线的历史",
    "title": "15-生存分析",
    "section": "2 K-M曲线的历史",
    "text": "2 K-M曲线的历史\n\n1958年，Dr. Kaplan和DrMeier 介绍了一种全新的、解决随访期间RightCensoring问题的生存分析方法\n特点:精确地记录并利用每个个体发生终点事件的具体时间，在任何一个终点事件发生的时间点计算出一个新的、基于之前所有信息的Cumulative survival\n优点:\n\n相比于Life-table method，更加充分地利用了信息，给出更准确的统计量\n非参数估计方法:不要求总体的分布形式，因此非常适合生存分析时使用\nK-M曲线可以很直观地表现出两组或多组的生存率或死亡率，适合在文章中展示",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#导入数据",
    "href": "Guide/Stata/25-05-08-survival-ana.html#导入数据",
    "title": "15-生存分析",
    "section": "3 导入数据",
    "text": "3 导入数据\n使用 Patient Survival in Drug Trial 数据集\n\n%%stata\nwebuse drugtr,clear\n\n(Patient survival in drug trial)\n\n\n将数据恢复成普通的数据格式：stset,clear\n\nStata已经将这个数据集设置成了生存数据的格式，导入数据集后，将数 据集恢复成普通的数据格式，这样才是我们在临床研究中见到的数据结构。\n\n\n%%stata\nstset,clear",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#数据集的初步观察",
    "href": "Guide/Stata/25-05-08-survival-ana.html#数据集的初步观察",
    "title": "15-生存分析",
    "section": "4 数据集的初步观察",
    "text": "4 数据集的初步观察\n\n%%stata\nlist in 5/10\n\n\n     +------------------------------+\n     | studyt~e   died   drug   age |\n     |------------------------------|\n  5. |        4      1      0    56 |\n  6. |        4      1      0    67 |\n  7. |        5      1      0    63 |\n  8. |        5      1      0    58 |\n  9. |        8      1      0    56 |\n     |------------------------------|\n 10. |        8      0      0    58 |\n     +------------------------------+\n\n\n\n%%stata\ncodebook drug\n\n\n-------------------------------------------------------------------------------\ndrug                                                      Drug type (0=placebo)\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [0,1]                         Units: 1\n         Unique values: 2                         Missing .: 0/48\n\n            Tabulation: Freq.  Value\n                           20  0\n                           28  1\n\n\n\n%%stata\ncodebook studytime\n\n\n-------------------------------------------------------------------------------\nstudytime                                        Months to death or end of exp.\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [1,39]                        Units: 1\n         Unique values: 28                        Missing .: 0/48\n\n                  Mean:    15.5\n             Std. dev.: 10.2563\n\n           Percentiles:     10%       25%       50%       75%       90%\n                              4       7.5      12.5        23        32\n\n\n\n%%stata\ncodebook died\n\n\n-------------------------------------------------------------------------------\ndied                                                          1 if patient died\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [0,1]                         Units: 1\n         Unique values: 2                         Missing .: 0/48\n\n            Tabulation: Freq.  Value\n                           17  0\n                           31  1\n\n\n\n%%stata\ncodebook age\n\n\n-------------------------------------------------------------------------------\nage                                              Patient's age at start of exp.\n-------------------------------------------------------------------------------\n\n                  Type: Numeric (byte)\n\n                 Range: [47,67]                       Units: 1\n         Unique values: 18                        Missing .: 0/48\n\n                  Mean: 55.875\n             Std. dev.: 5.6592\n\n           Percentiles:    10%       25%       50%       75%       90%\n                            49      50.5        56        60        65",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#数据申明代码操作",
    "href": "Guide/Stata/25-05-08-survival-ana.html#数据申明代码操作",
    "title": "15-生存分析",
    "section": "5 数据申明——代码操作",
    "text": "5 数据申明——代码操作\n\n告诉Stata: 终点事件(Failure variable),随访时间(Time variable)\nstset timevar, failure(failvar[==numlist])\n\ntimevar: 随访时间变量\nfailvar: 终点事件变量\nnumlist: 终点时间变量中，哪个(哪些)值代表发生了终点事件?\n\n\n\n%%stata\nstset studytime,failure(died==1)\n\n\nSurvival-time data settings\n\n         Failure event: died==1\nObserved time interval: (0, studytime]\n     Exit on or before: failure\n\n--------------------------------------------------------------------------\n         48  total observations\n          0  exclusions\n--------------------------------------------------------------------------\n         48  observations remaining, representing\n         31  failures in single-record/single-failure data\n        744  total analysis time at risk and under observation\n                                                At risk from t =         0\n                                     Earliest observed entry t =         0\n                                          Last observed exit t =        39",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#生存数据再观测",
    "href": "Guide/Stata/25-05-08-survival-ana.html#生存数据再观测",
    "title": "15-生存分析",
    "section": "6 生存数据再观测",
    "text": "6 生存数据再观测\nNotice：必须要在指定数据集为生存分析数据集之后(stset 之后)才能使用任何其他的 st 开始的命令。\n\n%%stata\nstsum\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n         |               Incidence     Number of   |------ Survival time -----|\n         | Time at risk       rate      subjects        25%       50%       75%\n---------+---------------------------------------------------------------------\n   Total |          744   .0416667            48          8        17        33\n\n\n\n%%stata\nstdescribe\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n                                   |-------------- Per subject --------------|\nCategory                   Total        Mean         Min     Median        Max\n------------------------------------------------------------------------------\nNumber of subjects            48   \nNumber of records             48           1           1          1          1\n\nEntry time (first)                         0           0          0          0\nExit time (final)                       15.5           1       12.5         39\n\nSubjects with gap              0   \nTime on gap                    0   \nTime at risk                 744        15.5           1       12.5         39\n\nFailures                      31    .6458333           0          1          1\n------------------------------------------------------------------------------\n\n\n\n%%stata\nsts list\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\nKaplan–Meier survivor function\n\n             At                  Survivor      Std.\n  Time     risk   Fail   Lost    function     error     [95% conf. int.]\n------------------------------------------------------------------------\n     1       48      2      0      0.9583    0.0288     0.8435    0.9894\n     2       46      1      0      0.9375    0.0349     0.8186    0.9794\n     3       45      1      0      0.9167    0.0399     0.7930    0.9679\n     4       44      2      0      0.8750    0.0477     0.7427    0.9418\n     5       42      2      0      0.8333    0.0538     0.6943    0.9129\n     6       40      2      1      0.7917    0.0586     0.6474    0.8820\n     7       37      1      0      0.7703    0.0608     0.6236    0.8656\n     8       36      3      1      0.7061    0.0661     0.5546    0.8143\n     9       32      0      1      0.7061    0.0661     0.5546    0.8143\n    10       31      1      1      0.6833    0.0678     0.5302    0.7957\n    11       29      2      1      0.6362    0.0708     0.4807    0.7564\n    12       26      2      0      0.5872    0.0733     0.4304    0.7145\n    13       24      1      0      0.5628    0.0742     0.4060    0.6931\n    15       23      1      1      0.5383    0.0749     0.3821    0.6712\n    16       21      1      0      0.5127    0.0756     0.3570    0.6483\n    17       20      1      1      0.4870    0.0761     0.3326    0.6249\n    19       18      0      2      0.4870    0.0761     0.3326    0.6249\n    20       16      0      1      0.4870    0.0761     0.3326    0.6249\n    22       15      2      0      0.4221    0.0786     0.2680    0.5684\n    23       13      2      0      0.3572    0.0788     0.2087    0.5083\n    24       11      1      0      0.3247    0.0780     0.1809    0.4771\n    25       10      1      1      0.2922    0.0767     0.1543    0.4449\n    28        8      1      1      0.2557    0.0753     0.1247    0.4093\n    32        6      0      2      0.2557    0.0753     0.1247    0.4093\n    33        4      1      0      0.1918    0.0791     0.0676    0.3634\n    34        3      0      1      0.1918    0.0791     0.0676    0.3634\n    35        2      0      1      0.1918    0.0791     0.0676    0.3634\n    39        1      0      1      0.1918    0.0791     0.0676    0.3634\n------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#k-m曲线的绘制",
    "href": "Guide/Stata/25-05-08-survival-ana.html#k-m曲线的绘制",
    "title": "15-生存分析",
    "section": "7 K-M曲线的绘制",
    "text": "7 K-M曲线的绘制\n语法：\nsts graph [if] [in] [,options]\n[,options] 不是必须，可以形如：,by(var)，这样就会按照 var 的分类绘制不同的线\n\n%%stata\nsts graph,by(drug)\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n7.1 图像展现更多的信息\n\n%%stata\nsts graph if age&lt;50,by(drug)\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n%%stata\nsts graph,by(drug) risktable\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n\n7.2 复杂绘图示例\n\n%%stata\nlabel drop drug_label\nlabel define drug_label 0 \"安慰剂\" 1 \"试验药\"\nlabel values drug drug_label\n\n\n. label drop drug_label\n\n. label define drug_label 0 \"安慰剂\" 1 \"试验药\"\n\n. label values drug drug_label\n\n. \n\n\n\n%%stata\nsts graph, ///\n    by(drug) ci atrisk ///\n    xlabel(0(5)40) ylabel(0(0.2)1) ///\n    legend(position(6) ring(1) cols(2) rowgap(1) colgap(1)) ///\n    xtitle(\"x轴的标签\") ytitle(\"y轴的标签\") ///\n    title(\"这里是标题\", size(medsmall)) ///\n    subtitle(\"这里是副标题\", size(small)) ///\n    caption(\"注释\", size(vsmall)) ///\n    note(\"数据来源：xxx\", size(vsmall)) ///\n    graphregion(margin(10 10 10 10)) ///\n    plotregion(margin(5 5 5 5))\n\n\n. sts graph, ///\n&gt;     by(drug) ci atrisk ///\n&gt;     xlabel(0(5)40) ylabel(0(0.2)1) ///\n&gt;     legend(position(6) ring(1) cols(2) rowgap(1) colgap(1)) ///\n&gt;     xtitle(\"x轴的标签\") ytitle(\"y轴的标签\") ///\n&gt;     title(\"这里是标题\", size(medsmall)) ///\n&gt;     subtitle(\"这里是副标题\", size(small)) ///\n&gt;     caption(\"注释\", size(vsmall)) ///\n&gt;     note(\"数据来源：xxx\", size(vsmall)) ///\n&gt;     graphregion(margin(10 10 10 10)) ///\n&gt;     plotregion(margin(5 5 5 5))\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\n.",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-survival-ana.html#检验组间差别log-rank-test",
    "href": "Guide/Stata/25-05-08-survival-ana.html#检验组间差别log-rank-test",
    "title": "15-生存分析",
    "section": "8 检验组间差别（Log-Rank Test）",
    "text": "8 检验组间差别（Log-Rank Test）\n\n%%stata\nsts test drug\n\n\n        Failure _d: died==1\n  Analysis time _t: studytime\n\nEquality of survivor functions\nLog-rank test\n\n      |  Observed       Expected\ndrug  |    events         events\n------+-------------------------\n    0 |        19           7.25\n    1 |        12          23.75\n------+-------------------------\nTotal |        31          31.00\n\n                chi2(1) =  28.27\n                Pr&gt;chi2 = 0.0000",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "15-生存分析"
    ]
  },
  {
    "objectID": "Guide/SAS/25-02-16-SAS-intro.html",
    "href": "Guide/SAS/25-02-16-SAS-intro.html",
    "title": "00-SAS-Record",
    "section": "",
    "text": "SAS的历史很长，很强大，但是现代化做的一般（交互界面）。\n使用场景也是有限的，至少一般情况下用不上这么高级的工具。\n但是在某些领域又是极其重要的，像银行和药企，他们要追求足够的稳定和严谨，那么多年不曾有重大改变且一向以稳定著称的 SAS 自然可以很好的满足这一需求。\n\nSAS 对学术研究的支持是比较不友好的。正版太贵，除非学校有提供，个人基本不可能使用正版，这里下载破解版，搞SID(SAS的授权证书)需要时间成本，还容易有安装问题，没错你可以选择使用SAS的教育版，不过谁用谁知道。\n安装比较麻烦，尤其是在Linux上，我曾用两周的时间折腾在 Linux 上安装一个 SAS ，最后以失败告终，且在互联网上找不到解决的方案，AI也束手无策。\nSAS不开源，意味着你看到某些论文，里面使用一些比较新的统计分析方法，SAS不大可能有现成代码可以使用，而 Python 和 R 则大几率有现成的包可以调用，这里也会节省不少时间。\nSAS的强大一方面是性能稳定，可以处理几十上百GB的数据而不容易崩溃，但是医学数据一般容量比较小，并不是非得SAS才能跑的动。\nSAS相比其他编程语言来说是独树一帜(奇葩)的存在（proc和data步独步天下），从语法上面来说并没有什么和它相接近的语言，相反 R 和 Python 则会和一般的编程语言例如 Java, C 等有一些类似的地方，对以后万一还需要学习其他语言或者学习以后新诞生的编程软件诞有一定帮助。\nSAS 的支持有限，互联网上关于 SAS 的使用信息较少，一般都在出版的书中有可复现的内容，也没有像 Python 和 R 等活跃的社区可以提供较多的互动和支持，编程遇上问题不容易找到答案。\n\n用肯定能用，但是在使用中占多大的比重，就需要权衡一下，在 AI 时代，不一定要全部掌握，看得懂，知道怎么做，应该也可以了，当然如果要深入，那就另说。\n每个人的资源和时间都是有限的，用最少的资源和时间做最多的事才是最重要的。",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS-Record"
    ]
  },
  {
    "objectID": "Guide/SAS/25-02-16-SAS-intro.html#footnotes",
    "href": "Guide/SAS/25-02-16-SAS-intro.html#footnotes",
    "title": "00-SAS-Record",
    "section": "脚注",
    "text": "脚注\n\n\nSAS 程式(SAS 9.4) 繁體中文出現亂碼怎麼辦?↩︎",
    "crumbs": [
      "Home",
      "统计软件",
      "SAS",
      "00-SAS-Record"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html",
    "href": "Guide/Stata/25-05-08-GEE.html",
    "title": "17-广义估计方程(GEE)",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#广义估计方程",
    "href": "Guide/Stata/25-05-08-GEE.html#广义估计方程",
    "title": "17-广义估计方程(GEE)",
    "section": "1 广义估计方程",
    "text": "1 广义估计方程\n广义估计方程（Generalized Estimating Equations, GEE）是一种用于分析具有相关性数据的统计方法，特别适用于纵向数据和重复测量数据的分析。\n\n1.1 基本概念\n广义估计方程是一种扩展的回归模型，旨在处理因变量之间可能存在的相关性。它于1986年由Liang和Zeger首次提出，主要用于估计广义线性模型的参数，尤其是在数据存在重复测量或相关性时。GEE通过准似然估计方法来处理这些数据，适用于多种类型的因变量，包括连续型、二分类和计数数据等。\n\n\n1.2 应用场景\n\n纵向数据分析：如临床试验中对同一组受试者在不同时间点的测量数据进行分析。\n重复测量数据：例如在心理学和社会科学研究中，研究同一对象在不同条件下的表现。\n组间比较：用于比较不同组别在某一时间点或多个时间点的差异。\n\n\n\n1.3 优势与特点\n\n处理相关性：GEE能够有效处理因变量之间的相关性，避免了传统方法（如方差分析）在数据不独立时可能导致的偏差。\n灵活性：适用于多种类型的因变量，且对自变量的数据类型没有严格要求，可以处理定类和定量数据。\n模型选择：根据因变量的分布特征选择合适的模型，如泊松回归、负二项回归等。",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#什么是纵向数据分析",
    "href": "Guide/Stata/25-05-08-GEE.html#什么是纵向数据分析",
    "title": "17-广义估计方程(GEE)",
    "section": "2 什么是纵向数据分析",
    "text": "2 什么是纵向数据分析\nExamples oflongitudinal dataanalysis(LDA,纵向数据分析)\n\nChild BP measured at each annual visit from 3 to 9 years old\nInfant weight measured at 3, 6, 9, 12 months\nIn a 3-arm cross-over trial, short chain fatty acid measured at the end ofeach\n\n\n2.1 纵向数据 vs. 横向数据\n纵向数据是指对同一组受试个体或者受试单元在不同时间点上的重复观测若干次，得到由截面和时间序列融合在一起的数据。\n因而纵向数据具有自相关性、生态单位聚集性、测量次数与测量时间间隔的非均衡性等特点。\n\nEstablish temporaltrends(时序:A在B之前发生)\nSeparate cohort effects from aging effects\nChildren have different baseline values in reading abilities\nThe trajectories of reading abilities differs by people\n\n\n\n2.2 方差公式\n\\[Var(X-Y)\nVar((1)X+(-1)Y)=(1^2)Var(X)+(-1^2)Var(Y)+2(1)(-1)Cov(X,Y)\\]\n在非配对 t 检验中，\\(Cov(X,Y)\\)为0，此时 \\(Var(X-Y)\\)\n打破线性相关的假设",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#gee-模型",
    "href": "Guide/Stata/25-05-08-GEE.html#gee-模型",
    "title": "17-广义估计方程(GEE)",
    "section": "3 GEE 模型",
    "text": "3 GEE 模型\n\n\n\ncorrelation\ndescription\n\n\n\n\nexchangeable\nexchangeable\n\n\nindependent\nindependent\n\n\nunstructured\nunstructured\n\n\nfixed matname\nuser-specified\n\n\nar #\nautoregressive of order #\n\n\nstationary #\nstationary of order #\n\n\nnonstationary #\nnonstationary of order #\n\n\n\n\n3.1 GEE 常用步骤\n\n使用 independent 回归结构\n稳健标准误估计（robust variance estimate）；如果用错模型，稳健后的结果也会相差不远\n使用 QIC（AIC for GEE）评估GEE模型",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#数据导入",
    "href": "Guide/Stata/25-05-08-GEE.html#数据导入",
    "title": "17-广义估计方程(GEE)",
    "section": "4 数据导入",
    "text": "4 数据导入\nData:NLS Women 14-24 in 1968\n\n%%stata\nwebuse union,clear\n\n(NLS Women 14-24 in 1968)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#数据观测",
    "href": "Guide/Stata/25-05-08-GEE.html#数据观测",
    "title": "17-广义估计方程(GEE)",
    "section": "5 数据观测",
    "text": "5 数据观测\n\n%%stata\nlist in 1/10\n\n\n     +----------------------------------------------------------------+\n     | idcode   year   age   grade   not_smsa   south   union   black |\n     |----------------------------------------------------------------|\n  1. |      1     72    20      12          0       0       1       1 |\n  2. |      1     77    25      12          0       0       0       1 |\n  3. |      1     80    28      12          0       0       1       1 |\n  4. |      1     83    31      12          0       0       1       1 |\n  5. |      1     85    33      12          0       0       1       1 |\n     |----------------------------------------------------------------|\n  6. |      1     87    35      12          0       0       1       1 |\n  7. |      1     88    37      12          0       0       1       1 |\n  8. |      2     71    19      12          0       0       0       1 |\n  9. |      2     77    25      12          0       0       1       1 |\n 10. |      2     78    26      12          0       0       1       1 |\n     +----------------------------------------------------------------+",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-GEE.html#用法",
    "href": "Guide/Stata/25-05-08-GEE.html#用法",
    "title": "17-广义估计方程(GEE)",
    "section": "6 用法",
    "text": "6 用法\n\n6.1 GEE 准备\nxtset studyid timevar\n\nstudyid: unique ID for each participant\ntimevar: timevar will usually be a variable that counts 1,2,…, and is to be interpreted as firstyear ofsurvey, second year,…, or first month oftreatment, second month.\n\n\n%%stata\nxtset id year\n\n\nPanel variable: idcode (unbalanced)\n Time variable: year, 70 to 88, but with gaps\n         Delta: 1 unit\n\n\n\n\n6.2 GEE 命令\nxtgee y x_1,x_2,x_3,…[if] [in] [weight] ,family(family) link(link) corr(correlation structure) robust\n可以使用 help gee 查看更多信息\n\n%%stata\nxtgee union age grade not_smsa south, family(binomial) link(logit) corr(ind) robust\n\n\nIteration 1:  Tolerance = 1.940e-12\n\nGEE population-averaged model                      Number of obs    =   26,200\nGroup variable: idcode                             Number of groups =    4,434\nFamily: Binomial                                   Obs per group:  \nLink:   Logit                                                   min =        1\nCorrelation: independent                                        avg =      5.9\n                                                                max =       12\n                                                   Wald chi2(4)     =   160.30\nScale parameter = 1                                Prob &gt; chi2      =   0.0000\n\nPearson chi2(26200)  = 26242.04                    Deviance         = 27093.19\nDispersion (Pearson) = 1.001604                    Dispersion       = 1.034091\n\n                                 (Std. err. adjusted for clustering on idcode)\n------------------------------------------------------------------------------\n             |               Robust\n       union | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |    .011683   .0033035     3.54   0.000     .0052082    .0181577\n       grade |    .048511   .0139346     3.48   0.000     .0211997    .0758223\n    not_smsa |  -.2214007   .0713343    -3.10   0.002    -.3612134    -.081588\n       south |  -.6470985   .0629803   -10.27   0.000    -.7705376   -.5236594\n       _cons |  -1.941974   .1973105    -9.84   0.000    -2.328695   -1.555253\n------------------------------------------------------------------------------",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "17-广义估计方程(GEE)"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-Cox-reg.html",
    "href": "Guide/Stata/25-05-08-Cox-reg.html",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-Cox-reg.html#数据导入",
    "href": "Guide/Stata/25-05-08-Cox-reg.html#数据导入",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "1 数据导入",
    "text": "1 数据导入\n在一个抗癌药物的Clinical Trial中，48名患者被随机分配到新药组(28人)和安慰剂组(20人)，研究人员想知道新药是否影响患者的生存情况。\nData：Patient Survival in Drug Trial\n\n%%stata\nwebuse drugtr,clear\n\n(Patient survival in drug trial)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-Cox-reg.html#cox回归",
    "href": "Guide/Stata/25-05-08-Cox-reg.html#cox回归",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "2 Cox回归",
    "text": "2 Cox回归\nCox回归分析，也称为比例风险回归模型（Proportional Hazards Model，简称Cox模型），是由英国统计学家D.R.Cox于1972年提出的一种半参数回归模型。\n该模型主要用于生存分析，能够同时分析多个因素对生存期的影响，并且可以处理带有截尾生存时间的数据。\n\n2.1 基本概念\n在介绍Cox回归模型之前，需要了解几个相关的概念：\n\n生存函数：表示观察对象的生存时间大于某时刻的概率。\n死亡函数：表示观察对象的生存时间不大于某时刻的概率。\n死亡密度函数：表示观察对象在某时刻的瞬时死亡率。\n危险率函数：表示生存时间已达到某时刻的观察对象在该时刻的瞬时死亡率1。\n\n\n\n2.2 基本原理\nCox回归模型的基本形式为： \\[h(t|X) = h_0(t) \\exp(\\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p)\\]\n其中， \\(h(t|X)\\) 是在给定协变量 \\(X\\) 时的危险率，\\(h_0(t)\\) 是基准危险率，\\(\\beta_i\\) 是需要估计的回归系数。\n\n\n2.3 Cox回归模型的假设包括：\n\n比例风险假设：各危险因素的作用不随时间变化。\n对数线性假设：协变量与对数风险比呈线性关系。\n\n\n\n2.4 偏回归系数的意义\n偏回归系数 \\(\\beta_i\\) 的流行病学含义是在其他协变量不变的情况下，协变量每增加一个测定单位时所引起的相对危险度的自然对数的改变量。\n\n\n2.5 假设检验\nCox回归模型中的偏回归系数可以通过建立偏似然函数，利用Newton-Raphson迭代法求得。常用的假设检验方法包括似然比检验、Wald检验和记分检验。\nCox回归模型由于其灵活性和广泛应用，成为生存分析中最常用的多因素分析方法之一。\n\n%%stata\nstcox drug\n\n\n        Failure _d: died\n  Analysis time _t: studytime\n\nIteration 0:  Log likelihood = -99.911448\nIteration 1:  Log likelihood = -88.254734\nIteration 2:  Log likelihood = -88.001551\nIteration 3:  Log likelihood =  -88.00019\nRefining estimates:\nIteration 0:  Log likelihood =  -88.00019\n\nCox regression with Breslow method for ties\n\nNo. of subjects =  48                                   Number of obs =     48\nNo. of failures =  31\nTime at risk    = 744\n                                                        LR chi2(1)    =  23.82\nLog likelihood = -88.00019                              Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n          _t | Haz. ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        drug |   .1327581   .0584002    -4.59   0.000     .0560555    .3144157\n------------------------------------------------------------------------------\n\n\n这里 Drug 的 coefficient(\\(\\beta\\)) : 新药组终点事件发生风险是安慰剂组的 13.3%(95% CI: 5.6%,31.4%)\n控制年龄：\n\n%%stata\nstcox drug age\n\n\n        Failure _d: died\n  Analysis time _t: studytime\n\nIteration 0:  Log likelihood = -99.911448\nIteration 1:  Log likelihood = -83.551879\nIteration 2:  Log likelihood = -83.324009\nIteration 3:  Log likelihood = -83.323546\nRefining estimates:\nIteration 0:  Log likelihood = -83.323546\n\nCox regression with Breslow method for ties\n\nNo. of subjects =  48                                   Number of obs =     48\nNo. of failures =  31\nTime at risk    = 744\n                                                        LR chi2(2)    =  33.18\nLog likelihood = -83.323546                             Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n          _t | Haz. ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        drug |   .1048772   .0477017    -4.96   0.000     .0430057    .2557622\n         age |   1.120325   .0417711     3.05   0.002     1.041375     1.20526\n------------------------------------------------------------------------------\n\n\n这里 Drug 的 coefficient(\\(\\beta_1\\)) : 在控制了患者的年龄后，新药组终点事件发生风险是安慰剂组的 10.5%(95% CI: 4.3%,25.6%)\n反过来，这里 Age 的 coefficient(\\(\\beta_2\\)) : 在控制了治疗方法后，患者年龄每增加 1 岁，发生终点事件风险增加 12%(95% CI: 4.1%,20.5%)\n\n\n2.6 Cox回归的命令语法\nstcox var1 var2 var3 ... [if] [in] [, options]\nNotice\n\n必须要在指定Data为Suvivaldata之后(stset之后)才能使用任何st开头的命令\n由于我们已经在一开始将数据转化为Survivaldata的时候指定过终点事件(Failure variable)、时间变量(Time variable)，我们在这里只需要设置需要在回归方程中independent variable即可\nExample\n\nstcox drug age\nstcox drug age if age &lt; 50",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-Cox-reg.html#ph-假定检验",
    "href": "Guide/Stata/25-05-08-Cox-reg.html#ph-假定检验",
    "title": "16-Cox回归与比例风险假定检验",
    "section": "3 PH 假定检验",
    "text": "3 PH 假定检验\n\n3.1 使用统计检验法\nCox回归模型在应用时，有一个非常重要的前提条件，即比例风险（Proportional hazards）假定，简称PH假定。\nNotice:PH假定的检验基于上一步进行的Cox回归。\n\\(H_0\\)：纳入Cox回归模型的变量满足PH假定\n\\(P&gt;0.05\\)，不能拒绝\\(H_0\\)\n\n%%stata\nestat phtest\n\n\nTest of proportional-hazards assumption\n\nTime function: Analysis time\n------------------------------------------------\n             |     chi2       df       Prob&gt;chi2\n-------------+----------------------------------\n Global test |     0.43        2          0.8064\n------------------------------------------------\n\n\n\n\n3.2 PH假定 使用图像法\n使用- ln(-ln(生存))图法，判断标准是如果待评价的变量分成的亚组曲线平行或近似平行，则满足PH假定。\nstphplot,by(bar1) adjust(var2 var3)\nvar1 是自变量名，var2 等是希望控制的变量。\nNotice:这个命令不一定要跟在cox回归之后\n\n%%stata\nstphplot,by(drug) adjust(age)\n\n\n        Failure _d: died\n  Analysis time _t: studytime\n\n\n\n\n\n\n\n\n\n\n\n3.3 如果不满足PH假定\n\n一般只要两组生存曲线趋势一致、不明显交叉即可判定PH假定成立\n如果PH假定不成立，可以加上时间(time)和暴露 (exposure,比如本例之中的drug)的交互项(interaction term), time*exposure\n也可以对于不同的时间段分别分析(e.g.0-10，10-20,&gt;20)\n参数生存分析模型:streg进行参数生存分析",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "16-Cox回归与比例风险假定检验"
    ]
  },
  {
    "objectID": "Guide/Stata/25-05-08-Ord-Logistic-Reg.html",
    "href": "Guide/Stata/25-05-08-Ord-Logistic-Reg.html",
    "title": "18-有序多分类Logistic回归",
    "section": "",
    "text": "import stata_setup\nstata_setup.config('C:/Program Files/Stata18', 'mp', splash=False)",
    "crumbs": [
      "Home",
      "统计软件",
      "Stata",
      "18-有序多分类Logistic回归"
    ]
  }
]